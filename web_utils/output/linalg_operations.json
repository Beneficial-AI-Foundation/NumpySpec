{
  "numpy_linalg_operations": {
    "matrix_vector_products": [
      {
        "name": "numpy.dot",
        "description": "Dot product of two arrays",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.dot.html",
        "add": {
          "doc": "numpy.dot(a, b, out=None)\n\nDot product of two arrays. Specifically,\n\n* If both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n* If both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n* If either a or b is 0-D (scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a * b is preferred.\n* If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n* If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b: dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n\nIt uses an optimized BLAS library when possible.\n\nParameters:\na : array_like - First argument.\nb : array_like - Second argument.\nout : ndarray, optional - Output argument.\n\nReturns:\noutput : ndarray - Returns the dot product of a and b.\n\nRaises:\nValueError - If the last dimension of a is not the same size as the second-to-last dimension of b.\n\nExamples:\n>>> import numpy as np\n>>> np.dot(3, 4)\n12\n>>> np.dot([2j, 3j], [2j, 3j])\n(-13+0j)\n>>> a = [[1, 0], [0, 1]]\n>>> b = [[4, 1], [2, 2]]\n>>> np.dot(a, b)\narray([[4, 1],\n       [2, 2]])",
          "code": "Source code implementation in C - available at https://github.com/numpy/numpy/blob/main/numpy/core/src/multiarray/multiarraymodule.c"
        }
      },
      {
        "name": "numpy.linalg.multi_dot",
        "description": "Compute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.multi_dot.html",
        "add": {
          "doc": "linalg.multi_dot(arrays, *, out=None)\n\nCompute the dot product of two or more arrays in a single function call, while automatically selecting the fastest evaluation order.\n\nmulti_dot chains numpy.dot and uses optimal parenthesization of the matrices. Depending on the shapes of the matrices, this can speed up the multiplication a lot.\n\nIf the first argument is 1-D it is treated as a row vector. If the last argument is 1-D it is treated as a column vector. The other arguments must be 2-D.\n\nThink of multi_dot as:\ndef multi_dot(arrays): return functools.reduce(np.dot, arrays)\n\nParameters:\narrays : sequence of array_like - If the first argument is 1-D it is treated as row vector. If the last argument is 1-D it is treated as column vector. The other arguments must be 2-D.\nout : ndarray, optional - Output argument.\n\nReturns:\noutput : ndarray - Returns the dot product of the supplied arrays.\n\nNotes:\nThe cost for a matrix multiplication can be calculated with the following function:\ndef cost(A, B):\n    return A.shape[0] * A.shape[1] * B.shape[1]\n\nExamples:\n>>> import numpy as np\n>>> from numpy.linalg import multi_dot\n>>> A = np.random.random((10000, 100))\n>>> B = np.random.random((100, 1000))\n>>> C = np.random.random((1000, 5))\n>>> D = np.random.random((5, 333))\n>>> _ = multi_dot([A, B, C, D])",
          "code": "@array_function_dispatch(_multidot_dispatcher)\ndef multi_dot(arrays, *, out=None):\n    \"\"\"\n    Compute the dot product of two or more arrays in a single function call,\n    while automatically selecting the fastest evaluation order.\n\n    `multi_dot` chains `numpy.dot` and uses optimal parenthesization\n    of the matrices [1]_ [2]_. Depending on the shapes of the matrices,\n    this can speed up the multiplication a lot.\n\n    If the first argument is 1-D it is treated as a row vector.\n    If the last argument is 1-D it is treated as a column vector.\n    The other arguments must be 2-D.\n\n    Think of `multi_dot` as::\n\n        def multi_dot(arrays): return functools.reduce(np.dot, arrays)\n\n\n    Parameters\n    ----------\n    arrays : sequence of array_like\n        If the first argument is 1-D it is treated as row vector.\n        If the last argument is 1-D it is treated as column vector.\n        The other arguments must be 2-D.\n    out : ndarray, optional\n        Output argument. This must have the exact kind that would be returned\n        if it was not used. In particular, it must have the right type, must be\n        C-contiguous, and its dtype must be the dtype that would be returned\n        for `dot(a, b)`. This is a performance feature. Therefore, if these\n        conditions are not met, an exception is raised, instead of attempting\n        to be flexible.\n\n    Returns\n    -------\n    output : ndarray\n        Returns the dot product of the supplied arrays.\n\n    See Also\n    --------\n    numpy.dot : dot multiplication with two arguments.\n\n    References\n    ----------\n\n    .. [1] Cormen, \"Introduction to Algorithms\", Chapter 15.2, p. 370-378\n    .. [2] https://en.wikipedia.org/wiki/Matrix_chain_multiplication\n\n    Examples\n    --------\n    `multi_dot` allows you to write::\n\n    >>> import numpy as np\n    >>> from numpy.linalg import multi_dot\n    >>> # Prepare some data\n    >>> A = np.random.random((10000, 100))\n    >>> B = np.random.random((100, 1000))\n    >>> C = np.random.random((1000, 5))\n    >>> D = np.random.random((5, 333))\n    >>> # the actual dot multiplication\n    >>> _ = multi_dot([A, B, C, D])\n\n    instead of::\n\n    >>> _ = np.dot(np.dot(np.dot(A, B), C), D)\n    >>> # or\n    >>> _ = A.dot(B).dot(C).dot(D)\n\n    Notes\n    -----\n    The cost for a matrix multiplication can be calculated with the\n    following function::\n\n        def cost(A, B):\n            return A.shape[0] * A.shape[1] * B.shape[1]\n\n    Assume we have three matrices\n    :math:`A_{10 \\times 100}, B_{100 \\times 5}, C_{5 \\times 50}`.\n\n    The costs for the two different parenthesizations are as follows::\n\n        cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500\n        cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000\n\n    \"\"\"\n    n = len(arrays)\n    # optimization only makes sense for len(arrays) > 2\n    if n < 2:\n        raise ValueError(\"Expecting at least two arrays.\")\n    elif n == 2:\n        return dot(arrays[0], arrays[1], out=out)\n\n    arrays = [asanyarray(a) for a in arrays]\n\n    # save original ndim to reshape the result array into the proper form later\n    ndim_first, ndim_last = arrays[0].ndim, arrays[-1].ndim\n    # Explicitly convert vectors to 2D arrays to keep the logic of the internal\n    # _multi_dot_* functions as simple as possible.\n    if arrays[0].ndim == 1:\n        arrays[0] = atleast_2d(arrays[0])\n    if arrays[-1].ndim == 1:\n        arrays[-1] = atleast_2d(arrays[-1]).T\n    _assert_2d(*arrays)\n\n    # _multi_dot_three is much faster than _multi_dot_matrix_chain_order\n    if n == 3:\n        result = _multi_dot_three(arrays[0], arrays[1], arrays[2], out=out)\n    else:\n        order = _multi_dot_matrix_chain_order(arrays)\n        result = _multi_dot(arrays, order, 0, n - 1, out=out)\n\n    # return proper shape\n    if ndim_first == 1 and ndim_last == 1:\n        return result[0, 0]  # scalar\n    elif ndim_first == 1 or ndim_last == 1:\n        return result.ravel()  # 1-D\n    else:\n        return result"
        }
      },
      {
        "name": "numpy.vdot",
        "description": "Return the dot product of two vectors with complex conjugation",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.vdot.html",
        "add": {
          "doc": "numpy.vdot(a, b, /)\n\nReturn the dot product of two vectors.\n\nThe vdot function handles complex numbers differently than dot: if the first argument is complex, it is replaced by its complex conjugate in the dot product calculation. vdot also handles multidimensional arrays differently than dot: it does not perform a matrix product, but flattens the arguments to 1-D arrays before taking a vector dot product.\n\nConsequently, when the arguments are 2-D arrays of the same shape, this function effectively returns their Frobenius inner product (also known as the trace inner product or the standard inner product on a vector space of matrices).\n\nParameters:\na : array_like - If a is complex the complex conjugate is taken before calculation of the dot product.\nb : array_like - Second argument to the dot product.\n\nReturns:\noutput : ndarray - Dot product of a and b. Can be an int, float, or complex depending on the types of a and b.\n\nExamples:\n>>> import numpy as np\n>>> a = np.array([1+2j,3+4j])\n>>> b = np.array([5+6j,7+8j])\n>>> np.vdot(a, b)\n(70-8j)\n>>> np.vdot(b, a)\n(70+8j)\n>>> a = np.array([[1, 4], [5, 6]])\n>>> b = np.array([[4, 1], [2, 2]])\n>>> np.vdot(a, b)\n30",
          "code": "C implementation - source code available at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.vecdot",
        "description": "Vector dot product of two arrays",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.vecdot.html",
        "add": {
          "doc": "numpy.vecdot(x1, x2, /, out=None, *, casting='same_kind', order='K', dtype=None, subok=True[, signature, axes, axis]) = <ufunc 'vecdot'>\n\nVector dot product of two arrays.\n\nLet a be a vector in x1 and b be a corresponding vector in x2. The dot product is defined as:\na \u00b7 b = \u03a3(\u0101\u1d62b\u1d62) for i=0 to n-1\n\nwhere the sum is over the last dimension (unless axis is specified) and where \u0101\u1d62 denotes the complex conjugate if a\u1d62 is complex and the identity otherwise.\n\nNew in version 2.0.0.\n\nParameters:\nx1, x2 : array_like - Input arrays, scalars not allowed.\nout : ndarray, optional - A location into which the result is stored.\n**kwargs - For other keyword-only arguments, see the ufunc docs.\n\nReturns:\ny : ndarray - The vector dot product of the inputs. This is a scalar only when both x1, x2 are 1-d vectors.\n\nRaises:\nValueError - If the last dimension of x1 is not the same size as the last dimension of x2. If a scalar value is passed in.\n\nExamples:\n>>> import numpy as np\n>>> v = np.array([[0., 5., 0.], [0., 0., 10.], [0., 6., 8.]])\n>>> n = np.array([0., 0.6, 0.8])\n>>> np.vecdot(v, n)\narray([ 3.,  8., 10.])",
          "code": "Universal function implementation - C code available at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.linalg.vecdot",
        "description": "Array API standard compatible version of vecdot",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.vecdot.html",
        "add": {
          "doc": "numpy.linalg.vecdot - Array API standard compatible version of numpy.vecdot. This is an alias for numpy.vecdot with additional validation for Array API compatibility.",
          "code": "Likely an alias or wrapper for numpy.vecdot - source at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.inner",
        "description": "Inner product of two arrays",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.inner.html",
        "add": {
          "doc": "numpy.inner(a, b, /)\n\nInner product of two arrays.\n\nOrdinary inner product of vectors for 1-D arrays (without complex conjugation), in higher dimensions a sum product over the last axes.\n\nParameters:\na, b : array_like - If a and b are nonscalar, their last dimensions must match.\n\nReturns:\nout : ndarray - If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. out.shape = (*a.shape[:-1], *b.shape[:-1])\n\nRaises:\nValueError - If both a and b are nonscalar and their last dimensions have different sizes.\n\nNotes:\nFor vectors (1-D arrays) it computes the ordinary inner-product:\nnp.inner(a, b) = sum(a[:]*b[:])\n\nMore generally, if ndim(a) = r > 0 and ndim(b) = s > 0:\nnp.inner(a, b) = np.tensordot(a, b, axes=(-1,-1))\n\nExamples:\n>>> import numpy as np\n>>> a = np.array([1,2,3])\n>>> b = np.array([0,1,0])\n>>> np.inner(a, b)\n2\n>>> a = np.arange(24).reshape((2,3,4))\n>>> b = np.arange(4)\n>>> c = np.inner(a, b)\n>>> c.shape\n(2, 3)\n>>> c\narray([[ 14,  38,  62],\n       [ 86, 110, 134]])",
          "code": "C implementation - source code available at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.outer",
        "description": "Compute the outer product of two vectors",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.outer.html",
        "add": {
          "doc": "numpy.outer(a, b, out=None)\n\nCompute the outer product of two vectors.\n\nGiven two vectors a and b of length M and N, respectively, the outer product is:\n[[a_0*b_0  a_0*b_1 ... a_0*b_{N-1} ]\n [a_1*b_0    .\n [ ...          .\n [a_{M-1}*b_0            a_{M-1}*b_{N-1} ]]\n\nParameters:\na : (M,) array_like - First input vector. Input is flattened if not already 1-dimensional.\nb : (N,) array_like - Second input vector. Input is flattened if not already 1-dimensional.\nout : (M, N) ndarray, optional - A location where the result is stored\n\nReturns:\nout : (M, N) ndarray - out[i, j] = a[i] * b[j]\n\nExamples:\n>>> import numpy as np\n>>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n>>> rl\narray([[-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.],\n       [-2., -1.,  0.,  1.,  2.]])\n>>> x = np.array(['a', 'b', 'c'], dtype=object)\n>>> np.outer(x, [1, 2, 3])\narray([['a', 'aa', 'aaa'],\n       ['b', 'bb', 'bbb'],\n       ['c', 'cc', 'ccc']], dtype=object)",
          "code": "@array_function_dispatch(_outer_dispatcher)\ndef outer(a, b, out=None):\n    \"\"\"\n    Compute the outer product of two vectors.\n\n    Given two vectors `a` and `b` of length ``M`` and ``N``, respectively,\n    the outer product [1]_ is::\n\n      [[a_0*b_0  a_0*b_1 ... a_0*b_{N-1} ]\n       [a_1*b_0    .\n       [ ...          .\n       [a_{M-1}*b_0            a_{M-1}*b_{N-1} ]]\n\n    Parameters\n    ----------\n    a : (M,) array_like\n        First input vector.  Input is flattened if\n        not already 1-dimensional.\n    b : (N,) array_like\n        Second input vector.  Input is flattened if\n        not already 1-dimensional.\n    out : (M, N) ndarray, optional\n        A location where the result is stored\n\n    Returns\n    -------\n    out : (M, N) ndarray\n        ``out[i, j] = a[i] * b[j]``\n\n    See also\n    --------\n    inner\n    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.\n    ufunc.outer : A generalization to dimensions other than 1D and other\n                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``\n                  is the equivalent.\n    linalg.outer : An Array API compatible variation of ``np.outer``,\n                   which accepts 1-dimensional inputs only.\n    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``\n                is the equivalent.\n\n    References\n    ----------\n    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd\n           ed., Baltimore, MD, Johns Hopkins University Press, 1996,\n           pg. 8.\n\n    Examples\n    --------\n    Make a (*very* coarse) grid for computing a Mandelbrot set:\n\n    >>> import numpy as np\n    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))\n    >>> rl\n    array([[-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.],\n           [-2., -1.,  0.,  1.,  2.]])\n    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))\n    >>> im\n    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],\n           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],\n           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],\n           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])\n    >>> grid = rl + im\n    >>> grid\n    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],\n           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],\n           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],\n           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],\n           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])\n\n    An example using a \"vector\" of letters:\n\n    >>> x = np.array(['a', 'b', 'c'], dtype=object)\n    >>> np.outer(x, [1, 2, 3])\n    array([['a', 'aa', 'aaa'],\n           ['b', 'bb', 'bbb'],\n           ['c', 'cc', 'ccc']], dtype=object)\n\n    \"\"\"\n    a = asarray(a)\n    b = asarray(b)\n    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)"
        }
      },
      {
        "name": "numpy.linalg.outer",
        "description": "Array API compatible variation of np.outer",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.outer.html",
        "add": {
          "doc": "numpy.linalg.outer - An Array API compatible variation of np.outer, which accepts 1-dimensional inputs only. This is part of the Array API standard compliance.",
          "code": "Source code available at https://github.com/numpy/numpy - likely in linalg module"
        }
      },
      {
        "name": "numpy.matmul",
        "description": "Matrix product of two arrays",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.matmul.html",
        "add": {
          "doc": "numpy.matmul(x1, x2, /, out=None, *, casting='same_kind', order='K', dtype=None, subok=True[, signature, axes, axis]) = <ufunc 'matmul'>\n\nMatrix product of two arrays.\n\nThe behavior depends on the arguments in the following way:\n* If both arguments are 2-D they are multiplied like conventional matrices.\n* If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.\n* If the first argument is 1-D, it is promoted to a matrix by prepending a 1 to its dimensions. After matrix multiplication the prepended 1 is removed.\n* If the second argument is 1-D, it is promoted to a matrix by appending a 1 to its dimensions. After matrix multiplication the appended 1 is removed.\n\nmatmul differs from dot in two important ways:\n* Multiplication by scalars is not allowed, use * instead.\n* Stacks of matrices are broadcast together as if the matrices were elements, respecting the signature (n,k),(k,m)->(n,m)\n\nParameters:\nx1, x2 : array_like - Input arrays, scalars not allowed.\nout : ndarray, optional - A location into which the result is stored.\n**kwargs - For other keyword-only arguments, see the ufunc docs.\n\nReturns:\ny : ndarray - The matrix product of the inputs. This is a scalar only when both x1, x2 are 1-d vectors.",
          "code": "Universal function (ufunc) implementation in C - source at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.linalg.matmul",
        "description": "Array API compatible version of matmul",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.matmul.html",
        "add": {
          "doc": "numpy.linalg.matmul - Array API standard compatible version of numpy.matmul. This is likely an alias or wrapper for numpy.matmul with additional validation for Array API compatibility.",
          "code": "Likely an alias or wrapper for numpy.matmul - source at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.matvec",
        "description": "Matrix-vector product",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.matvec.html",
        "add": {
          "doc": "numpy.matvec(x1, x2, /, out=None, *, casting='same_kind', order='K', dtype=None, subok=True[, signature, axes, axis]) = <ufunc 'matvec'>\n\nMatrix-vector dot product of two arrays.\n\nGiven a matrix (or stack of matrices) A in x1 and a vector (or stack of vectors) v in x2, the matrix-vector product is defined as:\n\nA \u22c5 b = \u03a3(j=0 to n-1) A_ij * v_j\n\nwhere the sum is over the last dimensions in x1 and x2 (unless axes is specified). (For a matrix-vector product with the vector conjugated, use np.vecmat(x2, x1.mT).)\n\nNew in version 2.2.0.\n\nParameters:\nx1, x2 : array_like - Input arrays, scalars not allowed.\nout : ndarray, optional - A location into which the result is stored.\n**kwargs - For other keyword-only arguments, see the ufunc docs.\n\nReturns:\ny : ndarray - The matrix-vector product of the inputs.\n\nRaises:\nValueError - If the last dimensions of x1 and x2 are not the same size. If a scalar value is passed in.\n\nSee also:\nvecdot - Vector-vector product.\nvecmat - Vector-matrix product.\nmatmul - Matrix-matrix product.\neinsum - Einstein summation convention.\n\nExamples:\n>>> import numpy as np\n>>> a = np.array([[0., 1., 0.], [-1., 0., 0.], [0., 0., 1.]])\n>>> v = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 6., 8.]])\n>>> np.matvec(a, v)\narray([[ 0., -1., 0.],\n       [ 1., 0., 0.],\n       [ 0., 0., 1.],\n       [ 6., 0., 8.]])",
          "code": "Universal function (ufunc) implementation in C - source at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.vecmat",
        "description": "Vector-matrix product",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.vecmat.html",
        "add": {
          "doc": "numpy.vecmat(x1, x2, /, out=None, *, casting='same_kind', order='K', dtype=None, subok=True[, signature, axes, axis]) = <ufunc 'vecmat'>\n\nVector-matrix dot product of two arrays.\n\nGiven a vector (or stack of vector) v in x1 and a matrix (or stack of matrices) A in x2, the vector-matrix product is defined as:\n\nb \u22c5 A = \u03a3(i=0 to n-1) v\u0304_i * A_ij\n\nwhere the sum is over the last dimension of x1 and the one-but-last dimensions in x2 (unless axes is specified) and where v\u0304_i denotes the complex conjugate if v is complex and the identity otherwise. (For a non-conjugated vector-matrix product, use np.matvec(x2.mT, x1).)\n\nNew in version 2.2.0.\n\nParameters:\nx1, x2 : array_like - Input arrays, scalars not allowed.\nout : ndarray, optional - A location into which the result is stored.\n**kwargs - For other keyword-only arguments, see the ufunc docs.\n\nReturns:\ny : ndarray - The vector-matrix product of the inputs.\n\nRaises:\nValueError - If the last dimensions of x1 and the one-but-last dimension of x2 are not the same size. If a scalar value is passed in.\n\nSee also:\nvecdot - Vector-vector product.\nmatvec - Matrix-vector product.\nmatmul - Matrix-matrix product.\neinsum - Einstein summation convention.\n\nExamples:\n>>> import numpy as np\n>>> v = np.array([0., 4., 2.])\n>>> a = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 0.]])\n>>> np.vecmat(v, a)\narray([ 0., 4., 0.])",
          "code": "Universal function (ufunc) implementation in C - source at https://github.com/numpy/numpy"
        }
      },
      {
        "name": "numpy.tensordot",
        "description": "Compute tensor dot product along specified axes",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.tensordot.html",
        "add": {
          "doc": "numpy.tensordot(a, b, axes=2)\n\nCompute tensor dot product along specified axes.\n\nGiven two tensors, a and b, and an array_like object containing two array_like objects, (a_axes, b_axes), sum the products of a's and b's elements (components) over the axes specified by a_axes and b_axes. The third argument can be a single non-negative integer_like scalar, N; if it is such, then the last N dimensions of a and the first N dimensions of b are summed over.\n\nParameters:\na, b : array_like - Tensors to \"dot\".\naxes : int or (2,) array_like\n  - integer_like: If an int N, sum over the last N axes of a and the first N axes of b in order. The sizes of the corresponding axes must match.\n  - (2,) array_like: Or, a list of axes to be summed over, first sequence applying to a, second to b. Both elements array_like must be of the same length.\n\nReturns:\noutput : ndarray - The tensor dot product of the input.\n\nSee also:\ndot, einsum\n\nNotes:\nThree common use cases are:\n- axes = 0 : tensor product a \u2297 b\n- axes = 1 : tensor dot product a \u22c5 b\n- axes = 2 : (default) tensor double contraction a : b\n\nWhen axes is integer_like, the sequence of axes for evaluation will be: from the -Nth axis to the -1th axis in a, and from the 0th axis to (N-1)th axis in b.\n\nExamples:\n>>> import numpy as np\n>>> a = np.arange(60.).reshape(3,4,5)\n>>> b = np.arange(24.).reshape(4,3,2)\n>>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n>>> c.shape\n(5, 2)\n>>> c\narray([[4400., 4730.],\n       [4532., 4874.],\n       [4664., 5018.],\n       [4796., 5162.],\n       [4928., 5306.]])",
          "code": "@array_function_dispatch(_tensordot_dispatcher)\ndef tensordot(a, b, axes=2):\n    \"\"\"\n    Compute tensor dot product along specified axes.\n\n    Given two tensors, `a` and `b`, and an array_like object containing\n    two array_like objects, ``(a_axes, b_axes)``, sum the products of\n    `a`'s and `b`'s elements (components) over the axes specified by\n    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative\n    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions\n    of `a` and the first ``N`` dimensions of `b` are summed over.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Tensors to \"dot\".\n\n    axes : int or (2,) array_like\n        * integer_like\n          If an int N, sum over the last N axes of `a` and the first N axes\n          of `b` in order. The sizes of the corresponding axes must match.\n        * (2,) array_like\n          Or, a list of axes to be summed over, first sequence applying to `a`,\n          second to `b`. Both elements array_like must be of the same length.\n\n    Returns\n    -------\n    output : ndarray\n        The tensor dot product of the input.\n\n    See Also\n    --------\n    dot, einsum\n\n    Notes\n    -----\n    Three common use cases are:\n        * ``axes = 0`` : tensor product :math:`a\\\\otimes b`\n        * ``axes = 1`` : tensor dot product :math:`a\\\\cdot b`\n        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`\n\n    When `axes` is integer_like, the sequence of axes for evaluation\n    will be: from the -Nth axis to the -1th axis in `a`,\n    and from the 0th axis to (N-1)th axis in `b`.\n    For example, ``axes = 2`` is the equal to\n    ``axes = [[-2, -1], [0, 1]]``.\n    When N-1 is smaller than 0, or when -N is larger than -1,\n    the element of `a` and `b` are defined as the `axes`.\n\n    When there is more than one axis to sum over - and they are not the last\n    (first) axes of `a` (`b`) - the argument `axes` should consist of\n    two sequences of the same length, with the first axis to sum over given\n    first in both sequences, the second axis second, and so forth.\n    The calculation can be referred to ``numpy.einsum``.\n\n    The shape of the result consists of the non-contracted axes of the\n    first tensor, followed by the non-contracted axes of the second.\n\n    Examples\n    --------\n    An example on integer_like:\n\n    >>> a_0 = np.array([[1, 2], [3, 4]])\n    >>> b_0 = np.array([[5, 6], [7, 8]])\n    >>> c_0 = np.tensordot(a_0, b_0, axes=0)\n    >>> c_0.shape\n    (2, 2, 2, 2)\n    >>> c_0\n    array([[[[ 5,  6],\n             [ 7,  8]],\n            [[10, 12],\n             [14, 16]]],\n           [[[15, 18],\n             [21, 24]],\n            [[20, 24],\n             [28, 32]]]])\n\n    An example on array_like:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))\n    >>> c.shape\n    (5, 2)\n    >>> c\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n\n    A slower but equivalent way of computing the same...\n\n    >>> d = np.zeros((5,2))\n    >>> for i in range(5):\n    ...   for j in range(2):\n    ...     for k in range(3):\n    ...       for n in range(4):\n    ...         d[i,j] += a[k,n,i] * b[n,k,j]\n    >>> c == d\n    array([[ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True],\n           [ True,  True]])\n\n    An extended example taking advantage of the overloading of + and \\\\*:\n\n    >>> a = np.array(range(1, 9))\n    >>> a.shape = (2, 2, 2)\n    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)\n    >>> A.shape = (2, 2)\n    >>> a; A\n    array([[[1, 2],\n            [3, 4]],\n           [[5, 6],\n            [7, 8]]])\n    array([['a', 'b'],\n           ['c', 'd']], dtype=object)\n\n    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction\n    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, 1)\n    array([[['acc', 'bdd'],\n            ['aaacccc', 'bbbdddd']],\n           [['aaaaacccccc', 'bbbbbdddddd'],\n            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)\n    array([[[[['a', 'b'],\n              ['c', 'd']],\n              ...\n\n    >>> np.tensordot(a, A, (0, 1))\n    array([[['abbbbb', 'cddddd'],\n            ['aabbbbbb', 'ccdddddd']],\n           [['aaabbbbbbb', 'cccddddddd'],\n            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, (2, 1))\n    array([[['abb', 'cdd'],\n            ['aaabbbb', 'cccdddd']],\n           [['aaaaabbbbbb', 'cccccdddddd'],\n            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)\n\n    >>> np.tensordot(a, A, ((0, 1), (0, 1)))\n    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)\n\n    >>> np.tensordot(a, A, ((2, 1), (1, 0)))\n    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)\n\n    \"\"\"\n    try:\n        iter(axes)\n    except Exception:\n        axes_a = list(range(-axes, 0))\n        axes_b = list(range(axes))\n    else:\n        axes_a, axes_b = axes\n    try:\n        na = len(axes_a)\n        axes_a = list(axes_a)\n    except TypeError:\n        axes_a = [axes_a]\n        na = 1\n    try:\n        nb = len(axes_b)\n        axes_b = list(axes_b)\n    except TypeError:\n        axes_b = [axes_b]\n        nb = 1\n\n    a, b = asarray(a), asarray(b)\n    as_ = a.shape\n    nda = a.ndim\n    bs = b.shape\n    ndb = b.ndim\n    equal = True\n    if na != nb:\n        equal = False\n    else:\n        for k in range(na):\n            if as_[axes_a[k]] != bs[axes_b[k]]:\n                equal = False\n                break\n            if axes_a[k] < 0:\n                axes_a[k] += nda\n            if axes_b[k] < 0:\n                axes_b[k] += ndb\n    if not equal:\n        raise ValueError(\"shape-mismatch for sum\")\n\n    # Move the axes to sum over to the end of \"a\"\n    # and to the front of \"b\"\n    notin = [k for k in range(nda) if k not in axes_a]\n    newaxes_a = notin + axes_a\n    N2 = math.prod(as_[axis] for axis in axes_a)\n    newshape_a = (math.prod(as_[ax] for ax in notin), N2)\n    olda = [as_[axis] for axis in notin]\n\n    notin = [k for k in range(ndb) if k not in axes_b]\n    newaxes_b = axes_b + notin\n    N2 = math.prod(bs[axis] for axis in axes_b)\n    newshape_b = (N2, math.prod(bs[ax] for ax in notin))\n    oldb = [bs[axis] for axis in notin]\n\n    at = a.transpose(newaxes_a).reshape(newshape_a)\n    bt = b.transpose(newaxes_b).reshape(newshape_b)\n    res = dot(at, bt)\n    return res.reshape(olda + oldb)"
        }
      },
      {
        "name": "numpy.linalg.tensordot",
        "description": "Array API compatible version of tensordot",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.tensordot.html",
        "add": {
          "doc": "linalg.tensordot(x1, x2, /, *, axes=2)\n\nCompute tensor dot product along specified axes.\n\nGiven two tensors, a and b, and an array_like object containing two array_like objects, (a_axes, b_axes), sum the products of a's and b's elements (components) over the axes specified by a_axes and b_axes. The third argument can be a single non-negative integer_like scalar, N; if it is such, then the last N dimensions of a and the first N dimensions of b are summed over.\n\nParameters:\na, b : array_like - Tensors to \"dot\".\naxes : int or (2,) array_like\n  - integer_like: If an int N, sum over the last N axes of a and the first N axes of b in order. The sizes of the corresponding axes must match.\n  - (2,) array_like: Or, a list of axes to be summed over, first sequence applying to a, second to b. Both elements array_like must be of the same length.\n\nReturns:\noutput : ndarray - The tensor dot product of the input.\n\nSee also:\ndot, einsum\n\nNotes:\nThree common use cases are:\n- axes = 0 : tensor product a \u2297 b\n- axes = 1 : tensor dot product a \u22c5 b\n- axes = 2 : (default) tensor double contraction a : b",
          "code": "@array_function_dispatch(_tensordot_dispatcher)\ndef tensordot(x1, x2, /, *, axes=2):\n    return _core_tensordot(x1, x2, axes=axes)"
        }
      },
      {
        "name": "numpy.einsum",
        "description": "Evaluates the Einstein summation convention on the operands",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.einsum.html",
        "add": {
          "doc": "numpy.einsum(subscripts, *operands, out=None, dtype=None, order='K', casting='safe', optimize=False)\n\nEvaluates the Einstein summation convention on the operands.\n\nUsing the Einstein summation convention, many common multi-dimensional, linear algebraic array operations can be represented in a simple fashion. In implicit mode einsum computes these values.\n\nIn explicit mode, einsum provides further flexibility to compute other array operations that might not be considered classical Einstein summation operations, by disabling, or forcing summation over specified subscript labels.\n\nParameters:\nsubscripts : str - Specifies the subscripts for summation as comma separated list of subscript labels. An implicit (classical Einstein summation) calculation is performed unless the explicit indicator '->' is included as well as subscript labels of the precise output form.\noperands : list of array_like - These are the arrays for the operation.\nout : ndarray, optional - If provided, the calculation is done into this array.\ndtype : {data-type, None}, optional - If provided, forces the calculation to use the data type specified.\norder : {'C', 'F', 'A', 'K'}, optional - Controls the memory layout of the output.\ncasting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional - Controls what kind of data casting may occur.\noptimize : {False, True, 'greedy', 'optimal'}, optional - Controls if intermediate optimization should occur.\n\nReturns:\noutput : ndarray - The calculation based on the Einstein summation convention.\n\nSee also:\neinsum_path, dot, inner, outer, tensordot, linalg.multi_dot\n\nExamples:\n>>> import numpy as np\n>>> a = np.arange(25).reshape(5,5)\n>>> b = np.arange(5)\n>>> np.einsum('ii', a)\n60\n>>> np.einsum('ii->i', a)\narray([ 0,  6, 12, 18, 24])\n>>> np.einsum('ij,j', a, b)\narray([ 30,  80, 130, 180, 230])\n>>> np.einsum('ji', c)\narray([[0, 3],\n       [1, 4],\n       [2, 5]])",
          "code": "@array_function_dispatch(_einsum_dispatcher, module='numpy')\ndef einsum(*operands, out=None, optimize=False, **kwargs):\n    \"\"\"\n    einsum(subscripts, *operands, out=None, dtype=None, order='K',\n           casting='safe', optimize=False)\n\n    Evaluates the Einstein summation convention on the operands.\n\n    Using the Einstein summation convention, many common multi-dimensional,\n    linear algebraic array operations can be represented in a simple fashion.\n    In *implicit* mode `einsum` computes these values.\n\n    In *explicit* mode, `einsum` provides further flexibility to compute\n    other array operations that might not be considered classical Einstein\n    summation operations, by disabling, or forcing summation over specified\n    subscript labels.\n\n    See the notes and examples for clarification.\n\n    Parameters\n    ----------\n    subscripts : str\n        Specifies the subscripts for summation as comma separated list of\n        subscript labels. An implicit (classical Einstein summation)\n        calculation is performed unless the explicit indicator '->' is\n        included as well as subscript labels of the precise output form.\n    operands : list of array_like\n        These are the arrays for the operation.\n    out : ndarray, optional\n        If provided, the calculation is done into this array.\n    dtype : {data-type, None}, optional\n        If provided, forces the calculation to use the data type specified.\n        Note that you may have to also give a more liberal `casting`\n        parameter to allow the conversions. Default is None.\n    order : {'C', 'F', 'A', 'K'}, optional\n        Controls the memory layout of the output. 'C' means it should\n        be C contiguous. 'F' means it should be Fortran contiguous,\n        'A' means it should be 'F' if the inputs are all 'F', 'C' otherwise.\n        'K' means it should be as close to the layout as the inputs as\n        is possible, including arbitrarily permuted axes.\n        Default is 'K'.\n    casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n        Controls what kind of data casting may occur.  Setting this to\n        'unsafe' is not recommended, as it can adversely affect accumulations.\n\n        * 'no' means the data types should not be cast at all.\n        * 'equiv' means only byte-order changes are allowed.\n        * 'safe' means only casts which can preserve values are allowed.\n        * 'same_kind' means only safe casts or casts within a kind,\n          like float64 to float32, are allowed.\n        * 'unsafe' means any data conversions may be done.\n\n        Default is 'safe'.\n    optimize : {False, True, 'greedy', 'optimal'}, optional\n        Controls if intermediate optimization should occur. No optimization\n        will occur if False and True will default to the 'greedy' algorithm.\n        Also accepts an explicit contraction list from the ``np.einsum_path``\n        function. See ``np.einsum_path`` for more details. Defaults to False.\n\n    Returns\n    -------\n    output : ndarray\n        The calculation based on the Einstein summation convention.\n\n    See Also\n    --------\n    einsum_path, dot, inner, outer, tensordot, linalg.multi_dot\n    einsum:\n        Similar verbose interface is provided by the\n        `einops <https://github.com/arogozhnikov/einops>`_ package to cover\n        additional operations: transpose, reshape/flatten, repeat/tile,\n        squeeze/unsqueeze and reductions.\n        The `opt_einsum <https://optimized-einsum.readthedocs.io/en/stable/>`_\n        optimizes contraction order for einsum-like expressions\n        in backend-agnostic manner.\n\n    Notes\n    -----\n    The Einstein summation convention can be used to compute\n    many multi-dimensional, linear algebraic array operations. `einsum`\n    provides a succinct way of representing these.\n\n    A non-exhaustive list of these operations,\n    which can be computed by `einsum`, is shown below along with examples:\n\n    * Trace of an array, :py:func:`numpy.trace`.\n    * Return a diagonal, :py:func:`numpy.diag`.\n    * Array axis summations, :py:func:`numpy.sum`.\n    * Transpositions and permutations, :py:func:`numpy.transpose`.\n    * Matrix multiplication and dot product, :py:func:`numpy.matmul`\n        :py:func:`numpy.dot`.\n    * Vector inner and outer products, :py:func:`numpy.inner`\n        :py:func:`numpy.outer`.\n    * Broadcasting, element-wise and scalar multiplication,\n        :py:func:`numpy.multiply`.\n    * Tensor contractions, :py:func:`numpy.tensordot`.\n    * Chained array operations, in efficient calculation order,\n        :py:func:`numpy.einsum_path`.\n\n    The subscripts string is a comma-separated list of subscript labels,\n    where each label refers to a dimension of the corresponding operand.\n    Whenever a label is repeated it is summed, so ``np.einsum('i,i', a, b)``\n    is equivalent to :py:func:`np.inner(a,b) <numpy.inner>`. If a label\n    appears only once, it is not summed, so ``np.einsum('i', a)``\n    produces a view of ``a`` with no changes. A further example\n    ``np.einsum('ij,jk', a, b)`` describes traditional matrix multiplication\n    and is equivalent to :py:func:`np.matmul(a,b) <numpy.matmul>`.\n    Repeated subscript labels in one operand take the diagonal.\n    For example, ``np.einsum('ii', a)`` is equivalent to\n    :py:func:`np.trace(a) <numpy.trace>`.\n\n    In *implicit mode*, the chosen subscripts are important\n    since the axes of the output are reordered alphabetically.  This\n    means that ``np.einsum('ij', a)`` doesn't affect a 2D array, while\n    ``np.einsum('ji', a)`` takes its transpose. Additionally,\n    ``np.einsum('ij,jk', a, b)`` returns a matrix multiplication, while,\n    ``np.einsum('ij,jh', a, b)`` returns the transpose of the\n    multiplication since subscript 'h' precedes subscript 'i'.\n\n    In *explicit mode* the output can be directly controlled by\n    specifying output subscript labels.  This requires the\n    identifier '->' as well as the list of output subscript labels.\n    This feature increases the flexibility of the function since\n    summing can be disabled or forced when required. The call\n    ``np.einsum('i->', a)`` is like :py:func:`np.sum(a) <numpy.sum>`\n    if ``a`` is a 1-D array, and ``np.einsum('ii->i', a)``\n    is like :py:func:`np.diag(a) <numpy.diag>` if ``a`` is a square 2-D array.\n    The difference is that `einsum` does not allow broadcasting by default.\n    Additionally ``np.einsum('ij,jh->ih', a, b)`` directly specifies the\n    order of the output subscript labels and therefore returns matrix\n    multiplication, unlike the example above in implicit mode.\n\n    To enable and control broadcasting, use an ellipsis.  Default\n    NumPy-style broadcasting is done by adding an ellipsis\n    to the left of each term, like ``np.einsum('...ii->...i', a)``.\n    ``np.einsum('...i->...', a)`` is like\n    :py:func:`np.sum(a, axis=-1) <numpy.sum>` for array ``a`` of any shape.\n    To take the trace along the first and last axes,\n    you can do ``np.einsum('i...i', a)``, or to do a matrix-matrix\n    product with the left-most indices instead of rightmost, one can do\n    ``np.einsum('ij...,jk...->ik...', a, b)``.\n\n    When there is only one operand, no axes are summed, and no output\n    parameter is provided, a view into the operand is returned instead\n    of a new array.  Thus, taking the diagonal as ``np.einsum('ii->i', a)``\n    produces a view (changed in version 1.10.0).\n\n    `einsum` also provides an alternative way to provide the subscripts and\n    operands as ``einsum(op0, sublist0, op1, sublist1, ..., [sublistout])``.\n    If the output shape is not provided in this format `einsum` will be\n    calculated in implicit mode, otherwise it will be performed explicitly.\n    The examples below have corresponding `einsum` calls with the two\n    parameter methods.\n\n    Views returned from einsum are now writeable whenever the input array\n    is writeable. For example, ``np.einsum('ijk...->kji...', a)`` will now\n    have the same effect as :py:func:`np.swapaxes(a, 0, 2) <numpy.swapaxes>`\n    and ``np.einsum('ii->i', a)`` will return a writeable view of the diagonal\n    of a 2D array.\n\n    Added the ``optimize`` argument which will optimize the contraction order\n    of an einsum expression. For a contraction with three or more operands\n    this can greatly increase the computational efficiency at the cost of\n    a larger memory footprint during computation.\n\n    Typically a 'greedy' algorithm is applied which empirical tests have shown\n    returns the optimal path in the majority of cases. In some cases 'optimal'\n    will return the superlative path through a more expensive, exhaustive\n    search. For iterative calculations it may be advisable to calculate\n    the optimal path once and reuse that path by supplying it as an argument.\n    An example is given below.\n\n    See :py:func:`numpy.einsum_path` for more details.\n\n    Examples\n    --------\n    >>> a = np.arange(25).reshape(5,5)\n    >>> b = np.arange(5)\n    >>> c = np.arange(6).reshape(2,3)\n\n    Trace of a matrix:\n\n    >>> np.einsum('ii', a)\n    60\n    >>> np.einsum(a, [0,0])\n    60\n    >>> np.trace(a)\n    60\n\n    Extract the diagonal (requires explicit form):\n\n    >>> np.einsum('ii->i', a)\n    array([ 0,  6, 12, 18, 24])\n    >>> np.einsum(a, [0,0], [0])\n    array([ 0,  6, 12, 18, 24])\n    >>> np.diag(a)\n    array([ 0,  6, 12, 18, 24])\n\n    Sum over an axis (requires explicit form):\n\n    >>> np.einsum('ij->i', a)\n    array([ 10,  35,  60,  85, 110])\n    >>> np.einsum(a, [0,1], [0])\n    array([ 10,  35,  60,  85, 110])\n    >>> np.sum(a, axis=1)\n    array([ 10,  35,  60,  85, 110])\n\n    For higher dimensional arrays summing a single axis can be done\n    with ellipsis:\n\n    >>> np.einsum('...j->...', a)\n    array([ 10,  35,  60,  85, 110])\n    >>> np.einsum(a, [Ellipsis,1], [Ellipsis])\n    array([ 10,  35,  60,  85, 110])\n\n    Compute a matrix transpose, or reorder any number of axes:\n\n    >>> np.einsum('ji', c)\n    array([[0, 3],\n           [1, 4],\n           [2, 5]])\n    >>> np.einsum('ij->ji', c)\n    array([[0, 3],\n           [1, 4],\n           [2, 5]])\n    >>> np.einsum(c, [1,0])\n    array([[0, 3],\n           [1, 4],\n           [2, 5]])\n    >>> np.transpose(c)\n    array([[0, 3],\n           [1, 4],\n           [2, 5]])\n\n    Vector inner products:\n\n    >>> np.einsum('i,i', b, b)\n    30\n    >>> np.einsum(b, [0], b, [0])\n    30\n    >>> np.inner(b,b)\n    30\n\n    Matrix vector multiplication:\n\n    >>> np.einsum('ij,j', a, b)\n    array([ 30,  80, 130, 180, 230])\n    >>> np.einsum(a, [0,1], b, [1])\n    array([ 30,  80, 130, 180, 230])\n    >>> np.dot(a, b)\n    array([ 30,  80, 130, 180, 230])\n    >>> np.einsum('...j,j', a, b)\n    array([ 30,  80, 130, 180, 230])\n\n    Broadcasting and scalar multiplication:\n\n    >>> np.einsum('..., ...', 3, c)\n    array([[ 0,  3,  6],\n           [ 9, 12, 15]])\n    >>> np.einsum(',ij', 3, c)\n    array([[ 0,  3,  6],\n           [ 9, 12, 15]])\n    >>> np.einsum(3, [Ellipsis], c, [Ellipsis])\n    array([[ 0,  3,  6],\n           [ 9, 12, 15]])\n    >>> np.multiply(3, c)\n    array([[ 0,  3,  6],\n           [ 9, 12, 15]])\n\n    Vector outer product:\n\n    >>> np.einsum('i,j', np.arange(2)+1, b)\n    array([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n    >>> np.einsum(np.arange(2)+1, [0], b, [1])\n    array([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n    >>> np.outer(np.arange(2)+1, b)\n    array([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n\n    Tensor contraction:\n\n    >>> a = np.arange(60.).reshape(3,4,5)\n    >>> b = np.arange(24.).reshape(4,3,2)\n    >>> np.einsum('ijk,jil->kl', a, b)\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n    >>> np.einsum(a, [0,1,2], b, [1,0,3], [2,3])\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n    >>> np.tensordot(a,b, axes=([1,0],[0,1]))\n    array([[4400., 4730.],\n           [4532., 4874.],\n           [4664., 5018.],\n           [4796., 5162.],\n           [4928., 5306.]])\n\n    Writeable returned arrays (since version 1.10.0):\n\n    >>> a = np.zeros((3, 3))\n    >>> np.einsum('ii->i', a)[:] = 1\n    >>> a\n    array([[1., 0., 0.],\n           [0., 1., 0.],\n           [0., 0., 1.]])\n\n    Example of ellipsis use:\n\n    >>> a = np.arange(6).reshape((3,2))\n    >>> b = np.arange(12).reshape((4,3))\n    >>> np.einsum('ki,jk->ij', a, b)\n    array([[10, 28, 46, 64],\n           [13, 40, 67, 94]])\n    >>> np.einsum('ki,...k->i...', a, b)\n    array([[10, 28, 46, 64],\n           [13, 40, 67, 94]])\n    >>> np.einsum('k...,jk', a, b)\n    array([[10, 28, 46, 64],\n           [13, 40, 67, 94]])\n\n    Chained array operations. For more complicated contractions, speed ups\n    might be achieved by repeatedly computing a 'greedy' path or pre-computing\n    the 'optimal' path and repeatedly applying it, using an `einsum_path`\n    insertion (since version 1.12.0). Performance improvements can be\n    particularly significant with larger arrays:\n\n    >>> a = np.ones(64).reshape(2,4,8)\n\n    Basic `einsum`: ~1520ms  (benchmarked on 3.1GHz Intel i5.)\n\n    >>> for iteration in range(500):\n    ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a)\n\n    Sub-optimal `einsum` (due to repeated path calculation time): ~330ms\n\n    >>> for iteration in range(500):\n    ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a,\n    ...         optimize='optimal')\n\n    Greedy `einsum` (faster optimal path approximation): ~160ms\n\n    >>> for iteration in range(500):\n    ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize='greedy')\n\n    Optimal `einsum` (best usage pattern in some use cases): ~110ms\n\n    >>> path = np.einsum_path('ijk,ilm,njm,nlk,abc->',a,a,a,a,a,\n    ...     optimize='optimal')[0]\n    >>> for iteration in range(500):\n    ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize=path)\n\n    \"\"\"\n    # Special handling if out is specified\n    specified_out = out is not None\n\n    # If no optimization, run pure einsum\n    if optimize is False:\n        if specified_out:\n            kwargs['out'] = out\n        return c_einsum(*operands, **kwargs)\n\n    # Check the kwargs to avoid a more cryptic error later, without having to\n    # repeat default values here\n    valid_einsum_kwargs = ['dtype', 'order', 'casting']\n    unknown_kwargs = [k for (k, v) in kwargs.items() if\n                      k not in valid_einsum_kwargs]\n    if len(unknown_kwargs):\n        raise TypeError(f\"Did not understand the following kwargs: {unknown_kwargs}\")\n\n    # Build the contraction list and operand\n    operands, contraction_list = einsum_path(*operands, optimize=optimize,\n                                             einsum_call=True)\n\n    # Handle order kwarg for output array, c_einsum allows mixed case\n    output_order = kwargs.pop('order', 'K')\n    if output_order.upper() == 'A':\n        if all(arr.flags.f_contiguous for arr in operands):\n            output_order = 'F'\n        else:\n            output_order = 'C'\n\n    # Start contraction loop\n    for num, contraction in enumerate(contraction_list):\n        inds, idx_rm, einsum_str, remaining, blas = contraction\n        tmp_operands = [operands.pop(x) for x in inds]\n\n        # Do we need to deal with the output?\n        handle_out = specified_out and ((num + 1) == len(contraction_list))\n\n        # Call tensordot if still possible\n        if blas:\n            # Checks have already been handled\n            input_str, results_index = einsum_str.split('->')\n            input_left, input_right = input_str.split(',')\n\n            tensor_result = input_left + input_right\n            for s in idx_rm:\n                tensor_result = tensor_result.replace(s, \"\")\n\n            # Find indices to contract over\n            left_pos, right_pos = [], []\n            for s in sorted(idx_rm):\n                left_pos.append(input_left.find(s))\n                right_pos.append(input_right.find(s))\n\n            # Contract!\n            new_view = tensordot(\n                *tmp_operands, axes=(tuple(left_pos), tuple(right_pos))\n            )\n\n            # Build a new view if needed\n            if (tensor_result != results_index) or handle_out:\n                if handle_out:\n                    kwargs[\"out\"] = out\n                new_view = c_einsum(\n                    tensor_result + '->' + results_index, new_view, **kwargs\n                )\n\n        # Call einsum\n        else:\n            # If out was specified\n            if handle_out:\n                kwargs[\"out\"] = out\n\n            # Do the contraction\n            new_view = c_einsum(einsum_str, *tmp_operands, **kwargs)\n\n        # Append new items and dereference what we can\n        operands.append(new_view)\n        del tmp_operands, new_view\n\n    if specified_out:\n        return out\n    else:\n        return asanyarray(operands[0], order=output_order)"
        }
      },
      {
        "name": "numpy.einsum_path",
        "description": "Evaluates the lowest cost contraction order for an einsum expression",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.einsum_path.html",
        "add": {
          "doc": "numpy.einsum_path(subscripts, *operands, optimize='greedy')\n\nEvaluates the lowest cost contraction order for an einsum expression by considering the creation of intermediate arrays.\n\nParameters:\nsubscripts : str - Specifies the subscripts for summation.\n*operands : list of array_like - These are the arrays for the operation.\noptimize : {bool, list, tuple, 'greedy', 'optimal'} - Choose the type of path. If a tuple is provided, the second argument is assumed to be the maximum intermediate size created. If only a single argument is provided the largest input or output array size is used as a maximum intermediate size.\n  - if a list is given that starts with einsum_path, uses this as the contraction path\n  - if False no optimization is taken\n  - if True defaults to the 'greedy' algorithm\n  - 'optimal' An algorithm that combinatorially explores all possible ways of contracting the listed tensors and chooses the least costly path. Scales exponentially with the number of terms in the contraction.\n  - 'greedy' An algorithm that chooses the best pair contraction at each step. Effectively, this algorithm searches the largest inner, Hadamard, and then outer products at each step. Scales cubically with the number of terms in the contraction. Equivalent to the 'optimal' path for most contractions.\n\nReturns:\npath : list of tuples - A list representation of the einsum path.\nstring_repr : str - A printable representation of the einsum path.\n\nSee also:\neinsum, linalg.multi_dot\n\nExamples:\n>>> import numpy as np\n>>> a = np.random.rand(2, 2)\n>>> b = np.random.rand(2, 5)\n>>> c = np.random.rand(5, 2)\n>>> path_info = np.einsum_path('ij,jk,kl->il', a, b, c, optimize='greedy')\n>>> print(path_info[0])\n['einsum_path', (1, 2), (0, 1)]\n>>> print(path_info[1])\nComplete contraction:  ij,jk,kl->il\n         Naive scaling:  4\n     Optimized scaling:  3\n      Naive FLOP count:  1.600e+02\n  Optimized FLOP count:  5.600e+01\n   Theoretical speedup:  2.857\n  Largest intermediate:  4.000e+00 elements",
          "code": "@array_function_dispatch(_einsum_path_dispatcher, module='numpy')\ndef einsum_path(*operands, optimize='greedy', einsum_call=False):\n    \"\"\"\n    einsum_path(subscripts, *operands, optimize='greedy')\n\n    Evaluates the lowest cost contraction order for an einsum expression by\n    considering the creation of intermediate arrays.\n\n    Parameters\n    ----------\n    subscripts : str\n        Specifies the subscripts for summation.\n    *operands : list of array_like\n        These are the arrays for the operation.\n    optimize : {bool, list, tuple, 'greedy', 'optimal'}\n        Choose the type of path. If a tuple is provided, the second argument is\n        assumed to be the maximum intermediate size created. If only a single\n        argument is provided the largest input or output array size is used\n        as a maximum intermediate size.\n\n        * if a list is given that starts with ``einsum_path``, uses this as the\n          contraction path\n        * if False no optimization is taken\n        * if True defaults to the 'greedy' algorithm\n        * 'optimal' An algorithm that combinatorially explores all possible\n          ways of contracting the listed tensors and chooses the least costly\n          path. Scales exponentially with the number of terms in the\n          contraction.\n        * 'greedy' An algorithm that chooses the best pair contraction\n          at each step. Effectively, this algorithm searches the largest inner,\n          Hadamard, and then outer products at each step. Scales cubically with\n          the number of terms in the contraction. Equivalent to the 'optimal'\n          path for most contractions.\n\n        Default is 'greedy'.\n\n    Returns\n    -------\n    path : list of tuples\n        A list representation of the einsum path.\n    string_repr : str\n        A printable representation of the einsum path.\n\n    Notes\n    -----\n    The resulting path indicates which terms of the input contraction should be\n    contracted first, the result of this contraction is then appended to the\n    end of the contraction list. This list can then be iterated over until all\n    intermediate contractions are complete.\n\n    See Also\n    --------\n    einsum, linalg.multi_dot\n\n    Examples\n    --------\n\n    We can begin with a chain dot example. In this case, it is optimal to\n    contract the ``b`` and ``c`` tensors first as represented by the first\n    element of the path ``(1, 2)``. The resulting tensor is added to the end\n    of the contraction and the remaining contraction ``(0, 1)`` is then\n    completed.\n\n    >>> np.random.seed(123)\n    >>> a = np.random.rand(2, 2)\n    >>> b = np.random.rand(2, 5)\n    >>> c = np.random.rand(5, 2)\n    >>> path_info = np.einsum_path('ij,jk,kl->il', a, b, c, optimize='greedy')\n    >>> print(path_info[0])\n    ['einsum_path', (1, 2), (0, 1)]\n    >>> print(path_info[1])\n      Complete contraction:  ij,jk,kl->il # may vary\n             Naive scaling:  4\n         Optimized scaling:  3\n          Naive FLOP count:  1.600e+02\n      Optimized FLOP count:  5.600e+01\n       Theoretical speedup:  2.857\n      Largest intermediate:  4.000e+00 elements\n    -------------------------------------------------------------------------\n    scaling                  current                                remaining\n    -------------------------------------------------------------------------\n       3                   kl,jk->jl                                ij,jl->il\n       3                   jl,ij->il                                   il->il\n\n\n    A more complex index transformation example.\n\n    >>> I = np.random.rand(10, 10, 10, 10)\n    >>> C = np.random.rand(10, 10)\n    >>> path_info = np.einsum_path('ea,fb,abcd,gc,hd->efgh', C, C, I, C, C,\n    ...                            optimize='greedy')\n\n    >>> print(path_info[0])\n    ['einsum_path', (0, 2), (0, 3), (0, 2), (0, 1)]\n    >>> print(path_info[1])\n      Complete contraction:  ea,fb,abcd,gc,hd->efgh # may vary\n             Naive scaling:  8\n         Optimized scaling:  5\n          Naive FLOP count:  8.000e+08\n      Optimized FLOP count:  8.000e+05\n       Theoretical speedup:  1000.000\n      Largest intermediate:  1.000e+04 elements\n    --------------------------------------------------------------------------\n    scaling                  current                                remaining\n    --------------------------------------------------------------------------\n       5               abcd,ea->bcde                      fb,gc,hd,bcde->efgh\n       5               bcde,fb->cdef                         gc,hd,cdef->efgh\n       5               cdef,gc->defg                            hd,defg->efgh\n       5               defg,hd->efgh                               efgh->efgh\n    \"\"\"\n\n    # Figure out what the path really is\n    path_type = optimize\n    if path_type is True:\n        path_type = 'greedy'\n    if path_type is None:\n        path_type = False\n\n    explicit_einsum_path = False\n    memory_limit = None\n\n    # No optimization or a named path algorithm\n    if (path_type is False) or isinstance(path_type, str):\n        pass\n\n    # Given an explicit path\n    elif len(path_type) and (path_type[0] == 'einsum_path'):\n        explicit_einsum_path = True\n\n    # Path tuple with memory limit\n    elif ((len(path_type) == 2) and isinstance(path_type[0], str) and\n            isinstance(path_type[1], (int, float))):\n        memory_limit = int(path_type[1])\n        path_type = path_type[0]\n\n    else:\n        raise TypeError(f\"Did not understand the path: {str(path_type)}\")\n\n    # Hidden option, only einsum should call this\n    einsum_call_arg = einsum_call\n\n    # Python side parsing\n    input_subscripts, output_subscript, operands = (\n        _parse_einsum_input(operands)\n    )\n\n    # Build a few useful list and sets\n    input_list = input_subscripts.split(',')\n    input_sets = [set(x) for x in input_list]\n    output_set = set(output_subscript)\n    indices = set(input_subscripts.replace(',', ''))\n\n    # Get length of each unique dimension and ensure all dimensions are correct\n    dimension_dict = {}\n    broadcast_indices = [[] for x in range(len(input_list))]\n    for tnum, term in enumerate(input_list):\n        sh = operands[tnum].shape\n        if len(sh) != len(term):\n            raise ValueError(\"Einstein sum subscript %s does not contain the \"\n                             \"correct number of indices for operand %d.\"\n                             % (input_subscripts[tnum], tnum))\n        for cnum, char in enumerate(term):\n            dim = sh[cnum]\n\n            # Build out broadcast indices\n            if dim == 1:\n                broadcast_indices[tnum].append(char)\n\n            if char in dimension_dict.keys():\n                # For broadcasting cases we always want the largest dim size\n                if dimension_dict[char] == 1:\n                    dimension_dict[char] = dim\n                elif dim not in (1, dimension_dict[char]):\n                    raise ValueError(\"Size of label '%s' for operand %d (%d) \"\n                                     \"does not match previous terms (%d).\"\n                                     % (char, tnum, dimension_dict[char], dim))\n            else:\n                dimension_dict[char] = dim\n\n    # Convert broadcast inds to sets\n    broadcast_indices = [set(x) for x in broadcast_indices]\n\n    # Compute size of each input array plus the output array\n    size_list = [_compute_size_by_dict(term, dimension_dict)\n                 for term in input_list + [output_subscript]]\n    max_size = max(size_list)\n\n    if memory_limit is None:\n        memory_arg = max_size\n    else:\n        memory_arg = memory_limit\n\n    # Compute naive cost\n    # This isn't quite right, need to look into exactly how einsum does this\n    inner_product = (sum(len(x) for x in input_sets) - len(indices)) > 0\n    naive_cost = _flop_count(\n        indices, inner_product, len(input_list), dimension_dict\n    )\n\n    # Compute the path\n    if explicit_einsum_path:\n        path = path_type[1:]\n    elif (\n        (path_type is False)\n        or (len(input_list) in [1, 2])\n        or (indices == output_set)\n    ):\n        # Nothing to be optimized, leave it to einsum\n        path = [tuple(range(len(input_list)))]\n    elif path_type == \"greedy\":\n        path = _greedy_path(\n            input_sets, output_set, dimension_dict, memory_arg\n        )\n    elif path_type == \"optimal\":\n        path = _optimal_path(\n            input_sets, output_set, dimension_dict, memory_arg\n        )\n    else:\n        raise KeyError(\"Path name %s not found\", path_type)\n\n    cost_list, scale_list, size_list, contraction_list = [], [], [], []\n\n    # Build contraction tuple (positions, gemm, einsum_str, remaining)\n    for cnum, contract_inds in enumerate(path):\n        # Make sure we remove inds from right to left\n        contract_inds = tuple(sorted(contract_inds, reverse=True))\n\n        contract = _find_contraction(contract_inds, input_sets, output_set)\n        out_inds, input_sets, idx_removed, idx_contract = contract\n\n        cost = _flop_count(\n            idx_contract, idx_removed, len(contract_inds), dimension_dict\n        )\n        cost_list.append(cost)\n        scale_list.append(len(idx_contract))\n        size_list.append(_compute_size_by_dict(out_inds, dimension_dict))\n\n        bcast = set()\n        tmp_inputs = []\n        for x in contract_inds:\n            tmp_inputs.append(input_list.pop(x))\n            bcast |= broadcast_indices.pop(x)\n\n        new_bcast_inds = bcast - idx_removed\n\n        # If we're broadcasting, nix blas\n        if not len(idx_removed & bcast):\n            do_blas = _can_dot(tmp_inputs, out_inds, idx_removed)\n        else:\n            do_blas = False\n\n        # Last contraction\n        if (cnum - len(path)) == -1:\n            idx_result = output_subscript\n        else:\n            sort_result = [(dimension_dict[ind], ind) for ind in out_inds]\n            idx_result = \"\".join([x[1] for x in sorted(sort_result)])\n\n        input_list.append(idx_result)\n        broadcast_indices.append(new_bcast_inds)\n        einsum_str = \",\".join(tmp_inputs) + \"->\" + idx_result\n\n        contraction = (\n            contract_inds, idx_removed, einsum_str, input_list[:], do_blas\n        )\n        contraction_list.append(contraction)\n\n    opt_cost = sum(cost_list) + 1\n\n    if len(input_list) != 1:\n        # Explicit \"einsum_path\" is usually trusted, but we detect this kind of\n        # mistake in order to prevent from returning an intermediate value.\n        raise RuntimeError(\n            f\"Invalid einsum_path is specified: {len(input_list) - 1} more \"\n            \"operands has to be contracted.\")\n\n    if einsum_call_arg:\n        return (operands, contraction_list)\n\n    # Return the path along with a nice string representation\n    overall_contraction = input_subscripts + \"->\" + output_subscript\n    header = (\"scaling\", \"current\", \"remaining\")\n\n    speedup = naive_cost / opt_cost\n    max_i = max(size_list)\n\n    path_print = f\"  Complete contraction:  {overall_contraction}\\n\"\n    path_print += f\"         Naive scaling:  {len(indices)}\\n\"\n    path_print += \"     Optimized scaling:  %d\\n\" % max(scale_list)\n    path_print += f\"      Naive FLOP count:  {naive_cost:.3e}\\n\"\n    path_print += f\"  Optimized FLOP count:  {opt_cost:.3e}\\n\"\n    path_print += f\"   Theoretical speedup:  {speedup:3.3f}\\n\"\n    path_print += f\"  Largest intermediate:  {max_i:.3e} elements\\n\"\n    path_print += \"-\" * 74 + \"\\n\"\n    path_print += \"%6s %24s %40s\\n\" % header\n    path_print += \"-\" * 74\n\n    for n, contraction in enumerate(contraction_list):\n        inds, idx_rm, einsum_str, remaining, blas = contraction\n        remaining_str = \",\".join(remaining) + \"->\" + output_subscript\n        path_run = (scale_list[n], einsum_str, remaining_str)\n        path_print += \"\\n%4d    %24s %40s\" % path_run\n\n    path = ['einsum_path'] + path\n    return (path, path_print)"
        }
      },
      {
        "name": "numpy.linalg.matrix_power",
        "description": "Raise a square matrix to the (integer) power n",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html",
        "add": {
          "doc": "linalg.matrix_power(a, n)\n\nRaise a square matrix to the (integer) power n.\n\nFor positive integers n, the power is computed by repeated matrix squarings and matrix multiplications. If n == 0, the identity matrix of the same shape as M is returned. If n < 0, the inverse is computed and then raised to the abs(n).\n\nNote: Stacks of object matrices are not currently supported.\n\nParameters:\na : (..., M, M) array_like - Matrix to be \"powered\".\nn : int - The exponent can be any integer or long integer, positive, negative, or zero.\n\nReturns:\na**n : (..., M, M) ndarray or matrix object - The return value is the same shape and type as M; if the exponent is positive or zero then the type of the elements is the same as those of M. If the exponent is negative the elements are floating-point.\n\nRaises:\nLinAlgError - For matrices that are not square or that (for negative powers) cannot be inverted numerically.\n\nExamples:\n>>> import numpy as np\n>>> from numpy.linalg import matrix_power\n>>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit\n>>> matrix_power(i, 3) # should = -i\narray([[ 0, -1],\n       [ 1,  0]])\n>>> matrix_power(i, 0)\narray([[1, 0],\n       [0, 1]])\n>>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements\narray([[ 0.,  1.],\n       [-1.,  0.]])",
          "code": "@array_function_dispatch(_matrix_power_dispatcher)\ndef matrix_power(a, n):\n    \"\"\"\n    Raise a square matrix to the (integer) power `n`.\n\n    For positive integers `n`, the power is computed by repeated matrix\n    squarings and matrix multiplications. If ``n == 0``, the identity matrix\n    of the same shape as M is returned. If ``n < 0``, the inverse\n    is computed and then raised to the ``abs(n)``.\n\n    .. note:: Stacks of object matrices are not currently supported.\n\n    Parameters\n    ----------\n    a : (..., M, M) array_like\n        Matrix to be \"powered\".\n    n : int\n        The exponent can be any integer or long integer, positive,\n        negative, or zero.\n\n    Returns\n    -------\n    a**n : (..., M, M) ndarray or matrix object\n        The return value is the same shape and type as `M`;\n        if the exponent is positive or zero then the type of the\n        elements is the same as those of `M`. If the exponent is\n        negative the elements are floating-point.\n\n    Raises\n    ------\n    LinAlgError\n        For matrices that are not square or that (for negative powers) cannot\n        be inverted numerically.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy.linalg import matrix_power\n    >>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit\n    >>> matrix_power(i, 3) # should = -i\n    array([[ 0, -1],\n           [ 1,  0]])\n    >>> matrix_power(i, 0)\n    array([[1, 0],\n           [0, 1]])\n    >>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements\n    array([[ 0.,  1.],\n           [-1.,  0.]])\n\n    Somewhat more sophisticated example\n\n    >>> q = np.zeros((4, 4))\n    >>> q[0:2, 0:2] = -i\n    >>> q[2:4, 2:4] = i\n    >>> q # one of the three quaternion units not equal to 1\n    array([[ 0., -1.,  0.,  0.],\n           [ 1.,  0.,  0.,  0.],\n           [ 0.,  0.,  0.,  1.],\n           [ 0.,  0., -1.,  0.]])\n    >>> matrix_power(q, 2) # = -np.eye(4)\n    array([[-1.,  0.,  0.,  0.],\n           [ 0., -1.,  0.,  0.],\n           [ 0.,  0., -1.,  0.],\n           [ 0.,  0.,  0., -1.]])\n\n    \"\"\"\n    a = asanyarray(a)\n    _assert_stacked_square(a)\n\n    try:\n        n = operator.index(n)\n    except TypeError as e:\n        raise TypeError(\"exponent must be an integer\") from e\n\n    # Fall back on dot for object arrays. Object arrays are not supported by\n    # the current implementation of matmul using einsum\n    if a.dtype != object:\n        fmatmul = matmul\n    elif a.ndim == 2:\n        fmatmul = dot\n    else:\n        raise NotImplementedError(\n            \"matrix_power not supported for stacks of object arrays\")\n\n    if n == 0:\n        a = empty_like(a)\n        a[...] = eye(a.shape[-2], dtype=a.dtype)\n        return a\n\n    elif n < 0:\n        a = inv(a)\n        n = abs(n)\n\n    # short-cuts.\n    if n == 1:\n        return a\n\n    elif n == 2:\n        return fmatmul(a, a)\n\n    elif n == 3:\n        return fmatmul(fmatmul(a, a), a)\n\n    # Use binary decomposition to reduce the number of matrix multiplications.\n    # Here, we iterate over the bits of n, from LSB to MSB, raise `a` to\n    # increasing powers of 2, and multiply into the result as needed.\n    z = result = None\n    while n > 0:\n        z = a if z is None else fmatmul(z, z)\n        n, bit = divmod(n, 2)\n        if bit:\n            result = z if result is None else fmatmul(result, z)\n\n    return result"
        }
      },
      {
        "name": "numpy.kron",
        "description": "Kronecker product of two arrays",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.kron.html",
        "add": {
          "doc": "numpy.kron(a, b)\n\nKronecker product of two arrays.\n\nComputes the Kronecker product, a composite array made of blocks of the second array scaled by the first.\n\nParameters:\na, b : array_like\n\nReturns:\nout : ndarray\n\nSee also:\nouter - The outer product\n\nNotes:\nThe function assumes that the number of dimensions of a and b are the same, if necessary prepending the smallest with ones. If a.shape = (r0,r1,..,rN) and b.shape = (s0,s1,...,sN), the Kronecker product has shape (r0*s0, r1*s1, ..., rN*SN). The elements are products of elements from a and b, organized explicitly by:\n\nkron(a,b)[k0,k1,...,kN] = a[i0,i1,...,iN] * b[j0,j1,...,jN]\n\nwhere:\nkt = it * st + jt,  t = 0,...,N\n\nIn the common 2-D case (N=1), the block structure can be visualized:\n[[ a[0,0]*b,   a[0,1]*b,  ... , a[0,-1]*b  ],\n [  ...           ...             ...      ],\n [ a[-1,0]*b, a[-1,1]*b, ... , a[-1,-1]*b ]]\n\nExamples:\n>>> import numpy as np\n>>> np.kron([1,10,100], [5,6,7])\narray([  5,   6,   7, ..., 500, 600, 700])\n>>> np.kron([5,6,7], [1,10,100])\narray([  5,  50, 500, ...,   7,  70, 700])\n>>> np.kron(np.eye(2), np.ones((2,2)))\narray([[1., 1., 0., 0.],\n       [1., 1., 0., 0.],\n       [0., 0., 1., 1.],\n       [0., 0., 1., 1.]])",
          "code": "@array_function_dispatch(_kron_dispatcher)\ndef kron(a, b):\n    \"\"\"\n    Kronecker product of two arrays.\n\n    Computes the Kronecker product, a composite array made of blocks of the\n    second array scaled by the first.\n\n    Parameters\n    ----------\n    a, b : array_like\n\n    Returns\n    -------\n    out : ndarray\n\n    See Also\n    --------\n    outer : The outer product\n\n    Notes\n    -----\n    The function assumes that the number of dimensions of `a` and `b`\n    are the same, if necessary prepending the smallest with ones.\n    If ``a.shape = (r0,r1,..,rN)`` and ``b.shape = (s0,s1,...,sN)``,\n    the Kronecker product has shape ``(r0*s0, r1*s1, ..., rN*SN)``.\n    The elements are products of elements from `a` and `b`, organized\n    explicitly by::\n\n        kron(a,b)[k0,k1,...,kN] = a[i0,i1,...,iN] * b[j0,j1,...,jN]\n\n    where::\n\n        kt = it * st + jt,  t = 0,...,N\n\n    In the common 2-D case (N=1), the block structure can be visualized::\n\n        [[ a[0,0]*b,   a[0,1]*b,  ... , a[0,-1]*b  ],\n         [  ...                              ...   ],\n         [ a[-1,0]*b,  a[-1,1]*b, ... , a[-1,-1]*b ]]\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.kron([1,10,100], [5,6,7])\n    array([  5,   6,   7, ..., 500, 600, 700])\n    >>> np.kron([5,6,7], [1,10,100])\n    array([  5,  50, 500, ...,   7,  70, 700])\n\n    >>> np.kron(np.eye(2), np.ones((2,2)))\n    array([[1.,  1.,  0.,  0.],\n           [1.,  1.,  0.,  0.],\n           [0.,  0.,  1.,  1.],\n           [0.,  0.,  1.,  1.]])\n\n    >>> a = np.arange(100).reshape((2,5,2,5))\n    >>> b = np.arange(24).reshape((2,3,4))\n    >>> c = np.kron(a,b)\n    >>> c.shape\n    (2, 10, 6, 20)\n    >>> I = (1,3,0,2)\n    >>> J = (0,2,1)\n    >>> J1 = (0,) + J             # extend to ndim=4\n    >>> S1 = (1,) + b.shape\n    >>> K = tuple(np.array(I) * np.array(S1) + np.array(J1))\n    >>> c[K] == a[I]*b[J]\n    True\n\n    \"\"\"\n    # Working:\n    # 1. Equalise the shapes by prepending smaller array with 1s\n    # 2. Expand shapes of both the arrays by adding new axes at\n    #    odd positions for 1st array and even positions for 2nd\n    # 3. Compute the product of the modified array\n    # 4. The inner most array elements now contain the rows of\n    #    the Kronecker product\n    # 5. Reshape the result to kron's shape, which is same as\n    #    product of shapes of the two arrays.\n    b = asanyarray(b)\n    a = array(a, copy=None, subok=True, ndmin=b.ndim)\n    is_any_mat = isinstance(a, matrix) or isinstance(b, matrix)\n    ndb, nda = b.ndim, a.ndim\n    nd = max(ndb, nda)\n\n    if (nda == 0 or ndb == 0):\n        return _nx.multiply(a, b)\n\n    as_ = a.shape\n    bs = b.shape\n    if not a.flags.contiguous:\n        a = reshape(a, as_)\n    if not b.flags.contiguous:\n        b = reshape(b, bs)\n\n    # Equalise the shapes by prepending smaller one with 1s\n    as_ = (1,) * max(0, ndb - nda) + as_\n    bs = (1,) * max(0, nda - ndb) + bs\n\n    # Insert empty dimensions\n    a_arr = expand_dims(a, axis=tuple(range(ndb - nda)))\n    b_arr = expand_dims(b, axis=tuple(range(nda - ndb)))\n\n    # Compute the product\n    a_arr = expand_dims(a_arr, axis=tuple(range(1, nd * 2, 2)))\n    b_arr = expand_dims(b_arr, axis=tuple(range(0, nd * 2, 2)))\n    # In case of `mat`, convert result to `array`\n    result = _nx.multiply(a_arr, b_arr, subok=(not is_any_mat))\n\n    # Reshape back\n    result = result.reshape(_nx.multiply(as_, bs))\n\n    return result if not is_any_mat else matrix(result, copy=False)"
        }
      },
      {
        "name": "numpy.linalg.cross",
        "description": "Return the cross product of two (arrays of) vectors",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.cross.html",
        "add": {
          "doc": "linalg.cross(x1, x2, /, *, axis=-1)\n\nReturns the cross product of 3-element vectors.\n\nIf x1 and/or x2 are multi-dimensional arrays, then the cross-product of each pair of corresponding 3-element vectors is independently computed.\n\nThis function is Array API compatible, contrary to numpy.cross.\n\nParameters:\nx1 : array_like - The first input array.\nx2 : array_like - The second input array. Must be compatible with x1 for all non-compute axes. The size of the axis over which to compute the cross-product must be the same size as the respective axis in x1.\naxis : int, optional - The axis (dimension) of x1 and x2 containing the vectors for which to compute the cross-product. Default: -1.\n\nReturns:\nout : ndarray - An array containing the cross products.\n\nSee also:\nnumpy.cross\n\nExamples:\n>>> import numpy as np\n>>> x = np.array([1, 2, 3])\n>>> y = np.array([4, 5, 6])\n>>> np.linalg.cross(x, y)\narray([-3,  6, -3])\n\n>>> x = np.array([[1,2,3], [4,5,6]])\n>>> y = np.array([[4,5,6], [1,2,3]])\n>>> np.linalg.cross(x, y)\narray([[-3,  6, -3],\n       [ 3, -6,  3]])\n\n>>> x = np.array([[1, 2], [3, 4], [5, 6]])\n>>> y = np.array([[4, 5], [6, 1], [2, 3]])\n>>> np.linalg.cross(x, y, axis=0)\narray([[-24,   6],\n       [ 18,  24],\n       [ -6, -18]])",
          "code": "@array_function_dispatch(_cross_dispatcher)\ndef cross(x1, x2, /, *, axis=-1):\n    \"\"\"\n    Returns the cross product of 3-element vectors.\n\n    If ``x1`` and/or ``x2`` are multi-dimensional arrays, then\n    the cross-product of each pair of corresponding 3-element vectors\n    is independently computed.\n\n    This function is Array API compatible, contrary to\n    :func:`numpy.cross`.\n\n    Parameters\n    ----------\n    x1 : array_like\n        The first input array.\n    x2 : array_like\n        The second input array. Must be compatible with ``x1`` for all\n        non-compute axes. The size of the axis over which to compute\n        the cross-product must be the same size as the respective axis\n        in ``x1``.\n    axis : int, optional\n        The axis (dimension) of ``x1`` and ``x2`` containing the vectors for\n        which to compute the cross-product. Default: ``-1``.\n\n    Returns\n    -------\n    out : ndarray\n        An array containing the cross products.\n\n    See Also\n    --------\n    numpy.cross\n\n    Examples\n    --------\n    Vector cross-product.\n\n    >>> x = np.array([1, 2, 3])\n    >>> y = np.array([4, 5, 6])\n    >>> np.linalg.cross(x, y)\n    array([-3,  6, -3])\n\n    Multiple vector cross-products. Note that the direction of the cross\n    product vector is defined by the *right-hand rule*.\n\n    >>> x = np.array([[1,2,3], [4,5,6]])\n    >>> y = np.array([[4,5,6], [1,2,3]])\n    >>> np.linalg.cross(x, y)\n    array([[-3,  6, -3],\n           [ 3, -6,  3]])\n\n    >>> x = np.array([[1, 2], [3, 4], [5, 6]])\n    >>> y = np.array([[4, 5], [6, 1], [2, 3]])\n    >>> np.linalg.cross(x, y, axis=0)\n    array([[-24,  6],\n           [ 18, 24],\n           [-6,  -18]])\n\n    \"\"\"\n    x1 = asanyarray(x1)\n    x2 = asanyarray(x2)\n\n    if x1.shape[axis] != 3 or x2.shape[axis] != 3:\n        raise ValueError(\n            \"Both input arrays must be (arrays of) 3-dimensional vectors, \"\n            f\"but they are {x1.shape[axis]} and {x2.shape[axis]} \"\n            \"dimensional instead.\"\n        )\n\n    return _core_cross(x1, x2, axis=axis)"
        }
      }
    ],
    "decompositions": [
      {
        "name": "numpy.linalg.cholesky",
        "description": "Cholesky decomposition. Return the lower or upper Cholesky decomposition, L * L.H or U.H * U, of the square matrix a.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html",
        "add": {
          "doc": "numpy.linalg.cholesky\n\nlinalg.cholesky(a, /, *, upper=False)\n\nCholesky decomposition.\n\nReturn the lower or upper Cholesky decomposition, `L * L.H` or `U.H * U`, of the square matrix `a`, where `L` is lower-triangular, `U` is upper-triangular, and `.H` is the conjugate transpose operator (which is the ordinary transpose if `a` is real-valued). `a` must be Hermitian (symmetric if real-valued) and positive-definite. No checking is performed to verify whether `a` is Hermitian or not. In addition, only the lower or upper-triangular and diagonal elements of `a` are used. Only `L` or `U` is actually returned.\n\nParameters:\n- a : (..., M, M) array_like\n  Hermitian (symmetric if all elements are real), positive-definite input matrix.\n- upper : bool\n  If `True`, the result must be the upper-triangular Cholesky factor. If `False`, the result must be the lower-triangular Cholesky factor. Default: `False`.\n\nReturns:\n- L : (..., M, M) array_like\n  Lower or upper-triangular Cholesky factor of a. Returns a matrix object if a is a matrix object.\n\nRaises:\n- LinAlgError\n  If the decomposition fails, for example, if a is not positive-definite.\n\nNotes:\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe Cholesky decomposition is often used as a fast way of solving A x = b (when A is both Hermitian/symmetric and positive-definite).\n\nExamples:\n>>> import numpy as np\n>>> A = np.array([[1,-2j],[2j,5]])\n>>> A\narray([[ 1.+0.j, -0.-2.j],\n       [ 0.+2.j,  5.+0.j]])\n>>> L = np.linalg.cholesky(A)\n>>> L\narray([[1.+0.j, 0.+0.j],\n       [0.+2.j, 1.+0.j]])\n>>> np.dot(L, L.T.conj()) # verify that L * L.H = A\narray([[1.+0.j, 0.-2.j],\n       [0.+2.j, 5.+0.j]])",
          "code": "def cholesky(a, /, *, upper=False):\n    \"\"\"\n    Cholesky decomposition.\n\n    Return the lower or upper Cholesky decomposition, ``L * L.H`` or\n    ``U.H * U``, of the square matrix ``a``, where ``L`` is lower-triangular,\n    ``U`` is upper-triangular, and ``.H`` is the conjugate transpose operator\n    (which is the ordinary transpose if ``a`` is real-valued). ``a`` must be\n    Hermitian (symmetric if real-valued) and positive-definite. No checking is\n    performed to verify whether ``a`` is Hermitian or not. In addition, only\n    the lower or upper-triangular and diagonal elements of ``a`` are used.\n    Only ``L`` or ``U`` is actually returned.\n    \"\"\"\n    gufunc = _umath_linalg.cholesky_up if upper else _umath_linalg.cholesky_lo\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    t, result_t = _commonType(a)\n    signature = 'D->D' if isComplexType(t) else 'd->d'\n    with errstate(call=_raise_linalgerror_nonposdef, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        r = gufunc(a, signature=signature)\n    return wrap(r.astype(result_t, copy=False))"
        }
      },
      {
        "name": "numpy.linalg.qr",
        "description": "Compute the qr factorization of a matrix. Factor the matrix a as qr, where q is orthonormal and r is upper-triangular.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.qr.html",
        "add": {
          "doc": "numpy.linalg.qr\n\nlinalg.qr(a, mode='reduced')\n\nCompute the qr factorization of a matrix.\n\nFactor the matrix `a` as *qr*, where `q` is orthonormal and `r` is upper-triangular.\n\nParameters:\n- a : array_like, shape (..., M, N)\n  An array-like object with the dimensionality of at least 2.\n- mode : {'reduced', 'complete', 'r', 'raw'}, optional, default: 'reduced'\n  If K = min(M, N), then\n  * 'reduced' : returns Q, R with dimensions (..., M, K), (..., K, N)\n  * 'complete' : returns Q, R with dimensions (..., M, M), (..., M, N)\n  * 'r' : returns R only with dimensions (..., K, N)\n  * 'raw' : returns h, tau with dimensions (..., N, M), (..., K,)\n\nReturns:\nWhen mode is 'reduced' or 'complete', the result will be a namedtuple with the attributes Q and R.\n- Q : ndarray of float or complex, optional\n  A matrix with orthonormal columns. When mode = 'complete' the result is an orthogonal/unitary matrix depending on whether or not a is real/complex.\n- R : ndarray of float or complex, optional\n  The upper-triangular matrix or a stack of upper-triangular matrices if the number of dimensions in the input array is greater than 2.\n- (h, tau) : ndarrays of np.double or np.cdouble, optional\n  The array h contains the Householder reflectors that generate q along with r. The tau array contains scaling factors for the reflectors.\n\nRaises:\n- LinAlgError\n  If factoring fails.\n\nNotes:\nThis is an interface to the LAPACK routines `dgeqrf`, `zgeqrf`, `dorgqr`, and `zungqr`.\n\nExamples:\n>>> import numpy as np\n>>> rng = np.random.default_rng()\n>>> a = rng.normal(size=(9, 6))\n>>> Q, R = np.linalg.qr(a)\n>>> np.allclose(a, np.dot(Q, R))  # a does equal QR\nTrue",
          "code": "def qr(a, mode='reduced'):\n    \"\"\"\n    Compute the qr factorization of a matrix.\n\n    Factor the matrix `a` as *qr*, where `q` is orthonormal and `r` is\n    upper-triangular.\n    \"\"\"\n    if mode not in ('reduced', 'complete', 'r', 'raw'):\n        if mode in ('f', 'full'):\n            # 2013-04-01, 1.8\n            msg = (\n                \"The 'full' option is deprecated in favor of 'reduced'.\\n\"\n                \"For backward compatibility let mode default.\"\n            )\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n            mode = 'reduced'\n        elif mode in ('e', 'economic'):\n            # 2013-04-01, 1.8\n            msg = \"The 'economic' option is deprecated.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n            mode = 'economic'\n        else:\n            raise ValueError(f\"Unrecognized mode '{mode}'\")\n\n    a, wrap = _makearray(a)\n    _assert_stacked_2d(a)\n    m, n = a.shape[-2:]\n    t, result_t = _commonType(a)\n    a = a.astype(t, copy=True)\n    a = _to_native_byte_order(a)\n    mn = min(m, n)\n\n    signature = 'D->D' if isComplexType(t) else 'd->d'\n    with errstate(call=_raise_linalgerror_qr, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        tau = _umath_linalg.qr_r_raw(a, signature=signature)\n\n    # handle modes that don't return q\n    if mode == 'r':\n        r = triu(a[..., :mn, :])\n        r = r.astype(result_t, copy=False)\n        return wrap(r)\n\n    if mode == 'raw':\n        q = transpose(a)\n        q = q.astype(result_t, copy=False)\n        tau = tau.astype(result_t, copy=False)\n        return wrap(q), tau\n\n    if mode == 'economic':\n        a = a.astype(result_t, copy=False)\n        return wrap(a)\n\n    # mc is the number of columns in the resulting q\n    # matrix. If the mode is complete then it is\n    # same as number of rows, and if the mode is reduced,\n    # then it is the minimum of number of rows and columns.\n    if mode == 'complete' and m > n:\n        mc = m\n        gufunc = _umath_linalg.qr_complete\n    else:\n        mc = mn\n        gufunc = _umath_linalg.qr_reduced\n\n    signature = 'DD->D' if isComplexType(t) else 'dd->d'\n    with errstate(call=_raise_linalgerror_qr, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        q = gufunc(a, tau, signature=signature)\n    r = triu(a[..., :mc, :])\n\n    q = q.astype(result_t, copy=False)\n    r = r.astype(result_t, copy=False)\n\n    return QRResult(wrap(q), wrap(r))"
        }
      },
      {
        "name": "numpy.linalg.svd",
        "description": "Singular Value Decomposition. When a is a 2D array, and full_matrices=False, then it is factorized as u @ np.diag(s) @ vh = (u * s) @ vh.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html",
        "add": {
          "doc": "numpy.linalg.svd\n\nlinalg.svd(a, full_matrices=True, compute_uv=True, hermitian=False)\n\nSingular Value Decomposition.\n\nWhen `a` is a 2D array, and `full_matrices=False`, then it is factorized as `u @ np.diag(s) @ vh = (u * s) @ vh`, where `u` and the Hermitian transpose of `vh` are 2D arrays with orthonormal columns and `s` is a 1D array of `a`'s singular values. When `a` is higher-dimensional, SVD is applied in stacked mode as explained below.\n\nParameters:\n- a : (..., M, N) array_like\n  A real or complex array with `a.ndim >= 2`.\n- full_matrices : bool, optional\n  If True (default), `u` and `vh` have the shapes `(..., M, M)` and `(..., N, N)`, respectively. Otherwise, the shapes are `(..., M, K)` and `(..., K, N)`, respectively, where `K = min(M, N)`.\n- compute_uv : bool, optional\n  Whether or not to compute `u` and `vh` in addition to `s`. True by default.\n- hermitian : bool, optional\n  If True, `a` is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.\n\nReturns:\nWhen compute_uv is True, the result is a namedtuple with the following attribute names:\n- U : {(..., M, M), (..., M, K)} array\n  Unitary array(s). The first `a.ndim - 2` dimensions have the same size as those of the input `a`. The size of the last two dimensions depends on the value of `full_matrices`. Only returned when `compute_uv` is True.\n- S : (..., K) array\n  Vector(s) with the singular values, within each vector sorted in descending order. The first `a.ndim - 2` dimensions have the same size as those of the input `a`.\n- Vh : {(..., N, N), (..., K, N)} array\n  Unitary array(s). The first `a.ndim - 2` dimensions have the same size as those of the input `a`. The size of the last two dimensions depends on the value of `full_matrices`. Only returned when `compute_uv` is True.\n\nRaises:\n- LinAlgError\n  If SVD computation does not converge.\n\nNotes:\nThe decomposition is performed using LAPACK routine `_gesdd`.\n\nExamples:\n>>> import numpy as np\n>>> rng = np.random.default_rng()\n>>> a = rng.normal(size=(9, 6)) + 1j*rng.normal(size=(9, 6))\n>>> U, S, Vh = np.linalg.svd(a, full_matrices=True)\n>>> U.shape, S.shape, Vh.shape\n((9, 9), (6,), (6, 6))\n>>> np.allclose(a, np.dot(U[:, :6] * S, Vh))\nTrue",
          "code": "# Note: The complete SVD implementation is complex and relies on LAPACK routines.\n# The Python wrapper function delegates to _umath_linalg.svd which interfaces with\n# LAPACK's GESDD routine for the actual computation. The core implementation is in C/Fortran.\n\n# The main Python function structure would be:\ndef svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n    \"\"\"\n    Singular Value Decomposition.\n    \"\"\"\n    a, wrap = _makearray(a)\n    _assert_stacked_2d(a)\n    t, result_t = _commonType(a)\n    \n    # The actual SVD computation is performed by LAPACK routines\n    # through the _umath_linalg module, which contains the C/Fortran\n    # implementation interfacing with LAPACK's _gesdd routines.\n    \n    # Note: Complete source code is not available as the core computation\n    # is implemented in LAPACK (Fortran) and accessed through NumPy's\n    # C extension modules."
        }
      },
      {
        "name": "numpy.linalg.svdvals",
        "description": "Returns the singular values of a matrix (or a stack of matrices) x. This function is Array API compatible.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.svdvals.html",
        "add": {
          "doc": "numpy.linalg.svdvals\n\nlinalg.svdvals(x, /)\n\nReturns the singular values of a matrix (or a stack of matrices) `x`. When x is a stack of matrices, the function will compute the singular values for each matrix in the stack.\n\nThis function is Array API compatible.\n\nCalling `np.svdvals(x)` to get singular values is the same as `np.svd(x, compute_uv=False, hermitian=False)`.\n\nParameters:\n- x : (..., M, N) array_like\n  Input array having shape (..., M, N) and whose last two dimensions form matrices on which to perform singular value decomposition. Should have a floating-point data type.\n\nReturns:\n- out : ndarray\n  An array with shape (..., K) that contains the vector(s) of singular values of length K, where K = min(M, N).\n\nExamples:\n>>> np.linalg.svdvals([[1, 2, 3, 4, 5],\n...                    [1, 4, 9, 16, 25],\n...                    [1, 8, 27, 64, 125]])\narray([146.68862757,   5.57510612,   0.60393245])\n\nDetermine the rank of a matrix using singular values:\n>>> s = np.linalg.svdvals([[1, 2, 3],\n...                        [2, 4, 6],\n...                        [-1, 1, -1]]); s\narray([8.38434191e+00, 1.64402274e+00, 2.31534378e-16])\n>>> np.count_nonzero(s > 1e-10)  # Matrix of rank 2\n2",
          "code": "# Note: Based on the source code location referenced in the documentation,\n# the svdvals function is located at lines 1881-1928 in numpy/linalg/_linalg.py\n# However, the complete implementation relies on the same LAPACK routines as svd.\n\n# The function essentially calls svd with compute_uv=False and returns only the singular values.\ndef svdvals(x, /):\n    \"\"\"\n    Returns the singular values of a matrix (or a stack of matrices) `x`.\n    \"\"\"\n    # The implementation delegates to the svd function with compute_uv=False\n    # which in turn uses LAPACK routines for the actual computation.\n    \n    # Note: Complete source code is not available as the core computation\n    # is implemented in LAPACK (Fortran) and accessed through NumPy's\n    # C extension modules. The function essentially calls:\n    # return svd(x, compute_uv=False, hermitian=False)\n    pass  # Placeholder - actual implementation uses LAPACK via _umath_linalg"
        }
      }
    ],
    "eigenvalues_eigenvectors": [
      {
        "name": "numpy.linalg.eig",
        "description": "Compute the eigenvalues and right eigenvectors of a square array.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html",
        "add": {
          "doc": "linalg.eig(a)\n\nCompute the eigenvalues and right eigenvectors of a square array.\n\nParameters:\n----------\na : (..., M, M) array\n    Matrices for which the eigenvalues and right eigenvectors will be computed\n\nReturns:\n-------\nA namedtuple with the following attributes:\n\neigenvalues : (..., M) array\n    The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. The resulting array will be of complex type, unless the imaginary part is zero in which case it will be cast to a real type. When a is real the resulting eigenvalues will be real (0 imaginary part) or occur in conjugate pairs\n\neigenvectors : (..., M, M) array\n    The normalized (unit \"length\") eigenvectors, such that the column eigenvectors[:,i] is the eigenvector corresponding to the eigenvalue eigenvalues[i].\n\nRaises:\n------\nLinAlgError\n    If the eigenvalue computation does not converge.\n\nSee Also:\n--------\neigvals : eigenvalues of a non-symmetric array.\neigh : eigenvalues and eigenvectors of a real symmetric or complex Hermitian (conjugate symmetric) array.\neigvalsh : eigenvalues of a real symmetric or complex Hermitian (conjugate symmetric) array.\nscipy.linalg.eig : Similar function in SciPy that also solves the generalized eigenvalue problem.\nscipy.linalg.schur : Best choice for unitary and other non-Hermitian normal matrices.\n\nNotes:\n-----\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThis is implemented using the _geev LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays.\n\nThe number w is an eigenvalue of a if there exists a vector v such that a @ v = w * v. Thus, the arrays a, eigenvalues, and eigenvectors satisfy the equations a @ eigenvectors[:,i] = eigenvalues[i] * eigenvectors[:,i] for i \u2208 {0,...,M-1}.\n\nThe array eigenvectors may not be of maximum rank, that is, some of the columns may be linearly dependent, although round-off error may obscure that fact. If the eigenvalues are all different, then theoretically the eigenvectors are linearly independent and a can be diagonalized by a similarity transformation using eigenvectors, i.e, inv(eigenvectors) @ a @ eigenvectors is diagonal.\n\nFor non-Hermitian normal matrices the SciPy function scipy.linalg.schur is preferred because the matrix eigenvectors is guaranteed to be unitary, which is not the case when using eig. The Schur factorization produces an upper triangular matrix rather than a diagonal matrix, but for normal matrices only the diagonal of the upper triangular matrix is needed, the rest is roundoff error.\n\nFinally, it is emphasized that eigenvectors consists of the right (as in right-hand side) eigenvectors of a. A vector y satisfying y.T @ a = z * y.T for some number z is called a left eigenvector of a, and, in general, the left and right eigenvectors of a matrix are not necessarily the (perhaps conjugate) transposes of each other.\n\nExamples:\n--------\n>>> import numpy as np\n>>> from numpy import linalg as LA\n\n(Almost) trivial example with real eigenvalues and eigenvectors.\n\n>>> eigenvalues, eigenvectors = LA.eig(np.diag((1, 2, 3)))\n>>> eigenvalues\narray([1., 2., 3.])\n>>> eigenvectors\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\nReal matrix possessing complex eigenvalues and eigenvectors; note that the eigenvalues are complex conjugates of each other.\n\n>>> eigenvalues, eigenvectors = LA.eig(np.array([[1, -1], [1, 1]]))\n>>> eigenvalues\narray([1.+1.j, 1.-1.j])\n>>> eigenvectors\narray([[0.70710678+0.j        , 0.70710678-0.j        ],\n       [0.        -0.70710678j, 0.        +0.70710678j]])\n\nComplex-valued matrix with real eigenvalues (but complex-valued eigenvectors); note that a.conj().T == a, i.e., a is Hermitian.\n\n>>> a = np.array([[1, 1j], [-1j, 1]])\n>>> eigenvalues, eigenvectors = LA.eig(a)\n>>> eigenvalues\narray([2.+0.j, 0.+0.j])\n>>> eigenvectors\narray([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary\n       [ 0.70710678+0.j        , -0.        +0.70710678j]])\n\nBe careful about round-off error!\n\n>>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])\n>>> # Theor. eigenvalues are 1 +/- 1e-9\n>>> eigenvalues, eigenvectors = LA.eig(a)\n>>> eigenvalues\narray([1., 1.])\n>>> eigenvectors\narray([[1., 0.],\n       [0., 1.]])",
          "code": "@array_function_dispatch(_unary_dispatcher)\ndef eig(a):\n    \"\"\"\n    Compute the eigenvalues and right eigenvectors of a square array.\n\n    Parameters\n    ----------\n    a : (..., M, M) array\n        Matrices for which the eigenvalues and right eigenvectors will\n        be computed\n\n    Returns\n    -------\n    A namedtuple with the following attributes:\n\n    eigenvalues : (..., M) array\n        The eigenvalues, each repeated according to its multiplicity.\n        The eigenvalues are not necessarily ordered. The resulting\n        array will be of complex type, unless the imaginary part is\n        zero in which case it will be cast to a real type. When `a`\n        is real the resulting eigenvalues will be real (0 imaginary\n        part) or occur in conjugate pairs\n\n    eigenvectors : (..., M, M) array\n        The normalized (unit \"length\") eigenvectors, such that the\n        column ``eigenvectors[:,i]`` is the eigenvector corresponding to the\n        eigenvalue ``eigenvalues[i]``.\n\n    Raises\n    ------\n    LinAlgError\n        If the eigenvalue computation does not converge.\n\n    See Also\n    --------\n    eigvals : eigenvalues of a non-symmetric array.\n    eigh : eigenvalues and eigenvectors of a real symmetric or complex\n           Hermitian (conjugate symmetric) array.\n    eigvalsh : eigenvalues of a real symmetric or complex Hermitian\n               (conjugate symmetric) array.\n    scipy.linalg.eig : Similar function in SciPy that also solves the\n                       generalized eigenvalue problem.\n    scipy.linalg.schur : Best choice for unitary and other non-Hermitian\n                         normal matrices.\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    This is implemented using the ``_geev`` LAPACK routines which compute\n    the eigenvalues and eigenvectors of general square arrays.\n\n    The number `w` is an eigenvalue of `a` if there exists a vector `v` such\n    that ``a @ v = w * v``. Thus, the arrays `a`, `eigenvalues`, and\n    `eigenvectors` satisfy the equations ``a @ eigenvectors[:,i] =\n    eigenvalues[i] * eigenvectors[:,i]`` for :math:`i \\\\in \\\\{0,...,M-1\\\\}`.\n\n    The array `eigenvectors` may not be of maximum rank, that is, some of the\n    columns may be linearly dependent, although round-off error may obscure\n    that fact. If the eigenvalues are all different, then theoretically the\n    eigenvectors are linearly independent and `a` can be diagonalized by a\n    similarity transformation using `eigenvectors`, i.e, ``inv(eigenvectors) @\n    a @ eigenvectors`` is diagonal.\n\n    For non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`\n    is preferred because the matrix `eigenvectors` is guaranteed to be\n    unitary, which is not the case when using `eig`. The Schur factorization\n    produces an upper triangular matrix rather than a diagonal matrix, but for\n    normal matrices only the diagonal of the upper triangular matrix is\n    needed, the rest is roundoff error.\n\n    Finally, it is emphasized that `eigenvectors` consists of the *right* (as\n    in right-hand side) eigenvectors of `a`. A vector `y` satisfying ``y.T @ a\n    = z * y.T`` for some number `z` is called a *left* eigenvector of `a`,\n    and, in general, the left and right eigenvectors of a matrix are not\n    necessarily the (perhaps conjugate) transposes of each other.\n\n    References\n    ----------\n    G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,\n    Academic Press, Inc., 1980, Various pp.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n\n    (Almost) trivial example with real eigenvalues and eigenvectors.\n\n    >>> eigenvalues, eigenvectors = LA.eig(np.diag((1, 2, 3)))\n    >>> eigenvalues\n    array([1., 2., 3.])\n    >>> eigenvectors\n    array([[1., 0., 0.],\n           [0., 1., 0.],\n           [0., 0., 1.]])\n\n    Real matrix possessing complex eigenvalues and eigenvectors; note that the\n    eigenvalues are complex conjugates of each other.\n\n    >>> eigenvalues, eigenvectors = LA.eig(np.array([[1, -1], [1, 1]]))\n    >>> eigenvalues\n    array([1.+1.j, 1.-1.j])\n    >>> eigenvectors\n    array([[0.70710678+0.j        , 0.70710678-0.j        ],\n           [0.        -0.70710678j, 0.        +0.70710678j]])\n\n    Complex-valued matrix with real eigenvalues (but complex-valued\n    eigenvectors); note that `a.conj().T == a`, i.e., `a` is Hermitian.\n\n    >>> a = np.array([[1, 1j], [-1j, 1]])\n    >>> eigenvalues, eigenvectors = LA.eig(a)\n    >>> eigenvalues\n    array([2.+0.j, 0.+0.j])\n    >>> eigenvectors\n    array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary\n           [ 0.70710678+0.j        , -0.        +0.70710678j]])\n\n    Be careful about round-off error!\n\n    >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])\n    >>> # Theor. eigenvalues are 1 +/- 1e-9\n    >>> eigenvalues, eigenvectors = LA.eig(a)\n    >>> eigenvalues\n    array([1., 1.])\n    >>> eigenvectors\n    array([[1., 0.],\n           [0., 1.]])\n\n    \"\"\"\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    _assert_finite(a)\n    t, result_t = _commonType(a)\n\n    signature = 'D->DD' if isComplexType(t) else 'd->DD'\n    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n                  invalid='call', over='ignore', divide='ignore',\n                  under='ignore'):\n        w, vt = _umath_linalg.eig(a, signature=signature)\n\n    if not isComplexType(t):\n        if all(w.imag == 0):\n            w = w.real\n            vt = vt.real\n            result_t = _realType(result_t)\n        else:\n            result_t = _complexType(result_t)\n\n    vt = vt.astype(result_t, copy=False)\n    w = w.astype(result_t, copy=False)\n    return EigResult(w, wrap(vt))"
        }
      },
      {
        "name": "numpy.linalg.eigh",
        "description": "Return the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html",
        "add": {
          "doc": "linalg.eigh(a, UPLO='L')\n\nReturn the eigenvalues and eigenvectors of a complex Hermitian (conjugate symmetric) or a real symmetric matrix.\n\nReturns two objects, a 1-D array containing the eigenvalues of a, and a 2-D square array or matrix (depending on the input type) of the corresponding eigenvectors (in columns).\n\nParameters:\n----------\na : (..., M, M) array\n    Hermitian or real symmetric matrices whose eigenvalues and eigenvectors are to be computed.\nUPLO : {'L', 'U'}, optional\n    Specifies whether the calculation is done with the lower triangular part of a ('L', default) or the upper triangular part ('U'). Irrespective of this value only the real parts of the diagonal will be considered in the computation to preserve the notion of a Hermitian matrix. It therefore follows that the imaginary part of the diagonal will always be treated as zero.\n\nReturns:\n-------\nA namedtuple with the following attributes:\n\neigenvalues : (..., M) ndarray\n    The eigenvalues in ascending order, each repeated according to its multiplicity.\neigenvectors : {(..., M, M) ndarray, (..., M, M) matrix}\n    The column eigenvectors[:, i] is the normalized eigenvector corresponding to the eigenvalue eigenvalues[i]. Will return a matrix object if a is a matrix object.\n\nRaises:\n------\nLinAlgError\n    If the eigenvalue computation does not converge.\n\nSee Also:\n--------\neigvalsh : eigenvalues of real symmetric or complex Hermitian (conjugate symmetric) arrays.\neig : eigenvalues and right eigenvectors for non-symmetric arrays.\neigvals : eigenvalues of non-symmetric arrays.\nscipy.linalg.eigh : Similar function in SciPy (but also solves the generalized eigenvalue problem).\n\nNotes:\n-----\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe eigenvalues/eigenvectors are computed using LAPACK routines _syevd, _heevd.\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always real. The array eigenvalues of (column) eigenvectors is unitary and a, eigenvalues, and eigenvectors satisfy the equations dot(a, eigenvectors[:, i]) = eigenvalues[i] * eigenvectors[:, i].\n\nExamples:\n--------\n>>> import numpy as np\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, -2j], [2j, 5]])\n>>> a\narray([[ 1.+0.j, -0.-2.j],\n       [ 0.+2.j,  5.+0.j]])\n>>> eigenvalues, eigenvectors = LA.eigh(a)\n>>> eigenvalues\narray([0.17157288, 5.82842712])\n>>> eigenvectors\narray([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n       [ 0.        +0.38268343j,  0.        -0.92387953j]])\n\n>>> (np.dot(a, eigenvectors[:, 0]) -\n... eigenvalues[0] * eigenvectors[:, 0])  # verify 1st eigenval/vec pair\narray([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])\n>>> (np.dot(a, eigenvectors[:, 1]) -\n... eigenvalues[1] * eigenvectors[:, 1])  # verify 2nd eigenval/vec pair\narray([0.+0.j, 0.+0.j])\n\n>>> A = np.matrix(a) # what happens if input is a matrix object\n>>> A\nmatrix([[ 1.+0.j, -0.-2.j],\n        [ 0.+2.j,  5.+0.j]])\n>>> eigenvalues, eigenvectors = LA.eigh(A)\n>>> eigenvalues\narray([0.17157288, 5.82842712])\n>>> eigenvectors\nmatrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n        [ 0.        +0.38268343j,  0.        -0.92387953j]])\n\n>>> # demonstrate the treatment of the imaginary part of the diagonal\n>>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n>>> a\narray([[5.+2.j, 9.-2.j],\n       [0.+2.j, 2.-1.j]])\n>>> # with UPLO='L' this is numerically equivalent to using LA.eig() with:\n>>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n>>> b\narray([[5.+0.j, 0.-2.j],\n       [0.+2.j, 2.+0.j]])\n>>> wa, va = LA.eigh(a)\n>>> wb, vb = LA.eig(b)\n>>> wa\narray([1., 6.])\n>>> wb\narray([6.+0.j, 1.+0.j])\n>>> va\narray([[-0.4472136 +0.j        , -0.89442719+0.j        ], # may vary\n       [ 0.        +0.89442719j,  0.        -0.4472136j ]])\n>>> vb\narray([[ 0.89442719+0.j       , -0.        +0.4472136j],\n       [-0.        +0.4472136j,  0.89442719+0.j       ]])",
          "code": "@array_function_dispatch(_eigvalsh_dispatcher)\ndef eigh(a, UPLO='L'):\n    \"\"\"\n    Return the eigenvalues and eigenvectors of a complex Hermitian\n    (conjugate symmetric) or a real symmetric matrix.\n\n    Returns two objects, a 1-D array containing the eigenvalues of `a`, and\n    a 2-D square array or matrix (depending on the input type) of the\n    corresponding eigenvectors (in columns).\n\n    Parameters\n    ----------\n    a : (..., M, M) array\n        Hermitian or real symmetric matrices whose eigenvalues and\n        eigenvectors are to be computed.\n    UPLO : {'L', 'U'}, optional\n        Specifies whether the calculation is done with the lower triangular\n        part of `a` ('L', default) or the upper triangular part ('U').\n        Irrespective of this value only the real parts of the diagonal will\n        be considered in the computation to preserve the notion of a Hermitian\n        matrix. It therefore follows that the imaginary part of the diagonal\n        will always be treated as zero.\n\n    Returns\n    -------\n    A namedtuple with the following attributes:\n\n    eigenvalues : (..., M) ndarray\n        The eigenvalues in ascending order, each repeated according to\n        its multiplicity.\n    eigenvectors : {(..., M, M) ndarray, (..., M, M) matrix}\n        The column ``eigenvectors[:, i]`` is the normalized eigenvector\n        corresponding to the eigenvalue ``eigenvalues[i]``.  Will return a\n        matrix object if `a` is a matrix object.\n\n    Raises\n    ------\n    LinAlgError\n        If the eigenvalue computation does not converge.\n\n    See Also\n    --------\n    eigvalsh : eigenvalues of real symmetric or complex Hermitian\n               (conjugate symmetric) arrays.\n    eig : eigenvalues and right eigenvectors for non-symmetric arrays.\n    eigvals : eigenvalues of non-symmetric arrays.\n    scipy.linalg.eigh : Similar function in SciPy (but also solves the\n                        generalized eigenvalue problem).\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    The eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,\n    ``_heevd``.\n\n    The eigenvalues of real symmetric or complex Hermitian matrices are always\n    real. [1]_ The array `eigenvalues` of (column) eigenvectors is unitary and\n    `a`, `eigenvalues`, and `eigenvectors` satisfy the equations ``dot(a,\n    eigenvectors[:, i]) = eigenvalues[i] * eigenvectors[:, i]``.\n\n    References\n    ----------\n    .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,\n           FL, Academic Press, Inc., 1980, pg. 222.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n    >>> a = np.array([[1, -2j], [2j, 5]])\n    >>> a\n    array([[ 1.+0.j, -0.-2.j],\n           [ 0.+2.j,  5.+0.j]])\n    >>> eigenvalues, eigenvectors = LA.eigh(a)\n    >>> eigenvalues\n    array([0.17157288, 5.82842712])\n    >>> eigenvectors\n    array([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n           [ 0.        +0.38268343j,  0.        -0.92387953j]])\n\n    >>> (np.dot(a, eigenvectors[:, 0]) -\n    ... eigenvalues[0] * eigenvectors[:, 0])  # verify 1st eigenval/vec pair\n    array([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])\n    >>> (np.dot(a, eigenvectors[:, 1]) -\n    ... eigenvalues[1] * eigenvectors[:, 1])  # verify 2nd eigenval/vec pair\n    array([0.+0.j, 0.+0.j])\n\n    >>> A = np.matrix(a) # what happens if input is a matrix object\n    >>> A\n    matrix([[ 1.+0.j, -0.-2.j],\n            [ 0.+2.j,  5.+0.j]])\n    >>> eigenvalues, eigenvectors = LA.eigh(A)\n    >>> eigenvalues\n    array([0.17157288, 5.82842712])\n    >>> eigenvectors\n    matrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary\n            [ 0.        +0.38268343j,  0.        -0.92387953j]])\n\n    >>> # demonstrate the treatment of the imaginary part of the diagonal\n    >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n    >>> a\n    array([[5.+2.j, 9.-2.j],\n           [0.+2.j, 2.-1.j]])\n    >>> # with UPLO='L' this is numerically equivalent to using LA.eig() with:\n    >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n    >>> b\n    array([[5.+0.j, 0.-2.j],\n           [0.+2.j, 2.+0.j]])\n    >>> wa, va = LA.eigh(a)\n    >>> wb, vb = LA.eig(b)\n    >>> wa\n    array([1., 6.])\n    >>> wb\n    array([6.+0.j, 1.+0.j])\n    >>> va\n    array([[-0.4472136 +0.j        , -0.89442719+0.j        ], # may vary\n           [ 0.        +0.89442719j,  0.        -0.4472136j ]])\n    >>> vb\n    array([[ 0.89442719+0.j       , -0.        +0.4472136j],\n           [-0.        +0.4472136j,  0.89442719+0.j       ]])\n\n    \"\"\"\n    UPLO = UPLO.upper()\n    if UPLO not in ('L', 'U'):\n        raise ValueError(\"UPLO argument must be 'L' or 'U'\")\n\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    _assert_finite(a)\n    t, result_t = _commonType(a)\n\n    if UPLO == 'L':\n        gufunc = _umath_linalg.eigh_lo\n    else:\n        gufunc = _umath_linalg.eigh_up\n\n    signature = 'D->dD' if isComplexType(t) else 'd->dd'\n    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n                  invalid='call', over='ignore', divide='ignore',\n                  under='ignore'):\n        w, vt = gufunc(a, signature=signature)\n    w = w.astype(_realType(result_t), copy=False)\n    vt = vt.astype(result_t, copy=False)\n    return EighResult(w, wrap(vt))"
        }
      },
      {
        "name": "numpy.linalg.eigvals",
        "description": "Compute the eigenvalues of a general matrix.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvals.html",
        "add": {
          "doc": "linalg.eigvals(a)\n\nCompute the eigenvalues of a general matrix.\n\nMain difference between eigvals and eig: the eigenvectors aren't returned.\n\nParameters:\n----------\na : (..., M, M) array_like\n    A complex- or real-valued matrix whose eigenvalues will be computed.\n\nReturns:\n-------\nw : (..., M,) ndarray\n    The eigenvalues, each repeated according to its multiplicity. They are not necessarily ordered, nor are they necessarily real for real matrices.\n\nRaises:\n------\nLinAlgError\n    If the eigenvalue computation does not converge.\n\nSee Also:\n--------\neig : eigenvalues and right eigenvectors of general arrays\neigvalsh : eigenvalues of real symmetric or complex Hermitian (conjugate symmetric) arrays.\neigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian (conjugate symmetric) arrays.\nscipy.linalg.eigvals : Similar function in SciPy.\n\nNotes:\n-----\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThis is implemented using the _geev LAPACK routines which compute the eigenvalues and eigenvectors of general square arrays.\n\nExamples:\n--------\nIllustration, using the fact that the eigenvalues of a diagonal matrix are its diagonal elements, that multiplying a matrix on the left by an orthogonal matrix, Q, and on the right by Q.T (the transpose of Q), preserves the eigenvalues of the \"middle\" matrix. In other words, if Q is orthogonal, then Q * A * Q.T has the same eigenvalues as A:\n\n>>> import numpy as np\n>>> from numpy import linalg as LA\n>>> x = np.random.random()\n>>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])\n>>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])\n(1.0, 1.0, 0.0)\n\nNow multiply a diagonal matrix by Q on one side and by Q.T on the other:\n\n>>> D = np.diag((-1,1))\n>>> LA.eigvals(D)\narray([-1.,  1.])\n>>> A = np.dot(Q, D)\n>>> A = np.dot(A, Q.T)\n>>> LA.eigvals(A)\narray([ 1., -1.]) # random",
          "code": "@array_function_dispatch(_unary_dispatcher)\ndef eigvals(a):\n    \"\"\"\n    Compute the eigenvalues of a general matrix.\n\n    Main difference between `eigvals` and `eig`: the eigenvectors aren't\n    returned.\n\n    Parameters\n    ----------\n    a : (..., M, M) array_like\n        A complex- or real-valued matrix whose eigenvalues will be computed.\n\n    Returns\n    -------\n    w : (..., M,) ndarray\n        The eigenvalues, each repeated according to its multiplicity.\n        They are not necessarily ordered, nor are they necessarily\n        real for real matrices.\n\n    Raises\n    ------\n    LinAlgError\n        If the eigenvalue computation does not converge.\n\n    See Also\n    --------\n    eig : eigenvalues and right eigenvectors of general arrays\n    eigvalsh : eigenvalues of real symmetric or complex Hermitian\n               (conjugate symmetric) arrays.\n    eigh : eigenvalues and eigenvectors of real symmetric or complex\n           Hermitian (conjugate symmetric) arrays.\n    scipy.linalg.eigvals : Similar function in SciPy.\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    This is implemented using the ``_geev`` LAPACK routines which compute\n    the eigenvalues and eigenvectors of general square arrays.\n\n    Examples\n    --------\n    Illustration, using the fact that the eigenvalues of a diagonal matrix\n    are its diagonal elements, that multiplying a matrix on the left\n    by an orthogonal matrix, `Q`, and on the right by `Q.T` (the transpose\n    of `Q`), preserves the eigenvalues of the \"middle\" matrix. In other words,\n    if `Q` is orthogonal, then ``Q * A * Q.T`` has the same eigenvalues as\n    ``A``:\n\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n    >>> x = np.random.random()\n    >>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])\n    >>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])\n    (1.0, 1.0, 0.0)\n\n    Now multiply a diagonal matrix by ``Q`` on one side and\n    by ``Q.T`` on the other:\n\n    >>> D = np.diag((-1,1))\n    >>> LA.eigvals(D)\n    array([-1.,  1.])\n    >>> A = np.dot(Q, D)\n    >>> A = np.dot(A, Q.T)\n    >>> LA.eigvals(A)\n    array([ 1., -1.]) # random\n\n    \"\"\"\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    _assert_finite(a)\n    t, result_t = _commonType(a)\n\n    signature = 'D->D' if isComplexType(t) else 'd->D'\n    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n                  invalid='call', over='ignore', divide='ignore',\n                  under='ignore'):\n        w = _umath_linalg.eigvals(a, signature=signature)\n\n    if not isComplexType(t):\n        if all(w.imag == 0):\n            w = w.real\n            result_t = _realType(result_t)\n        else:\n            result_t = _complexType(result_t)\n\n    return w.astype(result_t, copy=False)"
        }
      },
      {
        "name": "numpy.linalg.eigvalsh",
        "description": "Compute the eigenvalues of a complex Hermitian or real symmetric matrix.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigvalsh.html",
        "add": {
          "doc": "linalg.eigvalsh(a, UPLO='L')\n\nCompute the eigenvalues of a complex Hermitian or real symmetric matrix.\n\nMain difference from eigh: the eigenvectors are not computed.\n\nParameters:\n----------\na : (..., M, M) array_like\n    A complex- or real-valued matrix whose eigenvalues are to be computed.\nUPLO : {'L', 'U'}, optional\n    Specifies whether the calculation is done with the lower triangular part of a ('L', default) or the upper triangular part ('U'). Irrespective of this value only the real parts of the diagonal will be considered in the computation to preserve the notion of a Hermitian matrix. It therefore follows that the imaginary part of the diagonal will always be treated as zero.\n\nReturns:\n-------\nw : (..., M,) ndarray\n    The eigenvalues in ascending order, each repeated according to its multiplicity.\n\nRaises:\n------\nLinAlgError\n    If the eigenvalue computation does not converge.\n\nSee Also:\n--------\neigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian (conjugate symmetric) arrays.\neigvals : eigenvalues of general real or complex arrays.\neig : eigenvalues and right eigenvectors of general real or complex arrays.\nscipy.linalg.eigvalsh : Similar function in SciPy.\n\nNotes:\n-----\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe eigenvalues are computed using LAPACK routines _syevd, _heevd.\n\nExamples:\n--------\n>>> import numpy as np\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, -2j], [2j, 5]])\n>>> LA.eigvalsh(a)\narray([ 0.17157288,  5.82842712]) # may vary\n\n>>> # demonstrate the treatment of the imaginary part of the diagonal\n>>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n>>> a\narray([[5.+2.j, 9.-2.j],\n       [0.+2.j, 2.-1.j]])\n>>> # with UPLO='L' this is numerically equivalent to using LA.eigvals()\n>>> # with:\n>>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n>>> b\narray([[5.+0.j, 0.-2.j],\n       [0.+2.j, 2.+0.j]])\n>>> wa = LA.eigvalsh(a)\n>>> wb = LA.eigvals(b)\n>>> wa\narray([1., 6.])\n>>> wb\narray([6.+0.j, 1.+0.j])",
          "code": "@array_function_dispatch(_eigvalsh_dispatcher)\ndef eigvalsh(a, UPLO='L'):\n    \"\"\"\n    Compute the eigenvalues of a complex Hermitian or real symmetric matrix.\n\n    Main difference from eigh: the eigenvectors are not computed.\n\n    Parameters\n    ----------\n    a : (..., M, M) array_like\n        A complex- or real-valued matrix whose eigenvalues are to be\n        computed.\n    UPLO : {'L', 'U'}, optional\n        Specifies whether the calculation is done with the lower triangular\n        part of `a` ('L', default) or the upper triangular part ('U').\n        Irrespective of this value only the real parts of the diagonal will\n        be considered in the computation to preserve the notion of a Hermitian\n        matrix. It therefore follows that the imaginary part of the diagonal\n        will always be treated as zero.\n\n    Returns\n    -------\n    w : (..., M,) ndarray\n        The eigenvalues in ascending order, each repeated according to\n        its multiplicity.\n\n    Raises\n    ------\n    LinAlgError\n        If the eigenvalue computation does not converge.\n\n    See Also\n    --------\n    eigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian\n           (conjugate symmetric) arrays.\n    eigvals : eigenvalues of general real or complex arrays.\n    eig : eigenvalues and right eigenvectors of general real or complex\n          arrays.\n    scipy.linalg.eigvalsh : Similar function in SciPy.\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    The eigenvalues are computed using LAPACK routines ``_syevd``, ``_heevd``.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n    >>> a = np.array([[1, -2j], [2j, 5]])\n    >>> LA.eigvalsh(a)\n    array([ 0.17157288,  5.82842712]) # may vary\n\n    >>> # demonstrate the treatment of the imaginary part of the diagonal\n    >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])\n    >>> a\n    array([[5.+2.j, 9.-2.j],\n           [0.+2.j, 2.-1.j]])\n    >>> # with UPLO='L' this is numerically equivalent to using LA.eigvals()\n    >>> # with:\n    >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])\n    >>> b\n    array([[5.+0.j, 0.-2.j],\n           [0.+2.j, 2.+0.j]])\n    >>> wa = LA.eigvalsh(a)\n    >>> wb = LA.eigvals(b)\n    >>> wa\n    array([1., 6.])\n    >>> wb\n    array([6.+0.j, 1.+0.j])\n\n    \"\"\"\n    UPLO = UPLO.upper()\n    if UPLO not in ('L', 'U'):\n        raise ValueError(\"UPLO argument must be 'L' or 'U'\")\n\n    if UPLO == 'L':\n        gufunc = _umath_linalg.eigvalsh_lo\n    else:\n        gufunc = _umath_linalg.eigvalsh_up\n\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    _assert_finite(a)\n    t, result_t = _commonType(a)\n    signature = 'D->d' if isComplexType(t) else 'd->d'\n    with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,\n                  invalid='call', over='ignore', divide='ignore',\n                  under='ignore'):\n        w = gufunc(a, signature=signature)\n    return w.astype(_realType(result_t), copy=False)"
        }
      }
    ],
    "norms_numbers": [
      {
        "name": "numpy.linalg.norm",
        "description": "Matrix or vector norm. Returns one of eight different matrix norms, or one of an infinite number of vector norms, depending on the value of the ord parameter.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html",
        "add": {
          "doc": "linalg.norm(x, ord=None, axis=None, keepdims=False)\n\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the `ord` parameter.\n\nParameters:\n**x** : array_like\n    Input array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of `x.ravel` will be returned.\n\n**ord** : {int, float, inf, -inf, 'fro', 'nuc'}, optional\n    Order of the norm (see table under `Notes` for what values are supported for matrices and vectors respectively). inf means numpy's inf object. The default is None.\n\n**axis** : {None, int, 2-tuple of ints}, optional.\n    If axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\n**keepdims** : bool, optional\n    If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nReturns:\n**n** : float or ndarray\n    Norm of the matrix or vector(s).\n\nNotes:\nFor values of `ord < 1`, the result is, strictly speaking, not a mathematical 'norm', but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord | norm for matrices | norm for vectors\n----|-------------------|------------------\nNone | Frobenius norm | 2-norm\n'fro' | Frobenius norm | \u2013\n'nuc' | nuclear norm | \u2013\ninf | max(sum(abs(x), axis=1)) | max(abs(x))\n-inf | min(sum(abs(x), axis=1)) | min(abs(x))\n0 | \u2013 | sum(x != 0)\n1 | max(sum(abs(x), axis=0)) | as below\n-1 | min(sum(abs(x), axis=0)) | as below\n2 | 2-norm (largest sing. value) | as below\n-2 | smallest singular value | as below\nother | \u2013 | sum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by: ||A||_F = [\u2211_{i,j} abs(a_{i,j})^2]^{1/2}\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when `x.ndim != 2`.\n\nExamples:\n>>> import numpy as np\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) - 4\n>>> a\narray([-4, -3, -2, ...,  2,  3,  4])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\n\n>>> LA.norm(a)\n7.745966692414834\n>>> LA.norm(b)\n7.745966692414834\n>>> LA.norm(b, 'fro')\n7.745966692414834\n>>> LA.norm(a, np.inf)\n4.0\n>>> LA.norm(b, np.inf)\n9.0\n>>> LA.norm(a, -np.inf)\n0.0\n>>> LA.norm(b, -np.inf)\n2.0",
          "code": "@array_function_dispatch(_norm_dispatcher)\ndef norm(x, ord=None, axis=None, keepdims=False):\n    \"\"\"\n    Matrix or vector norm.\n\n    This function is able to return one of eight different matrix norms,\n    or one of an infinite number of vector norms (described below), depending\n    on the value of the ``ord`` parameter.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.  If `axis` is None, `x` must be 1-D or 2-D, unless `ord`\n        is None. If both `axis` and `ord` are None, the 2-norm of\n        ``x.ravel`` will be returned.\n    ord : {int, float, inf, -inf, 'fro', 'nuc'}, optional\n        Order of the norm (see table under ``Notes`` for what values are\n        supported for matrices and vectors respectively). inf means numpy's\n        `inf` object. The default is None.\n    axis : {None, int, 2-tuple of ints}, optional.\n        If `axis` is an integer, it specifies the axis of `x` along which to\n        compute the vector norms.  If `axis` is a 2-tuple, it specifies the\n        axes that hold 2-D matrices, and the matrix norms of these matrices\n        are computed.  If `axis` is None then either a vector norm (when `x`\n        is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default\n        is None.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are normed over are left in the\n        result as dimensions with size one.  With this option the result will\n        broadcast correctly against the original `x`.\n\n    Returns\n    -------\n    n : float or ndarray\n        Norm of the matrix or vector(s).\n\n    See Also\n    --------\n    scipy.linalg.norm : Similar function in SciPy.\n\n    Notes\n    -----\n    For values of ``ord < 1``, the result is, strictly speaking, not a\n    mathematical 'norm', but it may still be useful for various numerical\n    purposes.\n\n    The following norms can be calculated:\n\n    =====  ============================  ==========================\n    ord    norm for matrices             norm for vectors\n    =====  ============================  ==========================\n    None   Frobenius norm                2-norm\n    'fro'  Frobenius norm                --\n    'nuc'  nuclear norm                  --\n    inf    max(sum(abs(x), axis=1))      max(abs(x))\n    -inf   min(sum(abs(x), axis=1))      min(abs(x))\n    0      --                            sum(x != 0)\n    1      max(sum(abs(x), axis=0))      as below\n    -1     min(sum(abs(x), axis=0))      as below\n    2      2-norm (largest sing. value)  as below\n    -2     smallest singular value       as below\n    other  --                            sum(abs(x)**ord)**(1./ord)\n    =====  ============================  ==========================\n\n    The Frobenius norm is given by [1]_:\n\n    :math:`||A||_F = [\\\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`\n\n    The nuclear norm is the sum of the singular values.\n\n    Both the Frobenius and nuclear norm orders are only defined for\n    matrices and raise a ValueError when ``x.ndim != 2``.\n\n    References\n    ----------\n    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,\n           Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\n    Examples\n    --------\n\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n    >>> a = np.arange(9) - 4\n    >>> a\n    array([-4, -3, -2, ...,  2,  3,  4])\n    >>> b = a.reshape((3, 3))\n    >>> b\n    array([[-4, -3, -2],\n           [-1,  0,  1],\n           [ 2,  3,  4]])\n\n    >>> LA.norm(a)\n    7.745966692414834\n    >>> LA.norm(b)\n    7.745966692414834\n    >>> LA.norm(b, 'fro')\n    7.745966692414834\n    >>> LA.norm(a, np.inf)\n    4.0\n    >>> LA.norm(b, np.inf)\n    9.0\n    >>> LA.norm(a, -np.inf)\n    0.0\n    >>> LA.norm(b, -np.inf)\n    2.0\n\n    >>> LA.norm(a, 1)\n    20.0\n    >>> LA.norm(b, 1)\n    7.0\n    >>> LA.norm(a, -1)\n    -4.6566128774142013e-010\n    >>> LA.norm(b, -1)\n    6.0\n    >>> LA.norm(a, 2)\n    7.745966692414834\n    >>> LA.norm(b, 2)\n    7.3484692283495345\n\n    >>> LA.norm(a, -2)\n    0.0\n    >>> LA.norm(b, -2)\n    1.8570331885190563e-016 # may vary\n    >>> LA.norm(a, 3)\n    5.8480354764257312 # may vary\n    >>> LA.norm(a, -3)\n    0.0\n\n    Using the `axis` argument to compute vector norms:\n\n    >>> c = np.array([[ 1, 2, 3],\n    ...               [-1, 1, 4]])\n    >>> LA.norm(c, axis=0)\n    array([ 1.41421356,  2.23606798,  5.        ])\n    >>> LA.norm(c, axis=1)\n    array([ 3.74165739,  4.24264069])\n    >>> LA.norm(c, ord=1, axis=1)\n    array([ 6.,  6.])\n\n    Using the `axis` argument to compute matrix norms:\n\n    >>> m = np.arange(8).reshape(2,2,2)\n    >>> LA.norm(m, axis=(1,2))\n    array([  3.74165739,  11.22497216])\n    >>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n    (3.7416573867739413, 11.224972160321824)\n\n    \"\"\"\n    x = asarray(x)\n\n    if not issubclass(x.dtype.type, (inexact, object_)):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (\n            (ord is None) or\n            (ord in ('f', 'fro') and ndim == 2) or\n            (ord == 2 and ndim == 1)\n        ):\n            x = x.ravel(order='K')\n            if isComplexType(x.dtype.type):\n                x_real = x.real\n                x_imag = x.imag\n                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n            else:\n                sqnorm = x.dot(x)\n            ret = sqrt(sqnorm)\n            if keepdims:\n                ret = ret.reshape(ndim * [1])\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception as e:\n            raise TypeError(\n                \"'axis' must be None, an integer or a tuple of integers\"\n            ) from e\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == inf:\n            return abs(x).max(axis=axis, keepdims=keepdims, initial=0)\n        elif ord == -inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            return (\n                (x != 0)\n                .astype(x.real.dtype)\n                .sum(axis=axis, keepdims=keepdims)\n            )\n        elif ord == 1:\n            # special case for speedup\n            return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            s = (x.conj() * x).real\n            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n        # None of the str-type keywords for ord ('fro', 'nuc')\n        # are valid for vectors\n        elif isinstance(ord, str):\n            raise ValueError(f\"Invalid norm order '{ord}' for vectors\")\n        else:\n            absx = abs(x)\n            absx **= ord\n            ret = add.reduce(absx, axis=axis, keepdims=keepdims)\n            ret **= reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        row_axis = normalize_axis_index(row_axis, nd)\n        col_axis = normalize_axis_index(col_axis, nd)\n        if row_axis == col_axis:\n            raise ValueError('Duplicate axes given.')\n        if ord == 2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amax, 0)\n        elif ord == -2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amin)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis, initial=0)\n        elif ord == inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis, initial=0)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)\n        elif ord == -inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)\n        elif ord in [None, 'fro', 'f']:\n            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))\n        elif ord == 'nuc':\n            ret = _multi_svd_norm(x, row_axis, col_axis, sum, 0)\n        else:\n            raise ValueError(\"Invalid norm order for matrices.\")\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\"Improper number of dimensions to norm.\")"
        }
      },
      {
        "name": "numpy.linalg.matrix_norm",
        "description": "Computes the matrix norm of a matrix (or a stack of matrices) x. This function is Array API compatible.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_norm.html",
        "add": {
          "doc": "linalg.matrix_norm(x, /, *, keepdims=False, ord='fro')\n\nComputes the matrix norm of a matrix (or a stack of matrices) `x`.\n\nThis function is Array API compatible.\n\nParameters:\n**x** : array_like\n    Input array having shape (\u2026, M, N) and whose two innermost dimensions form `MxN` matrices.\n\n**keepdims** : bool, optional\n    If this is set to True, the axes which are normed over are left in the result as dimensions with size one. Default: False.\n\n**ord** : {1, -1, 2, -2, inf, -inf, 'fro', 'nuc'}, optional\n    The order of the norm. For details see the table under `Notes` in numpy.linalg.norm.\n\nSee also:\nnumpy.linalg.norm : Generic norm function\n\nExamples:\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) - 4\n>>> a\narray([-4, -3, -2, ...,  2,  3,  4])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\n\n>>> LA.matrix_norm(b)\n7.745966692414834\n>>> LA.matrix_norm(b, ord='fro')\n7.745966692414834\n>>> LA.matrix_norm(b, ord=np.inf)\n9.0\n>>> LA.matrix_norm(b, ord=-np.inf)\n2.0\n\n>>> LA.matrix_norm(b, ord=1)\n7.0\n>>> LA.matrix_norm(b, ord=-1)\n6.0\n>>> LA.matrix_norm(b, ord=2)\n7.3484692283495345\n>>> LA.matrix_norm(b, ord=-2)\n1.8570331885190563e-016  # may vary",
          "code": "@array_function_dispatch(_matrix_norm_dispatcher)\ndef matrix_norm(x, /, *, keepdims=False, ord=\"fro\"):\n    \"\"\"\n    Computes the matrix norm of a matrix (or a stack of matrices) ``x``.\n\n    This function is Array API compatible.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array having shape (..., M, N) and whose two innermost\n        dimensions form ``MxN`` matrices.\n    keepdims : bool, optional\n        If this is set to True, the axes which are normed over are left in\n        the result as dimensions with size one. Default: False.\n    ord : {1, -1, 2, -2, inf, -inf, 'fro', 'nuc'}, optional\n        The order of the norm. For details see the table under ``Notes``\n        in `numpy.linalg.norm`.\n\n    See Also\n    --------\n    numpy.linalg.norm : Generic norm function\n\n    Examples\n    --------\n    >>> from numpy import linalg as LA\n    >>> a = np.arange(9) - 4\n    >>> a\n    array([-4, -3, -2, ...,  2,  3,  4])\n    >>> b = a.reshape((3, 3))\n    >>> b\n    array([[-4, -3, -2],\n           [-1,  0,  1],\n           [ 2,  3,  4]])\n\n    >>> LA.matrix_norm(b)\n    7.745966692414834\n    >>> LA.matrix_norm(b, ord='fro')\n    7.745966692414834\n    >>> LA.matrix_norm(b, ord=np.inf)\n    9.0\n    >>> LA.matrix_norm(b, ord=-np.inf)\n    2.0\n\n    >>> LA.matrix_norm(b, ord=1)\n    7.0\n    >>> LA.matrix_norm(b, ord=-1)\n    6.0\n    >>> LA.matrix_norm(b, ord=2)\n    7.3484692283495345\n    >>> LA.matrix_norm(b, ord=-2)\n    1.8570331885190563e-016 # may vary\n\n    \"\"\"\n    x = asanyarray(x)\n    return norm(x, axis=(-2, -1), keepdims=keepdims, ord=ord)"
        }
      },
      {
        "name": "numpy.linalg.vector_norm",
        "description": "Computes the vector norm of a vector (or batch of vectors) x. This function is Array API compatible.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.vector_norm.html",
        "add": {
          "doc": "linalg.vector_norm(x, /, *, axis=None, keepdims=False, ord=2)\n\nComputes the vector norm of a vector (or batch of vectors) `x`.\n\nThis function is Array API compatible.\n\nParameters:\n**x** : array_like\n    Input array.\n\n**axis** : {None, int, 2-tuple of ints}, optional\n    If an integer, `axis` specifies the axis (dimension) along which to compute vector norms. If an n-tuple, `axis` specifies the axes (dimensions) along which to compute batched vector norms. If `None`, the vector norm must be computed over all array values (i.e., equivalent to computing the vector norm of a flattened array). Default: `None`.\n\n**keepdims** : bool, optional\n    If this is set to True, the axes which are normed over are left in the result as dimensions with size one. Default: False.\n\n**ord** : {int, float, inf, -inf}, optional\n    The order of the norm. For details see the table under `Notes` in numpy.linalg.norm.\n\nSee also:\nnumpy.linalg.norm : Generic norm function\n\nExamples:\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) + 1\n>>> a\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n>>> LA.vector_norm(b)\n16.881943016134134\n>>> LA.vector_norm(b, ord=np.inf)\n9.0\n>>> LA.vector_norm(b, ord=-np.inf)\n1.0\n\n>>> LA.vector_norm(b, ord=0)\n9.0\n>>> LA.vector_norm(b, ord=1)\n45.0\n>>> LA.vector_norm(b, ord=-1)\n0.3534857623790153\n>>> LA.vector_norm(b, ord=2)\n16.881943016134134\n>>> LA.vector_norm(b, ord=-2)\n0.8058837395885292",
          "code": "@array_function_dispatch(_vector_norm_dispatcher)\ndef vector_norm(x, /, *, axis=None, keepdims=False, ord=2):\n    \"\"\"\n    Computes the vector norm of a vector (or batch of vectors) ``x``.\n\n    This function is Array API compatible.\n\n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    axis : {None, int, 2-tuple of ints}, optional\n        If an integer, ``axis`` specifies the axis (dimension) along which\n        to compute vector norms. If an n-tuple, ``axis`` specifies the axes\n        (dimensions) along which to compute batched vector norms. If ``None``,\n        the vector norm must be computed over all array values (i.e.,\n        equivalent to computing the vector norm of a flattened array).\n        Default: ``None``.\n    keepdims : bool, optional\n        If this is set to True, the axes which are normed over are left in\n        the result as dimensions with size one. Default: False.\n    ord : {int, float, inf, -inf}, optional\n        The order of the norm. For details see the table under ``Notes``\n        in `numpy.linalg.norm`.\n\n    See Also\n    --------\n    numpy.linalg.norm : Generic norm function\n\n    Examples\n    --------\n    >>> from numpy import linalg as LA\n    >>> a = np.arange(9) + 1\n    >>> a\n    array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> b = a.reshape((3, 3))\n    >>> b\n    array([[1, 2, 3],\n           [4, 5, 6],\n           [7, 8, 9]])\n\n    >>> LA.vector_norm(b)\n    16.881943016134134\n    >>> LA.vector_norm(b, ord=np.inf)\n    9.0\n    >>> LA.vector_norm(b, ord=-np.inf)\n    1.0\n\n    >>> LA.vector_norm(b, ord=0)\n    9.0\n    >>> LA.vector_norm(b, ord=1)\n    45.0\n    >>> LA.vector_norm(b, ord=-1)\n    0.3534857623790153\n    >>> LA.vector_norm(b, ord=2)\n    16.881943016134134\n    >>> LA.vector_norm(b, ord=-2)\n    0.8058837395885292\n\n    \"\"\"\n    x = asanyarray(x)\n    shape = list(x.shape)\n    if axis is None:\n        # Note: np.linalg.norm() doesn't handle 0-D arrays\n        x = x.ravel()\n        _axis = 0\n    elif isinstance(axis, tuple):\n        # Note: The axis argument supports any number of axes, whereas\n        # np.linalg.norm() only supports a single axis for vector norm.\n        normalized_axis = normalize_axis_tuple(axis, x.ndim)\n        rest = tuple(i for i in range(x.ndim) if i not in normalized_axis)\n        newshape = axis + rest\n        x = _core_transpose(x, newshape).reshape(\n            (\n                prod([x.shape[i] for i in axis], dtype=int),\n                *[x.shape[i] for i in rest]\n            )\n        )\n        _axis = 0\n    else:\n        _axis = axis\n\n    res = norm(x, axis=_axis, ord=ord)\n\n    if keepdims:\n        # We can't reuse np.linalg.norm(keepdims) because of the reshape hacks\n        # above to avoid matrix norm logic.\n        _axis = normalize_axis_tuple(\n            range(len(shape)) if axis is None else axis, len(shape)\n        )\n        for i in _axis:\n            shape[i] = 1\n        res = res.reshape(tuple(shape))\n\n    return res"
        }
      },
      {
        "name": "numpy.linalg.cond",
        "description": "Compute the condition number of a matrix. This function is capable of returning the condition number using one of seven different norms.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html",
        "add": {
          "doc": "linalg.cond(x, p=None)\n\nCompute the condition number of a matrix.\n\nThis function is capable of returning the condition number using one of seven different norms, depending on the value of p (see Parameters below).\n\nParameters:\n**x** : (\u2026, M, N) array_like\n    The matrix whose condition number is sought.\n\n**p** : {None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional\n    Order of the norm used in the condition number computation:\n    \n    p     | norm for matrices\n    ------|------------------\n    None  | 2-norm, computed directly using the SVD\n    'fro' | Frobenius norm\n    inf   | max(sum(abs(x), axis=1))\n    -inf  | min(sum(abs(x), axis=1))\n    1     | max(sum(abs(x), axis=0))\n    -1    | min(sum(abs(x), axis=0))\n    2     | 2-norm (largest sing. value)\n    -2    | smallest singular value\n    \n    inf means the numpy.inf object, and the Frobenius norm is the root-of-sum-of-squares norm.\n\nReturns:\n**c** : {float, inf}\n    The condition number of the matrix. May be infinite.\n\nSee also:\nnumpy.linalg.norm\n\nNotes:\nThe condition number of x is defined as the norm of x times the norm of the inverse of x; the norm can be the usual L2-norm (root-of-sum-of-squares) or one of a number of other matrix norms.\n\nExamples:\n>>> import numpy as np\n>>> from numpy import linalg as LA\n>>> a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])\n>>> a\narray([[ 1,  0, -1],\n       [ 0,  1,  0],\n       [ 1,  0,  1]])\n>>> LA.cond(a)\n1.4142135623730951\n>>> LA.cond(a, 'fro')\n3.1622776601683795\n>>> LA.cond(a, np.inf)\n2.0\n>>> LA.cond(a, -np.inf)\n1.0\n>>> LA.cond(a, 1)\n2.0\n>>> LA.cond(a, -1)\n1.0\n>>> LA.cond(a, 2)\n1.4142135623730951\n>>> LA.cond(a, -2)\n0.70710678118654746  # may vary",
          "code": "@array_function_dispatch(_cond_dispatcher)\ndef cond(x, p=None):\n    \"\"\"\n    Compute the condition number of a matrix.\n\n    This function is capable of returning the condition number using\n    one of seven different norms, depending on the value of `p` (see\n    Parameters below).\n\n    Parameters\n    ----------\n    x : (..., M, N) array_like\n        The matrix whose condition number is sought.\n    p : {None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional\n        Order of the norm used in the condition number computation:\n\n        =====  ============================\n        p      norm for matrices\n        =====  ============================\n        None   2-norm, computed directly using the ``SVD``\n        'fro'  Frobenius norm\n        inf    max(sum(abs(x), axis=1))\n        -inf   min(sum(abs(x), axis=1))\n        1      max(sum(abs(x), axis=0))\n        -1     min(sum(abs(x), axis=0))\n        2      2-norm (largest sing. value)\n        -2     smallest singular value\n        =====  ============================\n\n        inf means the `numpy.inf` object, and the Frobenius norm is\n        the root-of-sum-of-squares norm.\n\n    Returns\n    -------\n    c : {float, inf}\n        The condition number of the matrix. May be infinite.\n\n    See Also\n    --------\n    numpy.linalg.norm\n\n    Notes\n    -----\n    The condition number of `x` is defined as the norm of `x` times the\n    norm of the inverse of `x` [1]_; the norm can be the usual L2-norm\n    (root-of-sum-of-squares) or one of a number of other matrix norms.\n\n    References\n    ----------\n    .. [1] G. Strang, *Linear Algebra and Its Applications*, Orlando, FL,\n           Academic Press, Inc., 1980, pg. 285.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy import linalg as LA\n    >>> a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])\n    >>> a\n    array([[ 1,  0, -1],\n           [ 0,  1,  0],\n           [ 1,  0,  1]])\n    >>> LA.cond(a)\n    1.4142135623730951\n    >>> LA.cond(a, 'fro')\n    3.1622776601683795\n    >>> LA.cond(a, np.inf)\n    2.0\n    >>> LA.cond(a, -np.inf)\n    1.0\n    >>> LA.cond(a, 1)\n    2.0\n    >>> LA.cond(a, -1)\n    1.0\n    >>> LA.cond(a, 2)\n    1.4142135623730951\n    >>> LA.cond(a, -2)\n    0.70710678118654746 # may vary\n    >>> (min(LA.svd(a, compute_uv=False)) *\n    ... min(LA.svd(LA.inv(a), compute_uv=False)))\n    0.70710678118654746 # may vary\n\n    \"\"\"\n    x = asarray(x)  # in case we have a matrix\n    if _is_empty_2d(x):\n        raise LinAlgError(\"cond is not defined on empty arrays\")\n    if p is None or p in {2, -2}:\n        s = svd(x, compute_uv=False)\n        with errstate(all='ignore'):\n            if p == -2:\n                r = s[..., -1] / s[..., 0]\n            else:\n                r = s[..., 0] / s[..., -1]\n    else:\n        # Call inv(x) ignoring errors. The result array will\n        # contain nans in the entries where inversion failed.\n        _assert_stacked_square(x)\n        t, result_t = _commonType(x)\n        signature = 'D->D' if isComplexType(t) else 'd->d'\n        with errstate(all='ignore'):\n            invx = _umath_linalg.inv(x, signature=signature)\n            r = norm(x, p, axis=(-2, -1)) * norm(invx, p, axis=(-2, -1))\n        r = r.astype(result_t, copy=False)\n\n    # Convert nans to infs unless the original array had nan entries\n    r = asarray(r)\n    nan_mask = isnan(r)\n    if nan_mask.any():\n        nan_mask &= ~isnan(x).any(axis=(-2, -1))\n        if r.ndim > 0:\n            r[nan_mask] = inf\n        elif nan_mask:\n            r[()] = inf\n\n    # Convention is to return scalars instead of 0d arrays\n    if r.ndim == 0:\n        r = r[()]\n\n    return r"
        }
      },
      {
        "name": "numpy.linalg.det",
        "description": "Compute the determinant of an array using LU factorization via LAPACK.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.det.html",
        "add": {
          "doc": "linalg.det(a)\n\nCompute the determinant of an array.\n\nParameters:\n**a** : (\u2026, M, M) array_like\n    Input array to compute determinants for.\n\nReturns:\n**det** : (\u2026) array_like\n    Determinant of a.\n\nSee also:\nslogdet : Another way to represent the determinant, more suitable for large matrices where underflow/overflow may occur.\nscipy.linalg.det : Similar function in SciPy.\n\nNotes:\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe determinant is computed via LU factorization using the LAPACK routine `z/dgetrf`.\n\nExamples:\nThe determinant of a 2-D array [[a, b], [c, d]] is ad - bc:\n\n>>> import numpy as np\n>>> a = np.array([[1, 2], [3, 4]])\n>>> np.linalg.det(a)\n-2.0  # may vary\n\nComputing determinants for a stack of matrices:\n\n>>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n>>> a.shape\n(3, 2, 2)\n>>> np.linalg.det(a)\narray([-2., -3., -8.])",
          "code": "@array_function_dispatch(_unary_dispatcher)\ndef det(a):\n    \"\"\"\n    Compute the determinant of an array.\n\n    Parameters\n    ----------\n    a : (..., M, M) array_like\n        Input array to compute determinants for.\n\n    Returns\n    -------\n    det : (...) array_like\n        Determinant of `a`.\n\n    See Also\n    --------\n    slogdet : Another way to represent the determinant, more suitable\n      for large matrices where underflow/overflow may occur.\n    scipy.linalg.det : Similar function in SciPy.\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    The determinant is computed via LU factorization using the LAPACK\n    routine ``z/dgetrf``.\n\n    Examples\n    --------\n    The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:\n\n    >>> import numpy as np\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> np.linalg.det(a)\n    -2.0 # may vary\n\n    Computing determinants for a stack of matrices:\n\n    >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n    >>> a.shape\n    (3, 2, 2)\n    >>> np.linalg.det(a)\n    array([-2., -3., -8.])\n\n    \"\"\"\n    a = asarray(a)\n    _assert_stacked_square(a)\n    t, result_t = _commonType(a)\n    signature = 'D->D' if isComplexType(t) else 'd->d'\n    r = _umath_linalg.det(a, signature=signature)\n    r = r.astype(result_t, copy=False)\n    return r"
        }
      },
      {
        "name": "numpy.linalg.matrix_rank",
        "description": "Return matrix rank of array using SVD method. Rank is the number of singular values greater than the tolerance threshold.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html",
        "add": {
          "doc": "linalg.matrix_rank(A, tol=None, hermitian=False, *, rtol=None)\n\nReturn matrix rank of array using SVD method\n\nRank of the array is the number of singular values of the array that are greater than tol.\n\nParameters:\n**A** : {(M,), (\u2026, M, N)} array_like\n    Input vector or stack of matrices.\n\n**tol** : (\u2026) array_like, float, optional\n    Threshold below which SVD values are considered zero. If tol is None, and `S` is an array with singular values for M, and `eps` is the epsilon value for datatype of `S`, then tol is set to `S.max() * max(M, N) * eps`.\n\n**hermitian** : bool, optional\n    If True, A is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.\n\n**rtol** : (\u2026) array_like, float, optional\n    Parameter for the relative tolerance component. Only `tol` or `rtol` can be set at a time. Defaults to `max(M, N) * eps`.\n\nReturns:\n**rank** : (\u2026) array_like\n    Rank of A.\n\nNotes:\nThe default threshold to detect rank deficiency is a test on the magnitude of the singular values of A. By default, we identify singular values less than `S.max() * max(M, N) * eps` as indicating rank deficiency (with the symbols defined above). This is the algorithm MATLAB uses. It also appears in Numerical recipes in the discussion of SVD solutions for linear least squares.\n\nThis default threshold is designed to detect rank deficiency accounting for the numerical errors of the SVD computation. Imagine that there is a column in A that is an exact (in floating point) linear combination of other columns in A. Computing the SVD on A will not produce a singular value exactly equal to 0 in general: any difference of the smallest SVD value from 0 will be caused by numerical imprecision in the calculation of the SVD.\n\nExamples:\n>>> import numpy as np\n>>> from numpy.linalg import matrix_rank\n>>> matrix_rank(np.eye(4))  # Full rank matrix\n4\n>>> I=np.eye(4); I[-1,-1] = 0.  # rank deficient matrix\n>>> matrix_rank(I)\n3\n>>> matrix_rank(np.ones((4,)))  # 1 dimension - rank 1 unless all 0\n1\n>>> matrix_rank(np.zeros((4,)))\n0",
          "code": "@array_function_dispatch(_matrix_rank_dispatcher)\ndef matrix_rank(A, tol=None, hermitian=False, *, rtol=None):\n    \"\"\"\n    Return matrix rank of array using SVD method\n\n    Rank of the array is the number of singular values of the array that are\n    greater than `tol`.\n\n    Parameters\n    ----------\n    A : {(M,), (..., M, N)} array_like\n        Input vector or stack of matrices.\n    tol : (...) array_like, float, optional\n        Threshold below which SVD values are considered zero. If `tol` is\n        None, and ``S`` is an array with singular values for `M`, and\n        ``eps`` is the epsilon value for datatype of ``S``, then `tol` is\n        set to ``S.max() * max(M, N) * eps``.\n    hermitian : bool, optional\n        If True, `A` is assumed to be Hermitian (symmetric if real-valued),\n        enabling a more efficient method for finding singular values.\n        Defaults to False.\n    rtol : (...) array_like, float, optional\n        Parameter for the relative tolerance component. Only ``tol`` or\n        ``rtol`` can be set at a time. Defaults to ``max(M, N) * eps``.\n\n        .. versionadded:: 2.0.0\n\n    Returns\n    -------\n    rank : (...) array_like\n        Rank of A.\n\n    Notes\n    -----\n    The default threshold to detect rank deficiency is a test on the magnitude\n    of the singular values of `A`.  By default, we identify singular values\n    less than ``S.max() * max(M, N) * eps`` as indicating rank deficiency\n    (with the symbols defined above). This is the algorithm MATLAB uses [1].\n    It also appears in *Numerical recipes* in the discussion of SVD solutions\n    for linear least squares [2].\n\n    This default threshold is designed to detect rank deficiency accounting\n    for the numerical errors of the SVD computation. Imagine that there\n    is a column in `A` that is an exact (in floating point) linear combination\n    of other columns in `A`. Computing the SVD on `A` will not produce\n    a singular value exactly equal to 0 in general: any difference of\n    the smallest SVD value from 0 will be caused by numerical imprecision\n    in the calculation of the SVD. Our threshold for small SVD values takes\n    this numerical imprecision into account, and the default threshold will\n    detect such numerical rank deficiency. The threshold may declare a matrix\n    `A` rank deficient even if the linear combination of some columns of `A`\n    is not exactly equal to another column of `A` but only numerically very\n    close to another column of `A`.\n\n    We chose our default threshold because it is in wide use. Other thresholds\n    are possible.  For example, elsewhere in the 2007 edition of *Numerical\n    recipes* there is an alternative threshold of ``S.max() *\n    np.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe\n    this threshold as being based on \"expected roundoff error\" (p 71).\n\n    The thresholds above deal with floating point roundoff error in the\n    calculation of the SVD.  However, you may have more information about\n    the sources of error in `A` that would make you consider other tolerance\n    values to detect *effective* rank deficiency. The most useful measure\n    of the tolerance depends on the operations you intend to use on your\n    matrix. For example, if your data come from uncertain measurements with\n    uncertainties greater than floating point epsilon, choosing a tolerance\n    near that uncertainty may be preferable. The tolerance may be absolute\n    if the uncertainties are absolute rather than relative.\n\n    References\n    ----------\n    .. [1] MATLAB reference documentation, \"Rank\"\n           https://www.mathworks.com/help/techdoc/ref/rank.html\n    .. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,\n           \"Numerical Recipes (3rd edition)\", Cambridge University Press, 2007,\n           page 795.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from numpy.linalg import matrix_rank\n    >>> matrix_rank(np.eye(4)) # Full rank matrix\n    4\n    >>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix\n    >>> matrix_rank(I)\n    3\n    >>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0\n    1\n    >>> matrix_rank(np.zeros((4,)))\n    0\n    \"\"\"\n    if rtol is not None and tol is not None:\n        raise ValueError(\"`tol` and `rtol` can't be both set.\")\n\n    A = asarray(A)\n    if A.ndim < 2:\n        return int(not all(A == 0))\n    S = svd(A, compute_uv=False, hermitian=hermitian)\n\n    if tol is None:\n        if rtol is None:\n            rtol = max(A.shape[-2:]) * finfo(S.dtype).eps\n        else:\n            rtol = asarray(rtol)[..., newaxis]\n        tol = S.max(axis=-1, keepdims=True) * rtol\n    else:\n        tol = asarray(tol)[..., newaxis]\n\n    return count_nonzero(S > tol, axis=-1)"
        }
      },
      {
        "name": "numpy.linalg.slogdet",
        "description": "Compute the sign and (natural) logarithm of the determinant of an array. More numerically stable than det for very large or small determinants.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.slogdet.html",
        "add": {
          "doc": "linalg.slogdet(a)\n\nCompute the sign and (natural) logarithm of the determinant of an array.\n\nIf an array has a very small or very large determinant, then a call to det may overflow or underflow. This routine is more robust against such issues, because it computes the logarithm of the determinant rather than the determinant itself.\n\nParameters:\n**a** : (\u2026, M, M) array_like\n    Input array, has to be a square 2-D array.\n\nReturns:\nA namedtuple with the following attributes:\n\n**sign** : (\u2026) array_like\n    A number representing the sign of the determinant. For a real matrix, this is 1, 0, or -1. For a complex matrix, this is a complex number with absolute value 1 (i.e., it is on the unit circle), or else 0.\n\n**logabsdet** : (\u2026) array_like\n    The natural log of the absolute value of the determinant.\n    \n    If the determinant is zero, then sign will be 0 and logabsdet will be -inf. In all cases, the determinant is equal to `sign * np.exp(logabsdet)`.\n\nSee also:\ndet\n\nNotes:\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe determinant is computed via LU factorization using the LAPACK routine `z/dgetrf`.\n\nExamples:\nThe determinant of a 2-D array `[[a, b], [c, d]]` is `ad - bc`:\n\n>>> import numpy as np\n>>> a = np.array([[1, 2], [3, 4]])\n>>> (sign, logabsdet) = np.linalg.slogdet(a)\n>>> (sign, logabsdet)\n(-1, 0.69314718055994529)  # may vary\n>>> sign * np.exp(logabsdet)\n-2.0\n\nComputing log-determinants for a stack of matrices:\n\n>>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n>>> a.shape\n(3, 2, 2)\n>>> sign, logabsdet = np.linalg.slogdet(a)\n>>> (sign, logabsdet)\n(array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))\n>>> sign * np.exp(logabsdet)\narray([-2., -3., -8.])\n\nThis routine succeeds where ordinary det does not:\n\n>>> np.linalg.det(np.eye(500) * 0.1)\n0.0\n>>> np.linalg.slogdet(np.eye(500) * 0.1)\n(1, -1151.2925464970228)",
          "code": "@array_function_dispatch(_unary_dispatcher)\ndef slogdet(a):\n    \"\"\"\n    Compute the sign and (natural) logarithm of the determinant of an array.\n\n    If an array has a very small or very large determinant, then a call to\n    `det` may overflow or underflow. This routine is more robust against such\n    issues, because it computes the logarithm of the determinant rather than\n    the determinant itself.\n\n    Parameters\n    ----------\n    a : (..., M, M) array_like\n        Input array, has to be a square 2-D array.\n\n    Returns\n    -------\n    A namedtuple with the following attributes:\n\n    sign : (...) array_like\n        A number representing the sign of the determinant. For a real matrix,\n        this is 1, 0, or -1. For a complex matrix, this is a complex number\n        with absolute value 1 (i.e., it is on the unit circle), or else 0.\n    logabsdet : (...) array_like\n        The natural log of the absolute value of the determinant.\n\n    If the determinant is zero, then `sign` will be 0 and `logabsdet`\n    will be -inf. In all cases, the determinant is equal to\n    ``sign * np.exp(logabsdet)``.\n\n    See Also\n    --------\n    det\n\n    Notes\n    -----\n    Broadcasting rules apply, see the `numpy.linalg` documentation for\n    details.\n\n    The determinant is computed via LU factorization using the LAPACK\n    routine ``z/dgetrf``.\n\n    Examples\n    --------\n    The determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:\n\n    >>> import numpy as np\n    >>> a = np.array([[1, 2], [3, 4]])\n    >>> (sign, logabsdet) = np.linalg.slogdet(a)\n    >>> (sign, logabsdet)\n    (-1, 0.69314718055994529) # may vary\n    >>> sign * np.exp(logabsdet)\n    -2.0\n\n    Computing log-determinants for a stack of matrices:\n\n    >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])\n    >>> a.shape\n    (3, 2, 2)\n    >>> sign, logabsdet = np.linalg.slogdet(a)\n    >>> (sign, logabsdet)\n    (array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))\n    >>> sign * np.exp(logabsdet)\n    array([-2., -3., -8.])\n\n    This routine succeeds where ordinary `det` does not:\n\n    >>> np.linalg.det(np.eye(500) * 0.1)\n    0.0\n    >>> np.linalg.slogdet(np.eye(500) * 0.1)\n    (1, -1151.2925464970228)\n\n    \"\"\"\n    a = asarray(a)\n    _assert_stacked_square(a)\n    t, result_t = _commonType(a)\n    real_t = _realType(result_t)\n    signature = 'D->Dd' if isComplexType(t) else 'd->dd'\n    sign, logdet = _umath_linalg.slogdet(a, signature=signature)\n    sign = sign.astype(result_t, copy=False)\n    logdet = logdet.astype(real_t, copy=False)\n    return SlogdetResult(sign, logdet)"
        }
      },
      {
        "name": "numpy.trace",
        "description": "Return the sum along diagonals of the array. For 2-D arrays, returns the sum of diagonal elements.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.trace.html",
        "add": {
          "doc": "numpy.trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None)\n\nReturn the sum along diagonals of the array.\n\nIf a is 2-D, the sum along its diagonal with the given offset is returned, i.e., the sum of elements `a[i,i+offset]` for all i.\n\nIf a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-arrays whose traces are returned. The shape of the resulting array is the same as that of a with axis1 and axis2 removed.\n\nParameters:\n**a** : array_like\n    Input array, from which the diagonals are taken.\n\n**offset** : int, optional\n    Offset of the diagonal from the main diagonal. Can be both positive and negative. Defaults to 0.\n\n**axis1, axis2** : int, optional\n    Axes to be used as the first and second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults are the first two axes of a.\n\n**dtype** : dtype, optional\n    Determines the data-type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and a is of integer type of precision less than the default integer precision, then the default integer precision is used. Otherwise, the precision is the same as that of a.\n\n**out** : ndarray, optional\n    Array into which the output is placed. Its type is preserved and it must be of the right shape to hold the output.\n\nReturns:\n**sum_along_diagonals** : ndarray\n    If a is 2-D, the sum along the diagonal is returned. If a has larger dimensions, then an array of sums along diagonals is returned.\n\nSee also:\ndiag, diagonal, diagflat\n\nExamples:\n>>> import numpy as np\n>>> np.trace(np.eye(3))\n3.0\n>>> a = np.arange(8).reshape((2,2,2))\n>>> np.trace(a)\narray([6, 8])\n\n>>> a = np.arange(24).reshape((2,2,2,3))\n>>> np.trace(a).shape\n(2, 3)",
          "code": "@array_function_dispatch(_trace_dispatcher)\ndef trace(a, offset=0, axis1=0, axis2=1, dtype=None, out=None):\n    \"\"\"\n    Return the sum along diagonals of the array.\n\n    If `a` is 2-D, the sum along its diagonal with the given offset\n    is returned, i.e., the sum of elements ``a[i,i+offset]`` for all i.\n\n    If `a` has more than two dimensions, then the axes specified by axis1 and\n    axis2 are used to determine the 2-D sub-arrays whose traces are returned.\n    The shape of the resulting array is the same as that of `a` with `axis1`\n    and `axis2` removed.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array, from which the diagonals are taken.\n    offset : int, optional\n        Offset of the diagonal from the main diagonal. Can be both positive\n        and negative. Defaults to 0.\n    axis1, axis2 : int, optional\n        Axes to be used as the first and second axis of the 2-D sub-arrays\n        from which the diagonals should be taken. Defaults are the first two\n        axes of `a`.\n    dtype : dtype, optional\n        Determines the data-type of the returned array and of the accumulator\n        where the elements are summed. If dtype has the value None and `a` is\n        of integer type of precision less than the default integer\n        precision, then the default integer precision is used. Otherwise,\n        the precision is the same as that of `a`.\n    out : ndarray, optional\n        Array into which the output is placed. Its type is preserved and\n        it must be of the right shape to hold the output.\n\n    Returns\n    -------\n    sum_along_diagonals : ndarray\n        If `a` is 2-D, the sum along the diagonal is returned.  If `a` has\n        larger dimensions, then an array of sums along diagonals is returned.\n\n    See Also\n    --------\n    diag, diagonal, diagflat\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> np.trace(np.eye(3))\n    3.0\n    >>> a = np.arange(8).reshape((2,2,2))\n    >>> np.trace(a)\n    array([6, 8])\n\n    >>> a = np.arange(24).reshape((2,2,2,3))\n    >>> np.trace(a).shape\n    (2, 3)\n\n    \"\"\"\n    if isinstance(a, np.matrix):\n        # Get trace of matrix via an array to preserve backward compatibility.\n        return asarray(a).trace(\n            offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out\n        )\n    else:\n        return asanyarray(a).trace(\n            offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out\n        )"
        }
      },
      {
        "name": "numpy.linalg.trace",
        "description": "Returns the sum along the specified diagonals of a matrix (or a stack of matrices) x. This function is Array API compatible, contrary to numpy.trace.",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.trace.html",
        "add": {
          "doc": "linalg.trace(x, /, *, offset=0, dtype=None)\n\nReturns the sum along the specified diagonals of a matrix (or a stack of matrices) `x`.\n\nThis function is Array API compatible, contrary to numpy.trace.\n\nParameters:\n**x** : (\u2026,M,N) array_like\n    Input array having shape (\u2026, M, N) and whose innermost two dimensions form MxN matrices.\n\n**offset** : int, optional\n    Offset specifying the off-diagonal relative to the main diagonal, where:\n    - offset = 0: the main diagonal.\n    - offset > 0: off-diagonal above the main diagonal.\n    - offset < 0: off-diagonal below the main diagonal.\n\n**dtype** : dtype, optional\n    Data type of the returned array.\n\nReturns:\n**out** : ndarray\n    An array containing the traces and whose shape is determined by removing the last two dimensions and storing the traces in the last array dimension. For example, if x has rank k and shape: (I, J, K, \u2026, L, M, N), then an output array has rank k-2 and shape: (I, J, K, \u2026, L) where:\n    \n    out[i, j, k, ..., l] = trace(a[i, j, k, ..., l, :, :])\n    \n    The returned array must have a data type as described by the dtype parameter above.\n\nSee also:\nnumpy.trace\n\nExamples:\n>>> np.linalg.trace(np.eye(3))\n3.0\n>>> a = np.arange(8).reshape((2, 2, 2))\n>>> np.linalg.trace(a)\narray([3, 11])\n\nTrace is computed with the last two axes as the 2-d sub-arrays. This behavior differs from numpy.trace which uses the first two axes by default.\n\n>>> a = np.arange(24).reshape((3, 2, 2, 2))\n>>> np.linalg.trace(a).shape\n(3, 2)\n\nTraces adjacent to the main diagonal can be obtained by using the offset argument:\n\n>>> a = np.arange(9).reshape((3, 3)); a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> np.linalg.trace(a, offset=1)  # First superdiagonal\n6\n>>> np.linalg.trace(a, offset=2)  # Second superdiagonal\n2\n>>> np.linalg.trace(a, offset=-1)  # First subdiagonal\n10\n>>> np.linalg.trace(a, offset=-2)  # Second subdiagonal\n6",
          "code": "@array_function_dispatch(_trace_dispatcher)\ndef trace(x, /, *, offset=0, dtype=None):\n    \"\"\"\n    Returns the sum along the specified diagonals of a matrix\n    (or a stack of matrices) ``x``.\n\n    This function is Array API compatible, contrary to\n    :py:func:`numpy.trace`.\n\n    Parameters\n    ----------\n    x : (...,M,N) array_like\n        Input array having shape (..., M, N) and whose innermost two\n        dimensions form MxN matrices.\n    offset : int, optional\n        Offset specifying the off-diagonal relative to the main diagonal,\n        where::\n\n            * offset = 0: the main diagonal.\n            * offset > 0: off-diagonal above the main diagonal.\n            * offset < 0: off-diagonal below the main diagonal.\n\n    dtype : dtype, optional\n        Data type of the returned array.\n\n    Returns\n    -------\n    out : ndarray\n        An array containing the traces and whose shape is determined by\n        removing the last two dimensions and storing the traces in the last\n        array dimension. For example, if x has rank k and shape:\n        (I, J, K, ..., L, M, N), then an output array has rank k-2 and shape:\n        (I, J, K, ..., L) where::\n\n            out[i, j, k, ..., l] = trace(a[i, j, k, ..., l, :, :])\n\n        The returned array must have a data type as described by the dtype\n        parameter above.\n\n    See Also\n    --------\n    numpy.trace\n\n    Examples\n    --------\n    >>> np.linalg.trace(np.eye(3))\n    3.0\n    >>> a = np.arange(8).reshape((2, 2, 2))\n    >>> np.linalg.trace(a)\n    array([3, 11])\n\n    Trace is computed with the last two axes as the 2-d sub-arrays.\n    This behavior differs from :py:func:`numpy.trace` which uses the first two\n    axes by default.\n\n    >>> a = np.arange(24).reshape((3, 2, 2, 2))\n    >>> np.linalg.trace(a).shape\n    (3, 2)\n\n    Traces adjacent to the main diagonal can be obtained by using the\n    `offset` argument:\n\n    >>> a = np.arange(9).reshape((3, 3)); a\n    array([[0, 1, 2],\n           [3, 4, 5],\n           [6, 7, 8]])\n    >>> np.linalg.trace(a, offset=1)  # First superdiagonal\n    6\n    >>> np.linalg.trace(a, offset=2)  # Second superdiagonal\n    2\n    >>> np.linalg.trace(a, offset=-1)  # First subdiagonal\n    10\n    >>> np.linalg.trace(a, offset=-2)  # Second subdiagonal\n    6\n\n    \"\"\"\n    return _core_trace(x, offset, axis1=-2, axis2=-1, dtype=dtype)"
        }
      }
    ],
    "solving_equations": [
      {
        "name": "numpy.linalg.solve",
        "description": "Solve a linear matrix equation, or system of linear scalar equations",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html",
        "add": {
          "doc": "linalg.solve(a, b)\n\nSolve a linear matrix equation, or system of linear scalar equations.\n\nComputes the \"exact\" solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.\n\nParameters:\na : (..., M, M) array_like\n    Coefficient matrix.\nb : {(M,), (..., M, K)}, array_like\n    Ordinate or \"dependent variable\" values.\n\nReturns:\nx : {..., M,), (..., M, K)} ndarray\n    Solution to the system a x = b. Returned shape is (..., M) if b is shape (M,) and (..., M, K) if b is (..., M, K), where the \"...\" part is broadcasted between a and b.\n\nRaises:\nLinAlgError\n    If a is singular or not square.\n\nSee also:\nscipy.linalg.solve : Similar function in SciPy.\n\nNotes:\nBroadcasting rules apply, see the numpy.linalg documentation for details.\n\nThe solutions are computed using LAPACK routine _gesv.\n\na must be square and of full-rank, i.e., all rows (or, equivalently, columns) must be linearly independent; if either is not true, use lstsq for the least-squares best \"solution\" of the system/equation.\n\nChanged in version 2.0: The b array is only treated as a shape (M,) column vector if it is exactly 1-dimensional. In all other instances it is treated as a stack of (M, K) matrices. Previously b would be treated as a stack of (M,) vectors if b.ndim was equal to a.ndim - 1.\n\nReferences:\n[1] G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pg. 22.\n\nExamples:\nSolve the system of equations: x0 + 2 * x1 = 1 and 3 * x0 + 5 * x1 = 2:\n\n>>> import numpy as np\n>>> a = np.array([[1, 2], [3, 5]])\n>>> b = np.array([1, 2])\n>>> x = np.linalg.solve(a, b)\n>>> x\narray([-1.,  1.])\n\nCheck that the solution is correct:\n>>> np.allclose(np.dot(a, x), b)\nTrue",
          "code": "def solve(a, b):\n    a, _ = _makearray(a)\n    _assert_stacked_square(a)\n    b, wrap = _makearray(b)\n    t, result_t = _commonType(a, b)\n\n    # We use the b = (..., M,) logic, only if the number of extra dimensions\n    # match exactly\n    if b.ndim == 1:\n        gufunc = _umath_linalg.solve1\n    else:\n        gufunc = _umath_linalg.solve\n\n    signature = 'DD->D' if isComplexType(t) else 'dd->d'\n    with errstate(call=_raise_linalgerror_singular, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        r = gufunc(a, b, signature=signature)\n\n    return wrap(r.astype(result_t, copy=False))"
        }
      },
      {
        "name": "numpy.linalg.tensorsolve",
        "description": "Solve the tensor equation a x = b for x",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.tensorsolve.html",
        "add": {
          "doc": "linalg.tensorsolve(a, b, axes=None)\n\nSolve the tensor equation `a x = b` for x.\n\nIt is assumed that all indices of x are summed over in the product, together with the rightmost indices of a, as is done in, for example, `tensordot(a, x, axes=x.ndim)`.\n\nParameters:\na : array_like\n    Coefficient tensor, of shape `b.shape + Q`. Q, a tuple, equals the shape of that sub-tensor of a consisting of the appropriate number of its rightmost indices, and must be such that `prod(Q) == prod(b.shape)` (in which sense a is said to be 'square').\nb : array_like\n    Right-hand tensor, which can be of any shape.\naxes : tuple of ints, optional\n    Axes in a to reorder to the right, before inversion. If None (default), no reordering is done.\n\nReturns:\nx : ndarray, shape Q\n\nRaises:\nLinAlgError\n    If a is singular or not 'square' (in the above sense).\n\nSee also:\nnumpy.tensordot, tensorinv, numpy.einsum\n\nExamples:\n>>> import numpy as np\n>>> a = np.eye(2*3*4)\n>>> a.shape = (2*3, 4, 2, 3, 4)\n>>> rng = np.random.default_rng()\n>>> b = rng.normal(size=(2*3, 4))\n>>> x = np.linalg.tensorsolve(a, b)\n>>> x.shape\n(2, 3, 4)\n>>> np.allclose(np.tensordot(a, x, axes=3), b)\nTrue",
          "code": "def tensorsolve(a, b, axes=None):\n    a = asarray(a)\n    b = asarray(b)\n    an = a.ndim\n\n    if axes is not None:\n        allaxes = list(range(0, an))\n        for k in axes:\n            allaxes.remove(k)\n            allaxes.insert(an, k)\n        a = a.transpose(allaxes)\n\n    oldshape = a.shape[-(an-b.ndim):]\n    prod = 1\n    for k in oldshape:\n        prod *= k\n\n    a = a.reshape(-1, prod)\n    b = b.flatten()\n    res = solve(a, b)\n    res = res.reshape(oldshape)\n    return res"
        }
      },
      {
        "name": "numpy.linalg.lstsq",
        "description": "Return the least-squares solution to a linear matrix equation",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html",
        "add": {
          "doc": "linalg.lstsq(a, b, rcond=None)\n\nReturn the least-squares solution to a linear matrix equation.\n\nComputes the vector x that approximately solves the equation `a @ x = b`. The equation may be under-, well-, or over-determined (i.e., the number of linearly independent rows of a can be less than, equal to, or greater than its number of linearly independent columns). If a is square and of full rank, then x (but for round-off error) is the \"exact\" solution of the equation. Else, x minimizes the Euclidean 2-norm ||b - ax||. If there are multiple minimizing solutions, the one with the smallest 2-norm ||x|| is returned.\n\nParameters:\na : (M, N) array_like\n    \"Coefficient\" matrix.\nb : {(M,), (M, K)} array_like\n    Ordinate or \"dependent variable\" values. If b is two-dimensional, the least-squares solution is calculated for each of the K columns of b.\nrcond : float, optional\n    Cut-off ratio for small singular values of a. For the purposes of rank determination, singular values are treated as zero if they are smaller than rcond times the largest singular value of a. The default uses the machine precision times `max(M, N)`. Passing `-1` will use machine precision.\n\nChanged in version 2.0: Previously, the default was `-1`, but a warning was given that this would change.\n\nReturns:\nx : {(N,), (N, K)} ndarray\n    Least-squares solution. If b is two-dimensional, the solutions are in the K columns of x.\nresiduals : {(1,), (K,), (0,)} ndarray\n    Sums of squared residuals: Squared Euclidean 2-norm for each column in `b - a @ x`. If the rank of a is < N or M <= N, this is an empty array. If b is 1-dimensional, this is a (1,) shape array. Otherwise the shape is (K,).\nrank : int\n    Rank of matrix a.\ns : (min(M, N),) ndarray\n    Singular values of a.\n\nRaises:\nLinAlgError\n    If computation does not converge.\n\nSee also:\nscipy.linalg.lstsq : Similar function in SciPy.\n\nNotes:\nIf b is a matrix, then all array results are returned as matrices.\n\nExamples:\nFit a line, `y = mx + c`, through some noisy data-points:\n\n>>> import numpy as np\n>>> x = np.array([0, 1, 2, 3])\n>>> y = np.array([-1, 0.2, 0.9, 2.1])\n\nBy examining the coefficients, we see that the line should have a gradient of roughly 1 and cut the y-axis at, more or less, -1.\n\nWe can rewrite the line equation as `y = Ap`, where `A = [[x 1]]` and `p = [[m], [c]]`. Now use lstsq to solve for p:\n\n>>> A = np.vstack([x, np.ones(len(x))]).T\n>>> A\narray([[ 0.,  1.],\n       [ 1.,  1.],\n       [ 2.,  1.],\n       [ 3.,  1.]])\n\n>>> m, c = np.linalg.lstsq(A, y)[0]\n>>> m, c\n(1.0 -0.95) # may vary",
          "code": "def lstsq(a, b, rcond=None):\n    a, _ = _makearray(a)\n    b, wrap = _makearray(b)\n    is_1d = b.ndim == 1\n    if is_1d:\n        b = b[:, newaxis]\n    _assert_stacked_2d(a)\n    _assert_stacked_2d(b)\n    m, n = a.shape[-2:]\n    bshape = b.shape\n    if bshape[-2] != m:\n        raise LinAlgError('1-d arrays must have same length, '\n                         '2-d arrays must have same first dimension')\n    t, result_t = _commonType(a, b)\n    a_real = np.isrealobj(a)\n    b_real = np.isrealobj(b)\n    real_t = t in (single, double)\n    if rcond is None:\n        rcond = finfo(t).eps * max(n, m)\n    if a_real and b_real and real_t:\n        gufunc = _umath_linalg.lstsq\n    else:\n        gufunc = _umath_linalg.lstsq\n    signature = 'DDd->Ddid' if isComplexType(t) else 'ddd->ddid'\n    with errstate(call=_raise_linalgerror_lstsq, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        ret = gufunc(a, b, rcond, signature=signature)\n\n    if is_1d:\n        ret = (ret[0][:, 0], ret[1], ret[2], ret[3])\n    return wrap(ret)"
        }
      },
      {
        "name": "numpy.linalg.inv",
        "description": "Compute the inverse of a matrix",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html",
        "add": {
          "doc": "linalg.inv(a)\n\nCompute the inverse of a matrix.\n\nGiven a square matrix `a`, return the matrix `ainv` satisfying `a @ ainv = ainv @ a = eye(a.shape[0])`.\n\nParameters:\na : (..., M, M) array_like\n    Matrix to be inverted.\n\nReturns:\nainv : (..., M, M) ndarray or matrix\n    Inverse of the matrix `a`.\n\nRaises:\nLinAlgError\n    If `a` is not square or inversion fails.\n\nSee also:\nscipy.linalg.inv : Similar function in SciPy.\nnumpy.linalg.cond : Compute the condition number of a matrix.\nnumpy.linalg.svd : Compute the singular value decomposition of a matrix.\n\nNotes:\nBroadcasting rules apply, see the `numpy.linalg` documentation for details.\n\nIf `a` is detected to be singular, a `LinAlgError` is raised. If `a` is ill-conditioned, a `LinAlgError` may or may not be raised, and results may be inaccurate due to floating-point errors.\n\nReferences:\n[1] Wikipedia, \"Condition number\", https://en.wikipedia.org/wiki/Condition_number\n\nExamples:\n>>> import numpy as np\n>>> from numpy.linalg import inv\n>>> a = np.array([[1., 2.], [3., 4.]])\n>>> ainv = inv(a)\n>>> np.allclose(a @ ainv, np.eye(2))\nTrue\n>>> np.allclose(ainv @ a, np.eye(2))\nTrue\n\nIf a is a matrix object, then the return value is a matrix as well:\n>>> ainv = inv(np.matrix(a))\n>>> ainv\nmatrix([[-2. ,  1. ],\n        [ 1.5, -0.5]])\n\nInverses of several matrices can be computed at once:\n>>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])\n>>> inv(a)\narray([[[-2.  ,  1.  ],\n        [ 1.5 , -0.5 ]],\n       [[-1.25,  0.75],\n        [ 0.75, -0.25]]])",
          "code": "def inv(a):\n    a, wrap = _makearray(a)\n    _assert_stacked_square(a)\n    t, result_t = _commonType(a)\n\n    signature = 'D->D' if isComplexType(t) else 'd->d'\n    with errstate(call=_raise_linalgerror_singular, invalid='call',\n                  over='ignore', divide='ignore', under='ignore'):\n        ainv = _umath_linalg.inv(a, signature=signature)\n    return wrap(ainv.astype(result_t, copy=False))"
        }
      },
      {
        "name": "numpy.linalg.pinv",
        "description": "Compute the (Moore-Penrose) pseudo-inverse of a matrix",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html",
        "add": {
          "doc": "linalg.pinv(a, rcond=None, hermitian=False, *, rtol=<no value>)\n\nCompute the (Moore-Penrose) pseudo-inverse of a matrix.\n\nCalculate the generalized inverse of a matrix using its singular-value decomposition (SVD) and including all large singular values.\n\nParameters:\na : (..., M, N) array_like\n    Matrix or stack of matrices to be pseudo-inverted.\nrcond : (...) array_like of float, optional\n    Cutoff for small singular values. Singular values less than or equal to `rcond * largest_singular_value` are set to zero. Broadcasts against the stack of matrices. Default: `1e-15`.\nhermitian : bool, optional\n    If True, a is assumed to be Hermitian (symmetric if real-valued), enabling a more efficient method for finding singular values. Defaults to False.\nrtol : (...) array_like of float, optional\n    Same as rcond, but it's an Array API compatible parameter name. Only rcond or rtol can be set at a time. If none of them are provided then NumPy's `1e-15` default is used. If `rtol=None` is passed then the API standard default is used.\n    New in version 2.0.0.\n\nReturns:\nB : (..., N, M) ndarray\n    The pseudo-inverse of a. If a is a matrix instance, then so is B.\n\nRaises:\nLinAlgError\n    If the SVD computation does not converge.\n\nSee also:\nscipy.linalg.pinv : Similar function in SciPy.\nscipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a Hermitian matrix.\n\nNotes:\nThe pseudo-inverse of a matrix A, denoted A^+, is defined as: \"the matrix that 'solves' [the least-squares problem] Ax = b,\" i.e., if x\u0304 is said solution, then A^+ is that matrix such that x\u0304 = A^+b.\n\nIt can be shown that if Q\u2081 \u03a3 Q\u2082\u1d40 = A is the singular value decomposition of A, then A^+ = Q\u2082 \u03a3^+ Q\u2081\u1d40, where Q\u2081,\u2082 are orthogonal matrices, \u03a3 is a diagonal matrix consisting of A's so-called singular values, (followed, typically, by zeros), and then \u03a3^+ is simply the diagonal matrix consisting of the reciprocals of A's singular values (again, followed by zeros).\n\nReferences:\n[1] G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pp. 139-142.\n\nExamples:\nThe following example checks that `a * a+ * a == a` and `a+ * a * a+ == a+`:\n\n>>> import numpy as np\n>>> rng = np.random.default_rng()\n>>> a = rng.normal(size=(9, 6))\n>>> B = np.linalg.pinv(a)\n>>> np.allclose(a, np.dot(a, np.dot(B, a)))\nTrue\n>>> np.allclose(B, np.dot(B, np.dot(a, B)))\nTrue",
          "code": "def pinv(a, rcond=None, hermitian=False, *, rtol=_NoValue):\n    a, wrap = _makearray(a)\n    if rcond is not None and rtol is not _NoValue:\n        raise ValueError(\"Only one of {rcond, rtol} can be specified.\")\n    if rtol is not _NoValue:\n        rcond = rtol\n    elif rcond is None:\n        rcond = 1e-15\n\n    _assert_stacked_2d(a)\n    t, result_t = _commonType(a)\n\n    if hermitian:\n        # For Hermitian matrices, use eigenvalue decomposition\n        # which is more efficient than SVD\n        u, s, vt = eigvals(a, hermitian=True)\n        cutoff = rcond * np.maximum.reduce(s, axis=-1, keepdims=True)\n        large_s = s > cutoff\n        s = np.where(large_s, 1 / s, 0)\n        res = matmul(u * s[..., None, :], u.swapaxes(-2, -1).conj())\n    else:\n        # Use SVD for general matrices\n        u, s, vt = svd(a, full_matrices=False, hermitian=False)\n        cutoff = rcond * np.maximum.reduce(s, axis=-1, keepdims=True)\n        large_s = s > cutoff\n        s = np.divide(1, s, out=s, where=large_s)\n        s[~large_s] = 0\n        res = matmul(vt.swapaxes(-2, -1) * s[..., None, :], u.swapaxes(-2, -1).conj())\n\n    return wrap(res.astype(result_t, copy=False))"
        }
      },
      {
        "name": "numpy.linalg.tensorinv",
        "description": "Compute the 'inverse' of an N-dimensional array",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.tensorinv.html",
        "add": {
          "doc": "linalg.tensorinv(a, ind=2)\n\nCompute the 'inverse' of an N-dimensional array.\n\nThe result is an inverse for `a` relative to the tensordot operation `tensordot(a, b, ind)`, i. e., up to floating-point accuracy, `tensordot(tensorinv(a), a, ind)` is the \"identity\" tensor for the tensordot operation.\n\nParameters:\na : array_like\n    Tensor to 'invert'. Its shape must be 'square', i. e., `prod(a.shape[:ind]) == prod(a.shape[ind:])`.\nind : int, optional\n    Number of first indices that are involved in the inverse sum. Must be a positive integer, default is 2.\n\nReturns:\nb : ndarray\n    a's tensordot inverse, shape `a.shape[ind:] + a.shape[:ind]`.\n\nRaises:\nLinAlgError\n    If a is singular or not 'square' (in the above sense).\n\nSee also:\nnumpy.tensordot, tensorsolve\n\nExamples:\n>>> import numpy as np\n>>> a = np.eye(4*6)\n>>> a.shape = (4, 6, 8, 3)\n>>> ainv = np.linalg.tensorinv(a, ind=2)\n>>> ainv.shape\n(8, 3, 4, 6)\n>>> rng = np.random.default_rng()\n>>> b = rng.normal(size=(4, 6))\n>>> np.allclose(np.tensordot(ainv, b), np.linalg.tensorsolve(a, b))\nTrue\n\n>>> a = np.eye(4*6)\n>>> a.shape = (24, 8, 3)\n>>> ainv = np.linalg.tensorinv(a, ind=1)\n>>> ainv.shape\n(8, 3, 24)\n>>> rng = np.random.default_rng()\n>>> b = rng.normal(size=24)\n>>> np.allclose(np.tensordot(ainv, b, 1), np.linalg.tensorsolve(a, b))\nTrue",
          "code": "def tensorinv(a, ind=2):\n    a = asarray(a)\n    oldshape = a.shape\n    prod = 1\n    if ind > 0:\n        invshape = oldshape[ind:] + oldshape[:ind]\n        for k in oldshape[ind:]:\n            prod *= k\n    else:\n        raise ValueError(\"Invalid ind argument.\")\n    a = a.reshape(prod, -1)\n    ia = inv(a)\n    return ia.reshape(*invshape)"
        }
      }
    ],
    "other_operations": [
      {
        "name": "numpy.diagonal",
        "description": "Return specified diagonals from an array",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.diagonal.html",
        "add": {
          "doc": "numpy.diagonal(a, offset=0, axis1=0, axis2=1)\n\nReturn specified diagonals.\n\nIf a is 2-D, returns the diagonal of a with the given offset, i.e., the collection of elements of the form `a[i, i+offset]`. If a has more than two dimensions, then the axes specified by axis1 and axis2 are used to determine the 2-D sub-array whose diagonal is returned. The shape of the resulting array can be determined by removing axis1 and axis2 and appending an index to the right equal to the size of the resulting diagonals.\n\nIn versions of NumPy prior to 1.7, this function always returned a new, independent array containing a copy of the values in the diagonal.\n\nIn NumPy 1.7 and 1.8, it continues to return a copy of the diagonal, but depending on this fact is deprecated. Writing to the resulting array continues to work as it used to, but a FutureWarning is issued.\n\nStarting in NumPy 1.9 it returns a read-only view on the original array. Attempting to write to the resulting array will produce an error.\n\nIn some future release, it will return a read/write view and writing to the returned array will alter your original array. The returned array will have the same type as the input array.\n\nParameters:\n----------\na : array_like\n    Array from which the diagonals are taken.\noffset : int, optional\n    Offset of the diagonal from the main diagonal. Can be positive or negative. Defaults to main diagonal (0).\naxis1 : int, optional\n    Axis to be used as the first axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to first axis (0).\naxis2 : int, optional\n    Axis to be used as the second axis of the 2-D sub-arrays from which the diagonals should be taken. Defaults to second axis (1).\n\nReturns:\n-------\narray_of_diagonals : ndarray\n    If a is 2-D, then a 1-D array containing the diagonal and of the same type as a is returned unless a is a matrix, in which case a 1-D array rather than a (2-D) matrix is returned in order to maintain backward compatibility.\n    If `a.ndim > 2`, then the dimensions specified by axis1 and axis2 are removed, and a new axis inserted at the end corresponding to the diagonal.\n\nRaises:\n------\nValueError\n    If the dimension of a is less than 2.\n\nSee Also:\n--------\ndiag : MATLAB work-a-like for 1-D and 2-D arrays.\ndiagflat : Create diagonal arrays.\ntrace : Sum along diagonals.\n\nExamples:\n--------\n>>> import numpy as np\n>>> a = np.arange(4).reshape(2,2)\n>>> a\narray([[0, 1],\n       [2, 3]])\n>>> a.diagonal()\narray([0, 3])\n>>> a.diagonal(1)\narray([1])\n\nA 3-D example:\n>>> a = np.arange(8).reshape(2,2,2); a\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n>>> a.diagonal(0, 0, 1)\narray([[0, 6],\n       [1, 7]])",
          "code": "# Source code location: numpy/_core/fromnumeric.py#L1694-L1822\n# Note: This is a wrapper function that calls the actual implementation\n# The actual implementation is in the C code and ndarray methods\n# Source code not directly accessible due to GitHub access restrictions"
        }
      },
      {
        "name": "numpy.linalg.diagonal",
        "description": "Returns specified diagonals of a matrix (or a stack of matrices) - Array API compatible",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.diagonal.html",
        "add": {
          "doc": "linalg.diagonal(x, /, *, offset=0)\n\nReturns specified diagonals of a matrix (or a stack of matrices) `x`.\n\nThis function is Array API compatible, contrary to numpy.diagonal, the matrix is assumed to be defined by the last two dimensions.\n\nParameters:\n----------\nx : (...,M,N) array_like\n    Input array having shape (..., M, N) and whose innermost two dimensions form MxN matrices.\noffset : int, optional\n    Offset specifying the off-diagonal relative to the main diagonal, where:\n    - offset = 0: the main diagonal.\n    - offset > 0: off-diagonal above the main diagonal.\n    - offset < 0: off-diagonal below the main diagonal.\n\nReturns:\n-------\nout : (...,min(N,M)) ndarray\n    An array containing the diagonals and whose shape is determined by removing the last two dimensions and appending a dimension equal to the size of the resulting diagonals. The returned array must have the same data type as `x`.\n\nSee Also:\n--------\nnumpy.diagonal\n\nExamples:\n--------\n>>> a = np.arange(4).reshape(2, 2); a\narray([[0, 1],\n       [2, 3]])\n>>> np.linalg.diagonal(a)\narray([0, 3])\n\nA 3-D example:\n>>> a = np.arange(8).reshape(2, 2, 2); a\narray([[[0, 1],\n        [2, 3]],\n       [[4, 5],\n        [6, 7]]])\n>>> np.linalg.diagonal(a)\narray([[0, 3],\n       [4, 7]])\n\nDiagonals adjacent to the main diagonal can be obtained by using the offset argument:\n>>> a = np.arange(9).reshape(3, 3)\n>>> a\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n>>> np.linalg.diagonal(a, offset=1)  # First superdiagonal\narray([1, 5])\n>>> np.linalg.diagonal(a, offset=-1)  # First subdiagonal\narray([3, 7])",
          "code": "# Source code location: numpy/linalg/_linalg.py#L3089-L3176\n# Note: Source code not directly accessible due to GitHub access restrictions\n# This function is part of the Array API standard implementation"
        }
      },
      {
        "name": "numpy.linalg.matrix_transpose",
        "description": "Transposes a matrix (or a stack of matrices) - Array API compatible",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix%5Ftranspose.html",
        "add": {
          "doc": "linalg.matrix_transpose(x, /)\n\nTransposes a matrix (or a stack of matrices) `x`.\n\nThis function is Array API compatible.\n\nParameters:\n----------\nx : array_like\n    Input array having shape (..., M, N) and whose two innermost dimensions form `MxN` matrices.\n\nReturns:\n-------\nout : ndarray\n    An array containing the transpose for each matrix and having shape (..., N, M).\n\nSee Also:\n--------\ntranspose : Generic transpose method.\n\nNotes:\n-----\nThis function is an alias of numpy.matrix_transpose.\n\nExamples:\n--------\n>>> import numpy as np\n>>> np.matrix_transpose([[1, 2], [3, 4]])\narray([[1, 3],\n       [2, 4]])\n\n>>> np.matrix_transpose([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\narray([[[1, 3],\n        [2, 4]],\n       [[5, 7],\n        [6, 8]]])",
          "code": "# Source code location: numpy/linalg/_linalg.py#L3452-L3454\n# Note: This is a very simple wrapper function (only 3 lines)\n# def matrix_transpose(x, /):\n#     return numpy.matrix_transpose(x)\n# Source code not directly accessible due to GitHub access restrictions"
        }
      },
      {
        "name": "numpy.linalg.LinAlgError",
        "description": "Generic Python-exception-derived object raised by linalg functions",
        "url": "https://numpy.org/doc/stable/reference/generated/numpy.linalg.LinAlgError.html",
        "add": {
          "doc": "exception linalg.LinAlgError\n\nGeneric Python-exception-derived object raised by linalg functions.\n\nGeneral purpose exception class, derived from Python's ValueError class, programmatically raised in linalg functions when a Linear Algebra-related condition would prevent further correct execution of the function.\n\nParameters:\n----------\nNone\n\nExamples:\n--------\n>>> from numpy import linalg as LA\n>>> LA.inv(np.zeros((2,2)))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"...linalg.py\", line 350,\n    in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))\n  File \"...linalg.py\", line 249,\n    in solve\n    raise LinAlgError('Singular matrix')\nnumpy.linalg.LinAlgError: Singular matrix",
          "code": "# Source code location: numpy/linalg/_linalg.py#L132-L159\n# This is a simple exception class that inherits from ValueError\n# class LinAlgError(ValueError):\n#     \"\"\"\n#     Generic Python-exception-derived object raised by linalg functions.\n#     \n#     General purpose exception class, derived from Python's ValueError\n#     class, programmatically raised in linalg functions when a Linear\n#     Algebra-related condition would prevent further correct execution of\n#     the function.\n#     \n#     Parameters\n#     ----------\n#     None\n#     \n#     Examples\n#     --------\n#     >>> from numpy import linalg as LA\n#     >>> LA.inv(np.zeros((2,2)))\n#     Traceback (most recent call last):\n#       File \"<stdin>\", line 1, in <module>\n#       File \"...linalg.py\", line 350,\n#         in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))\n#       File \"...linalg.py\", line 249,\n#         in solve\n#         raise LinAlgError('Singular matrix')\n#     numpy.linalg.LinAlgError: Singular matrix\n#     \n#     \"\"\"\n#     def __init__(self, *args):\n#         ValueError.__init__(self, *args)\n# Source code not directly accessible due to GitHub access restrictions"
        }
      }
    ]
  },
  "metadata": {
    "total_functions": 46,
    "categories": 6,
    "generated_date": "2025-06-30",
    "source": "NumPy v2.3 documentation and GitHub repository"
  }
}