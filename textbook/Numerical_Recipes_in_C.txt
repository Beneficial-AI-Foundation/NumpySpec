Numerical Recipes in C

The Art of Scientific Computing
Second Edition

William H. Press

Harvard-Smithsonian Center for Astrophysics

Saul A. Teukolsky

Department of Physics, Cornell University

William T. Vetterling

Polaroid Corporation

OMA YUON) €ZpZ-228-008

}ayNdwod Ja/uas Aue Oo}
lo swei6olg ‘ssalg Aysuaniuy eBpuquueg Aq 2661-8861 (D) 1yBUAdOD

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

Brian P. Flannery
EXXON Research and Engineering Company

“aremyos sedicay jeouewnn Aq 2661-8861 (O)

UY YON apis}no) Bio‘ebpuquies@AsasjsnojoeuIp OF
‘SIWOHGD 40 syoog sedio:

CAMBRIDGE UNIVERSITY PRESS
Cambridge New York Port Chester Melbourne Sydney

Published by the Press Syndicate of the University of Cambridge
The Pitt Building, Trumpington Street, Cambridge CB2 1RP

40 West 20th Street, New York, NY 10011-4211, USA

477 Williamstown Road, Port Melbourne, VIC, 3207, Australia

Copyright © Cambridge University Press 1988, 1992

except for 813.10 and Appendix B, which are placed into the public domain,
and except for all other computer programs and procedures, which are
Copyright © Numerical Recipes Software 1987, 1988, 1992, 1997, 2002
All Rights Reserved.

Some sections of this book were originally published, in different form, in Computers
in Physics magazine, Copyright © American Institute of Physics, 1988-1992.

First Edition originally published 1988; Second Edition originally published 1992.
Reprinted with corrections, 1993, 1994, 1995, 1997, 2002.
This reprinting is corrected to software version 2.10

Printed in the United States of America
Typeset in TEX

Without an additional license to use the contained software, this book is intended aq
a text and reference book, for reading purposes only. A free license for limited use of the}
software by the individual owner of a copy of this book who personally types one or more|
routines into a single computer is granted under terms described on p. xvii. See the sectior
“License Information” (pp. xvi-xviii) for information on obtaining more general licenses|
at low cost.

Machine-readable media containing the software in this book, with included licenses
for use on a single screen, are available from Cambridge University Press. See the|
order form at the back of the book, email to “orders@cup.org” (North America) o1
“directcustserv@cambridge.org” (rest of world), or write to Cambridge University Press.
110 Midland Avenue, Port Chester, NY 10573 (USA), for further information.

The software may also be downloaded, with immediate purchase of a license
also possible, from the Numerical Recipes Software Web Site (http: //www.nr.com)|
Unlicensed transfer of Numerical Recipes programs to any other format, or to any|
computer except one that is specifically licensed, is strictly prohibited. Technical questions
corrections, and requests for information should be addressed to Numerical Recipes|
Software, P.O. Box 380243, Cambridge, MA 02238-0243 (USA), email “info@nr.com”,
or fax 781 863-1739.

Library of Congress Cataloging in Publication Data

Numerical recipes in C : the art of scientific computing / William H. Press

. fet al.J. -— 2nd ed.

Includes bibliographical references (p. ) and index.

ISBN 0-521-43108-5

1. Numerical analysi:
3. C (Computer program language) I. Press, William H.
QA297.N866 1992
519.4'0285'53-de20 92-8876

A catalog record for this book is available from the British Library.

ISBN 0 521 431085 Book

ISBN 0 521 43720 2. Example book in C

ISBN 0 521 75037 7 C/C++ CDROM (Windows/Macintosh)
ISBN 0 521 75035 0 Complete CDROM (Windows/Macintosh)
ISBN 0 521 75036 9 Complete CDROM (UNIX/Linux)

s—Computer programs. 2. Science-Mathematics—Computer programs.

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'SGOM IISIA ‘SWWOHGD 40 Sy00q sedivay jEOUAWINN Japs0 OF “payiqiyoud Ajolys si ‘wayndwios Ja/as Aue 0} (auUO sy} Buipnjoul) s:
-aulyoeul jo BulAdoo Aue Jo ‘uolonpoidas sayjin4 ‘“asn jeuosiad UMO JIAaU} 10} Adoo Jaded auo ayew O} Suasn JeUJE\U! 10} payuRJB S|

“aremyog sedioay /eouewnn Aq 2661-886} (9) 1UBUAdOD swWesBoIg ‘ssalq AysianlU eBpUIquIeD Aq 2661-8861 (9) 1YBUAdOD

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

Contents

Preface to the Second Edition xi
Preface to the First Edition xiv
License Information xvi
Computer Programs by Chapter and Section xix
Preliminaries 1
1.0 Introduction 1
1.1 Program Organization and Control Structures 5
1.2 Some C Conventions for Scientific Computing 15
1.3 Error, Accuracy, and Stability 28
Solution of Linear Algebraic Equations 32
2.0 Introduction 32
2.1 Gauss-Jordan Elimination 36
2.2 Gaussian Elimination with Backsubstitution 41
2.3 LU Decomposition and Its Applications 43
2.4 Tridiagonal and Band Diagonal Systems of Equations 50
2.5 Iterative Improvement of a Solution to Linear Equations 55
2.6 Singular Value Decomposition 59
2.7 Sparse Linear Systems 71
2.8 Vandermonde Matrices and Toeplitz Matrices 90
2.9 Cholesky Decomposition 96
2.10 QR Decomposition 98
2.11 Is Matrix Inversion an N° Process? 102
Interpolation and Extrapolation 105
3.0 Introduction 105
3.1 Polynomial Interpolation and Extrapolation 108
3.2 Rational Function Interpolation and Extrapolation 111
3.3 Cubic Spline Interpolation 113
3.4 How to Search an Ordered Table 117
3.5 Coefficients of the Interpolating Polynomial 120
3.6 Interpolation in Two or More Dimensions 123

vU
g
3

S
B
Q
D
om
oO
2
a
=
g
i
2
S

z
4

z
=
=
=
3
Q
8
3
2
8
2
&
8
8
&
&
iM
x
FS
8
Z
E
a
3
>
3
g
g
3
3
2
Z

SI ‘“ayndwod JeMas Aue 0} (9U0 SI

aC

*(eoLeWY YON episyno) Bio‘eBpuquied@AJesysno}a1Ip 0} |eWa Pues JO

BJISGOM SIA ‘SWIOHGO 10 Syoog sedioay jeOUOWINN JapPso O| “payiqiyoud Aj 1
-auiyoeul jo BulAdoo Aue Jo ‘uolonpoidas Jayyin4 ‘asn jeuosiad UMO sau} 410} Adoo saded auo ayew 0} Siasn JaUEU! 10} payueB SI

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUOWNN Aq Z661-8B6l (D) IYBUAdOD swesBOlY “‘ssalg AysIaAIUN EBpLqueD Aq 2661-8861 (D) jyBUAdOD

vi

Contents

Integration of Functions

4.0 Introduction

4.1 Classical Formulas for Equally Spaced Abscissas
4.2 Elementary Algorithms

4.3 Romberg Integration

4.4 Improper Integrals

4.5 Gaussian Quadratures and Orthogonal Polynomials
4.6 Multidimensional Integrals

Evaluation of Functions

5.0 Introduction

5.1 Series and Their Convergence

5.2 Evaluation of Continued Fractions

5.3 Polynomials and Rational Functions

5.4 Complex Arithmetic

5.5 Recurrence Relations and Clenshaw’s Recurrence Formula
5.6 Quadratic and Cubic Equations

5.7 Numerical Derivatives

5.8 Chebyshev Approximation

5.9 Derivatives or Integrals of a Chebyshev-approximated Function
5.10 Polynomial Approximation from Chebyshev Coefficients
5.11 Economization of Power Series

5.12 Padé Approximants

5.13 Rational Chebyshev Approximation

5.14 Evaluation of Functions by Path Integration

Special Functions

6.0 Introduction

6.1 Gamma Function, Beta Function, Factorials, Binomial Coefficients

6.2 Incomplete Gamma Function, Error Function, Chi-Square
Probability Function, Cumulative Poisson Function

6.3 Exponential Integrals

6.4 Incomplete Beta Function, Student’s Distribution, F-Distribution,
Cumulative Binomial Distribution

6.5 Bessel Functions of Integer Order

6.6 Modified Bessel Functions of Integer Order

6.7 Bessel Functions of Fractional Order, Airy Functions, Spherical
Bessel Functions

6.8 Spherical Harmonics

6.9 Fresnel Integrals, Cosine and Sine Integrals

6.10 Dawson’s Integral

6.11 Elliptic Integrals and Jacobian Elliptic Functions

6.12 Hypergeometric Functions

Random Numbers

7.0 Introduction
7.1 Uniform Deviates

129

129
130
136
140
141
147
161

165

165
65
69
73
76
178
83
86
90
95
197
98
200
204
208

212

212
213

216
222

226
230
236

240
252
255
259
261
271

274

274
275

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
8
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uonjonpoides seyyin4 “asn yeuosied uMO

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

Contents

Vii

10

11

7.2 Transformation Method: Exponential and Normal Deviates
7.3 Rejection Method: Gamma, Poisson, Binomial Deviates
7A Generation of Random Bits

7.5 Random Sequences Based on Data Encryption

7.6 Simple Monte Carlo Integration

7.7 Quasi- (that is, Sub-) Random Sequences

7.8 Adaptive and Recursive Monte Carlo Methods

Sorting

8.0 Introduction

8.1 Straight Insertion and Shell’s Method
8.2 Quicksort

8.3 Heapsort

8.4 Indexing and Ranking

8.5 Selecting the /th Largest

8.6 Determination of Equivalence Classes

Root Finding and Nonlinear Sets of Equations

9.0 Introduction

9.1 Bracketing and Bisection

9.2 Secant Method, False Position Method, and Ridders’ Method

9.3 Van Wijngaarden—Dekker—Brent Method

9.4 Newton-Raphson Method Using Derivative

9.5 Roots of Polynomials

9.6 Newton-Raphson Method for Nonlinear Systems of Equations

9.7 Globally Convergent Methods for Nonlinear Systems of Equations

Minimization or Maximization of Functions

10.0 Introduction

10.1 Golden Section Search in One Dimension

10.2 Parabolic Interpolation and Brent’s Method in One Dimension
10.3 One-Dimensional Search with First Derivatives

10.4 Downhill Simplex Method in Multidimensions

10.5 Direction Set (Powell’s) Methods in Multidimensions

10.6 Conjugate Gradient Methods in Multidimensions

10.7 Variable Metric Methods in Multidimensions

10.8 Linear Programming and the Simplex Method

10.9 Simulated Annealing Methods

Eigensystems

11.0 Introduction

11.1 Jacobi Transformations of a Symmetric Matrix

11.2 Reduction of a Symmetric Matrix to Tridiagonal Form:
Givens and Householder Reductions

11.3 Eigenvalues and Eigenvectors of a Tridiagonal Matrix

11.4 Hermitian Matrices

11.5 Reduction of a General Matrix to Hessenberg Form

287
290
296
300
304
309
316

329

329
330
332
336
338
341
345

347

347
350
354
359
362
369
379
383

394

394
397
402
405
408
412
420
425
430
444

456

456
463

469
475
481
482

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
8
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uonjonpoides seyyin4 “asn yeuosied uMO

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Viii

Contents

12

13

14

15

11.6 The QR Algorithm for Real Hessenberg Matrices
11.7 Improving Eigenvalues and/or Finding Eigenvectors by
Inverse Iteration

Fast Fourier Transform

12.0 Introduction

12.1 Fourier Transform of Discretely Sampled Data

12.2 Fast Fourier Transform (FFT)

12.3 FFT of Real Functions, Sine and Cosine Transforms
12.4 FFT in Two or More Dimensions

12.5 Fourier Transforms of Real Data in Two and Three Dimensions

12.6 External Storage or Memory-Local FFTs

Fourier and Spectral Applications

13.0 Introduction

13.1 Convolution and Deconvolution Using the FFT

13.2 Correlation and Autocorrelation Using the FFT

13.3 Optimal (Wiener) Filtering with the FFT

13.4 Power Spectrum Estimation Using the FFT

13.5 Digital Filtering in the Time Domain

13.6 Linear Prediction and Linear Predictive Coding

13.7 Power Spectrum Estimation by the Maximum Entropy
(All Poles) Method

13.8 Spectral Analysis of Unevenly Sampled Data

13.9 Computing Fourier Integrals Using the FFT

13.10 Wavelet Transforms

13.11 Numerical Use of the Sampling Theorem

Statistical Description of Data

14.0 Introduction
14.1 Moments of a Distribution: Mean, Variance, Skewness,
and So Forth

14.2 Do Two Distributions Have the Same Means or Variances?

14.3 Are Two Distributions Different?

14.4 Contingency Table Analysis of Two Distributions
14.5 Linear Correlation

14.6 Nonparametric or Rank Correlation

14.7 Do Two-Dimensional Distributions Differ?

14.8 Savitzky-Golay Smoothing Filters

Modeling of Data

15.0 Introduction

15.1 Least Squares as a Maximum Likelihood Estimator
15.2 Fitting Data to a Straight Line

15.3 Straight-Line Data with Errors in Both Coordinates
15.4 General Linear Least Squares

15.5 Nonlinear Models

486
493

496

496
500
504
510
521
525
532

537

537
538
545
547
549
558
564

572
575
584
591
606

609
609

610
615
620
628
636
639
645
650

656

656
657
661
666
671
681

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
8
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uonjonpoides seyyin4 “asn yeuosied uMO

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

Contents

ix

16

17

18

19

20

15.6 Confidence Limits on Estimated Model Parameters
15.7 Robust Estimation

Integration of Ordinary Differential Equations

16.0 Introduction

16.1 Runge-Kutta Method

16.2 Adaptive Stepsize Control for Runge-Kutta

16.3 Modified Midpoint Method

16.4 Richardson Extrapolation and the Bulirsch-Stoer Method
16.5 Second-Order Conservative Equations

16.6 Stiff Sets of Equations

16.7 Multistep, Multivalue, and Predictor-Corrector Methods

Two Point Boundary Value Problems

17.0 Introduction

17.1 The Shooting Method

17.2 Shooting to a Fitting Point

17.3 Relaxation Methods

17.4 A Worked Example: Spheroidal Harmonics

17.5 Automated Allocation of Mesh Points

17.6 Handling Internal Boundary Conditions or Singular Points

Integral Equations and Inverse Theory

18.0 Introduction

18.1 Fredholm Equations of the Second Kind

18.2 Volterra Equations

18.3 Integral Equations with Singular Kernels

18.4 Inverse Problems and the Use of A Priori Information
18.5 Linear Regularization Methods

18.6 Backus-Gilbert Method

18.7 Maximum Entropy Image Restoration

Partial Differential Equations

19.0 Introduction

19.1 Flux-Conservative Initial Value Problems

19.2 Diffusive Initial Value Problems

19.3 Initial Value Problems in Multidimensions

19.4 Fourier and Cyclic Reduction Methods for Boundary
Value Problems

19.5 Relaxation Methods for Boundary Value Problems

19.6 Multigrid Methods for Boundary Value Problems

Less-Numerical Algorithms

20.0 Introduction
20.1 Diagnosing Machine Parameters
20.2 Gray Codes

689
699

707

707
710
714
722
724
732
734
TAT

753

753
757
760
762
7712
783
784

788

788
791
794
797
804
808
815
818

827

827
834
847
853

857
863
871

889

889
889
894

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
8
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uonjonpoides seyyin4 “asn yeuosied uMO

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

Contents

896
903

20.3 Cyclic Redundancy and Other Checksums
20.4 Huffman Coding and Compression of Data

20.5 Arithmetic Coding

910
915

20.6 Arithmetic at Arbitrary Precision

926

References

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

930
940
948
951
965

Appendix A: Table of Prototype Declarations
Appendix B: Utility Routines

Appendix C: Complex Arithmetic

Index of Programs and Dependencies

General Index

Preface to the Second Edition

Our aim in writing the original edition of Numerical Recipes was to provide a
book that combined general discussion, analytical mathematics, algorithmics, and
actual working programs. The success of the first edition puts us now in a difficult,
though hardly unenviable, position. We wanted, then and now, to write a book
that is informal, fearlessly editorial, unesoteric, and above all useful. There is a
danger that, if we are not careful, we might produce a second edition that is weighty,
balanced, scholarly, and boring.
It is a mixed blessing that we know more now than we did six years ago. Then,
we were making educated guesses, based on existing literature and our own research,
about which numerical techniques were the most important and robust. Now, we have
the benefit of direct feedback from a large reader community. Letters to our alter-ego
enterprise, Numerical Recipes Software, are in the thousands per year. (Please, don’t
telephone us.) Our post office box has become a magnet for letters pointing out
that we have omitted some particular technique, well known to be important in a
particular field of science or engineering. We value such letters, and digest them
carefully, especially when they point us to specific references in the literature.
The inevitable result of this input is that this Second Edition of Numerical
Recipes is substantially larger than its predecessor, in fact about 50% larger both in
words and number of included programs (the latter now numbering well over 300).
“Don’t let the book grow in size,” is the advice that we received from several wise
colleagues. We have tried to follow the intended spirit of that advice, even as we
violate the letter of it. We have not lengthened, or increased in difficulty, the book’s
principal discussions of mainstream topics. Many new topics are presented at this
same accessible level. Some topics, both from the earlier edition and new to this
one, are now set in smaller type that labels them as being “advanced.” The reader
who ignores such advanced sections completely will not, we think, find any lack of
continuity in the shorter volume that results.
Here are some highlights of the new material in this Second Edition:
e anew chapter on integral equations and inverse methods
e a detailed treatment of multigrid methods for solving elliptic partial
differential equations
routines for band diagonal linear systems
improved routines for linear algebra on sparse matrices
Cholesky and QR decomposition
orthogonal polynomials and Gaussian quadratures for arbitrary weight
functions
methods for calculating numerical derivatives
e Padé approximants, and rational Chebyshev approximation
e Bessel functions, and modified Bessel functions, of fractional order; and
several other new special functions

e improved random number routines

© quasi-random sequences

e routines for adaptive and recursive Monte Carlo integration in high-
dimensional spaces

e globally convergent methods for sets of nonlinear equations

xi

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

xii Preface to the Second Edition

simulated annealing minimization for continuous control spaces

fast Fourier transform (FFT) for real data in two and three dimensions
fast Fourier transform (FFT) using external storage

improved fast cosine transform routines

wavelet transforms

Fourier integrals with upper and lower limits

spectral analysis on unevenly sampled data

Savitzky-Golay smoothing filters

fitting straight line data with errors in both coordinates

a two-dimensional Kolmogorov-Smirnoff test

the statistical bootstrap method

embedded Runge-Kutta-Fehlberg methods for differential equations
high-order methods for stiff differential equations

a new chapter on “less-numerical” algorithms, including Huffman and
arithmetic coding, arbitrary precision arithmetic, and several other topics.
Consult the Preface to the First Edition, following, or the Table of Contents, for a
list of the more “basic” subjects treated.

Acknowledgments

It is not possible for us to list by name here all the readers who have made
useful suggestions; we are grateful for these. In the text, we attempt to give specific
attribution for ideas that appear to be original, and not known in the literature. We
apologize in advance for any omissions.

Some readers and colleagues have been particularly generous in providing
us with ideas, comments, suggestions, and programs for this Second Edition.
We especially want to thank George Rybicki, Philip Pinto, Peter Lepage, Robert
Lupton, Douglas Eardley, Ramesh Narayan, David Spergel, Alan Oppenheim, Sallie
Baliunas, Scott Tremaine, Glennys Farrar, Steven Block, John Peacock, Thomas
Loredo, Matthew Choptuik, Gregory Cook, L. Samuel Finn, P. Deuflhard, Harold
Lewis, Peter Weinberger, David Syer, Richard Ferch, Steven Ebstein, Bradley
Keister, and William Gould. We have been helped by Nancy Lee Snyder’s mastery
of a complicated TEX manuscript. We express appreciation to our editors Lauren
Cowles and Alan Harvey at Cambridge University Press, and to our production editor
Russell Hahn. We remain, of course, grateful to the individuals acknowledged in
the Preface to the First Edition.

Special acknowledgment is due to programming consultant Seth Finkelstein,
who wrote, rewrote, or influenced many of the routines in this book, as well as in
its FORTRAN-language twin and the companion Example books. Our project has
benefited enormously from Seth’s talent for detecting, and following the trail of, even
very slight anomalies (often compiler bugs, but occasionally our errors), and from
his good programming sense. To the extent that this edition of Numerical Recipes
in C has a more graceful and “C-like” programming style than its predecessor, most
of the credit goes to Seth. (Of course, we accept the blame for the FORTRANish
lapses that still remain.)

We prepared this book for publication on DEC and Sun workstations run-
ning the UNIX operating system, and on a 486/33 PC compatible running
MS-DOS 5.0/Windows 3.0. (See §1.0 for a list of additional computers used in

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Preface to the Second Edition xiii

program tests.) We enthusiastically recommend the principal software used: GNU
Emacs, TgX, Perl, Adobe Illustrator, and PostScript. Also used were a variety of C
compilers — too numerous (and sometimes too buggy) for individual acknowledg-
ment. It is a sobering fact that our standard test suite (exercising all the routines
in this book) has uncovered compiler bugs in many of the compilers tried. When
possible, we work with developers to see that such bugs get fixed; we encourage
interested compiler developers to contact us about such arrangements.

WHP and SAT acknowledge the continued support of the U.S. National Science
Foundation for their research on computational methods. D.A.R.P.A. support is
acknowledged for §13.10 on wavelets.

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

June, 1992 William H. Press
Saul A. Teukolsky
William T. Vetterling
Brian P. Flannery

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


Preface to the First Edition

We call this book Numerical Recipes for several reasons. In one sense, this book
is indeed a “cookbook” on numerical computation. However there is an important
distinction between a cookbook and a restaurant menu. The latter presents choices
among complete dishes in each of which the individual flavors are blended and
disguised. The former — and this book — reveals the individual ingredients and
explains how they are prepared and combined.

Another purpose of the title is to connote an eclectic mixture of presentational
techniques. This book is unique, we think, in offering, for each topic considered,
a certain amount of general discussion, a certain amount of analytical mathematics,
a certain amount of discussion of algorithmics, and (most important) actual imple-
mentations of these ideas in the form of working computer routines. Our task has
been to find the right balance among these ingredients for each topic. You will
find that for some topics we have tilted quite far to the analytic side; this where we
have felt there to be gaps in the “standard” mathematical training. For other topics,
where the mathematical prerequisites are universally held, we have tilted towards
more in-depth discussion of the nature of the computational algorithms, or towards
practical questions of implementation.

We admit, therefore, to some unevenness in the “level” of this book. About half
of it is suitable for an advanced undergraduate course on numerical computation for
science or engineering majors. The other half ranges from the level of a graduate
course to that of a professional reference. Most cookbooks have, after all, recipes at
varying levels of complexity. An attractive feature of this approach, we think, is that
the reader can use the book at increasing levels of sophistication as his/her experience
grows. Even inexperienced readers should be able to use our most advanced routines
as black boxes. Having done so, we hope that these readers will subsequently go
back and learn what secrets are inside.

If there is a single dominant theme in this book, it is that practical methods
of numerical computation can be simultaneously efficient, clever, and — important
— clear. The alternative viewpoint, that efficient computational methods must
necessarily be so arcane and complex as to be useful only in “black box” form,
we firmly reject.

Our purpose in this book is thus to open up a large number of computational
black boxes to your scrutiny. We want to teach you to take apart these black boxes
and to put them back together again, modifying them to suit your specific needs.
We assume that you are mathematically literate, i.e., that you have the normal
mathematical preparation associated with an undergraduate degree in a physical
science, or engineering, or economics, or a quantitative social science. We assume
that you know how to program a computer. We do not assume that you have any
prior formal knowledge of numerical analysis or numerical methods.

The scope of Numerical Recipes is supposed to be “everything up to, but
not including, partial differential equations.” We honor this in the breach: First,
we do have one introductory chapter on methods for partial differential equations
(Chapter 19). Second, we obviously cannot include everything else. All the so-called
“standard” topics of a numerical analysis course have been included in this book:

xiv

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

Preface to the First Edition xv

linear equations (Chapter 2), interpolation and extrapolation (Chaper 3), integration
(Chaper 4), nonlinear root-finding (Chapter 9), eigensystems (Chapter 11), and
ordinary differential equations (Chapter 16). Most of these topics have been taken
beyond their standard treatments into some advanced material which we have felt
to be particularly important or useful.

Some other subjects that we cover in detail are not usually found in the standard
numerical analysis texts. These include the evaluation of functions and of particular
special functions of higher mathematics (Chapters 5 and 6); random numbers and
Monte Carlo methods (Chapter 7); sorting (Chapter 8); optimization, including
multidimensional methods (Chapter 10); Fourier transform methods, including FFT
methods and other spectral methods (Chapters 12 and 13); two chapters on the
statistical description and modeling of data (Chapters 14 and 15); and two-point
boundary value problems, both shooting and relaxation methods (Chapter 17).

The programs in this book are included in ANSI-standard C. Versions of the
book in FORTRAN, Pascal, and BASIC are available separately. We have more
to say about the C language, and the computational environment assumed by our
routines, in §1.1 (Introduction).

Acknowledgments

Many colleagues have been generous in giving us the benefit of their numerical
and computational experience, in providing us with programs, in commenting on
the manuscript, or in general encouragement. We particularly wish to thank George
Rybicki, Douglas Eardley, Philip Marcus, Stuart Shapiro, Paul Horowitz, Bruce
Musicus, Irwin Shapiro, Stephen Wolfram, Henry Abarbanel, Larry Smarr, Richard
Muller, John Bahcall, and A.G.W. Cameron.

We also wish to acknowledge two individuals whom we have never met: Forman
Acton, whose 1970 textbook Numerical Methods that Work (New York: Harper and
Row) has surely left its stylistic mark on us; and Donald Knuth, both for his series
of books on The Art of Computer Programming (Reading, MA: Addison-Wesley),
and for TgX, the computer typesetting language which immensely aided production
of this book.

Research by the authors on computational methods was supported in part by
the U.S. National Science Foundation.

October, 1985 William H. Press
Brian P. Flannery
Saul A. Teukolsky
William T. Vetterling

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

License Information

Read this section if you want to use the programs in this book on a computer.
You’ll need to read the following Disclaimer of Warranty, get the programs onto your
computer, and acquire a Numerical Recipes software license. (Without this license,
which can be the free “immediate license” under terms described below, the book is
intended as a text and reference book, for reading purposes only.)

Disclaimer of Warranty

We make no warranties, express or implied, that the programs contained
in this volume are free of error, or are consistent with any particular standard
of merchantability, or that they will meet your requirements for any particular
application. They should not be relied on for solving a problem whose incorrect
solution could result in injury to a person or loss of property. If you do use the
programs in such a manner, it is at your own risk. The authors and publisher
disclaim all liability for direct or consequential damages resulting from your
use of the programs.

How to Get the Code onto Your Computer

Pick one of the following methods:

e You can type the programs from this book directly into your computer. In this
case, the only kind of license available to you is the free “immediate license”
(see below). You are not authorized to transfer or distribute a machine-readable
copy to any other person, nor to have any other person type the programs into a
computer on your behalf. We do not want to hear bug reports from you if you
choose this option, because experience has shown that virtually all reported bugs
in such cases are typing errors!

e You can download the Numerical Recipes programs electronically from the
Numerical Recipes On-Line Software Store, located at http: //www.nr.com, our
Web site. All the files (Recipes and demonstration programs) are packaged as
a single compressed file. You'll need to purchase a license to download and
unpack them. Any number of single-screen licenses can be purchased instantly
(with discount for multiple screens) from the On-Line Store, with fees that depend
on your operating system (Windows or Macintosh versus Linux or UNIX) and
whether you are affiliated with an educational institution. Purchasing a single-
screen license is also the way to start if you want to acquire a more general (site
or corporate) license; your single-screen cost will be subtracted from the cost of
any later license upgrade.

e You can purchase media containing the programs from Cambridge University Press.
A CD-ROM version in ISO-9660 format for Windows and Macintosh systems
contains the complete C software, and also the C++ version. More extensive CD-
ROMs in ISO-9660 format for Windows, Macintosh, and UNIX/Linux systems are
also available; these include the C, C++, and Fortran versions on a single CD-ROM
(as well as versions in Pascal and BASIC from the first edition). These CD-ROMs
are available with a single-screen license for Windows or Macintosh (order ISBN
0 521 750350), or (at a slightly higher price) with a single-screen license for
UNIX/Linux workstations (order ISBN 0 521 750369). Orders for media from
Cambridge University Press can be placed at 800 872-7423 (North America only)
or by email to orders@cup.org (North America) or directcustserv@cambridge.org
(rest of world). Or, visit the Web site http: //www.cambridge. org.

xvi

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

License Information xvii

Types of License Offered

Here are the types of licenses that we offer. Note that some types are
automatically acquired with the purchase of media from Cambridge University
Press, or of an unlocking password from the Numerical Recipes On-Line Software
Store, while other types of licenses require that you communicate specifically with
Numerical Recipes Software (email: orders@nr.com or fax: 781 863-1739). Our
Web site http: //www.nr.com has additional information.

e [Immediate License”] If you are the individual owner of a copy of this book and
you type one or more of its routines into your computer, we authorize you to use
them on that computer for your own personal and noncommercial purposes. You
are not authorized to transfer or distribute machine-readable copies to any other
person, or to use the routines on more than one machine, or to distribute executable
programs containing our routines. This is the only free license.

e [‘Single-Screen License”] This is the most common type of low-cost license, with
terms governed by our Single Screen (Shrinkwrap) License document (complete
terms available through our Web site). Basically, this license lets you use Numerical
Recipes routines on any one screen (PC, workstation, X-terminal, etc.). You may
also, under this license, transfer pre-compiled, executable programs incorporating
our routines to other, unlicensed, screens or computers, providing that (i) your
application is noncommercial (i.e., does not involve the selling of your program
for a fee), (ii) the programs were first developed, compiled, and successfully run
on a licensed screen, and (iii) our routines are bound into the programs in such a
manner that they cannot be accessed as individual routines and cannot practicably
be unbound and used in other programs. That is, under this license, your program
user must not be able to use our programs as part of a program library or “mix-and-
match” workbench. Conditions for other types of commercial or noncommercial
distribution may be found on our Web site (http: //www.nr.com).

e [“Multi-Screen, Server, Site, and Corporate Licenses”] The terms of the Single
Screen License can be extended to designated groups of machines, defined by
number of screens, number of machines, locations, or ownership. Significant
discounts from the corresponding single-screen prices are available when the
estimated number of screens exceeds 40. Contact Numerical Recipes Software
(email: orders@nr.com or fax: 781 863-1739) for details.

e [Course Right-to-Copy License”] Instructors at accredited educational institutions
who have adopted this book for a course, and who have already purchased a Single
Screen License (either acquired with the purchase of media, or from the Numerical
Recipes On-Line Software Store), may license the programs for use in that course
as follows: Mail your name, title, and address; the course name, number, dates,
and estimated enrollment; and advance payment of $5 per (estimated) student
to Numerical Recipes Software, at this address: P.O. Box 243, Cambridge, MA
02238 (USA). You will receive by return mail a license authorizing you to make
copies of the programs for use by your students, and/or to transfer the programs to
a machine accessible to your students (but only for the duration of the course).

About Copyrights on Computer Programs

Like artistic or literary compositions, computer programs are protected by
copyright. Generally it is an infringement for you to copy into your computer a
program from a copyrighted source. (It is also not a friendly thing to do, since it
deprives the program’s author of compensation for his or her creative effort.) Under

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

xviii License Information

copyright law, all “derivative works” (modified versions, or translations into another
computer language) also come under the same copyright as the original work.

Copyright does not protect ideas, but only the expression of those ideas in
a particular form. In the case of a computer program, the ideas consist of the
program’s methodology and algorithm, including the necessary sequence of steps
adopted by the programmer. The expression of those ideas is the program source
code (particularly any arbitrary or stylistic choices embodied in it), its derived object
code, and any other derivative works.

If you analyze the ideas contained in a program, and then express those
ideas in your own completely different implementation, then that new program
implementation belongs to you. That is what we have done for those programs in
this book that are not entirely of our own devising. When programs in this book are
said to be “based” on programs published in copyright sources, we mean that the
ideas are the same. The expression of these ideas as source code is our own. We
believe that no material in this book infringes on an existing copyright.

Trademarks

Several registered trademarks appear within the text of this book: Sun is a
trademark of Sun Microsystems, Inc. SPARC and SPARCstation are trademarks
of SPARC International, Inc. Microsoft, Windows 95, Windows NT, PowerStation,
and MS are trademarks of Microsoft Corporation. DEC, VMS, Alpha AXP, and
ULTRIX are trademarks of Digital Equipment Corporation. IBM is a trademark of
International Business Machines Corporation. Apple and Macintosh are trademarks
of Apple Computer, Inc. UNIX is a trademark licensed exclusively through X/Open
Co. Ltd. IMSL is a trademark of Visual Numerics, Inc. NAG refers to proprietary
computer software of Numerical Algorithms Group (USA) Inc. PostScript and
Adobe Illustrator are trademarks of Adobe Systems Incorporated. Last, and no doubt
least, Numerical Recipes (when identifying products) is a trademark of Numerical
Recipes Software.

Attributions

The fact that ideas are legally “free as air” in no way supersedes the ethical
requirement that ideas be credited to their known originators. When programs in
this book are based on known sources, whether copyrighted or in the public domain,
published or “handed-down,” we have attempted to give proper attribution. Unfor-
tunately, the lineage of many programs in common circulation is often unclear. We
would be grateful to readers for new or corrected information regarding attributions,
which we will attempt to incorporate in subsequent printings.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.0
1.1
1.1
1.1

2.1

23
23
24
24
24
24
25
2.6
2.6
2.6
27
27
27
27
27
27
27
27
27
27
27
28
28
2.9
2.9
2.10
2.10
2.10
2.10
2.10

3.1
3.2
33
33
34

Computer Programs

by Chapter and Section

flmoon
julday
badluk
caldat

gaussj

ludcmp
lubksb
tridag
banmul
bandec
banbks
mprove
svbksb
svdcmp
pythag
cyclic
sprsin
sprsax
sprstx
sprstp
sprspm
sprstm
linbcg
snrm
atimes
asolve
vander
toep1z
choldc
cholsl
qrdcmp
qrsolv
rsolv
qrupdt
rotate

polint
ratint
spline
splint
locate

calculate phases of the moon by date

Julian Day number from calendar date
Friday the 13th when the moon is full
calendar date from Julian day number

Gauss-Jordan matrix inversion and linear equation

solution

linear equation solution, LU decomposition
linear equation solution, backsubstitution
solution of tridiagonal systems

multiply vector by band diagonal matrix
band diagonal systems, decomposition
band diagonal systems, backsubstitution
linear equation solution, iterative improvement
singular value backsubstitution

singular value decomposition of a matrix
calculate (a? + b?)!/? without overflow
solution of cyclic tridiagonal systems
convert matrix to sparse format

product of sparse matrix and vector
product of transpose sparse matrix and vector
transpose of sparse matrix

pattern multiply two sparse matrices
threshold multiply two sparse matrices
biconjugate gradient solution of sparse systems
used by linbcg for vector norm

used by linbcg for sparse multiplication
used by linbcg for preconditioner

solve Vandermonde systems

solve Toeplitz systems

Cholesky decomposition

Cholesky backsubstitution

QR decomposition

QR backsubstitution

right triangular backsubstitution

update a QR decomposition

Jacobi rotation used by qrupdt

polynomial interpolation

rational function interpolation
construct a cubic spline

cubic spline interpolation

search an ordered table by bisection

xix

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees
-auiyoeul jo BuiAdoo Aue Jo ‘uononpoidas ayjin4 ‘asn jeuosiad UMO JIAaU} 10} Adoo saded auo ayeW 0} Suasn JEUJa\U! 10} Pa}UeJGH SI UOISSIULOg

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUOWNN Aq Z661-8B6l (D) IYBUAdOD swesBOlY “‘ssalg AysIaAIUN EBpLqueD Aq 2661-8861 (D) jyBUAdOD

XX Computer Programs by Chapter and Section
34 hunt search a table when calls are correlated
3.5 polcoe polynomial coefficients from table of values
3.5 polcof polynomial coefficients from table of values
3.6 polin2 two-dimensional polynomial interpolation
3.6 beucof construct two-dimensional bicubic
3.6 beuint two-dimensional bicubic interpolation
3.6 splie2 construct two-dimensional spline
3.6 splin2 two-dimensional spline interpolation
42 trapzd trapezoidal rule
42 qtrap integrate using trapezoidal rule
42 qsimp integrate using Simpson’s rule
43 qromb integrate using Romberg adaptive method
44 midpnt extended midpoint rule
44 qromo integrate using open Romberg adaptive method
44 midinf integrate a function on a semi-infinite interval
44 midsql integrate a function with lower square-root singularity
44 midsqu integrate a function with upper square-root singularity
44 midexp integrate a function that decreases exponentially
45 qgaus integrate a function by Gaussian quadratures
45 gauleg Gauss-Legendre weights and abscissas
45 gaulag Gauss-Laguerre weights and abscissas
45 gauher Gauss-Hermite weights and abscissas
45 gaujac Gauss-Jacobi weights and abscissas
45 gaucof quadrature weights from orthogonal polynomials
45 orthog construct nonclassical orthogonal polynomials
46 quad3d integrate a function over a three-dimensional space
5.1 eulsum sum a series by Euler—van Wijngaarden algorithm
5.3 ddpoly evaluate a polynomial and its derivatives
5.3 poldiv divide one polynomial by another
53 ratval evaluate a rational function
57 dfridr numerical derivative by Ridders’ method
5.8 chebft fit a Chebyshev polynomial to a function
5.8 chebev Chebyshev polynomial evaluation
5.9 chder derivative of a function already Chebyshev fitted
5.9 chint integrate a function already Chebyshev fitted
5.10 chebpc polynomial coefficients from a Chebyshev fit
5.10 peshft polynomial coefficients of a shifted polynomial
5.11 pccheb inverse of chebpc; use to economize power series
5.12 pade Padé approximant from power series coefficients
5.13 ratlsq rational fit by least-squares method
6. gammln logarithm of gamma function
6. factrl factorial function
6. bico binomial coefficients function
6. factln logarithm of factorial function

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Computer Programs by Chapter and Section xxi

ANNNNNNNNNNDNANNNNNN NNN NNN NNN NNN NW DW
CbeSm®YUUUNUUADDAADADAUUUUNUUARWWHNHHNHHNHNHE

DAAHRAAHRAAAA

~_

VHnveseeseeeees

beta
gammp
gammq
gser
gcf
erff
erffc
erfcc
expint
ei
betai
betacf
bessjO
bessy0
bessji
bessy1
bessy
bessj
bessi0
bessk0O
bessil
bessk1i
bessk
bessi
bessjy
beschb
bessik
airy
sphbes
plgndr
frenel
cisi
dawson
rf

rd

rj

re
ellf
elle
ellpi
sncndn
hypgeo
hypser
hypdrv

ran0O
rant

beta function

incomplete gamma function

complement of incomplete gamma function
series used by gammp and gammq

continued fraction used by gammp and gammq
error function

complementary error function
complementary error function, concise routine
exponential integral EF,

exponential integral Ei

incomplete beta function

continued fraction used by betai

Bessel function Jo

Bessel function Yo

Bessel function J;

Bessel function Y;

Bessel function Y of general integer order
Bessel function J of general integer order
modified Bessel function Jo

modified Bessel function Ko

modified Bessel function J,

modified Bessel function Ky;

modified Bessel function Ix of integer order
modified Bessel function I of integer order
Bessel functions of fractional order
Chebyshev expansion used by bessjy
modified Bessel functions of fractional order
Airy functions

spherical Bessel functions j,, and yp,
Legendre polynomials, associated (spherical harmonics)
Fresnel integrals S(a) and C(x)

cosine and sine integrals Ci and Si
Dawson’s integral

Carlson’s elliptic integral of the first kind
Carlson’s elliptic integral of the second kind
Carlson’s elliptic integral of the third kind
Carlson’s degenerate elliptic integral
Legendre elliptic integral of the first kind
Legendre elliptic integral of the second kind
Legendre elliptic integral of the third kind
Jacobian elliptic functions

complex hypergeometric function

complex hypergeometric function, series evaluation
complex hypergeometric function, derivative of

random deviate by Park and Miller minimal standard
random deviate, minimal standard plus shuffle

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

xxii Computer Programs by Chapter and Section
7A ran2 random deviate by L’Ecuyer long period plus shuffle
7A ran3 random deviate by Knuth subtractive method
72 expdev exponential random deviates
72 gasdev normally distributed random deviates
73 gamdev gamma-law distribution random deviates
73 poidev Poisson distributed random deviates
73 bnidev binomial distributed random deviates
TA irbit1 random bit sequence
TA irbit2 random bit sequence
75 psdes “pseudo-DES” hashing of 64 bits
75 ran4 random deviates from DES-like hashing
77 sobseq Sobol’s quasi-random sequence
78 vegas adaptive multidimensional Monte Carlo integration
78 rebin sample rebinning used by vegas
78 miser recursive multidimensional Monte Carlo integration
78 ranpt get random point, used by miser
8.1 piksrt sort an array by straight insertion
8.1 piksr2 sort two arrays by straight insertion
8.1 shell sort an array by Shell’s method
8.2 sort sort an array by quicksort method
8.2 sort2 sort two arrays by quicksort method
8.3 hpsort sort an array by heapsort method
84 indexx construct an index for an array
84 sort3 sort, use an index to sort 3 or more arrays
84 rank construct a rank table for an array
8.5 select find the Nth largest in an array
8.5 selip find the Nth largest, without altering an array
8.5 hpsel find M largest values, without altering an array
8.6 eclass determine equivalence classes from list
8.6 eclazz determine equivalence classes from procedure
9.0 scrsho graph a function to search for roots
91 zbrac outward search for brackets on roots
91 zbrak inward search for brackets on roots
91 rtbis find root of a function by bisection
92 rtflsp find root of a function by false-position
92 rtsec find root of a function by secant method
92 zriddr find root of a function by Ridders’ method
93 zbrent find root of a function by Brent’s method
94 rtnewt find root of a function by Newton-Raphson
94 rtsafe find root of a function by Newton-Raphson and bisection
95 laguer find a root of a polynomial by Laguerre’s method
95 zroots roots of a polynomial by Laguerre’s method with
deflation
95 zrhqr roots of a polynomial by eigenvalue methods
95 qroot complex or double root of a polynomial, Bairstow

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Computer Programs by Chapter and Section xxiii

9.6
97
97
97
97
97

0.1
0.1
0.2
0.3
04
04
0.5
0.5
0.5
0.6
0.6
0.6
0.7
0.8
0.8
0.8
0.8
09
09
09
09
09
09
09
09

1.1
1.1
12
13
15
15
1.6

2.2
23
23
23
23
23

mnewt
insrch
newt
fdjac
fmin
broydn

mnbrak
golden
brent
dbrent
amoeba
amotry
powell
linmin
fidim
frprmn
dlinmin
dfidim
dfpmin
simplx
simp1
simp2
simp3
anneal
revcst
reverse
trncst
trnspt
metrop
amebsa
amotsa

jacobi
eigsrt
tred2
tqli
balanc
elmhes
hqr

fourl
twofft
realft
sinft
cosft1
cosft2

Newton’s method for systems of equations

search along a line, used by newt

globally convergent multi-dimensional Newton’s method
finite-difference Jacobian, used by newt

norm of a vector function, used by newt

secant method for systems of equations

bracket the minimum of a function

find minimum of a function by golden section search
find minimum of a function by Brent’s method

find minimum of a function using derivative information
minimize in N-dimensions by downhill simplex method
evaluate a trial point, used by amoeba

minimize in -dimensions by Powell’s method
minimum of a function along a ray in V-dimensions
function used by linmin

minimize in V-dimensions by conjugate gradient
minimum of a function along a ray using derivatives
function used by dlinmin

minimize in V-dimensions by variable metric method
linear programming maximization of a linear function
linear programming, used by simp1x

linear programming, used by simp1x

linear programming, used by simp1x

traveling salesman problem by simulated annealing
cost of a reversal, used by anneal

do a reversal, used by anneal

cost of a transposition, used by anneal

do a transposition, used by anneal

Metropolis algorithm, used by anneal

simulated annealing in continuous spaces

evaluate a trial point, used by amebsa

eigenvalues and eigenvectors of a symmetric matrix
eigenvectors, sorts into order by eigenvalue
Householder reduction of a real, symmetric matrix
eigensolution of a symmetric tridiagonal matrix
balance a nonsymmetric matrix

reduce a general matrix to Hessenberg form
eigenvalues of a Hessenberg matrix

fast Fourier transform (FFT) in one dimension
fast Fourier transform of two real functions
fast Fourier transform of a single real function
fast sine transform

fast cosine transform with endpoints
“staggered” fast cosine transform

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Xxiv Computer Programs by Chapter and Section
24 fourn fast Fourier transform in multidimensions
25 rlft3 FFT of real data in two or three dimensions
2.6 fourfs FFT for huge data sets on external media
2.6 fourew rewind and permute files, used by fourfs
3.1 convlv convolution or deconvolution of data using FFT
3.2 correl correlation or autocorrelation of data using FFT
3.4 spctrm power spectrum estimation using FFT
3.6 memcof evaluate maximum entropy (MEM) coefficients
3.6 fixrts reflect roots of a polynomial into unit circle
3.6 predic linear prediction using MEM coefficients
3.7 evlmem power spectral estimation from MEM coefficients
3.8 period power spectrum of unevenly sampled data
3.8 fasper power spectrum of unevenly sampled larger data sets
3.8 spread extirpolate value into array, used by fasper
3.9 dftcor compute endpoint corrections for Fourier integrals
3.9 dftint high-accuracy Fourier integrals
3.10 wtt one-dimensional discrete wavelet transform
3.10 daub4 Daubechies 4-coefficient wavelet filter
3.10 pwtset initialize coefficients for pwt
3.10 pwt partial wavelet transform
3.10 wtn multidimensional discrete wavelet transform
4.1 moment calculate moments of a data set
42 ttest Student’s t-test for difference of means
42 avevar calculate mean and variance of a data set
42 tutest Student’s t-test for means, case of unequal variances
42 tptest Student’s t-test for means, case of paired data
42 ftest F-test for difference of variances
43 chsone chi-square test for difference between data and model
43 chstwo chi-square test for difference between two data sets
43 ksone Kolmogorov-Smirnov test of data against model
43 kstwo Kolmogorov-Smirnov test between two data sets
43 probks Kolmogorov-Smirnov probability function
44 cntabl contingency table analysis using chi-square
44 cntab2 contingency table analysis using entropy measure
45 pearsn Pearson’s correlation between two data sets
4.6 spear Spearman’s rank correlation between two data sets
4.6 crank replaces array elements by their rank
46 kendl11 correlation between two data sets, Kendall’s tau
4.6 kend12 contingency table analysis using Kendall’s tau
47 ks2d1s K-S test in two dimensions, data vs. model
47 quadct count points by quadrants, used by ks2d1s
47 quadvl quadrant probabilities, used by ks2d1s
47 ks2d2s K-S test in two dimensions, data vs. data
48 savgol Savitzky-Golay smoothing coefficients

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Computer Programs by Chapter and Section XXV

5.2
5.3
5.3
5.4
5.4
5.4
5.4
5.4
5.4
5.5
5.5
5.5
5.7
5.7

6.1
6.1
6.2
6.2
6.2
63
64
64
64
6.5
6.6
6.6
6.6
6.6
6.6

7A
72
73
73
73
73
7A
7A
7A
7A

8.1
8.1
8.2
8.3
8.3

fit
fitexy
chixy
1fit
covsrt
svdfit
svdvar
fpoly
fleg
mrqmin
mrqcof
fgauss
medfit
rofunc

rk4
rkdumb
rkqs
rkck
odeint
mmid
bsstep
pzextr
rzextr
stoerm
stiff
jacobn
derivs
simpr
stifbs

shoot
shootf
solvde
bksub
pinvs
red
sfroid
difeq
sphoot
sphfpt

fred2

fredin
voltra
wwghts
kermom

least-squares fit data to a straight line

fit data to a straight line, errors in both x and y

used by fitexy to calculate a 7

general linear least-squares fit by normal equations
rearrange covariance matrix, used by 1fit

linear least-squares fit by singular value decomposition
variances from singular value decomposition

fit a polynomial using 1fit or svdfit

fit a Legendre polynomial using 1fit or svdfit
nonlinear least-squares fit, Marquardt’s method

used by mrqmin to evaluate coefficients

fit a sum of Gaussians using mrqmin

fit data to a straight line robustly, least absolute deviation
fit data robustly, used by medfit

integrate one step of ODEs, fourth-order Runge-Kutta
integrate ODEs by fourth-order Runge-Kutta
integrate one step of ODEs with accuracy monitoring
Cash-Karp-Runge-Kutta step used by rkqs

integrate ODEs with accuracy monitoring

integrate ODEs by modified midpoint method
integrate ODEs, Bulirsch-Stoer step

polynomial extrapolation, used by bsstep

rational function extrapolation, used by bsstep
integrate conservative second-order ODEs

integrate stiff ODEs by fourth-order Rosenbrock
sample Jacobian routine for stiff

sample derivatives routine for stiff

integrate stiff ODEs by semi-implicit midpoint rule
integrate stiff ODEs, Bulirsch-Stoer step

solve two point boundary value problem by shooting
ditto, by shooting to a fitting point

two point boundary value problem, solve by relaxation
backsubstitution, used by solvde

diagonalize a sub-block, used by solvde

reduce columns of a matrix, used by solvde
spheroidal functions by method of solvde

spheroidal matrix coefficients, used by sfroid
spheroidal functions by method of shoot

spheroidal functions by method of shootf

solve linear Fredholm equations of the second kind
interpolate solutions obtained with fred2

linear Volterra equations of the second kind
quadrature weights for an arbitrarily singular kernel
sample routine for moments of a singular kernel

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Xxvi Computer Programs by Chapter and Section
8.3 quadmx sample routine for a quadrature matrix
8.3 fredex example of solving a singular Fredholm equation
95 sor elliptic PDE solved by successive overrelaxation method
9.6 mglin linear elliptic PDE solved by multigrid method
9.6 rstrct half-weighting restriction, used by mglin, mgfas
9.6 interp bilinear prolongation, used by mglin, mgfas
9.6 addint interpolate and add, used by mglin
9.6 slvsml solve on coarsest grid, used by mglin
9.6 relax Gauss-Seidel relaxation, used by mglin
9.6 resid calculate residual, used by mglin
9.6 copy utility used by mglin, mgfas
9.6 £1110 utility used by mglin
9.6 mgfas nonlinear elliptic PDE solved by multigrid method
9.6 relax2 Gauss-Seidel relaxation, used by mgfas
9.6 slvsm2 solve on coarsest grid, used by mgfas
9.6 lop applies nonlinear operator, used by mgfas
9.6 matadd utility used by mgfas
9.6 matsub utility used by mgfas
9.6 anorm2 utility used by mgfas
20.1 machar diagnose computer’s floating arithmetic
20.2 igray Gray code and its inverse
20.3 icrcl cyclic redundancy checksum, used by icre
20.3 icre cyclic redundancy checksum
20.3 decchk decimal check digit calculation or verification
20.4 hufmak construct a Huffman code
20.4 hufapp append bits to a Huffman code, used by hufmak
20.4 hufenc use Huffman code to encode and compress a character
20.4 hufdec use Huffman code to decode and decompress a character
20.5 arcmak construct an arithmetic code
20.5 arcode encode or decode a character using arithmetic coding
20.5 arcsum add integer to byte string, used by arcode
20.6 mpops multiple precision arithmetic, simpler operations
20.6 mpmul multiple precision multiply, using FFT methods
20.6 mpinv multiple precision reciprocal
20.6 mpdiv multiple precision divide and remainder
20.6 mpsqrt multiple precision square root
20.6 mp2dfr multiple precision conversion to decimal base
20.6 mppi multiple precision example, compute many digits of 7

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 1. Preliminaries

1.0 Introduction

This book, like its predecessor edition, is supposed to teach you methods of
numerical computing that are practical, efficient, and (insofar as possible) elegant.
We presume throughout this book that you, the reader, have particular tasks that you
want to get done. We view our job as educating you on how to proceed. Occasionally
we may try to reroute you briefly onto a particularly beautiful side road; but by and
large, we will guide you along main highways that lead to practical destinations.

Throughout this book, you will find us fearlessly editorializing, telling you
what you should and shouldn’t do. This prescriptive tone results from a conscious
decision on our part, and we hope that you will not find it irritating. We do not
claim that our advice is infallible! Rather, we are reacting against a tendency, in
the textbook literature of computation, to discuss every possible method that has
ever been invented, without ever offering a practical judgment on relative merit. We
do, therefore, offer you our practical judgments whenever we can. As you gain
experience, you will form your own opinion of how reliable our advice is.

We presume that you are able to read computer programs in C, that being
the language of this version of Numerical Recipes (Second Edition). The book
Numerical Recipes in FORTRAN (Second Edition) is separately available, if you
prefer to program in that language. Earlier editions of Numerical Recipes in Pascal
and Numerical Recipes Routines and Examples in BASIC are also available; while
not containing the additional material of the Second Edition versions in C and
FORTRAN, these versions are perfectly serviceable if Pascal or BASIC is your
language of choice.

When we include programs in the text, they look like this:

#include <math.h>
#define RAD (3.14159265/180.0)

void flmoon(int n, int nph, long *jd, float *frac)
Our programs begin with an introductory comment summarizing their purpose and explaining
their calling sequence. This routine calculates the phases of the moon. Given an integer n and
a code nph for the phase desired (nph = 0 for new moon, 1 for first quarter, 2 for full, 3 for last
quarter), the routine returns the Julian Day Number jd, and the fractional part of a day frac
to be added to it, of the nth such phase since January, 1900. Greenwich Mean Time is assumed.
{

void nrerror(char error_text[]);

int i;

float am,as,c,t,t2,xtra;

c=ntnph/4.0; This is how we comment an individual
line.

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'SGOM IISIA ‘SWWOHGD 40 Sy00q sedivay jEOUAWINN Japs0 OF “payiqiyoud Ajolys si ‘wayndwios Ja/as Aue 0} (auUO sy} Buipnjoul) s:
-aulyoeul jo BulAdoo Aue Jo ‘uolonpoidas sayjin4 ‘“asn jeuosiad UMO JIAaU} 10} Adoo Jaded auo ayew O} Suasn JeUJE\U! 10} payuRJB S|

vU
g
3
i
@.

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUOWNN Aq Z661-8B6l (D) IYBUAdOD swesBOlY “‘ssalg AysIaAIUN EBpLqueD Aq 2661-8861 (D) jyBUAdOD

2 Chapter 1. Preliminaries

t=c/1236.85;
t2=trt;
as=359.2242+29.105356*c; You aren't really intended to understand
am=306 .0253+385 .816918*c+0.010730*t2; this algorithm, but it does work!
*jd=2415020+28L*n+7L*nph ;
xtra=0.75933+1 .53058868*c+((1.178e-4)-(1.55e-7) *t) *t2;
if (mph == 0 || nph == 2)
xtra += (0.1734-3.93e-4*t)*sin(RAD¥as)-0.4068*sin(RAD*am) ;
else if (mph == 1 || nph == 3)
xtra += (0.1721-4.0e-4*t) *sin(RAD*as) -0.6280*sin(RAD*am) ;

else nrerror("nph is unknown in flmoon"); This is how we will indicate error
i=(int) (xtra >= 0.0 ? floor(xtra) : ceil(xtra-1.0)); conditions.
xjd += i;

*frac=xtra-i;

If the syntax of the function definition above looks strange to you, then you are
probably used to the older Kernighan and Ritchie (“K&R”) syntax, rather than that of
the newer ANSI C-. In this edition, we adopt ANSI C as our standard. You might want
to look ahead to $1.2 where ANSI C function prototypes are discussed in more detail.

Note our convention of handling all errors and exceptional cases with a statement
like nrerror("some error message") ;. The function nrerror() is part of a
small file of utility programs, nrutil.c, listed in Appendix B at the back of the
book. This Appendix includes a number of other utilities that we will describe later in
this chapter. Function nrerror () prints the indicated error message to your stderr
device (usually your terminal screen), and then invokes the function exit (), which
terminates execution. The function exit () is in every C library we know of; but if
you find it missing, you can modify nrerror () so that it does anything else that will
halt execution. For example, you can have it pause for input from the keyboard, and
then manually interrupt execution. In some applications, you will want to modify
nrerror () to do more sophisticated error handling, for example to transfer control
somewhere else, with an error flag or error code set.

We will have more to say about the C programming language, its conventions
and style, in §1.1 and §1.2.

Computational Environment and Program Validation

Our goal is that the programs in this book be as portable as possible, across
different platforms (models of computer), across different operating systems, and
across different C compilers. C was designed with this type of portability in
mind. Nevertheless, we have found that there is no substitute for actually checking
all programs on a variety of compilers, in the process uncovering differences in
library structure or contents, and even occasional differences in allowed syntax. As
surrogates for the large number of possible combinations, we have tested all the
programs in this book on the combinations of machines, operating systems, and
compilers shown on the accompanying table. More generally, the programs should
run without modification on any compiler that implements the ANSI C standard,
as described for example in Harbison and Steele’s excellent book [1]. With small
modifications, our programs should run on any compiler that implements the older,
de facto K&R standard [2]. An example of the kind of trivial incompatibility to
watch out for is that ANSI C requires the memory allocation functions malloc ()

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.0 Introduction

Tested Machines and Compilers

Hardware

O/S Version

Compiler Version

IBM PC compatible 486/33

MS-DOS 5.0/Windows 3.1

Microsoft C/C++ 7.0

IBM PC compatible 486/33 MS-DOS 5.0 Borland C/C++ 2.0

IBM RS/6000 AIX 3.2 IBM xlc 1.02

DECstation 5000/25 ULTRIX 4.2A CodeCenter (Saber) C 3.1.1
DECsystem 5400 ULTRIX 4.1 GNU C Compiler 2.1

Sun SPARCstation 2 SunOS 4.1 GNU C Compiler 1.40
DECstation 5000/200 ULTRIX 4.2 DEC RISC C 2.1*

Sun SPARCstation 2 SunOS 4.1 Sun ce 1.1*

*compiler version does not fully implement ANSI C; only K&R validated

and free() to be declared via the header stdlib.h; some older compilers require
them to be declared with the header file malloc.h, while others regard them as
inherent in the language and require no header file at all.

In validating the programs, we have taken the program source code directly
from the machine-readable form of the book’s manuscript, to decrease the chance
of propagating typographical errors. “Driver” or demonstration programs that we
used as part of our validations are available separately as the Numerical Recipes
Example Book (C), as well as in machine-readable form. If you plan to use more
than a few of the programs in this book, or if you plan to use programs in this book
on more than one different computer, then you may find it useful to obtain a copy
of these demonstration programs.

Of course we would be foolish to claim that there are no bugs in our programs,
and we do not make such a claim. We have been very careful, and have benefitted
from the experience of the many readers who have written to us. If you find a new
bug, please document it and tell us!

Compatibility with the First Edition

If you are accustomed to the Numerical Recipes routines of the First Edition, rest
assured: almost all of them are still here, with the same names and functionalities,
often with major improvements in the code itself. In addition, we hope that you
will soon become equally familiar with the added capabilities of the more than 100
routines that are new to this edition.

We have retired a small number of First Edition routines, those that we believe
to be clearly dominated by better methods implemented in this edition. A table,
following, lists the retired routines and suggests replacements.

First Edition users should also be aware that some routines common to both
editions have alterations in their calling interfaces, so are not directly “plug compat-
ible.” A fairly complete list is: chsone, chstwo, covsrt, dfpmin, laguer, 1fit,
memcof,mrqcof, mrqmin, pzextr, ran4, realft, rzextr, shoot, shootf. There
may be others (depending in part on which printing of the First Edition is taken
for the comparison). If you have written software of any appreciable complexity

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4 Chapter 1. Preliminaries

Previous Routines Omitted from This Edition

Name(s) Replacement(s) Comment
adi mglin or mgfas better method
cosft cosfti or cosft2 choice of boundary conditions
cel,e12 rf,rd,rj,rc better algorithms
des, desks ran4 now uses psdes was too slow
mdiani,mdian2 select, selip more general
qcksrt sort name change (sort is now hpsort)
rkqc rkqs better method
smooft use convlv with coefficients from savgol
sparse linbcg more general

that is dependent on First Edition routines, we do not recommend blindly replacing
them by the corresponding routines in this book. We do recommend that any new
programming efforts use the new routines.

About References

You will find references, and suggestions for further reading, listed at the
end of most sections of this book. References are cited in the text by bracketed
numbers like this [3].

Because computer algorithms often circulate informally for quite some time
before appearing in a published form, the task of uncovering “primary literature”
is sometimes quite difficult. We have not attempted this, and we do not pretend
to any degree of bibliographical completeness in this book. For topics where a
substantial secondary literature exists (discussion in textbooks, reviews, etc.) we
have consciously limited our references to a few of the more useful secondary
sources, especially those with good references to the primary literature. Where the
existing secondary literature is insufficient, we give references to a few primary
sources that are intended to serve as starting points for further reading, not as
complete bibliographies for the field.

The order in which references are listed is not necessarily significant. It reflects a
compromise between listing cited references in the order cited, and listing suggestions
for further reading in a roughly prioritized order, with the most useful ones first.

The remaining three sections of this chapter review some basic concepts of
programming (control structures, etc.), discuss a set of conventions specific to C
that we have adopted in this book, and introduce some fundamental concepts in
numerical analysis (roundoff error, etc.). Thereafter, we plunge into the substantive
material of the book.

CITED REFERENCES AND FURTHER READING:

Harbison, S.P., and Steele, G.L., Jr. 1991, C: A Reference Manual, 3rd ed. (Englewood Cliffs,
NJ: Prentice-Hall). [1]

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.1 Program Organization and Control Structures 5

Kernighan, B., and Ritchie, D. 1978, The C Programming Language (Englewood Cliffs, NJ:
Prentice-Hall). [2] [Reference for K&R “traditional” C. Later editions of this book conform
to the ANSI C standard.]

Meeus, J. 1982, Astronomical Formulae for Calculators, 2nd ed., revised and enlarged (Rich-
mond, VA: Willmann-Bell). [3]

1.1 Program Organization and Control
Structures

We sometimes like to point out the close analogies between computer programs,
on the one hand, and written poetry or written musical scores, on the other. All
three present themselves as visual media, symbols on a two-dimensional page or
computer screen. Yet, in all three cases, the visual, two-dimensional, frozen-in-time
representation communicates (or is supposed to communicate) something rather
different, namely a process that unfolds in time. A poem is meant to be read; music,
played; a program, executed as a sequential series of computer instructions.

In all three cases, the target of the communication, in its visual form, is a human
being. The goal is to transfer to him/her, as efficiently as can be accomplished,
the greatest degree of understanding, in advance, of how the process will unfold in
time. In poetry, this human target is the reader. In music, it is the performer. In
programming, it is the program user.

Now, you may object that the target of communication of a program is not
a human but a computer, that the program user is only an irrelevant intermediary,
a lackey who feeds the machine. This is perhaps the case in the situation where
the business executive pops a diskette into a desktop computer and feeds that
computer a black-box program in binary executable form. The computer, in this
case, doesn’t much care whether that program was written with “good programming
practice” or not.

We envision, however, that you, the readers of this book, are in quite a different
situation. You need, or want, to know not just what a program does, but also how
it does it, so that you can tinker with it and modify it to your particular application.
You need others to be able to see what you have done, so that they can criticize or
admire. In such cases, where the desired goal is maintainable or reusable code, the
targets of a program’s communication are surely human, not machine.

One key to achieving good programming practice is to recognize that pro-
gramming, music, and poetry — all three being symbolic constructs of the human
brain — are naturally structured into hierarchies that have many different nested
levels. Sounds (phonemes) form small meaningful units (morphemes) which in turn
form words; words group into phrases, which group into sentences; sentences make
paragraphs, and these are organized into higher levels of meaning. Notes form
musical phrases, which form themes, counterpoints, harmonies, etc.; which form
movements, which form concertos, symphonies, and so on.

The structure in programs is equally hierarchical. Appropriately, good program-
ming practice brings different techniques to bear on the different levels [1-3]. Ata low
level is the ascii character set. Then, constants, identifiers, operands, operators.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6 Chapter 1. Preliminaries

Then program statements, like a[j+1]=b+c/3.0;. Here, the best programming
advice is simply be clear, or (correspondingly) don’t be too tricky. You might
momentarily be proud of yourself at writing the single line

k=(2-j)*(1+3*j)/2;

if you want to permute cyclically one of the values j = (0,1, 2) into respectively
k = (1,2,0). You will regret it later, however, when you try to understand that
line. Better, and likely also faster, is

k=j+1;
if (k == 3) k=0;

Many programming stylists would even argue for the ploddingly literal

switch (j) {
case 0: k=1; break;
case 1: k=2; break;
case 2: k=0; break;
default: {
fprintf(stderr,"unexpected value for j");
exit(1);

on the grounds that it is both clear and additionally safeguarded from wrong assump-
tions about the possible values of j. Our preference among the implementations
is for the middle one.

In this simple example, we have in fact traversed several levels of hierarchy:
Statements frequently come in “groups” or “blocks” which make sense only taken
as a whole. The middle fragment above is one example. Another is

swap=a[j];
alj]=b(jl;
b[jl=swap;

which makes immediate sense to any programmer as the exchange of two variables,
while

ans=sum=0.0;
n=1;

is very likely to be an initialization of variables prior to some iterative process. This
level of hierarchy in a program is usually evident to the eye. It is good programming
practice to put in comments at this level, e.g., “initialize” or “exchange variables.”

The next level is that of control structures. These are things like the switch
construction in the example above, for loops, and so on. This level is sufficiently
important, and relevant to the hierarchical level of the routines in this book, that
we will come back to it just below.

At still higher levels in the hierarchy, we have functions and modules, and the
whole “global” organization of the computational task to be done. In the musical
analogy, we are now at the level of movements and complete works. At these levels,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.1 Program Organization and Control Structures 7

modularization and encapsulation become important programming concepts, the
general idea being that program units should interact with one another only through
clearly defined and narrowly circumscribed interfaces. Good modularization practice
is an essential prerequisite to the success of large, complicated software projects,
especially those employing the efforts of more than one programmer. It is also good
practice (if not quite as essential) in the less massive programming tasks that an
individual scientist, or reader of this book, encounters.

Some computer languages, such as Modula-2 and C++, promote good modular-
ization with higher-level language constructs absent in C. In Modula-2, for example,
functions, type definitions, and data structures can be encapsulated into “modules”
that communicate through declared public interfaces and whose internal workings
are hidden from the rest of the program [4]. In the C++ language, the key concept
is “class,” a user-definable generalization of data type that provides for data hiding,
automatic initialization of data, memory management, dynamic typing, and operator
overloading (i.e., the user-definable extension of operators like + and * so as to be
appropriate to operands in any particular class) [5]. Properly used in defining the data
structures that are passed between program units, classes can clarify and circumscribe
these units’ public interfaces, reducing the chances of programming error and also
allowing a considerable degree of compile-time and run-time error checking.

Beyond modularization, though depending on it, lie the concepts of object-
oriented programming. Here a programming language, such as C++ or Turbo Pascal
5.5 [6], allows a module’s public interface to accept redefinitions of types or actions,
and these redefinitions become shared all the way down through the module’s
hierarchy (so-called polymorphism). For example, a routine written to invert a matrix
of real numbers could — dynamically, at run time — be made able to handle complex
numbers by overloading complex data types and corresponding definitions of the
arithmetic operations. Additional concepts of inheritance (the ability to define a data
type that “inherits” all the structure of another type, plus additional structure of its
own), and object extensibility (the ability to add functionality to a module without
access to its source code, e.g., at run time), also come into play.

We have not attempted to modularize, or make objects out of, the routines in this
book, for at least two reasons. First, the chosen language, C, does not really make
this possible. Second, we envision that you, the reader, might want to incorporate
the algorithms in this book, a few at a time, into modules or objects with a structure
of your own choosing. There does not exist, at present, a standard or accepted set
of “classes” for scientific object-oriented computing. While we might have tried to
invent such a set, doing so would have inevitably tied the algorithmic content of the
book (which is its raison d’étre) to some rather specific, and perhaps haphazard, set
of choices regarding class definitions.

On the other hand, we are not unfriendly to the goals of modular and object-
oriented programming. Within the limits of C, we have therefore tried to structure
our programs to be “object friendly.” That is one reason we have adopted ANSI
C with its function prototyping as our default C dialect (see §1.2). Also, within
our implementation sections, we have paid particular attention to the practices of
structured programming, as we now discuss.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8 Chapter 1. Preliminaries

Control Structures

An executing program unfolds in time, but not strictly in the linear order in
which the statements are written. Program statements that affect the order in which
statements are executed, or that affect whether statements are executed, are called
control statements. Control statements never make useful sense by themselves. They
make sense only in the context of the groups or blocks of statements that they in turn
control. If you think of those blocks as paragraphs containing sentences, then the
control statements are perhaps best thought of as the indentation of the paragraph
and the punctuation between the sentences, not the words within the sentences.

We can now say what the goal of structured programming is. It is to make
program control manifestly apparent in the visual presentation of the program. You
see that this goal has nothing at all to do with how the computer sees the program.
As already remarked, computers don’t care whether you use structured programming
or not. Human readers, however, do care. You yourself will also care, once you
discover how much easier it is to perfect and debug a well-structured program than
one whose control structure is obscure.

You accomplish the goals of structured programming in two complementary
ways. First, you acquaint yourself with the small number of essential control
structures that occur over and over again in programming, and that are therefore
given convenient representations in most programming languages. You should learn
to think about your programming tasks, insofar as possible, exclusively in terms of
these standard control structures. In writing programs, you should get into the habit
of representing these standard control structures in consistent, conventional ways.

“Doesn’t this inhibit creativity?” our students sometimes ask. Yes, just
as Mozart’s creativity was inhibited by the sonata form, or Shakespeare’s by the
metrical requirements of the sonnet. The point is that creativity, when it is meant to
communicate, does well under the inhibitions of appropriate restrictions on format.

Second, you avoid, insofar as possible, control statements whose controlled
blocks or objects are difficult to discern at a glance. This means, in practice, that you
must try to avoid named labels on statements and goto’s. It is not the goto’s that
are dangerous (although they do interrupt one’s reading of a program); the named
statement labels are the hazard. In fact, whenever you encounter a named statement
label while reading a program, you will soon become conditioned to get a sinking
feeling in the pit of your stomach. Why? Because the following questions will, by
habit, immediately spring to mind: Where did control come from in a branch to this
label? It could be anywhere in the routine! What circumstances resulted in a branch
to this label? They could be anything! Certainty becomes uncertainty, understanding
dissolves into a morass of possibilities.

Some examples are now in order to make these considerations more concrete
(see Figure 1.1.1).

Catalog of Standard Structures

Iteration. In C, simple iteration is performed with a for loop, for example

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
Se
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ac
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
a3
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.1 Program Organization and Control Structures

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.
i granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
Ss (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs,

http:/Awww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

false

ck
(b)

blo
WHILE iteration

iteration
complete?
increment
index
FOR iteration
(a)
block

(5
2
S

BREAK iteration

DO WHILE iteration

isit website

(d)

Standard control structures used in structured programming: (a) for iteration; (b) while

iteration; (c) do while iteration; (d) break iteration; (e) if structure; (f) switch structure

Figure 1.1.1.

Preliminaries

Chapter 1.

10

false

false

condition

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer: Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

else block

IF structure
()

“4

2
s
3
&

By
3

SWITCH structure

(f)
Standard control structures used in structured programming (see caption on previous page).

Figure 1.1.1.

1.1 Program Organization and Control Structures 11

Notice how we always indent the block of code that is acted upon by the control
structure, leaving the structure itself unindented. Notice also our habit of putting the
initial curly brace on the same line as the for statement, instead of on the next line.
This saves a full line of white space, and our publisher loves us for it.

IF structure. This structure in C is similar to that found in Pascal, Algol,
FORTRAN and other languages, and typically looks like

if (4

+
else if (...) {

+

else {

+

Since compound-statement curly braces are required only when there is more
than one statement in a block, however, C’s if construction can be somewhat less
explicit than the corresponding structure in FORTRAN or Pascal. Some care must be
exercised in constructing nested if clauses. For example, consider the following:

if (b > 3)
if (a > 3) b += 1;
else b -= 1; /* questionable! */

As judged by the indentation used on successive lines, the intent of the writer of
this code is the following: ‘If b is greater than 3 and a is greater than 3, then
increment b. If b is not greater than 3, then decrement b.’ According to the rules
of C, however, the actual meaning is ‘If b is greater than 3, then evaluate a. If a is
greater than 3, then increment b, and if a is less than or equal to 3, decrement b.’ The
point is that an else clause is associated with the most recent open if statement,
no matter how you lay it out on the page. Such confusions in meaning are easily
resolved by the inclusion of braces. They may in some instances be technically
superfluous; nevertheless, they clarify your intent and improve the program. The
above fragment should be written as

if (b> 3) {

if (a > 3) b += 1;
} else {

b -= 1;
+

Here is a working program that consists dominantly of if control statements:

#include <math.h>
#define IGREG (15+31L*(10+12L*1582) ) Gregorian Calendar adopted Oct. 15, 1582.

long julday(int mm, int id, int iyyy)

In this routine julday returns the Julian Day Number that begins at noon of the calendar date
specified by month mm, day id, and year iyyy, all integer variables. Positive year signifies A.D.;
negative, B.C. Remember that the year after 1 B.C. was 1 A.D.

{

void nrerror(char error_text[]);

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12 Chapter 1. Preliminaries

long jul;

int ja, jy=iyyy, jm;

if (jy == 0) nrerror("julday: there is no year zero.");

if (jy < 0) ++jy;

if (mm > 2) { Here is an example of a block IF-structure.
jm=mm+1 ;

} else {
~Sys
jm=mm+13;

+

jul = (long) (floor(365.25*jy)+floor(30.6001* jm) +id+1720995) ;

if (id+31L*(mm+12L*iyyy) >= IGREG) { Test whether to change to Gregorian Cal-
ja=(int) (0.01*jy); endar.

jul += 2-jat(int) (0.25*ja);
+

return jul;

(Astronomers number each 24-hour period, starting and ending at noon, with
a unique integer, the Julian Day Number [7]. Julian Day Zero was a very long
time ago; a convenient reference point is that Julian Day 2440000 began at noon
of May 23, 1968. If you know the Julian Day Number that begins at noon of a
given calendar date, then the day of the week of that date is obtained by adding
1 and taking the result modulo base 7; a zero answer corresponds to Sunday, | to
Monday, ..., 6 to Saturday.)

While iteration. Most languages (though not FORTRAN, incidentally) provide
for structures like the following C example:

while (n < 1000) {
n *= 2;
j t= 1;

It is the particular feature of this structure that the control-clause (in this case
n < 1000) is evaluated before each iteration. If the clause is not true, the enclosed
statements will not be executed. In particular, if this code is encountered at a time
when n is greater than or equal to 1000, the statements will not even be executed once.

Do-While iteration. | Companion to the while iteration is a related control-
structure that tests its control-clause at the end of each iteration. In C, it looks
like this:

do {
n *= 2;
j t= 1;
} while (mn < 1000);

In this case, the enclosed statements will be executed at least once, independent
of the initial value of n.

Break. In this case, you have a loop that is repeated indefinitely until some
condition tested somewhere in the middle of the loop (and possibly tested in more

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.1 Program Organization and Control Structures 13

than one place) becomes true. At that point you wish to exit the loop and proceed
with what comes after it. In C the structure is implemented with the simple break
statement, which terminates execution of the innermost for, while, do, or switch
construction and proceeds to the next sequential instruction. (In Pascal and standard
FORTRAN, this structure requires the use of statement labels, to the detriment of clear
programming.) A typical usage of the break statement is:

for(;;) {
[statements before the test]
if (...) break;
[statements after the test]
+

[next sequential instruction]

Here is a program that uses several different iteration structures. One of us was
once asked, for a scavenger hunt, to find the date of a Friday the 13th on which the
moon was full. This is a program which accomplishes that task, giving incidentally
all other Fridays the 13th as a by-product.

#include <stdio.h>

#include <math.h>

#define ZON -5.0 Time zone —5 is Eastern Standard Time.
#tdefine IYBEG 1900 The range of dates to be searched.
#define IYEND 2000

int main(void) /* Program badluk */
{
void flmoon(int n, int nph, long *jd, float *frac);
long julday(int mm, int id, int iyyy);
int ic,icon,idwk,im,iyyy,n;
float timzon = ZON/24.0,frac;
long jd,jday;

printf("\nFull moons on Friday the 13th from %5d to %5d\n",IYBEG,IYEND) ;
for (iyyy=IYBEG;iyyy<=IYEND;iyyy++) { Loop over each year,
for (im=1;im<=12;im++) { and each month.
jday=julday(im,13,iyyy) ; Is the 13th a Friday?
idwk=(int) ((jdayt+1) % 7);
if (idwk == 5) {
n=(int) (12.37*(iyyy-1900+(im-0.5)/12.0));
This value n is a first approximation to how many full moons have occurred
since 1900. We will feed it into the phase routine and adjust it up or down
until we determine that our desired 13th was or was not a full moon. The
variable icon signals the direction of adjustment.

icon=0;
for (;;) {
flmoon(n,2,&jd,&frac) ; Get date of full moon n.
frac=24.0*(fracttimzon) ; Convert to hours in correct time zone.
if (frac < 0.0) { Convert from Julian Days beginning at
--jd; noon to civil days beginning at mid-
frac += 24.0; night.
+
if (frac > 12.0) {
++jd;
frac -= 12.0;
} else
frac += 12.0;
if (jd == jday) { Did we hit our target day?

printf ("\n42d/13/%4d\n" , im, iyyy) ;
printf("4s %5.1f %s\n","Full moon",frac,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14 Chapter 1. Preliminaries

"hrs after midnight (EST)");

break; Part of the break-structure, a match.
} else { Didn't hit it.
ic=(jday >= jd ? 1: -1);
if (ic == (-icon)) break; Another break, case of no match.
icon=ic;
n += ic;
+
}
}
}
+
return 0;

If you are merely curious, there were (or will be) occurrences of a full moon
on Friday the 13th (time zone GMT—5) on: 3/13/1903, 10/13/1905, 6/13/1919,
1/13/1922, 11/13/1970, 2/13/1987, 10/13/2000, 9/13/2019, and 8/13/2049.

Other “standard” structures. Our advice is to avoid them. Every pro-
gramming language has some number of “goodies” that the designer just couldn’t
resist throwing in. They seemed like a good idea at the time. Unfortunately they
don’t stand the test of time! Your program becomes difficult to translate into other
languages, and difficult to read (because rarely used structures are unfamiliar to the
reader). You can almost always accomplish the supposed conveniences of these
structures in other ways.

In C, the most problematic control structure is the switch...case...default
construction (see Figure 1.1.1), which has historically been burdened by uncertainty,
from compiler to compiler, about what data types are allowed in its control expression.
Data types char and int are universally supported. For other data types, e.g., float
or doub1e, the structure should be replaced by a more recognizable and translatable
if...else construction. ANSI C allows the control expression to be of type long,
but many older compilers do not.

The continue; construction, while benign, can generally be replaced by an
if construction with no loss of clarity.

About “Advanced Topics”

Material set in smaller type, like this, signals an “advanced topic,” either one outside of
the main argument of the chapter, or else one requiring of you more than the usual assumed
mathematical background, or else (in a few cases) a discussion that is more speculative or an
algorithm that is less well-tested. Nothing important will be lost if you skip the advanced
topics on a first reading of the book.

You may have noticed that, by its looping over the months and years, the program badluk
avoids using any algorithm for converting a Julian Day Number back into a calendar date. A
routine for doing just this is not very interesting structurally, but it is occasionally useful:

#include <math.h>
#tdefine IGREG 2299161

void caldat(long julian, int *mm, int *id, int *iyyy)
Inverse of the function julday given above. Here julian is input as a Julian Day Number,
and the routine outputs mm,id, and iyyy as the month, day, and year on which the specified
Julian Day started at noon.
t

long ja, jalpha, jb, jc, jd, je;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 15

if (julian >= IGREG) { Cross-over to Gregorian Calendar produces this correc-
jalpha=(long)(((double) (julian-1867216)-0.25)/36524.25) ; tion.
ja=julian+i+jalpha-(long) (0.25*jalpha) ;

} else if (julian < 0) { Make day number positive by adding integer number of

ja=julian+36525*(1-julian/36525) ; Julian centuries, then subtract them off
} else at the end.

ja=julian;
jb=jat1524;

jc=Clong) (6680.0+((double) (jb-2439870)-122.1)/365.25) ;
j= (long) (365+jc+(0.25+jc));

je=(long) ((jb-jd) /30.6001) ;

*id=jb-jd-(long) (30.6001*je) ;

*mm=je-1;

if (*mm > 12) *mm -= 12;

*iyyy=jc-4715;

aif (*mm > 2) --(*iyyy);

if (*iyyy <= 0) --(+iyyy);

if (julian < 0) *iyyy -= 100*(1-julian/36525) ;

(For additional calendrical algorithms, applicable to various historical calendars, see [8].)

CITED REFERENCES AND FURTHER READING:

Harbison, S.P., and Steele, G.L., Jr. 1991, C: A Reference Manual, 3rd ed. (Englewood Cliffs,
NJ: Prentice-Hall).

Kernighan, B.W. 1978, The Elements of Programming Style (New York: McGraw-Hill). [1]

Yourdon, E. 1975, Techniques of Program Structure and Design (Englewood Cliffs, NJ: Prentice-
Hall). [2]

Jones, R., and Stewart, I. 1987, The Art of C Programming (New York: Springer-Verlag). [3]

Hoare, C.A.R. 1981, Communications of the ACM, vol. 24, pp. 75-83.

Wirth, N. 1983, Programming in Modula-2, 3rd ed. (New York: Springer-Verlag). [4]

Stroustrup, B. 1986, The C++ Programming Language (Reading, MA: Addison-Wesley). [5]

Borland International, Inc. 1989, Turbo Pascal 5.5 Object-Oriented Programming Guide (Scotts
Valley, CA: Borland International). [6]

Meeus, J. 1982, Astronomical Formulae for Calculators, 2nd ed., revised and enlarged (Rich-
mond, VA: Willmann-Bell). [7]

Hatcher, D.A. 1984, Quarterly Journal of the Royal Astronomical Society, vol. 25, pp. 583-55; see
also op. cit. 1985, vol. 26, pp. 151-155, and 1986, vol. 27, pp. 506-507. [8]

1.2 Some C Conventions for Scientific
Computing

The C language was devised originally for systems programming work, not for
scientific computing. Relative to other high-level programming languages, C puts
the programmer “‘very close to the machine” in several respects. It is operator-rich,
giving direct access to most capabilities of a machine-language instruction set. It
has a large variety of intrinsic data types (short and long, signed and unsigned
integers; floating and double-precision reals; pointer types; etc.), and a concise
syntax for effecting conversions and indirections. It defines an arithmetic on pointers
(addresses) that relates gracefully to array addressing and is highly compatible with
the index register structure of many computers.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16 Chapter 1. Preliminaries

Portability has always been another strong point of the C language. C is the
underlying language of the UNIX operating system; both the language and the
operating system have by now been implemented on literally hundreds of different
computers. The language’s universality, portability, and flexibility have attracted
increasing numbers of scientists and engineers to it. It is commonly used for the
real-time control of experimental hardware, often in spite of the fact that the standard
UNIX kernel is less than ideal as an operating system for this purpose.

The use of C for higher level scientific calculations such as data analysis,
modeling, and floating-point numerical work has generally been slower in developing.
In part this is due to the entrenched position of FORTRAN as the mother-tongue of
virtually all scientists and engineers born before 1960, and most born after. In
part, also, the slowness of C’s penetration into scientific computing has been due to
deficiencies in the language that computer scientists have been (we think, stubbornly)
slow to recognize. Examples are the lack of a good way to raise numbers to small
integer powers, and the “implicit conversion of float to double” issue, discussed
below. Many, though not all, of these deficiencies are overcome in the ANSI C
Standard. Some remaining deficiencies will undoubtedly disappear over time.

Yet another inhibition to the mass conversion of scientists to the C cult has been,
up to the time of writing, the decided lack of high-quality scientific or numerical
libraries. That is the lacuna into which we thrust this edition of Numerical Recipes.
We certainly do not claim to be a complete solution to the problem. We do hope
to inspire further efforts, and to lay out by example a set of sensible, practical
conventions for scientific C programming.

The need for programming conventions in C is very great. Far from the problem
of overcoming constraints imposed by the language (our repeated experience with
Pascal), the problem in C is to choose the best and most natural techniques from
multiple opportunities — and then to use those techniques completely consistently
from program to program. In the rest of this section, we set out some of the issues,
and describe the adopted conventions that are used in all of the routines in this book.

Function Prototypes and Header Files

ANSI C allows functions to be defined with function prototypes, which specify
the type of each function parameter. If a function declaration or definition with
a prototype is visible, the compiler can check that a given function call invokes
the function with the correct argument types. All the routines printed in this book
are in ANSI C prototype form. For the benefit of readers with older “traditional
K&R” C compilers, the Numerical Recipes C Diskette includes two complete sets of
programs, one in ANSI, the other in K&R.

The easiest way to understand prototypes is by example. A function definition
that would be written in traditional C as

int g(x,y,z)
int x,y;
float z;

becomes in ANSI C

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 17

int g(int x, int y, float z)

A function that has no parameters has the parameter type list void.

A function declaration (as contrasted to a function definition) is used to
“introduce” a function to a routine that is going to call it. The calling routine needs
to know the number and type of arguments and the type of the returned value. In
a function declaration, you are allowed to omit the parameter names. Thus the
declaration for the above function is allowed to be written

int g(int, int, float);

If a C program consists of multiple source files, the compiler cannot check the
consistency of each function call without some additional assistance. The safest
way to proceed is as follows:

e Every external function should have a single prototype declaration in a
header (.h) file.

e The source file with the definition (body) of the function should also
include the header file so that the compiler can check that the prototypes
in the declaration and the definition match.

e Every source file that calls the function should include the appropriate
header (.h) file.

e Optionally, a routine that calls a function can also include that function’s
prototype declaration internally. This is often useful when you are
developing a program, since it gives you a visible reminder (checked by
the compiler through the common .nh file) of a function’s argument types.
Later, after your program is debugged, you can go back and delete the
supernumary internal declarations.

For the routines in this book, the header file containing all the prototypes is nr.h,
listed in Appendix A. You should put the statement #include nr.h at the top of
every source file that contains Numerical Recipes routines. Since, more frequently
than not, you will want to include more than one Numerical Recipes routine in a
single source file, we have not printed this #include statement in front of this
book’s individual program listings, but you should make sure that it is present in
your programs.

As backup, and in accordance with the last item on the indented list above,
we declare the function prototype of all Numerical Recipes routines that are called
by other Numerical Recipes routines internally to the calling routine. (That also
makes our routines much more readable.) The only exception to this rule is that
the small number of utility routines that we use repeatedly (described below) are
declared in the additional header file nrutil.h, and the line #include nrutil.h
is explicitly printed whenever it is needed.

A final important point about the header file nr.h is that, as furnished on
the diskette, it contains both ANSI C and traditional K&R-style declarations. The
ANSI forms are invoked if any of the following macros are defined: __STDC__,
ANSI, or NRANSI. (The purpose of the last name is to give you an invocation that
does not conflict with other possible uses of the first two names.) If you have an
ANSI compiler, it is essential that you invoke it with one or more of these macros

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18 Chapter 1. Preliminaries

defined. The typical means for doing so is to include a switch like “-DANSI” on
the compiler command line.
Some further details about the file nr.h are given in Appendix A.

Vectors and One-Dimensional Arrays

There is a close, and elegant, correspondence in C between pointers and arrays.
The value referenced by an expression like a[j] is defined to be *((a)+(j)),
that is, “the contents of the address obtained by incrementing the pointer a by
j-’ A consequence of this definition is that if a points to a legal data location,
the array element a[0] is always defined. Arrays in C are natively “zero-origin”
or “zero-offset.’ An array declared by the statement float b[4]; has the valid
references b[0], b[1], b[2], and b[3], but not b[4].

Right away we need a notation to indicate what is the valid range of an array
index. (The issue comes up about a thousand times in this book!) For the above
example, the index range of b will be henceforth denoted b[0..3], a notation
borrowed from Pascal. In general, the range of an array declared by float
a[M]; is al0..M — 1], and the same if float is replaced by any other data type.

One problem is that many algorithms naturally like to go from 1 to M, not
from 0 to M — 1. Sure, you can always convert them, but they then often acquire
a baggage of additional arithmetic in array indices that is, at best, distracting. It is
better to use the power of the C language, in a consistent way, to make the problem
disappear. Consider

float b[4] ,*bb;
bb=b-1;

The pointer bb now points one location before b. An immediate consequence is that
the array elements bb[1], bb[2], bb[3], and bb[4] all exist. In other words the
range of bb is bb[1..4]. We will refer to bb as a unit-offset vector. (See Appendix
B for some additional discussion of technical details.)

It is sometimes convenient to use zero-offset vectors, and sometimes convenient
to use unit-offset vectors in algorithms. The choice should be whichever is most
natural to the problem at hand. For example, the coefficients of a polynomial
ao + a,x + aga? +... + an2” clearly cry out for the zero-offset a0. .n], while
a vector of V data points x;, i = 1... N calls for a unit-offset x[1..N]. When a
routine in this book has an array as an argument, its header comment always gives
the expected index range. For example,

void someroutine(float bb[], int nn)
This routine does something with the vector bb[1..nn].

Now, suppose you want someroutine() to do its thing on your own vector,
of length 7, say. If your vector, call it aa, is already unit-offset (has the valid range
aa[1..7]), then you can invoke someroutine(aa,7) ; in the obvious way. That is
the recommended procedure, since someroutine() presumably has some logical,
or at least aesthetic, reason for wanting a unit-offset vector.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 19

But suppose that your vector of length 7, now call it a, is perversely a native C,
zero-offset array (has range a[0. .6]). Perhaps this is the case because you disagree
with our aesthetic prejudices, Heaven help you! To use our recipe, do you have to
copy a’s contents element by element into another, unit-offset vector? No! Do you
have to declare a new pointer aaa and set it equal to a-1? No! You simply invoke
someroutine(a-1,7);. Then a[1], as seen from within our recipe, is actually
a[0] as seen from your program. In other words, you can change conventions “on
the fly” with just a couple of keystrokes.

Forgive us for belaboring these points. We want to free you from the zero-offset
thinking that C encourages but (as we see) does not require. A final liberating point
is that the utility file nrutil.c, listed in full in Appendix B, includes functions
for allocating (using malloc()) arbitrary-offset vectors of arbitrary lengths. The
synopses of these functions are as follows:

float *vector(long nl, long nh)
Allocates a float vector with range [nl..nh].

int *ivector(long nl, long nh)
Allocates an int vector with range [nl..nh].

unsigned char *cvector(long nl, long nh)
Allocates an unsigned char vector with range [nl..nh].

unsigned long *lvector(long nl, long nh)
Allocates an unsigned long vector with range [nl..nh].

double *dvector(long nl, long nh)
Allocates a double vector with range [nl..nh].

A typical use of the above utilities is the declaration float *b; followed by
b=vector (1,7) ;, which makes the range b[1..7] come into existence and allows
b to be passed to any function calling for a unit-offset vector.

The file nrutil.c also contains the corresponding deallocation routines,

void free_vector(float *v, long nl, long nh)
void free_ivector(int *v, long nl, long nh)
void free_cvector(unsigned char *v, long nl, long nh)
void free_lvector(unsigned long *v, long nl, long nh)
void free_dvector(double *v, long nl, long nh)

with the typical use being free_vector(b,1,7);.

Our recipes use the above utilities extensively for the allocation and deallocation
of vector workspace. We also commend them to you for use in your main programs or
other procedures. Note that if you want to allocate vectors of length longer than 64k
on an IBM PC-compatible computer, you should replace all occurrences of malloc

in nrutil.c by your compiler’s special-purpose memory allocation function. This
applies also to matrix allocation, to be discussed next.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20 Chapter 1. Preliminaries

Matrices and Two-Dimensional Arrays

The zero- versus unit-offset issue arises here, too. Let us, however, defer it for
a moment in favor of an even more fundamental matter, that of variable dimension
arrays (FORTRAN terminology) or conformant arrays (Pascal terminology). These
are arrays that need to be passed to a function along with real-time information
about their two-dimensional size. The systems programmer rarely deals with two-
dimensional arrays, and almost never deals with two-dimensional arrays whose size
is variable and known only at run time. Such arrays are, however, the bread and
butter of scientific computing. Imagine trying to live with a matrix inversion routine
that could work with only one size of matrix!

There is no technical reason that a C compiler could not allow a syntax like

void someroutine(a,m,n)
float alm] [n]; /* ILLEGAL DECLARATION */

and emit code to evaluate the variable dimensions m and n (or any variable-dimension
expression) each time someroutine() is entered. Alas! the above fragment is
forbidden by the C language definition. The implementation of variable dimensions
in C instead requires some additional finesse; however, we will see that one is
rewarded for the effort.

There is a subtle near-ambiguity in the C syntax for two-dimensional array
references. Let us elucidate it, and then turn it to our advantage. Consider the
array reference to a (say) float value ali] [j], where i and j are expressions
that evaluate to type int. A C compiler will emit quite different machine code for
this reference, depending on how the identifier a has been declared. If a has been
declared as a fixed-size array,e.g., float a[5] [9] ;, then the machine code is: “to
the address a add 9 times i, then add j, return the value thus addressed.” Notice that
the constant 9 needs to be known in order to effect the calculation, and an integer
multiplication is required (see Figure 1.2.1).

Suppose, on the other hand, that a has been declared by float **a;. Then
the machine code for a[i] [j] is: “to the address of a add i, take the value thus
addressed as a new address, add j to it, return the value addressed by this new
address.” Notice that the underlying size of a[] [] does not enter this calculation
at all, and that there is no multiplication; an additional indirection replaces it. We
thus have, in general, a faster and more versatile scheme than the previous one. The
price that we pay is the storage requirement for one array of pointers (to the rows
of a[] []), and the slight inconvenience of remembering to initialize those pointers
when we declare an array.

Here is our bottom line: We avoid the fixed-size two-dimensional arrays of C as
being unsuitable data structures for representing matrices in scientific computing. We
adopt instead the convention “pointer to array of pointers,” with the array elements
pointing to the first element in the rows of each matrix. Figure 1.2.1 contrasts the
rejected and adopted schemes.

The following fragment shows how a fixed-size array a of size 13 by 9 is
converted to a “pointer to array of pointers” reference aa:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 21

(a)

*em eo 77

(b)
Figure 1.2.1. Two storage schemes for a matrix m. Dotted lines denote address reference, while solid

lines connect sequential memory locations. (a) Pointer to a fixed size two-dimensional array. (b) Pointer
to an array of pointers to rows; this is the scheme adopted in this book.

float a[13] [9] ,**aa;

int i;
aa=(float **) malloc((unsigned) 13*sizeof(float*)) ;
for (i=0;i<=12;it++) aalil=alil; ali] is a pointer to a[i] [0]

The identifier aa is now a matrix with index range aa[0..12] [0..8]. You can use
or modify its elements ad lib, and more importantly you can pass it as an argument
to any function by its name aa. That function, which declares the corresponding
dummy argument as float **aa;, can address its elements as aa[i] [j] without
knowing its physical size.

You may rightly not wish to clutter your programs with code like the above
fragment. Also, there is still the outstanding problem of how to treat unit-offset
indices, so that (for example) the above matrix aa could be addressed with the range
a[1..13][1..9]. Both of these problems are solved by additional utility routines
in nrutil.c (Appendix B) which allocate and deallocate matrices of arbitrary
range. The synopses are

float **matrix(long nrl, long nrh, long ncl, long nch)
Allocates a float matrix with range [nrl..nrh] [ncl..nch].

double **dmatrix(long nrl, long nrh, long ncl, long nch)
Allocates a double matrix with range [nrl..nrh] [ncl..nch].

int **imatrix(long nrl, long nrh, long ncl, long nch)
Allocates an int matrix with range [nrl..nrh] [ncl..nch].

void free_matrix(float **m, long nrl, long nrh, long ncl, long nch)
Frees a matrix allocated with matrix.

“(eoueWY YON apis}no) Bio abpuquieo@ Masjsno}9auIp 0} jee PUaS JO ‘(AJUO BOVEWY YON) EZ7Z-Z28-008- I I/29 40 Woo" U'MMy/:d}Y

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

22 Chapter 1. Preliminaries

void free_dmatrix(double **m, long nrl, long nrh, long ncl, long nch)
Frees a matrix allocated with dmatrix.

void free_imatrix(int **m, long nrl, long nrh, long ncl, long nch)
Frees a matrix allocated with imatrix.

A typical use is

float **a;
a=matrix(1,13,1,9);

a(3](S]=...
...+a[2] [9]/3.0...
someroutine(a,...);

free_matrix(a,1,13,1,9);

All matrices in Numerical Recipes are handled with the above paradigm, and we
commend it to you.

Some further utilities for handling matrices are also included in nrutil.c.
The first is a function submatrix() that sets up a new pointer reference to an
already-existing matrix (or sub-block thereof), along with new offsets if desired.
Its synopsis is

float **submatrix(float **a, long oldrl, long oldrh, long oldcl,

long oldch, long newrl, long newcl)
Point a submatrix [newrl..newrl+(oldrh-oldr1)] [newcl. .newcl+(oldch-oldcl)] to
the existing matrix range a[oldrl..oldrh] [oldcl..oldch].

Here oldrl and oldrh are respectively the lower and upper row indices of the
original matrix that are to be represented by the new matrix, oldcl and oldch are
the corresponding column indices, and newrl and newcl are the lower row and
column indices for the new matrix. (We don’t need upper row and column indices,
since they are implied by the quantities already given.)

Two sample uses might be, first, to select as a 2 x 2 submatrix b[1. .2]
[1..2] some interior range of an existing matrix, say a[4..5] [2..3],

float **a,**b;
a=matrix(1,13,1,9);

b=submatrix(a,4,5,2,3,1,1);

and second, to map an existing matrix a[1..13][1..9] into a new matrix
plO..12] [0..8],

float **a,**b;
a=matrix(1,13,1,9);

b=submatrix(a,1,13,1,9,0,0);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 23

Incidentally, you can use submatrix() for matrices of any type whose sizeof ()
is the same as sizeof (float) (often true for int, e.g.); just cast the first argument
to type float ** and cast the result to the desired type,e.g., int **.

The function

void free_submatrix(float **b, long nrl, long nrh, long ncl, long nch)

frees the array of row-pointers allocated by submatrix (). Note that it does not free
the memory allocated to the data in the submatrix, since that space still lies within
the memory allocation of some original matrix.

Finally, if you have a standard C matrix declared as a[nrow] [ncol], and you
want to convert it into a matrix declared in our pointer-to-row-of-pointers manner,
the following function does the trick:

float **convert_matrix(float *a, long nrl, long nrh, long ncl, long nch)
Allocate a float matrix m[nrl..nrh] [ncl..nch] that points to the matrix declared in the
standard C manner as a[nrow] [ncol] , where nrow=nrh-nrl+i1 and ncol=nch-ncl+i. The
routine should be called with the address &a[0] [0] as the first argument.

(You can use this function when you want to make use of C’s initializer syntax
to set values for a matrix, but then be able to pass the matrix to programs in this
book.) The function

void free_convert_matrix(float **b, long nrl, long nrh, long ncl, long nch)
Free a matrix allocated by convert_matrix().

frees the allocation, without affecting the original matrix a.

The only examples of allocating a three-dimensional array as a pointer-to-
pointer-to-pointer structure in this book are found in the routines r1£t3 in §12.5 and
sfroid in §17.4. The necessary allocation and deallocation functions are

float ***f3tensor(long nrl, long nrh, long ncl, long nch, long ndl, long ndh)
Allocate a float 3-dimensional array with subscript range [nrl. .nrh] [ncl..nch] [ndl. .ndh] .

void free_f3tensor(float ***t, long nrl, long nrh, long ncl, long nch,
long ndl, long ndh)
Free a float 3-dimensional array allocated by f3tensor().

Complex Arithmetic

C does not have complex data types, or predefined arithmetic operations on
complex numbers. That omission is easily remedied with the set of functions in
the file complex.c which is printed in full in Appendix C at the back of the book.
A synopsis is as follows:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

24 Chapter 1. Preliminaries

typedef struct FCOMPLEX {float r,i;} fcomplex;

fcomplex Cadd(fcomplex a, fcomplex b)
Returns the complex sum of two complex numbers.

fcomplex Csub(fcomplex a, fcomplex b)
Returns the complex difference of two complex numbers.

fcomplex Cmul(fcomplex a, fcomplex b)
Returns the complex product of two complex numbers.

fcomplex Cdiv(fcomplex a, fcomplex b)
Returns the complex quotient of two complex numbers.

fcomplex Csqrt(fcomplex z)
Returns the complex square root of a complex number.

fcomplex Conjg(fcomplex z)
Returns the complex conjugate of a complex number.

float Cabs(fcomplex z)
Returns the absolute value (modulus) of a complex number.

fcomplex Complex(float re, float im)
Returns a complex number with specified real and imaginary parts.

fcomplex RCmul(float x, fcomplex a)
Returns the complex product of a real number and a complex number.

The implementation of several of these complex operations in floating-point
arithmetic is not entirely trivial; see §5.4.

Only about half a dozen routines in this book make explicit use of these complex
arithmetic functions. The resulting code is not as readable as one would like, because
the familiar operations +-*/ are replaced by function calls. The C++ extension to
the C language allows operators to be redefined. That would allow more readable
code. However, in this book we are committed to standard C.

We should mention that the above functions assume the ability to pass, return,
and assign structures like FCOMPLEX (or types such as fcomplex that are defined
to be structures) by value. All recent C compilers have this ability, but it is not in
the original K&R C definition. If you are missing it, you will have to rewrite the
functions in complex.c, making them pass and return pointers to variables of type
fcomplex instead of the variables themselves. Likewise, you will need to modify
the recipes that use the functions.

Several other routines (e.g., the Fourier transforms four1 and fourn) do
complex arithmetic “by hand,” that is, they carry around real and imaginary parts as
float variables. This results in more efficient code than would be obtained by using
the functions in complex .c. But the code is even less readable. There is simply no
ideal solution to the complex arithmetic problem in C.

Implicit Conversion of Float to Double

In traditional (K&R) C, float variables are automatically converted to double
before any operation is attempted, including both arithmetic operations and passing
as arguments to functions. All arithmetic is then done in double precision. If a
float variable receives the result of such an arithmetic operation, the high precision

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 25

is immediately thrown away. A corollary of these rules is that all the real-number
standard C library functions are of type double and compute to double precision.

The justification for these conversion rules is, “well, there’s nothing wrong with
a little extra precision,” and “this way the libraries need only one version of each
function.” One does not need much experience in scientific computing to recognize
that the implicit conversion rules are, in fact, sheer madness! In effect, they make it
impossible to write efficient numerical programs. One of the cultural barriers that
separates computer scientists from “regular” scientists and engineers is a differing
point of view on whether a 30% or 50% loss of speed is worth worrying about. In
many real-time or state-of-the-art scientific applications, such a loss is catastrophic.
The practical scientist is trying to solve tomorrow’s problem with yesterday’s
computer; the computer scientist, we think, often has it the other way around.

The ANSI C standard happily does not allow implicit conversion for arithmetic
operations, but it does require it for function arguments, unless the function is fully
prototyped by an ANSI declaration as described earlier in this section. That is
another reason for our being rigorous about using the ANSI prototype mechanism,
and a good reason for you to use an ANSI-compatible compiler.

Some older C compilers do provide an optional compilation mode in which
the implicit conversion of float to double is suppressed. Use this if you can.
In this book, when we write float, we mean float; when we write double,
we mean double, i.e., there is a good algorithmic reason for having higher
precision. Our routines all can tolerate the traditional implicit conversion rules,
but they are more efficient without them. Of course, if your application actually
requires double precision, you can change our declarations from float to double
without difficulty. (The brute force approach is to add a preprocessor statement
#define float double !)

A Few Wrinkles

We like to keep code compact, avoiding unnecessary spaces unless they add
immediate clarity. We usually don’t put space around the assignment operator “=”.
Through a quirk of history, however, some C compilers recognize the (nonexistent)
operator “‘=-” as being equivalent to the subtractive assignment operator “-=”, and
“=«” as being the same as the multiplicative assignment operator “*=”. That is why
you will see us write y= -10.0; or y=(-10.0);, and y= *a; or y=(*a) ;.

We have the same viewpoint regarding unnecessary parentheses. You can’t write
(or read) C effectively unless you memorize its operator precedence and associativity
tules. Please study the accompanying table while you brush your teeth every night.

We never use the register storage class specifier. Good optimizing compilers
are quite sophisticated in making their own decisions about what to keep in registers,
and the best choices are sometimes rather counter-intuitive.

Different compilers use different methods of distinguishing between defining
and referencing declarations of the same external name in several files. We follow
the most common scheme, which is also the ANSI standard. The storage class
extern is explicitly included on all referencing top-level declarations. The storage
class is omitted from the single defining declaration for each external variable. We
have commented these declarations, so that if your compiler uses a different scheme
you can change the code. The various schemes are discussed in §4.8 of [1].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

26 Chapter 1. Preliminaries

Operator Precedence and Associativity Rules in C

oO function call left-to-right
0 array element
structure or union member
> pointer reference to structure
! logical not right-to-left
~ bitwise complement
- unary minus
++ increment
-- decrement
& address of
* contents of
(type) cast to type
sizeof size in bytes
* multiply left-to-right
/ divide
h remainder
+ add left-to-right
- subtract
<< bitwise left shift left-to-right
>> bitwise right shift
< arithmetic less than left-to-right
> arithmetic greater than
<= arithmetic less than or equal to
>= arithmetic greater than or equal to
== arithmetic equal left-to-right

f= arithmetic not equal

& bitwise and left-to-right

- bitwise exclusive or left-to-right

| bitwise or left-to-right

&& logical and left-to-right

I logical or left-to-right

Po: conditional expression right-to-left

= ignment operator right-to-left
also += -= *= /= Y=
<<= >>= &= 7= |=

, sequential expression left-to-right

We have already alluded to the problem of computing small integer powers of
numbers, most notably the square and cube. The omission of this operation from C
is perhaps the language’s most galling insult to the scientific programmer. All good
FORTRAN compilers recognize expressions like (A+B) **4 and produce in-line code,
in this case with only one add and two multiplies. It is typical for constant integer
powers up to 12 to be thus recognized.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.2 Some C Conventions for Scientific Computing 27

In C, the mere problem of squaring is hard enough! Some people “macro-ize”
the operation as

#define SQR(a) ((a)*(a))

However, this is likely to produce code where SQR(sin(x) ) results in two calls to
the sine routine! You might be tempted to avoid this by storing the argument of the
squaring function in a temporary variable:

static float sqrarg;
#define SQR(a) (sqrarg=(a) ,sqrarg*sqrarg)

The global variable sqrarg now has (and needs to keep) scope over the whole
module, which is a little dangerous. Also, one needs a completely different macro to
square expressions of type int. More seriously, this macro can fail if there are two
SQR operations in a single expression. Since in C the order of evaluation of pieces of
the expression is at the compiler’s discretion, the value of sqrarg in one evaluation
of SQR can be that from the other evaluation in the same expression, producing
nonsensical results. When we need a guaranteed-correct SQR macro, we use the
following, which exploits the guaranteed complete evaluation of subexpressions in
a conditional expression:

static float sqrarg;
#define SQR(a) ((sqrarg=(a)) == 0.0 ? 0.0 : sqrarg*sqrarg)

A collection of macros for other simple operations is included in the file nrutil.h
(see Appendix B) and used by many of our programs. Here are the synopses:

SQR(a) Square a float value.

DSQR(a) Square a double value.
FMAX(a,b) Maximum of two float values.
FMIN(a,b) Minimum of two float values.
DMAX(a,b) Maximum of two double values.
DMIN(a,b) Minimum of two double values.
IMAX(a,b) Maximum of two int values.
IMIN(a,b) Minimum of two int values.
LMAX(a,b) Maximum of two long values.
LMIN(a,b) Minimum of two long values.
SIGN(a,b) Magnitude of a times sign of b.

Scientific programming in C may someday become a bed of roses; for now,
watch out for the thorns!

CITED REFERENCES AND FURTHER READING:

Harbison, S.P., and Steele, G.L., Jr. 1991, C: A Reference Manual, 3rd ed. (Englewood Cliffs,
NJ: Prentice-Hall). [1]

AT&T Bell Laboratories 1985, The C Programmer’s Handbook (Englewood Cliffs, NJ: Prentice-
Hall).

Kernighan, B., and Ritchie, D. 1978, The C Programming Language (Englewood Cliffs, NJ:
Prentice-Hall). [Reference for K&R “traditional” C. Later editions of this book conform to
the ANSI C standard.]

Hogan, T. 1984, The C Programmer's Handbook (Bowie, MD: Brady Communications).

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

28 Chapter 1. Preliminaries

1.3 Error, Accuracy, and Stability

Although we assume no prior training of the reader in formal numerical analysis,
we will need to presume a common understanding of a few key concepts. We will
define these briefly in this section.

Computers store numbers not with infinite precision but rather in some approxi-
mation that can be packed into a fixed number of bits (binary digits) or bytes (groups
of 8 bits). Almost all computers allow the programmer a choice among several
different such representations or data types. Data types can differ in the number of
bits utilized (the wordlength), but also in the more fundamental respect of whether
the stored number is represented in fixed-point (int or long) or floating-point
(float or double) format.

A number in integer representation is exact. Arithmetic between numbers in
integer representation is also exact, with the provisos that (i) the answer is not outside
the range of (usually, signed) integers that can be represented, and (ii) that division
is interpreted as producing an integer result, throwing away any integer remainder.

In floating-point representation, a number is represented internally by a sign bit
s (interpreted as plus or minus), an exact integer exponent e, and an exact positive
integer mantissa /. Taken together these represent the number

sx Mx BOF (1.3.1)

where B is the base of the representation (usually B = 2, but sometimes B = 16),
and E is the bias of the exponent, a fixed integer constant for any given machine
and representation. An example is shown in Figure 1.3.1.

Several floating-point bit patterns can represent the same number. If B = 2,
for example, a mantissa with leading (high-order) zero bits can be left-shifted, i.e.,
multiplied by a power of 2, if the exponent is decreased by a compensating amount.
Bit patterns that are “as left-shifted as they can be” are termed normalized. Most
computers always produce normalized results, since these don’t waste any bits of
the mantissa and thus allow a greater accuracy of the representation. Since the
high-order bit of a properly normalized mantissa (when B = 2) is always one, some
computers don’t store this bit at all, giving one extra bit of significance.

Arithmetic among numbers in floating-point representation is not exact, even if
the operands happen to be exactly represented (i.e., have exact values in the form of
equation 1.3.1). For example, two floating numbers are added by first right-shifting
(dividing by two) the mantissa of the smaller (in magnitude) one, simultaneously
increasing its exponent, until the two operands have the same exponent. Low-order
(least significant) bits of the smaller operand are lost by this shifting. If the two
operands differ too greatly in magnitude, then the smaller operand is effectively
replaced by zero, since it is right-shifted to oblivion.

The smallest (in magnitude) floating-point number which, when added to the
floating-point number 1.0, produces a floating-point result different from 1.0 is
termed the machine accuracy €,,. A typical computer with B = 2 and a 32-bit
wordlength has ¢€,, around 3 x 10-8. (A more detailed discussion of machine
characteristics, and a program to determine them, is given in §20.1.) Roughly

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.3 Error, Accuracy, and Stability

29

de
>:
Cy SS oe
s SS s
eS oe wes roe
se SF MS cs
mS
%=0 10000000 10000000000000000000000 (a)
3=0 10000010 11000000000000000000000 (b)
Yo=0 OLLI1111 10000000000000000000000 (c)
10-7=0 01101001 11010110101111111001010 (@d
=0 10000010 00000000000000000000000) ¢@)
3+10-7=0 10000010 11000000000000000000000 (f)

Figure 1.3.1.

Floating point representations of numbers in a typical 32-bit (4-byte) format. (a) The

number 1/2 (note the bias in the exponent); (b) the number 3; (c) the number 1/4; (d) the number
1077, represented to machine accuracy; (e) the same number 10-7, but shifted so as to have the same
exponent as the number 3; with this shifting, all significance is lost and 10-7 becomes zero; shifting to
a common exponent must occur before two numbers can be added; (f) sum of the numbers 3 + 10-7,
which equals 3 to machine accuracy. Even though 10~7 can be represented accurately by itself, it cannot
accurately be added to a much larger number.

speaking, the machine accuracy €,, is the fractional accuracy to which floating-point
numbers are represented, corresponding to a change of one in the least significant
bit of the mantissa. Pretty much any arithmetic operation among floating numbers
should be thought of as introducing an additional fractional error of at least € m. This
type of error is called roundoff error.

It is important to understand that €,,, is not the smallest floating-point number
that can be represented on a machine. That number depends on how many bits there
are in the exponent, while €,,, depends on how many bits there are in the mantissa.

Roundoff errors accumulate with increasing amounts of calculation. If, in the
course of obtaining a calculated value, you perform NV such arithmetic operations,
you might be so lucky as to have a total roundoff error on the order of V/Nem, if
the roundoff errors come in randomly up or down. (The square root comes from a
random-walk.) However, this estimate can be very badly off the mark for two reasons:

(i) It very frequently happens that the regularities of your calculation, or the
peculiarities of your computer, cause the roundoff errors to accumulate preferentially
in one direction. In this case the total will be of order Ne».

(ii) Some especially unfavorable occurrences can vastly increase the roundoff
error of single operations. Generally these can be traced to the subtraction of two
very nearly equal numbers, giving a result whose only significant bits are those
(few) low-order ones in which the operands differed. You might think that such a
“coincidental” subtraction is unlikely to occur. Not always so. Some mathematical
expressions magnify its probability of occurrence tremendously. For example, in the
familiar formula for the solution of a quadratic equation,

—b + Vb? — 4dac
r= = (1.3.2)
a

the addition becomes delicate and roundoff-prone whenever ac < b?. (In §5.6 we
will learn how to avoid the problem in this particular case.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

30 Chapter 1. Preliminaries

Roundoff error is a characteristic of computer hardware. There is another,
different, kind of error that is a characteristic of the program or algorithm used,
independent of the hardware on which the program is executed. Many numerical
algorithms compute “discrete” approximations to some desired “continuous” quan-
tity. For example, an integral is evaluated numerically by computing a function
at a discrete set of points, rather than at “every” point. Or, a function may be
evaluated by summing a finite number of leading terms in its infinite series, rather
than all infinity terms. In cases like this, there is an adjustable parameter, e.g., the
number of points or of terms, such that the “true” answer is obtained only when
that parameter goes to infinity. Any practical calculation is done with a finite, but
sufficiently large, choice of that parameter.

The discrepancy between the true answer and the answer obtained in a practical
calculation is called the truncation error. Truncation error would persist even on a
hypothetical, “perfect” computer that had an infinitely accurate representation and no
roundoff error. As a general rule there is not much that a programmer can do about
roundoff error, other than to choose algorithms that do not magnify it unnecessarily
(see discussion of “stability” below). Truncation error, on the other hand, is entirely
under the programmer’s control. In fact, it is only a slight exaggeration to say
that clever minimization of truncation error is practically the entire content of the
field of numerical analysis!

Most of the time, truncation error and roundoff error do not strongly interact
with one another. A calculation can be imagined as having, first, the truncation error
that it would have if run on an infinite-precision computer, “plus” the roundoff error
associated with the number of operations performed.

Sometimes, however, an otherwise attractive method can be unstable. This
means that any roundoff error that becomes “mixed into” the calculation at an early
stage is successively magnified until it comes to swamp the true answer. An unstable
method would be useful on a hypothetical, perfect computer; but in this imperfect
world it is necessary for us to require that algorithms be stable — or if unstable
that we use them with great caution.

Here is a simple, if somewhat artificial, example of an unstable algorithm:
Suppose that it is desired to calculate all integer powers of the so-called “Golden
Mean,” the number given by

o © 0.61803398 (1.3.3)

V5-1
2

It turns out (you can easily verify) that the powers ¢” satisfy a simple recursion
relation,

orth = grt _ gn (1.3.4)

Thus, knowing the first two values ¢° = 1 and ¢! = 0.61803398, we can successively
apply (1.3.4) performing only a single subtraction, rather than a slower multiplication
by ¢, at each stage.

Unfortunately, the recurrence (1.3.4) also has another solution, namely the value
- 3 (V5 +1). Since the recurrence is linear, and since this undesired solution has
magnitude greater than unity, any small admixture of it introduced by roundoff errors
will grow exponentially. On a typical machine with 32-bit wordlength, (1.3.4) starts

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

1.3 Error, Accuracy, and Stability 31

to give completely wrong answers by about n = 16, at which point ¢” is down to only
10-4. The recurrence (1.3.4) is unstable, and cannot be used for the purpose stated.

We will encounter the question of stability in many more sophisticated guises,
later in this book.

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 1.

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software (Englewood Cliffs,
NJ: Prentice Hall), Chapter 2.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §1.3.

Wilkinson, J.H. 1964, Rounding Errors in Algebraic Processes (Englewood Cliffs, NJ: Prentice-
Hall).

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 2. Solution of Linear
Algebraic Equations

2.0 Introduction

A set of linear algebraic equations looks like this:

1121 + A202 + 1343 Qnty = by

G21%1 + A22%2 + a3%3

a2ntN = bg

a31%1 + 43202 + 43323

a3nrn = b3

(2.0.1)

amv, + aye%2 + ay3%3 +++: +ayntn = bu

Here the N unknowns xj, j = 1,2,...,.N are related by M equations. The
coefficients aj; with i = 1,2,...,M andj = 1,2,..., N are known numbers, as
are the right-hand side quantities b;,i = 1,2,...,M.

Nonsingular versus Singular Sets of Equations

If N = M then there are as many equations as unknowns, and there is a good
chance of solving for a unique solution set of a ;’s. Analytically, there can fail to
be a unique solution if one or more of the M equations is a linear combination of
the others, a condition called row degeneracy, or if all equations contain certain
variables only in exactly the same linear combination, called column degeneracy.
(For square matrices, a row degeneracy implies a column degeneracy, and vice
versa.) A set of equations that is degenerate is called singular. We will consider
singular matrices in some detail in §2.6.

Numerically, at least two additional things can go wrong:

e While not exact linear combinations of each other, some of the equations
may be so close to linearly dependent that roundoff errors in the machine
render them linearly dependent at some stage in the solution process. In
this case your numerical procedure will fail, and it can tell you that it
has failed.

32

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

2.0 Introduction 33

e Accumulated roundoff errors in the solution process can swamp the true
solution. This problem particularly emerges if N is too large. The
numerical procedure does not fail algorithmically. However, it returns a
set of x’s that are wrong, as can be discovered by direct substitution back
into the original equations. The closer a set of equations is to being singular,
the more likely this is to happen, since increasingly close cancellations
will occur during the solution. In fact, the preceding item can be viewed
as the special case where the loss of significance is unfortunately total.

Much of the sophistication of complicated “linear equation-solving packages”
is devoted to the detection and/or correction of these two pathologies. As you
work with large linear sets of equations, you will develop a feeling for when such
sophistication is needed. It is difficult to give any firm guidelines, since there is no
such thing as a “typical” linear problem. But here is a rough idea: Linear sets with
N as large as 20 or 50 can be routinely solved in single precision (32 bit floating
representations) without resorting to sophisticated methods, if the equations are not
close to singular. With double precision (60 or 64 bits), this number can readily
be extended to N as large as several hundred, after which point the limiting factor
is generally machine time, not accuracy.

Even larger linear sets, N in the thousands or greater, can be solved when the
coefficients are sparse (that is, mostly zero), by methods that take advantage of the
sparseness. We discuss this further in §2.7.

At the other end of the spectrum, one seems just as often to encounter linear
problems which, by their underlying nature, are close to singular. In this case, you
might need to resort to sophisticated methods even for the case of N = 10 (though
rarely for N = 5). Singular value decomposition (§2.6) is a technique that can
sometimes turn singular problems into nonsingular ones, in which case additional
sophistication becomes unnecessary.

Matrices
Equation (2.0.1) can be written in matrix form as
A-x=b (2.0.2)

Here the raised dot denotes matrix multiplication, A is the matrix of coefficients, and
b is the right-hand side written as a column vector,

a, «42. «ss QIN by
ay a22.«-. ap b

Ag | %1 22 2N pa | (2.03)
ami @M2 ... @Mn bu

By convention, the first index on an element aj; denotes its row, the second
index its column. For most purposes you don’t need to know how a matrix is stored
in a computer’s physical memory; you simply reference matrix elements by their
two-dimensional addresses, e.g., @34 = a[3] [4]. We have already seen, in §1.2,
that this C notation can in fact hide a rather subtle and versatile physical storage
scheme, “pointer to array of pointers to rows.” You might wish to review that section

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

34 Chapter 2. Solution of Linear Algebraic Equations

at this point. Occasionally it is useful to be able to peer through the veil, for example
to pass a whole row ali] [j], j=l...., N by the reference a[i].

Tasks of Computational Linear Algebra

We will consider the following tasks as falling in the general purview of this
chapter:

e Solution of the matrix equation A-x = b for an unknown vector x, where A
is a square matrix of coefficients, raised dot denotes matrix multiplication,
and b is a known right-hand side vector (§2.1-§2.10).

e Solution of more than one matrix equation A - x ; = bj, for a set of vectors
xj,j =1,2,...,each corresponding to a different, known right-hand side
vector b;. In this task the key simplification is that the matrix A is held
constant, while the right-hand sides, the b’s, are changed (§2.1-§2.10).

Calculation of the matrix A~! which is the matrix inverse of a square matrix
A,ie., A- AT=AT. A= 1, where | is the identity matrix (all zeros
except for ones on the diagonal). This task is equivalent, for an N x N
matrix A, to the previous task with N different b;’s (j = 1,2,...,.N),
namely the unit vectors (b; = all zero elements except for 1 in the jth
component). The corresponding x’s are then the columns of the matrix
inverse of A (§2.1 and §2.3).

e Calculation of the determinant of a square matrix A (§2.3).

If M < N, or if M = N but the equations are degenerate, then there
are effectively fewer equations than unknowns. In this case there can be either no
solution, or else more than one solution vector x. In the latter event, the solution space
consists of a particular solution x, added to any linear combination of (typically)
N — M vectors (which are said to be in the nullspace of the matrix A). The task
of finding the solution space of A involves

e Singular value decomposition of a matrix A.

This subject is treated in §2.6.

In the opposite case there are more equations than unknowns, M > N. When
this occurs there is, in general, no solution vector x to equation (2.0.1), and the set
of equations is said to be overdetermined. It happens frequently, however, that the
best “compromise” solution is sought, the one that comes closest to satisfying all
equations simultaneously. If closeness is defined in the least-squares sense, i.e., that
the sum of the squares of the differences between the left- and right-hand sides of
equation (2.0.1) be minimized, then the overdetermined linear problem reduces to
a (usually) solvable linear problem, called the

e Linear least-squares problem.

The reduced set of equations to be solved can be written as the N x N set of equations

(A? - A) -x =(A7 -b) (2.0.4)

where A? denotes the transpose of the matrix A. Equations (2.0.4) are called the
normal equations of the linear least-squares problem. There is a close connection

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.0 Introduction 35

between singular value decomposition and the linear least-squares problem, and the
latter is also discussed in §2.6. You should be warned that direct solution of the
normal equations (2.0.4) is not generally the best way to find least-squares solutions.

Some other topics in this chapter include

e Iterative improvement of a solution (§2.5)

e Various special forms: symmetric positive-definite (82.9), tridiagonal
(§2.4), band diagonal (§2.4), Toeplitz (§2.8), Vandermonde (§2.8), sparse
(§2.7)

e Strassen’s “fast matrix inversion” (§2.11).
Standard Subroutine Packages

We cannot hope, in this chapter or in this book, to tell you everything there is to
know about the tasks that have been defined above. In many cases you will have no
alternative but to use sophisticated black-box program packages. Several good ones
are available, though not always in C. LINPACK was developed at Argonne National
Laboratories and deserves particular mention because it is published, documented,
and available for free use. A successor to LINPACK, LAPACK, is now becoming
available. Packages available commercially (though not necessarily in C) include
those in the IMSL and NAG libraries.

You should keep in mind that the sophisticated packages are designed with very
large linear systems in mind. They therefore go to great effort to minimize not only
the number of operations, but also the required storage. Routines for the various
tasks are usually provided in several versions, corresponding to several possible
simplifications in the form of the input coefficient matrix: symmetric, triangular,
banded, positive definite, etc. If you have a large matrix in one of these forms,
you should certainly take advantage of the increased efficiency provided by these
different routines, and not just use the form provided for general matrices.

There is also a great watershed dividing routines that are direct (i.e., execute
in a predictable number of operations) from routines that are iterative (i.e., attempt
to converge to the desired answer in however many steps are necessary). Iterative
methods become preferable when the battle against loss of significance is in danger
of being lost, either due to large N or because the problem is close to singular. We
will treat iterative methods only incompletely in this book, in §2.7 and in Chapters
18 and 19. These methods are important, but mostly beyond our scope. We will,
however, discuss in detail a technique which is on the borderline between direct
and iterative methods, namely the iterative improvement of a solution that has been
obtained by direct methods (§2.5).

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press).

Gill, P.E., Murray, W., and Wright, M.H. 1991, Numerical Linear Algebra and Optimization, vol. 1
(Redwood City, CA: Addison-Wesley).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 4.

Dongarra, J.J., et al. 1979, LINPACK User’s Guide (Philadelphia: S.1.A.M.).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

36 Chapter 2. Solution of Linear Algebraic Equations

Coleman, T.F., and Van Loan, C. 1988, Handbook for Matrix Computations (Philadelphia: S.1.A.M.).

Forsythe, G.E., and Moler, C.B. 1967, Computer Solution of Linear Algebraic Systems (Engle-
wood Cliffs, NJ: Prentice-Hall).

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag).

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of Linear Equations
(New York: Wiley).

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), Chapter 2.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), Chapter 9.

2.1 Gauss-Jordan Elimination

For inverting a matrix, Gauss-Jordan elimination is about as efficient as any
other method. For solving sets of linear equations, Gauss-Jordan elimination
produces both the solution of the equations for one or more right-hand side vectors
b, and also the matrix inverse A~!. However, its principal weaknesses are (i) that it
requires all the right-hand sides to be stored and manipulated at the same time, and
(ii) that when the inverse matrix is not desired, Gauss-Jordan is three times slower
than the best alternative technique for solving a single linear set (§2.3). The method’s
principal strength is that it is as stable as any other direct method, perhaps even a
bit more stable when full pivoting is used (see below).

If you come along later with an additional right-hand side vector, you can
multiply it by the inverse matrix, of course. This does give an answer, but one that is
quite susceptible to roundoff error, not nearly as good as if the new vector had been
included with the set of right-hand side vectors in the first instance.

For these reasons, Gauss-Jordan elimination should usually not be your method
of first choice, either for solving linear equations or for matrix inversion. The
decomposition methods in §2.3 are better. Why do we give you Gauss-Jordan at all?
Because it is straightforward, understandable, solid as a rock, and an exceptionally
good “psychological” backup for those times that something is going wrong and you
think it might be your linear-equation solver.

Some people believe that the backup is more than psychological, that Gauss-
Jordan elimination is an “independent” numerical method. This turns out to be
mostly myth. Except for the relatively minor differences in pivoting, described
below, the actual sequence of operations performed in Gauss-Jordan elimination is
very closely related to that performed by the routines in the next two sections.

For clarity, and to avoid writing endless ellipses (- - -) we will write out equations
only for the case of four equations and four unknowns, and with three different right-
hand side vectors that are known in advance. You can write bigger matrices and
extend the equations to the case of N x N matrices, with MW sets of right-hand
side vectors, in completely analogous fashion. The routine implemented below is,
of course, general.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.1 Gauss-Jordan Elimination 37

Elimination on Column-Augmented Matrices

a1
a21
a31
a41

Consider the linear matrix equation

@12 413° 414 f11 ©12 T13 Yl Y12 Y13Z 14
a22  a23 aza | war | ) { 22 |) { 223 ] Lj { yar y22 yea yaa
a32 a33° 434 @31 ©32 233 y31 -Y32-Y33«34
a42 @43° aaa @A1 x42 @43 yal ya2 yas Yaa
bit biz bi3 100 0
_ baa bo2 bos 010 0
~ bar | | 32 J Uf 33 JU} 0 0 1 0 (2.1
bat baz baa 0001

1)

Here the raised dot (-) signifies matrix multiplication, while the operator L! just
signifies column augmentation, that is, removing the abutting parentheses and
making a wider matrix out of the operands of the LI operator.

It should not take you long to write out equation (2.1.1) and to see that it simply

states that aj; is the ith component (i = 1, 2,3, 4) of the vector solution of the jth
right-hand side (j = 1, 2,3), the one whose coefficients are bi;,i = 1,2,3,4; and

that the matrix of unknown coefficients y;; is the inverse matrix of a;;. In otl
words, the matrix solution of

where A and Y are square matrices, the b;’s and x;’s are column vectors, and
the identity matrix, simultaneously solves the linear sets

and

[A] - [x7 U x2 U x3 UY] = [by Li bg Ub3 U1] (2.1

A-x,j=by A:x9=b2 A-x3 =b3 (2.1

A-Y=1 (2.

Now it is also elementary to verify the following facts about (2.1.1):

e Interchanging any two rows of A and the corresponding rows of the b’s
and of 1, does not change (or scramble in any way) the solution x’s and
Y. Rather, it just corresponds to writing the same set of linear equations
in a different order.

e Likewise, the solution set is unchanged and in no way scrambled if we
replace any row in A by a linear combination of itself and any other row,
as long as we do the same linear combination of the rows of the b’s and 1
(which then is no longer the identity matrix, of course).

e Interchanging any two columns of A gives the same solution set only
if we simultaneously interchange corresponding rows of the x’s and of
Y. In other words, this interchange scrambles the order of the rows in
the solution. If we do this, we will need to unscramble the solution by
restoring the rows to their original order.

ner

2)

is

3)

4)

Gauss-Jordan elimination uses one or more of the above operations to reduce

the matrix A to the identity matrix. When this is accomplished, the right-hand side
becomes the solution set, as one sees instantly from (2.1.2).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

38 Chapter 2. Solution of Linear Algebraic Equations

Pivoting

In “Gauss-Jordan elimination with no pivoting,” only the second operation in
the above list is used. The first row is divided by the element a1, (this being a
trivial linear combination of the first row with any other row — zero coefficient for
the other row). Then the right amount of the first row is subtracted from each other
row to make all the remaining a;;’s zero. The first column of A now agrees with
the identity matrix. We move to the second column and divide the second row by
22, then subtract the right amount of the second row from rows 1, 3, and 4, so as to
make their entries in the second column zero. The second column is now reduced
to the identity form. And so on for the third and fourth columns. As we do these
operations to A, we of course also do the corresponding operations to the b’s and to
(which by now no longer resembles the identity matrix in any way!).

Obviously we will run into trouble if we ever encounter a zero element on the
(then current) diagonal when we are going to divide by the diagonal element. (The
element that we divide by, incidentally, is called the pivot element or pivot.) Not so
obvious, but true, is the fact that Gauss-Jordan elimination with no pivoting (no use of
the first or third procedures in the above list) is numerically unstable in the presence
of any roundoff error, even when a zero pivot is not encountered. You must never do
Gauss-Jordan elimination (or Gaussian elimination, see below) without pivoting!
So what is this magic pivoting? Nothing more than interchanging rows (partial
pivoting) or rows and columns (full pivoting), so as to put a particularly desirable
element in the diagonal position from which the pivot is about to be selected. Since
we don’t want to mess up the part of the identity matrix that we have already built up,
we can choose among elements that are both (i) on rows below (or on) the one that
is about to be normalized, and also (ii) on columns to the right (or on) the column
we are about to eliminate. Partial pivoting is easier than full pivoting, because we
don’t have to keep track of the permutation of the solution vector. Partial pivoting
makes available as pivots only the elements already in the correct column. It turns
out that partial pivoting is “almost” as good as full pivoting, in a sense that can be
made mathematically precise, but which need not concern us here (for discussion
and references, see [1]). To show you both variants, we do full pivoting in the routine
in this section, partial pivoting in §2.3.

We have to state how to recognize a particularly desirable pivot when we see
one. The answer to this is not completely known theoretically. It is known, both
theoretically and in practice, that simply picking the largest (in magnitude) available
element as the pivot is a very good choice. A curiosity of this procedure, however, is
that the choice of pivot will depend on the original scaling of the equations. If we take
the third linear equation in our original set and multiply it by a factor of a million, it
is almost guaranteed that it will contribute the first pivot; yet the underlying solution
of the equations is not changed by this multiplication! One therefore sometimes sees
routines which choose as pivot that element which would have been largest if the
original equations had all been scaled to have their largest coefficient normalized to
unity. This is called implicit pivoting. There is some extra bookkeeping to keep track
of the scale factors by which the rows would have been multiplied. (The routines in
§2.3 include implicit pivoting, but the routine in this section does not.)

Finally, let us consider the storage requirements of the method. With a little
reflection you will see that at every stage of the algorithm, either an element of A is

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.1 Gauss-Jordan Elimination 39

predictably a one or zero (if it is already in a part of the matrix that has been reduced
to identity form) or else the exactly corresponding element of the matrix that started
as | is predictably a one or zero (if its mate in A has not been reduced to the identity
form). Therefore the matrix 1 does not have to exist as separate storage: The matrix
inverse of A is gradually built up in A as the original A is destroyed. Likewise,
the solution vectors x can gradually replace the right-hand side vectors b and share
the same storage, since after each column in A is reduced, the corresponding row
entry in the b’s is never again used.

Here is the routine for Gauss-Jordan elimination with full pivoting:

#include <math.h>
#include "nrutil.h"
#define SWAP(a,b) {temp=(a) ; (a)=(b) ; (b)=temp;}

void gaussj(float **a, int n, float **b, int m)
Linear equation solution by Gauss-Jordan elimination, equation (2.1.1) above. a[1..n] [1..n]
is the input matrix. b[1..n][1..m] is input containing the m right-hand side vectors. On
output, a is replaced by its matrix inverse, and b is replaced by the corresponding set of solution
vectors.
{

int *indxc,*indxr,*ipiv;

int i,icol,irow,j,k,1,11;

float big,dum,pivinv,temp;

indxc=ivector(1,n); The integer arrays ipiv, indxr, and indxc are
indxr=ivector(1,n); used for bookkeeping on the pivoting.
ipiv=ivector(1,n);
for (j= =n;j++) ipiv[j]=0;
for (i=1;i<=n;i++) { This is the main loop over the columns to be
big=0.0; reduced.
for (j=1;j<=n;j++) This is the outer loop of the search for a pivot

if (ipiv([j] != 1) element.
for (k=1;k<=n;k++) {
if (ipiv[k] == 0) {
if (fabs(a[j][k]) >= big) {
big=fabs(a([j] [k]);
irow=j;
icol=k;

+
3
++(ipiv[icol]);
We now have the pivot element, so we interchange rows, if needed, to put the pivot
element on the diagonal. The columns are not physically interchanged, only relabeled:
indxc [i], the column of the ith pivot element, is the ith column that is reduced, while
indxr [i] is the row in which that pivot element was originally located. If indxr[i] 4
indxc [i] there is an implied column interchange. With this form of bookkeeping, the
solution b's will end up in the correct order, and the inverse matrix will be scrambled
by columns.
if (irow != icol) {
for ( n;1++) SWAP(a[irow] [1] ,a[icol] [1])

for (1=1;1<=m;1++) SWAP(b[irow] [1] ,b[icol] [1])
}
indxr [i]=irow; We are now ready to divide the pivot row by the
indxc [i]=icol; pivot element, located at irow and icol.
if (a[icol] [icol] == 0.0) nrerror("gaussj: Singular Matrix");

pivinv=1.0/a[icol] [icol] ;

a[icol] [icol]=1.0;

for (1=1;1<=n;1++) aficol][1] *= pivinv;
for (1=1;1<=m;1++) bficol][1] *= pivinv;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

40 Chapter 2. Solution of Linear Algebraic Equations

for (11=1;11<=n;11++) Next, we reduce the rows...
if (11 icol) { ...except for the pivot one, of course.
dum=a [11] [icol] ;
a[11] [icol]=0.0;
for (1=1;1<=n;1++) af11])[1] -= alicol] [1]*dum;
for (1=1;1<=m;1++) bf[11][1] -= blicol] [1]*dum;

3
+
This is the end of the main loop over columns of the reduction. It only remains to unscram-
ble the solution in view of the column interchanges. We do this by interchanging pairs of
columns in the reverse order that the permutation was built up.
for (1=n;1>=1;1--) {

if (indxr[1] != indxc[1])
for (k=1;k<=n;k++)
SWAP (a([k] Cindxr[1]] ,a(k] [indxc[1]]);

} And we are done.
free_ivector(ipiv,1,n);
free_ivector(indxr,1,n);
free_ivector(indxc,1,n);

Row versus Column Elimination Strategies

The above discussion can be amplified by a modest amount of formalism. Row
operations on a matrix A correspond to pre- (that is, left-) multiplication by some simple
matrix R. For example, the matrix R with components

1 ifi=jandi 42,4
_ Ja ifi=2,j=4
Ru=91 itinay a2 (2.1.5)

0 otherwise
effects the interchange of rows 2 and 4. Gauss-Jordan elimination by row operations alone
(including the possibility of partial pivoting) consists of a series of such left-multiplications,
yielding successively

A-x=b
(---Rg-Ro-Ri-A)-x=---Rg-Ro-Ri-b
(2.1.6)
(1)-x=---R3-R2-Ri-b
X=---R3-Ro-Ry-b

The key point is that since the R’s build from right to left, the right-hand side is simply
transformed at each stage from one vector to another.

Column operations, on the other hand, correspond to post-, or right-, multiplications
by simple matrices, call them C. The matrix in equation (2.1.5), if right-multiplied onto a
matrix A, will interchange A’s second and fourth columns. Elimination by column operations
involves (conceptually) inserting a column operator, and also its inverse, between the matrix
A and the unknown vector x:

A-x=b

A-Ci-Cy)-x=b
A-C1-Co-Cz!-Cy*-x=b (2.1.7)

(A-C1-Co-C3-+)--Cz1- Cp! -Cy!-x=b

(1)---e3!-cy!-cyt-x=b

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.2 Gaussian Elimination with Backsubstitution 41

which (peeling of the C~!’s one at a time) implies a solution
x=C1-Co-C3---b (2.1.8)
Notice the essential difference between equation (2.1.8) and equation (2.1.6). In the
latter case, the C’s must be applied to b in the reverse order from that in which they become
known. That is, they must all be stored along the way. This requirement greatly reduces

the usefulness of column operations, generally restricting them to simple permutations, for
example in support of full pivoting.

CITED REFERENCES AND FURTHER READING:
Wilkinson, J.H. 1965, The Algebraic Eigenvalue Problem (New York: Oxford University Press). [1]

Carnahan, B., Luther, H.A., and Wilkes, J.O. 1969, Applied Numerical Methods (New York:
Wiley), Example 5.2, p. 282.

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill), Program B-2, p. 298.

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of Linear Equations
(New York: Wiley).

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §9.3-1.

2.2 Gaussian Elimination with Backsubstitution

The usefulness of Gaussian elimination with backsubstitution is primarily
pedagogical. It stands between full elimination schemes such as Gauss-Jordan, and
triangular decomposition schemes such as will be discussed in the next section.
Gaussian elimination reduces a matrix not all the way to the identity matrix, but
only halfway, to a matrix whose components on the diagonal and above (say) remain
nontrivial. Let us now see what advantages accrue.

Suppose that in doing Gauss-Jordan elimination, as described in §2.1, we at
each stage subtract away rows only below the then-current pivot element. When a 22
is the pivot element, for example, we divide the second row by its value (as before),
but now use the pivot row to zero only ag2 and a4, not a;2 (see equation 2.1.1).
Suppose, also, that we do only partial pivoting, never interchanging columns, so that
the order of the unknowns never needs to be modified.

Then, when we have done this for all the pivots, we will be left with a reduced
equation that looks like this (in the case of a single right-hand side vector):

a, ay a3 a4 v1 ‘
0 ayy a3 Aba | e2} % (2.2.1)
0 0 O ay ra hi

Here the primes signify that the a’s and b’s do not have their original numerical
values, but have been modified by all the row operations in the elimination to this
point. The procedure up to this point is termed Gaussian elimination.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

42 Chapter 2. Solution of Linear Algebraic Equations

Backsubstitution

But how do we solve for the x’s? The last x (a4 in this example) is already
isolated, namely

ty = U4/a44 (2.2.2)
With the last 2 known we can move to the penultimate x,
1

x3 = ——[b5 — r4a44] (2.2.3)
a33

and then proceed with the x before that one. The typical step is

N
r= = SO aja; (2.2.4)

tt j=itl

The procedure defined by equation (2.2.4) is called backsubstitution. The com-
bination of Gaussian elimination and backsubstitution yields a solution to the set
of equations.

The advantage of Gaussian elimination and backsubstitution over Gauss-Jordan
elimination is simply that the former is faster in raw operations count: The
innermost loops of Gauss-Jordan elimination, each containing one subtraction and
one multiplication, are executed N? and N?M times (where there are N equations
and M unknowns). The corresponding loops in Gaussian elimination are executed
only 3N 3 times (only half the matrix is reduced, and the increasing numbers of
predictable zeros reduce the count to one-third), and aN 2M times, respectively.
Each backsubstitution of a right-hand side is 3N’ ? executions of a similar loop (one
multiplication plus one subtraction). For MZ < N (only a few right-hand sides)
Gaussian elimination thus has about a factor three advantage over Gauss-Jordan.
(We could reduce this advantage to a factor 1.5 by not computing the inverse matrix
as part of the Gauss-Jordan scheme.)

For computing the inverse matrix (which we can view as the case of M = N
right-hand sides, namely the N unit vectors which are the columns of the identity
matrix), Gaussian elimination and backsubstitution at first glance require iN 3 (matrix
reduction) + 3N 3 (right-hand side manipulations) + 3N' 3 (N backsubstitutions)
= aN 3 loop executions, which is more than the NV 3 for Gauss-Jordan. However, the
unit vectors are quite special in containing all zeros except for one element. If this
is taken into account, the right-side manipulations can be reduced to only aN 3 loop
executions, and, for matrix inversion, the two methods have identical efficiencies.

Both Gaussian elimination and Gauss-Jordan elimination share the disadvantage
that all right-hand sides must be known in advance. The LU decomposition method
in the next section does not share that deficiency, and also has an equally small
operations count, both for solution with any number of right-hand sides, and for
matrix inversion. For this reason we will not implement the method of Gaussian
elimination as a routine.

CITED REFERENCES AND FURTHER READING:

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §9.3-1.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.3 LU Decomposition and Its Applications 43

Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York: Wiley), §2.1.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §2.2.1.

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of Linear Equations
(New York: Wiley).

2.3 LU Decomposition and Its Applications

Suppose we are able to write the matrix A as a product of two matrices,
L-U=A (2.3.1)

where L is lower triangular (has elements only on the diagonal and below) and U
is upper triangular (has elements only on the diagonal and above). For the case of
a 4x 4 matrix A, for example, equation (2.3.1) would look like this:

ai 0 0 0 Bir Pi2 Bis Pra a1 412 413° @14

a21 a22 0 O | | O 622 B23 Goa} _ | @21 a22 423 2a

a31 932 433° (0 0 0 833 Bsa a31 @32 433° a34

Q41 O42 O43 OA 0 0 0 Baa @41 G42 443° G44
(2.3.2)

We can use a decomposition such as (2.3.1) to solve the linear set

A-x=(L-U)-x=L-(U-x)=b (2.3.3)
by first solving for the vector y such that

L-y=b (2.3.4)
and then solving
U-x=y (2.3.5)

What is the advantage of breaking up one linear set into two successive ones?
The advantage is that the solution of a triangular set of equations is quite trivial, as
we have already seen in §2.2 (equation 2.2.4). Thus, equation (2.3.4) can be solved
by forward substitution as follows,

bi
n= —_
an
1 i-1 (2.3.6)
ron wD aus i= 2,3,...,N
j=

while (2.3.5) can then be solved by backsubstitution exactly as in equations (2.2.2)—
(2.2.4),

ty = is
PNN
1 N (2.3.7)
v= a ly Se Bigs i=N-1,N—-2,...,1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

44 Chapter 2. Solution of Linear Algebraic Equations

Equations (2.3.6) and (2.3.7) total (for each right-hand side b) N ? executions
of an inner loop containing one multiply and one add. If we have N right-hand
sides which are the unit column vectors (which is the case when we are inverting a
matrix), then taking into account the leading zeros reduces the total execution count
of (2.3.6) from $N* to 2.N®, while (2.3.7) is unchanged at 4.3.

Notice that, once we have the LU decomposition of A, we can solve with as
many right-hand sides as we then care to, one at a time. This is a distinct advantage
over the methods of §2.1 and §2.2.

Performing the LU Decomposition
How then can we solve for L and U, given A? First, we write out the
i, jth component of equation (2.3.1) or (2.3.2). That component always is a sum
beginning with
a1 By + +++ = iy

The number of terms in the sum depends, however, on whether i or 7 is the smaller
number. We have, in fact, the three cases,

i<j: ai Piz + Ai2Baj +++ + an Big = aij (2.3.8
i=j: aij + Ai2Baj ++>+ + ain Bjy = aig (2.3.9)
i>j: ani Pry + A282; + +++ + a4 855 = aay (2.3.10

Equations (2.3.8)-(2.3.10) total N? equations for the N? + N unknown a’s and
3's (the diagonal being represented twice). Since the number of unknowns is greater
than the number of equations, we are invited to specify N of the unknowns arbitrarily
and then try to solve for the others. In fact, as we shall see, it is always possible to take

ai=l  i=1,...,N (2.3.11)

A surprising procedure, now, is Crout’s algorithm, which quite trivially solves
the set of N? + N equations (2.3.8)-(2.3.11) for all the a’s and ’s by just arranging
the equations in a certain order! That order is as follows:

e Seta; = 1,27 =1,..., N (equation 2.3.11).

e For each j = 1,2,3,..., N do these two procedures: First, for i =

1,2,..., »J, use (2. 3.8), (2.3.9), and (2.3.11) to solve for 3 ;;, namely

i-1

Big = ai3 — ain Bj. (2.3.12)

k=1

(When 7 = 1 in 2.3.12 the summation term is taken to mean zero.) Second,
fori = j7+1,j+2,...,N use (2.3.10) to solve for a;;, namely

1 a
ig Bij («. a oud ( )

Be sure to do both procedures before going on to the next 7.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

2.3 LU Decomposition and Its Applications 45

Figure 2.3.1. Crout’s algorithm for LU decomposition of a matrix. Elements of the original matrix are
modified in the order indicated by lower case letters: a, b, c, etc. Shaded boxes show the previously
modified elements that are used in modifying two typical elements, each indicated by an “x”.

If you work through a few iterations of the above procedure, you will see that
the a’s and ’s that occur on the right-hand side of equations (2.3.12) and (2.3.13)
are already determined by the time they are needed. You will also see that every a ;;
is used only once and never again. This means that the corresponding a ;; or i; can
be stored in the location that the a used to occupy: the decomposition is “in place.”
[The diagonal unity elements a;; (equation 2.3.11) are not stored at all.] In brief,
Crout’s method fills in the combined matrix of a’s and (’s,

Pr Pio Bis Pra

a21 B22 Bog Boa (23.14)
a31 32 33 Bsa ~
Q41 42 43 Baa

by columns from left to right, and within each column from top to bottom (see
Figure 2.3.1).

What about pivoting? Pivoting (i.e., selection of a salubrious pivot element for
the division in equation 2.3.13) is absolutely essential for the stability of Crout’s

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

46 Chapter 2. _ Solution of Linear Algebraic Equations

method. Only partial pivoting (interchange of rows) can be implemented efficiently.
However this is enough to make the method stable. This means, incidentally, that
we don’t actually decompose the matrix A into LU form, but rather we decompose
a rowwise permutation of A. (If we keep track of what that permutation is, this
decomposition is just as useful as the original one would have been.)

Pivoting is slightly subtle in Crout’s algorithm. The key point to notice is that
equation (2.3.12) in the case of i = j (its final application) is exactly the same as
equation (2.3.13) except for the division in the latter equation; in both cases the
upper limit of the sum is k = j — 1 (= i-— 1). This means that we don’t have to
commit ourselves as to whether the diagonal element (3; is the one that happens
to fall on the diagonal in the first instance, or whether one of the (undivided) a ;;’s
below it in the column,i = j +1,..., N, is to be “promoted” to become the diagonal
3. This can be decided after all the candidates in the column are in hand. As you
should be able to guess by now, we will choose the largest one as the diagonal 3
(pivot element), then do all the divisions by that element en masse. This is Crout’s
method with partial pivoting. Our implementation has one additional wrinkle: It
initially finds the largest element in each row, and subsequently (when it is looking
for the maximal pivot element) scales the comparison as if we had initially scaled all
the equations to make their maximum coefficient equal to unity; this is the implicit
pivoting mentioned in §2.1.

#include <math.h>
#include "nrutil.h"
#define TINY 1.0e-20 A small number.

void ludemp(float **a, int n, int *indx, float *d)
Given a matrix a[1..n] [1..n], this routine replaces it by the LU decomposition of a rowwise
permutation of itself. a and n are input. a is output, arranged as in equation (2.3.14) above;
indx[1..n] is an output vector that records the row permutation effected by the partial
pivoting; d is output as +1 depending on whether the number of row interchanges was even
or odd, respectively. This routine is used in combination with lubksb to solve linear equations
or invert a matrix.
{

int i,imax,j,k;

float big,dum,sum,temp;

float *vv; vv stores the implicit scaling of each row.

vv=vector(1,n);
¥d=1.0; No row interchanges yet.
for (i=1;i<=n;i++) { Loop over rows to get the implicit scaling informa-
big=0.0; tion.
for (j=1;j<=n; j++)
if ((temp=fabs(ali][j])) > big) big=temp;

if (big == 0.0) nrerror("Singular matrix in routine ludcmp") ;
No nonzero largest element.
vv [i]=1.0/big; Save the scaling.
}
for This is the loop over columns of Crout’s method.
i<jjit+) { This is equation (2.3.12) except for i = j.
sum=a [i] [j];
for (k=1;k<i;k++) sum -= a[i] [k]*alk] [j];
afi] (j]=sum;
}
big=0.0; Initialize for the search for largest pivot element.
for (i=j;is=n;it++) { This is i = j of equation (2.3.12) andi = j+1...N
sum=a[i] [j]; of equation (2.3.13).

for (k=1;k<j;k++)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.3 LU Decomposition and Its Applications 47

+

sum -= a[i] (k] *alk] [jl];

ali] [j]=sum;

if ( (dum=vv[i]*fabs(sum)) >= big) {

Is the figure of merit for the pivot better than the best so far?
big=dum ;
imax=i;

if (j != imax) { Do we need to interchange rows?
for (k=1;k<=n;k++) { Yes, do so...
dum=a [imax] [k] ;
a[{imax] [k]=a(j] [x];
a(j] [k]=dum;
3
xd = -(*d); ..and change the parity of d.
vv [imax] =vv[j]; Also interchange the scale factor.
}
indx [j]=imax;
if (alj][j] == 0.0) alj][jl=TINy;
If the pivot element is zero the matrix is singular (at least to the precision of the
algorithm). For some applications on singular matrices, it is desirable to substitute
TINY for zero.
if Gj !=m f{ Now, finally, divide by the pivot element.
dum=1.0/(alj] (jl);
for (i=j+1;i<=n;it+) ali] [j] *= dum;
}

Go back for the next column in the reduction.

free_vector(vv,1,n);

Here is the routine for forward substitution and backsubstitution, implementing
equations (2.3.6) and (2.3.7).

void lubksb(float **a, int n, int *indx, float b[])

Solves the set of n linear equations A-X = B. Herea[1..n] [1..n] is input, not as the matrix
A but rather as its LU decomposition, determined by the routine ludemp. indx[1. .n] is input
as the permutation vector returned by ludemp. b[1. .n] is input as the right-hand side vector
B, and returns with the solution vector X. a, n, and indx are not modified by this routine
and can be left in place for successive calls with different right-hand sides b. This routine takes
into account the possibility that b will begin with many zero elements, so it is efficient for use
in matrix inversion.

{

int i,ii=0,ip,j;

float sum;

for (i=1;i<=n;i++) { When ii is set to a positive value, it will become the
ip=indx [i]; index of the first nonvanishing element of b. We now
sum=b[ip] ; do the forward substitution, equation (2.3.6). The
blip] =b [il ; only new wrinkle is to unscramble the permutation
if (ii) as we go.

for (j=ii;j<=i-1;j++) sum -= ali] [jl]*b[j];

else if (sum) ii=i; A nonzero element was encountered, so from now on we
bl[i]=sum; will have to do the sums in the loop above.

}

for (i=n;i>=1;i--) { Now we do the backsubstitution, equation (2.3.7).
sum=b [i] ;
for (j=it1;j<=n;j++) sum -= ali] [j]*b(j];
bli]=sum/a[i] [i] ; Store a component of the solution vector X.

} All done!

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

48 Chapter 2. Solution of Linear Algebraic Equations

The LU decomposition in Ludcmp requires about 3N 3 executions of the inner
loops (each with one multiply and one add). This is thus the operation count
for solving one (or a few) right-hand sides, and is a factor of 3 better than the
Gauss-Jordan routine gaussj which was given in §2.1, and a factor of 1.5 better
than a Gauss-Jordan routine (not given) that does not compute the inverse matrix.
For inverting a matrix, the total count (including the forward and backsubstitution
as discussed following equation 2.3.7 above) is (3 + z + 3)N% = N°, the same
as gaussj.

To summarize, this is the preferred way to solve the linear set of equations
A-x =b:

float **a,*b,d;
int n,*indx;

ludemp(a,n, indx, &d) ;
lubksb(a,n, indx,b) ;

The answer x will be given back in b. Your original matrix A will have
been destroyed.

If you subsequently want to solve a set of equations with the same A but a
different right-hand side b, you repeat only

lubksb(a,n, indx,b) ;

not, of course, with the original matrix A, but with a and indx as were already
set by ludcmp.

Inverse of a Matrix

Using the above LU decomposition and backsubstitution routines, it is com-
pletely straightforward to find the inverse of a matrix column by column.

#define N ...
float **a,**y,d,*col;
int i,j,*indx;

ludcemp(a,N,indx, &d) ;
for(j=1;j<=N; j++) {
for (i=1;i<=N;i++) col[i]=0.0;
col[j]=1.0;
lubksb(a,N,indx,col) ;
for(i=1;i<=Nji++) ylil [j]=col [a];

Decompose the matrix just once.
Find inverse by columns.

The matrix y will now contain the inverse of the original matrix a, which will have
been destroyed. Alternatively, there is nothing wrong with using a Gauss-Jordan
routine like gaussj (§2.1) to invert a matrix in place, again destroying the original.
Both methods have practically the same operations count.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.3 LU Decomposition and Its Applications 49

Incidentally, if you ever have the need to compute A~! - B from matrices A
and B, you should LU decompose A and then backsubstitute with the columns of
B instead of with the unit vectors that would give A’s inverse. This saves a whole
matrix multiplication, and is also more accurate.

Determinant of a Matrix

The determinant of an LU decomposed matrix is just the product of the
diagonal elements,

N
det = [| 6; (2.3.15)
j=l

We don’t, recall, compute the decomposition of the original matrix, but rather a
decomposition of a rowwise permutation of it. Luckily, we have kept track of
whether the number of row interchanges was even or odd, so we just preface the
product by the corresponding sign. (You now finally know the purpose of setting
d in the routine ludcmp.)

Calculation of a determinant thus requires one call to ludcmp, with no subse-
quent backsubstitutions by lubksb.

#define N ...
float **a,d;
int j,*indx;

ludcemp(a,N,indx, &d) ; This returns d as +1.
for (j=1;j<=N; j++) d *= aljl][jl;

The variable d now contains the determinant of the original matrix a, which will
have been destroyed.

For a matrix of any substantial size, it is quite likely that the determinant will
overflow or underflow your computer’s floating-point dynamic range. In this case
you can modify the loop of the above fragment and (e.g.) divide by powers of ten,
to keep track of the scale separately, or (e.g.) accumulate the sum of logarithms of
the absolute values of the factors and the sign separately.

Complex Systems of Equations

If your matrix A is real, but the right-hand side vector is complex, say b + id, then (i)
LU decompose A in the usual way, (ii) backsubstitute b to get the real part of the solution
vector, and (iii) backsubstitute d to get the imaginary part of the solution vector.

If the matrix itself is complex, so that you want to solve the system

(A+ iC) - (x + ty) = (b + id) (2.3.16)

then there are two possible ways to proceed. The best way is to rewrite ludcmp and lubksb
as complex routines. Complex modulus substitutes for absolute value in the construction of
the scaling vector vv and in the search for the largest pivot elements. Everything else goes
through in the obvious way, with complex arithmetic used as needed. (See §§1.2 and 5.4 for
discussion of complex arithmetic in C.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

50 Chapter 2. Solution of Linear Algebraic Equations

A quick-and-dirty way to solve complex systems is to take the real and imaginary
parts of (2.3.16), giving
A-x—C-y=b
(2.3.17)
C-x+A-y=d

which can be written as a 2N x 2N set of real equations,

E00) ae

and then solved with ludcmp and lubksb in their present forms. This scheme is a factor of
2 inefficient in storage, since A and C are stored twice. It is also a factor of 2 inefficient
in time, since the complex multiplies in a complexified version of the routines would each
use 4 real multiplies, while the solution of a 2N x 2N problem involves 8 times the work of
an N x N one. If you can tolerate these factor-of-two inefficiencies, then equation (2.3.18)
is an easy way to proceed.

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), Chapter 4.

Dongarra, J.J., et al. 1979, LINPACK User’s Guide (Philadelphia: S.1.A.M.).

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), §3.3, and p. 50.

Forsythe, G.E., and Moler, C.B. 1967, Computer Solution of Linear Algebraic Systems (Engle-
wood Cliffs, NJ: Prentice-Hall), Chapters 9, 16, and 18.

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of Linear Equations
(New York: Wiley).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
84.2.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §9.11.

Horn, R.A., and Johnson, C.R. 1985, Matrix Analysis (Cambridge: Cambridge University Press).

2.4 Tridiagonal and Band Diagonal Systems
of Equations

The special case of a system of linear equations that is tridiagonal, that is, has
nonzero elements only on the diagonal plus or minus one column, is one that occurs
frequently. Also common are systems that are band diagonal, with nonzero elements
only along a few diagonal lines adjacent to the main diagonal (above and below).

For tridiagonal sets, the procedures of LU decomposition, forward- and back-
substitution each take only O(V) operations, and the whole solution can be encoded
very concisely. The resulting routine tridag is one that we will use in later chapters.

Naturally, one does not reserve storage for the full N x N matrix, but only for
the nonzero components, stored as three vectors. The set of equations to be solved is

bb a Of ut TL

a2 bz cg ++: uz T2
= (2.4.1)

an-1 bn-1 C€N-1 UN-1 TN-1

0 an bn uN rN

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.4 Tridiagonal and Band Diagonal Systems of Equations 51

Notice that a, and cy are undefined and are not referenced by the routine that follows.

#include "nrutil.h"

void tridag(float al], float b[], float c[], float r[], float uf],

unsigned long n)
Solves for a vector u[1..n] the tridiagonal linear set given by equation (2.4.1). a[1..n],
b[1..n], c[41..n], and r[1..n] are input vectors and are not modified.

unsigned long j;
float bet,*gam;

gam=vector(1,n); One vector of workspace, gam is needed.

if (b[1] == 0.0) nrerror("Error 1 in tridag");

If this happens then you should rewrite your equations as a set of order N — 1, with u2

trivially eliminated.

u(i1]=r([1]/(bet=b[1]);

for (j= =n; j++) { Decomposition and forward substitution.
gam[j]=c[j-1]/bet;
bet=b[j]-alj]*gam[j];

if (bet == 0.0) nrerror("Error 2 in tridag"); Algorithm fails; see be-
ulj]=(r[j]-alj]*ulj-1])/bet; low.

}

for (j=(n-1);j>=1;5--)
ulj] -= gam[j+1] *u[j+1]; Backsubstitution.

free_vector(gam,1,n);

There is no pivoting in tridag. It is for this reason that tridag can fail even
when the underlying matrix is nonsingular: A zero pivot can be encountered even for
a nonsingular matrix. In practice, this is not something to lose sleep about. The kinds
of problems that lead to tridiagonal linear sets usually have additional properties
which guarantee that the algorithm in tridag will succeed. For example, if

|bj| > |aj| +

ol j=l. N (2.4.2)

(called diagonal dominance) then it can be shown that the algorithm cannot encounter
a zero pivot.

It is possible to construct special examples in which the lack of pivoting in the
algorithm causes numerical instability. In practice, however, such instability is almost
never encountered — unlike the general matrix problem where pivoting is essential.

The tridiagonal algorithm is the rare case of an algorithm that, in practice, is
more robust than theory says it should be. Of course, should you ever encounter a
problem for which tridag fails, you can instead use the more general method for
band diagonal systems, now described (routines bandec and banbks).

Some other matrix forms consisting of tridiagonal with a small number of
additional elements (e.g., upper right and lower left corners) also allow rapid
solution; see §2.7.

Band Diagonal Systems

Where tridiagonal systems have nonzero elements only on the diagonal plus or minus
one, band diagonal systems are slightly more general and have (say) m1 > 0 nonzero elements
immediately to the left of (below) the diagonal and mz > 0 nonzero elements immediately to
its right (above it). Of course, this is only a useful classification if m1 and mz are both < N.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

52 Chapter 2. Solution of Linear Algebraic Equations

In that case, the solution of the linear system by LU decomposition can be accomplished
much faster, and in much less storage, than for the general N x N case.
The precise definition of a band diagonal matrix with elements a;; is that

aij =0 when j>i+me2 or i>jtm (2.4.3)

Band diagonal matrices are stored and manipulated in a so-called compact form, which results
if the matrix is tilted 45° clockwise, so that its nonzero elements lie in a long, narrow
matrix with m; + 1 + mz columns and N rows. This is best illustrated by an example:
The band diagonal matrix

3 1000 0 0
415 00 0 0
9 26 5 00 0
035 8 9 0 0 (244)
0079 3 2 0
000 3 8 4 6
00002 4 4
which has N = 7,m 1 = 2, and m2 = 1, is stored compactly as the 7 x 4 matrix,
a 415
9 2 6 5
3.5 8 9 (2.4.5)
79 3 2
3.8 4 6
24 4 @

Here x denotes elements that are wasted space in the compact format; these will not be
referenced by any manipulations and can have arbitrary values. Notice that the diagonal
of the original matrix appears in column mj; + 1, with subdiagonal elements to its left,
superdiagonal elements to its right.

The simplest manipulation of a band diagonal matrix, stored compactly, is to multiply
it by a vector to its right. Although this is algorithmically trivial, you might want to study
the following routine carefully, as an example of how to pull nonzero elements a;; out of the
compact storage format in an orderly fashion.

#include "nrutil.h"

void banmul(float **a, unsigned long n, int m1, int m2, float x[], float b{[])
Matrix multiply b = A- x, where A is band diagonal with m1 rows below the diagonal and m2
rows above. The input vector x and output vector b are stored as x[1..n] and b[1..n],
respectively. The array a[1..n][1..m1+m2+1] stores A as follows: The diagonal elements
are in a[1..n] [m1+1]. Subdiagonal elements are in al[j..n][1..m1] (with 7 > 1 ap-
propriate to the number of elements on each subdiagonal). Superdiagonal elements are in
a(1..j] [m1+2..m1+m2+1] with j <n appropriate to the number of elements on each su-
perdiagonal.
{

unsigned long i,j,k,tmploop;

tmploop=LMIN(mi+m2+1,n-k) ;
b[i]=0.0;
for (j=LMAX(1,1-k) ;j<=tmploop;j++) bli] += ali] [j]*x(j+k];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.4 Tridiagonal and Band Diagonal Systems of Equations 53

It is not possible to store the LU decomposition of a band diagonal matrix A quite
as compactly as the compact form of A itself. The decomposition (essentially by Crout’s
method, see §2.3) produces additional nonzero “fill-ins.” One straightforward storage scheme
is to return the upper triangular factor (U) in the same space that A previously occupied, and
to return the lower triangular factor (L) in a separate compact matrix of size N x mi. The
diagonal elements of U (whose product, times d= +1, gives the determinant) are returned
in the first column of A’s storage space.

The following routine, bandec, is the band-diagonal analog of ludcmp in §2.3:

#include <math.h>
#define SWAP(a,b) {dum=(a) ; (a)=(b) ; (b)=dum; }
#define TINY 1.0e-20

void bandec(float **a, unsigned long n, int m1, int m2, float **al,

unsigned long indx[], float *d)
Given an n x n band diagonal matrix A with m1 subdiagonal rows and m2 superdiagonal rows,
compactly stored in the array a[1..n] [1..m1+m2+1] as described in the comment for routine
banmul, this routine constructs an LU decomposition of a rowwise permutation of A. The upper
triangular matrix replaces a, while the lower triangular matrix is returned inal [1..n][1..m1].
indx[1..n] is an output vector which records the row permutation effected by the partial
pivoting; d is output as +1 depending on whether the number of row interchanges was even
or odd, respectively. This routine is used in combination with banbks to solve band-diagonal
sets of equations.
{

unsigned long i,j,k,1;

int mm;

float dum;

mm=mi+m2+1;

1=m1;
for (i=1;i<=m1;i++) { Rearrange the storage a bit.
for (j=m1+2-i;j<=mm;j++) ali] [j-1]=alil [jl];
1--;
for (j=mm-1;j<=mm; j++) ali] [j]=0.0;
+
¥*d=1.0;
1=m1;
for (k=1;k<=n;k++) { For each row...
dum=a [k] [1] ;
i=k;
if (1 <n) 1++;
for (j=k+1;j<=1;jt++) { Find the pivot element.
if (fabs(a[j][1]) > fabs(dum)) {
dum=a[j] [1];
irji
3
}
indx [k]
if (dum 0.0) alk] [1]=TINY;
Matrix is algorithmically singular, but proceed anyway with TINY pivot (desirable in
some applications).
if G !=k) { Interchange rows.
xd = -(*d);
for (j=1;j<=mm;j++) SWAP(a[k] [j],ali][j])
}
for (i=k+1;i<=l;it+) { Do the elimination.
dum=a [i] [1] /a(k] [1];
al [k] [i-k]=dum;
for ( mm; j++) ali] [j-1]=alil] [j]-dum*a[k] [5];
a[i] [mm]=0.0;
}
+

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

54 Chapter 2. Solution of Linear Algebraic Equations

Some pivoting is possible within the storage limitations of bandec, and the above
routine does take advantage of the opportunity. In general, when TINY is returned as a
diagonal element of U, then the original matrix (perhaps as modified by roundoff error)
is in fact singular. In this regard, bandec is somewhat more robust than tridag above,
which can fail algorithmically even for nonsingular matrices; bandec is thus also useful (with
m1 = m2 = 1) for some ill-behaved tridiagonal systems.

Once the matrix A has been decomposed, any number of right-hand sides can be solved in
turn by repeated calls to banbks, the backsubstitution routine whose analog in §2.3 is lubksb.

#define SWAP(a,b) {dum=(a) ; (a)=(b) ; (b)=dum; }

void banbks(float **a, unsigned long n, int m1, int m2, float **al,

unsigned long indx[], float b[])
Given the arrays a, al, and indx as returned from bandec, and given a right-hand side vector
b[1..n], solves the band diagonal linear equations A-x = b. The solution vector x overwrites
b[1..n]. The other input arrays are not modified, and can be left in place for successive calls
with different right-hand sides.

t
unsigned long i,k,1;
int mm;
float dum;
mm=mi+m2+1;
jk<=n;k++) { Forward substitution, unscrambling the permuted rows
ndx [k] ; as we go.
if (i != k) SWAP(b[k] ,b[i])
if (1 <n) 1++;
for (i=kt+1;i<=1;it++) bfi] -= al[k] [i-k] *b[k];
3
1=1;
for (i=n;i>=1;i--) { Backsubstitution.
dum=b [i] ;
for (k=2;k<=1;k++) dum -= a[i] [k] *b[k+i-1];
b{i]=dum/a [i] [1];
if (1 < mm) 1++;
+
}

The routines bandec and banbks are based on the Handbook routines bandetl and
bansoll in [1].

CITED REFERENCES AND FURTHER READING:

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems (Waltham, MA:
Blaisdell), p. 74.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
Example 5.4.3, p. 166.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §9.11.

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag), Chapter 1/6. [1]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), $4.3.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.5 Iterative Improvement of a Solution to Linear Equations 55

Figure 2.5.1. Iterative improvement of the solution to A - x = b. The first guess x + 6x is multiplied by
A to produce b + db. The known vector b is subtracted, giving db. The linear set with this right-hand
side is inverted, giving 5x. This is subtracted from the first guess giving an improved solution x.

2.5 Iterative Improvement of a Solution to
Linear Equations

Obviously it is not easy to obtain greater precision for the solution of a linear
set than the precision of your computer’s floating-point word. Unfortunately, for
large sets of linear equations, it is not always easy to obtain precision equal to, or
even comparable to, the computer’s limit. In direct methods of solution, roundoff
errors accumulate, and they are magnified to the extent that your matrix is close
to singular. You can easily lose two or three significant figures for matrices which
(you thought) were far from singular.

If this happens to you, there is a neat trick to restore the full machine precision,
called iterative improvement of the solution. The theory is very straightforward (see
Figure 2.5.1): Suppose that a vector x is the exact solution of the linear set

A-x=b (2.5.1)
You don’t, however, know x. You only know some slightly wrong solution x + 6x,
where Ox is the unknownerror. When multiplied by the matrix A, your slightly wrong
solution gives a product slightly discrepant from the desired right-hand side b , namely
A: (x + 6x) = b + 6b (2.5.2)

Subtracting (2.5.1) from (2.5.2) gives

A- 5x = db (2.5.3)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

56 Chapter 2. Solution of Linear Algebraic Equations

But (2.5.2) can also be solved, trivially, for db. Substituting this into (2.5.3) gives
A-6x =A: (x+dx)—b (2.5.4)

In this equation, the whole right-hand side is known, since x + 6x is the wrong
solution that you want to improve. It is essential to calculate the right-hand side
in double precision, since there will be a lot of cancellation in the subtraction of b.
Then, we need only solve (2.5.4) for the error 5x, then subtract this from the wrong
solution to get an improved solution.

An important extra benefit occurs if we obtained the original solution by LU
decomposition. In this case we already have the LU decomposed form of A, and all
we need do to solve (2.5.4) is compute the right-hand side and backsubstitute!

The code to do all this is concise and straightforward:

#include "nrutil.h"

void mprove(float **a, float **alud, int n, int indx[], float b[], float x{[])
Improves a solution vector x[1..n] of the linear set of equations A. X = B. The matrix
a[1..n][1..n], and the vectors b[1..n] and x[1..n] are input, as is the dimension n.
Also input is alud[1..n][1..n], the LU decomposition of a as returned by ludcmp, and
the vector indx[1..n] also returned by that routine. On output, only x[1..n] is modified,
to an improved set of values.

{
void lubksb(float **a, int n, int *indx, float b[]);
int j,i;
double sdp;
float *r;
r=vector(1,n);
for (i= =n;it+) { Calculate the right-hand side, accumulating
sdp = -b[il; the residual in double precision.
for (j=1;j<=n;jt++) sdp += ali] (j]*x[jl;
r[il]=sdp;
+
lubksb(alud,n, indx,r) ; Solve for the error term,
for (i=1;i<=n;it++) xfi] -= ri]; and subtract it from the old solution.
free_vector(r,1,n);
3

You should note that the routine ludcmp in §2.3 destroys the input matrix as
it LU decomposes it. Since iterative improvement requires both the original matrix
and its LU decomposition, you will need to copy A before calling ludemp. Likewise
lubksb destroys b in obtaining x, so make a copy of b also. If you don’t mind
this extra storage, iterative improvement is highly recommended: It is a process
of order only N? operations (multiply vector by matrix, and backsubstitute — see
discussion following equation 2.3.7); it never hurts; and it can really give you your
money’s worth if it saves an otherwise ruined solution on which you have already
spent of order N® operations.

You can call mprove several times in succession if you want. Unless you are
starting quite far from the true solution, one call is generally enough; but a second
call to verify convergence can be reassuring.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.5 Iterative Improvement of a Solution to Linear Equations 57

More on Iterative Improvement

It is illuminating (and will be useful later in the book) to give a somewhat more solid
analytical foundation for equation (2.5.4), and also to give some additional results. Implicit in
the previous discussion was the notion that the solution vector x + 5x has an error term; but
we neglected the fact that the LU decomposition of A is itself not exact.

A different analytical approach starts with some matrix Bo that is assumed to be an
approximate inverse of the matrix A, so that Bo - A is approximately the identity matrix 1.
Define the residual matrix R of Bo as

R=1-Bo-A (2.5.5)

which is supposed to be “small” (we will be more precise below). Note that therefore
Bo:-A=1-R (2.5.6)
Next consider the following formal manipulation:

A’'=A'-(By'-Bo) = (A! - By) - Bo = (Bo: A) ' - Bo

2.5.7
=(1—R)7'-Bo =(1+R+R?+R* +--+) Bo G57)

We can define the nth partial sum of the last expression by
Bn =(1+R+-::-+R")-Bo (2.5.8)

so that B. + A”, if the limit exists.
It now is straightforward to verify that equation (2.5.8) satisfies some interesting
recurrence relations. As regards solving A - x = b, where x and b are vectors, define

Xn =Bn-b (2.5.9)
Then it is easy to show that
Xn+1 = Xn + Bo: (b —A- Xn) (2.5.10)

This is immediately recognizable as equation (2.5.4), with —dx = Xn+41 — Xn, and with Bo
taking the role of A7~+. We see, therefore, that equation (2.5.4) does not require that the LU
decomposition of A be exact, but only that the implied residual R be small. In rough terms, if
the residual is smaller than the square root of your computer’s roundoff error, then after one
application of equation (2.5.10) (that is, going from x9 = Bo -b to x;) the first neglected term,
of order R?, will be smaller than the roundoff error. Equation (2.5.10), like equation (2.5.4),
moreover, can be applied more than once, since it uses only Bo, and not any of the higher B’s.

A much more surprising recurrence which follows from equation (2.5.8) is one that more
than doubles the order n at each stage:

Bony1 = 2Bn —Bn-A-Bn 2 =0,1,3,7,--- (2.5.11)

Repeated application of equation (2.5.11), from a suitable starting matrix Bo, converges
quadratically to the unknown inverse matrix A~ (see §9.4 for the definition of “quadrati-
cally”). Equation (2.5.11) goes by various names, including Schultz’s Method and Hotelling’s
Method; see Pan and Reif [1] for references. In fact, equation (2.5.11) is simply the iterative
Newton-Raphson method of root-finding (§9.4) applied to matrix inversion.

Before you get too excited about equation (2.5.11), however, you should notice that it
involves two full matrix multiplications at each iteration. Each matrix multiplication involves
N® adds and multiplies. But we already saw in §§2.1-2.3 that direct inversion of A requires
only N® adds and N* multiplies in toto. Equation (2.5.11) is therefore practical only when
special circumstances allow it to be evaluated much more rapidly than is the case for general
matrices. We will meet such circumstances later, in §13.10.

In the spirit of delayed gratification, let us nevertheless pursue the two related issues:
When does the series in equation (2.5.7) converge; and what is a suitable initial guess Bo (if,
for example, an initial LU decomposition is not feasible)?

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

58 Chapter 2. Solution of Linear Algebraic Equations

We can define the norm of a matrix as the largest amplification of length that it is
able to induce on a vector,

R-
v40— |v|

(2.5.12)
If we let equation (2.5.7) act on some arbitrary right-hand side b, as one wants a matrix inverse
to do, it is obvious that a sufficient condition for convergence is

|IR|| <1 (2.5.13)

Pan and Reif [1] point out that a suitable initial guess for Bo is any sufficiently small constant
€ times the matrix transpose of A, that is,

Bo=ceA’ or R=1-cA7-A (2.5.14)
To see why this is so involves concepts from Chapter 11; we give here only the briefest sketch:

AT. Aisa symmetric, positive definite matrix, so it has real, positive eigenvalues. In its
iagonal representation, R takes the form

R = diag(1 — €d1,1 — eA2,...,1 — €An) (2.5.15)

where all the \;’s are positive. Evidently any ¢ satisfying 0 < € < 2/(max; ;) will give
||R|| < 1. It is not difficult to show that the optimal choice for ¢, giving the most rapid
convergence for equation (2.5.11), is

€ = 2/(max A; + min Ai) (2.5.16)

Rarely does one know the eigenvalues of A? - A in equation (2.5.16). Pan and Reif.
lerive several interesting bounds, which are computable directly from A. The following
choices guarantee the convergence of B,, as n — oo,

e< 1/ Dots or e< 1/ (mae laij| x max > lay!) (2.5.17)
ak j i

The latter expression is truly a remarkable formula, which Pan and Reif derive by noting that
the vector norm in equation (2.5.12) need not be the usual Z2 norm, but can instead be either
the Z.. (max) norm, or the L, (absolute value) norm. See their work for details.

Another approach, with which we have had some success, is to estimate the largest
eigenvalue statistically, by calculating s; = |A - v;|° for several unit vector vi’s with randomly
chosen directions in N-space. The largest eigenvalue \ can then be bounded by the maximum
of 2max s; and 2NVar(s;)/j(si), where Var and js denote the sample variance and mean,
respectively.

CITED REFERENCES AND FURTHER READING:

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §2.3.4, p. 55.

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), p. 74.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§5.5.6, p. 183.

Forsythe, G.E., and Moler, C.B. 1967, Computer Solution of Linear Algebraic Systems (Engle-
wood Cliffs, NJ: Prentice-Hall), Chapter 13.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §9.5, p. 437.

Pan, V., and Reif, J. 1985, in Proceedings of the Seventeenth Annual ACM Symposium on Theory
of Computing (New York: Association for Computing Machinery). [1]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.6 Singular Value Decomposition 59

2.6 Singular Value Decomposition

There exists a very powerful set of techniques for dealing with sets of equations
or matrices that are either singular or else numerically very close to singular. In many
cases where Gaussian elimination and LU decomposition fail to give satisfactory
results, this set of techniques, known as singular value decomposition, or SVD,
will diagnose for you precisely what the problem is. In some cases, SVD will
not only diagnose the problem, it will also solve it, in the sense of giving you a
useful numerical answer, although, as we shall see, not necessarily “the” answer
that you thought you should get.

SVD is also the method of choice for solving most linear least-squares problems.
We will outline the relevant theory in this section, but defer detailed discussion of
the use of SVD in this application to Chapter 15, whose subject is the parametric
modeling of data.

SVD methods are based on the following theorem of linear algebra, whose proof
is beyond our scope: Any M x N matrix A whose number of rows is greater than
or equal to its number of columns JN, can be written as the product of an I x N
column-orthogonal matrix U, an N x N diagonal matrix W with positive or zero
elements (the singular values), and the transpose of an N x NV orthogonal matrix V.
The various shapes of these matrices will be made clearer by the following tableau:

wi
we

WN

(2.6.1)

The matrices U and V are each orthogonal in the sense that their columns are
orthonormal,

M

1<k<N
> VinUin = Skn <N (2.6.2)
i=l _ _
N
1<k<N
» VinVin= Sin Sey (2.6.3)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

60 Chapter 2. Solution of Linear Algebraic Equations

or as a tableau,

(2.6.4)

Since V is square, it is also row-orthonormal, V - vi =1.

The SVD decomposition can also be carried out when M < N. In this case
the singular values w; for 7 = M+ 1,...,N are all zero, and the corresponding
columns of U are also zero. Equation (2.6.2) then holds only for k,n < M.

The decomposition (2.6.1) can always be done, no matter how singular the
matrix is, and it is “almost” unique. That is to say, it is unique up to (i) making
the same permutation of the columns of U, elements of W, and columns of V (or
rows of V"), or (ii) forming linear combinations of any columns of U and V whose
corresponding elements of W happen to be exactly equal. An important consequence
of the permutation freedom is that for the case /Z < N, a numerical algorithm for
the decomposition need not return zero w;’s for j = M+1,...,N; the N— M
zero singular values can be scattered among all positions 7 = 1,2,...,.N.

At the end of this section, we give a routine, svdcmp, that performs SVD on
an arbitrary matrix A, replacing it by U (they are the same shape) and giving back
W and V separately. The routine svdcmp is based on a routine by Forsythe et
al. [1], which is in turn based on the original routine of Golub and Reinsch, found, in
various forms, in [2-4] and elsewhere. These references include extensive discussion
of the algorithm used. As much as we dislike the use of black-box routines, we are
going to ask you to accept this one, since it would take us too far afield to cover
its necessary background material here. Suffice it to say that the algorithm is very
stable, and that it is very unusual for it ever to misbehave. Most of the concepts that
enter the algorithm (Householder reduction to bidiagonal form, diagonalization by
QR procedure with shifts) will be discussed further in Chapter 11.

If you are as suspicious of black boxes as we are, you will want to verify yourself
that svdcmp does what we say it does. That is very easy to do: Generate an arbitrary
matrix A, call the routine, and then verify by matrix multiplication that (2.6.1) and
(2.6.4) are satisfied. Since these two equations are the only defining requirements
for SVD, this procedure is (for the chosen A) a complete end-to-end check.

Now let us find out what SVD is good for.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.6 Singular Value Decomposition 61

SVD of a Square Matrix

If the matrix A is square, N x N say, then U, V, and W are all square matrices
of the same size. Their inverses are also trivial to compute: U and V are orthogonal,
so their inverses are equal to their transposes; W is diagonal, so its inverse is the
diagonal matrix whose elements are the reciprocals of the elements w ;. From (2.6.1)
it now follows immediately that the inverse of A is

A’! =V- [diag (1/w;)]-U" (2.6.5)

The only thing that can go wrong with this construction is for one of the w ;’s
to be zero, or (numerically) for it to be so small that its value is dominated by
roundoff error and therefore unknowable. If more than one of the w ;’s have this
problem, then the matrix is even more singular. So, first of all, SVD gives you a
clear diagnosis of the situation.

Formally, the condition number of a matrix is defined as the ratio of the largest
(in magnitude) of the w;’s to the smallest of the w;’s. A matrix is singular if its
condition number is infinite, and it is ill-conditioned if its condition number is too
large, that is, if its reciprocal approaches the machine’s floating-point precision (for
example, less than 10~° for single precision or 10~!? for double).

For singular matrices, the concepts of nullspace and range are important.
Consider the familiar set of simultaneous equations

A-x=b (2.6.6)

where A is a square matrix, b and x are vectors. Equation (2.6.6) defines A as a
linear mapping from the vector space x to the vector space b. If A is singular, then
there is some subspace of x, called the nullspace, that is mapped to zero, A- x = 0.
The dimension of the nullspace (the number of linearly independent vectors x that
can be found in it) is called the nullity of A.

Now, there is also some subspace of b that can be “reached” by A, in the sense
that there exists some x which is mapped there. This subspace of b is called the range
of A. The dimension of the range is called the rank of A. If A is nonsingular, then its
range will be all of the vector space b, so its rank is NV. If A is singular, then the rank
will be less than NV. In fact, the relevant theorem is “rank plus nullity equals N.”

What has this to do with SVD? SVD explicitly constructs orthonormal bases
for the nullspace and range of a matrix. Specifically, the columns of U whose
same-numbered elements w are nonzero are an orthonormal set of basis vectors that
span the range; the columns of V whose same-numbered elements w j are zero are
an orthonormal basis for the nullspace.

Now let’s have another look at solving the set of simultaneous linear equations
(2.6.6) in the case that A is singular. First, the set of homogeneous equations, where
b = 0, is solved immediately by SVD: Any column of V whose corresponding w ;
is zero yields a solution.

When the vector b on the right-hand side is not zero, the important question is
whether it lies in the range of A or not. If it does, then the singular set of equations
does have a solution x; in fact it has more than one solution, since any vector in
the nullspace (any column of V with a corresponding zero w ;) can be added to x
in any linear combination.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

62 Chapter 2. Solution of Linear Algebraic Equations

If we want to single out one particular member of this solution-set of vectors as
a representative, we might want to pick the one with the smallest length |x| 2 Here is
how to find that vector using SVD: Simply replace 1/w, by zero if w; = 0. (It is not
very often that one gets to set oo = 0 !) Then compute (working from right to left)

x = V- [diag (1/w,)] -(U" -b) (2.6.7)

This will be the solution vector of smallest length; the columns of V that are in the
nullspace complete the specification of the solution set.

Proof: Consider |x + x’|, where x’ lies in the nullspace. Then, if W —1 denotes
the modified inverse of W with some elements zeroed,

Ix +x'|=|V-Wo!-U?-b+x’|
=|V-(W7!-U?-b+V"-x’)| (2.6.8)

=|W7t-U?-b+V7-x’|

Here the first equality follows from (2.6.7), the second and third from the orthonor-
mality of V. If you now examine the two terms that make up the sum on the
right-hand side, you will see that the first one has nonzero 7 components only where
w, # 0, while the second one, since x’ is in the nullspace, has nonzero j components
only where w; = 0. Therefore the minimum length obtains for x’ = 0, q.e.d.

If b is not in the range of the singular matrix A, then the set of equations (2.6.6)
has no solution. But here is some good news: If b is not in the range of A, then
equation (2.6.7) can still be used to construct a “solution” vector x. This vector x
will not exactly solve A. x = b. But, among all possible vectors x, it will do the
closest possible job in the least squares sense. In other words (2.6.7) finds

x which minimizes r= |A-x—b| (2.6.9)

The number r is called the residual of the solution.

The proof is similar to (2.6.8): Suppose we modify x by adding some arbitrary
x’. Then A- x — b is modified by adding some b’ = A- x’. Obviously b’ is in
the range of A. We then have

|A-x—b+b'|=|(U-W-V")-(V-W-?-U"-b) —b +b’

=|(U-W-Wt.UT—1)-b+b'|

=|U-[(W-w-t—1)-U?-b+U? -b)]| (2.6.10)

=|(W-W7!-1)-U7-b+U"-b'|

Now, (W - W~! — 1) is a diagonal matrix which has nonzero j components only for
w,; = 0, while U’b’ has nonzero j components only for w; #0, since b’ lies in the
range of A. Therefore the minimum obtains for b’ = 0, q.e.d.

Figure 2.6.1 summarizes our discussion of SVD thus far.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

2.6 Singular Value Decomposition 63

range of A

SVD solution of
A‘xe=d

(b)

Figure 2.6.1. (a) A nonsingular matrix A maps a vector space into one of the same dimension. The
vector x is mapped into b, so that x satisfies the equation A- x = b. (b) A singular matrix A maps a
vector space into one of lower dimensionality, here a plane into a line, called the “range” of A. The
“nullspace” of A is mapped to zero. The solutions of A - x = d consist of any one particular solution plus
any vector in the nullspace, here forming a line parallel to the nullspace. Singular value decomposition
(SVD) selects the particular solution closest to zero, as shown. The point c lies outside of the range
of A, so A: x = c has no solution. SVD finds the least-squares best compromise solution, namely a
solution of A. x = c’, as shown.

In the discussion since equation (2.6.6), we have been pretending that a matrix
either is singular or else isn’t. That is of course true analytically. Numerically,
however, the far more common situation is that some of the w,;’s are very small
but nonzero, so that the matrix is ill-conditioned. In that case, the direct solution
methods of LU decomposition or Gaussian elimination may actually give a formal
solution to the set of equations (that is, a zero pivot may not be encountered); but
the solution vector may have wildly large components whose algebraic cancellation,
when multiplying by the matrix A, may give a very poor approximation to the
right-hand vector b. In such cases, the solution vector x obtained by zeroing the

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

64 Chapter 2. _ Solution of Linear Algebraic Equations

small w,’s and then using equation (2.6.7) is very often better (in the sense of the
residual |A - x — b| being smaller) than both the direct-method solution and the SVD
solution where the small w,’s are left nonzero.

It may seem paradoxical that this can be so, since zeroing a singular value
corresponds to throwing away one linear combination of the set of equations that
we are trying to solve. The resolution of the paradox is that we are throwing away
precisely a combination of equations that is so corrupted by roundoff error as to be at
best useless; usually it is worse than useless since it “pulls” the solution vector way
off towards infinity along some direction that is almost a nullspace vector. In doing
this, it compounds the roundoff problem and makes the residual |A - x — b| larger.

SVD cannot be applied blindly, then. You have to exercise some discretion in
deciding at what threshold to zero the small w,’s, and/or you have to have some idea
what size of computed residual |A - x — b| is acceptable.

As an example, here is a “backsubstitution” routine svbksb for evaluating
equation (2.6.7) and obtaining a solution vector x from a right-hand side b, given
that the SVD of a matrix A has already been calculated by a call to svdcmp. Note
that this routine presumes that you have already zeroed the small w,’s. It does not
do this for you. If you haven’t zeroed the small w’s, then this routine is just as
ill-conditioned as any direct method, and you are misusing SVD.

#include "nrutil.h"

void svbksb(float **u, float wl], float **v, int m, int n, float b[], float x[])
Solves A: X = B for a vector X, where A is specified by the arrays u[1..m][1..n],w[1..n],
v(1..n] [1..n] as returned by svdcmp. mand n are the dimensions of a, and will be equal for
square matrices. b[1..m] is the input right-hand side. x[1..n] is the output solution vector.
No input quantities are destroyed, so the routine may be called sequentially with different b’s.
{

int jj.j.is

float s,*tmp;

tmp=vector(1,n) ;

for (j=1;j<=n;j++) { Calculate U7 B.
s=0.0;
if (wIj]l) t Nonzero result only if w; is nonzero.
for (i=1;i<=m;it+) s += uli] [j]*b[il;
s /= wljl; This is the divide by wy.

free_vector(tmp,1,n);

Note that a typical use of svdcmp and svbksb superficially resembles the
typical use of ludcmp and lubksb: In both cases, you decompose the left-hand
matrix A just once, and then can use the decomposition either once or many times
with different right-hand sides. The crucial difference is the “editing” of the singular
values before svbksb is called:

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.6 Singular Value Decomposition 65

#define N ...
float wmax,wmin, **a,**u, *w,**V,*b,*xX;
int i,j;
for (i=1;i<=N; i++) Copy a into u if you don't want it to be de-
for j=1;j<=N; j++) stroyed.
uli] (j]=alil [j];
svdemp(u,N,N,w,v); SVD the square matrix a.
wmax=0.0; Will be the maximum singular value obtained.
for(j=1;j<=N; j++) if (w[j] > wmax) wmax=w[j];
This is where we set the threshold for singular values allowed to be nonzero. The constant
is typical, but not universal. You have to experiment with your own application.
wmin=wmax*1.0e-6;
for (j=1;j<=N; j++) if (wlj] < wmin) w[j]=0.0;
svbksb(u,w,v,N,N,b,x); Now we can backsubstitute.

SVD for Fewer Equations than Unknowns

If you have fewer linear equations MM than unknowns JN, then you are not
expecting a unique solution. Usually there will be an N — M dimensional family
of solutions. If you want to find this whole solution space, then SVD can readily
do the job.

The SVD decomposition will yield N — M zero or negligible wj’s, since
M < N. There may be additional zero w,’s from any degeneracies in your M
equations. Be sure that you find this many small w,’s, and zero them before calling
svbksb, which will give you the particular solution vector x. As before, the columns
of V corresponding to zeroed w;’s are the basis vectors whose linear combinations,
added to the particular solution, span the solution space.

SVD for More Equations than Unknowns

This situation will occur in Chapter 15, when we wish to find the least-squares
solution to an overdetermined set of linear equations. In tableau, the equations
to be solved are

A -|x}= |b (2.6.11)

The proofs that we gave above for the square case apply without modification
to the case of more equations than unknowns. The least-squares solution vector x is

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

66 Chapter 2. Solution of Linear Algebraic Equations

given by (2.6.7), which, with nonsquare matrices, looks like this,

x| = Vv - | diag(1/w;) | - ul -|b

(2.6.12)

In general, the matrix W will not be singular, and no w,’s will need to be
set to zero. Occasionally, however, there might be column degeneracies in A. In
this case you will need to zero some small w,; values after all. The corresponding
column in V gives the linear combination of x’s that is then ill-determined even by
the supposedly overdetermined set.

Sometimes, although you do not need to zero any w,’s for computational
reasons, you may nevertheless want to take note of any that are unusually small:
Their corresponding columns in V are linear combinations of x’s which are insensitive
to your data. In fact, you may then wish to zero these w ;’s, to reduce the number of
free parameters in the fit. These matters are discussed more fully in Chapter 15.

Constructing an Orthonormal Basis

Suppose that you have N vectors in an M/-dimensional vector space, with
N < M. Then the N vectors span some subspace of the full vector space.
Often you want to construct an orthonormal set of N vectors that span the same
subspace. The textbook way to do this is by Gram-Schmidt orthogonalization,
starting with one vector and then expanding the subspace one dimension at a
time. Numerically, however, because of the build-up of roundoff errors, naive
Gram-Schmidt orthogonalization is terrible.

The right way to construct an orthonormal basis for a subspace is by SVD:
Form an M x N matrix A whose N columns are your vectors. Run the matrix
through svdcmp. The columns of the matrix U (which in fact replaces A on output
from svdcmp) are your desired orthonormal basis vectors.

You might also want to check the output wj’s for zero values. If any occur,
then the spanned subspace was not, in fact, N dimensional; the columns of U
corresponding to zero w;’s should be discarded from the orthonormal basis set.

(QR factorization, discussed in §2.10, also constructs an orthonormal basis,
see [5].)

Approximation of Matrices

Note that equation (2.6.1) can be rewritten to express any matrix Aj; as a sum
of outer products of columns of U and rows of V7, with the “weighting factors”
being the singular values w,,

N
Ay = > wr Vin Vix (2.6.13)

k=1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.6 Singular Value Decomposition 67

If you ever encounter a situation where most of the singular values wj; of a
matrix A are very small, then A will be well-approximated by only a few terms in the
sum (2.6.13). This means that you have to store only a few columns of U and V (the
same k ones) and you will be able to recover, with good accuracy, the whole matrix.

Note also that it is very efficient to multiply such an approximated matrix by a
vector x: You just dot x with each of the stored columns of V, multiply the resulting
scalar by the corresponding w;,, and accumulate that multiple of the corresponding
column of U. If your matrix is approximated by a small number kc of singular
values, then this computation of A - x takes only about K(M + N) multiplications,
instead of MN for the full matrix.

SVD Algorithm

Here is the algorithm for constructing the singular value decomposition of any
matrix. See §11.2-§11.3, and also [4-5], for discussion relating to the underlying
method.

#include <math.h>
#include "nrutil.h"

void svdemp(float **a, int m, int n, float w[], float **v)
Given a matrix a[1..m][1..n], this routine computes its singular value decomposition, A =
U-W-V?. The matrix U replaces a on output. The diagonal matrix of singular values W is out-
put as a vector w[1..n]. The matrix V (not the transpose V”’) is output as v[1..n] [1. .n].
{

float pythag(float a, float b);

int flag,i,its,j,jj,k,1,nm;

float anorm,c,f,g,h,s,scale,x,y,z,*rv1;

rvi=vector(1,n);
g=scale=anorm=0.0; Householder reduction to bidiagonal form.
for ( jis=n;it+) {
1=i+1;
rvi[i]=scalexg;
g=s=scale=0.0;
if G <=m) {
for (k=i;k<=m;k++) scale += fabs(a[k] [i]);
if (scale) {
for (k=i;k<=m;k++) {
alk] [i] /= scale;
s += a(k] [i] *alk] [i];

3

f=alil] 44];

g = -SIGN(sqrt(s) ,f);

h=f*g-s;

ali] [i]=f-g;

for (j=1;j<=n;jt+) {
for (s=0.0,k=i;k<=m;k++) s += alk] [i] *alk] [jl];
f=s/h;
for (k=i;k<=m;k++) alk] [j] += f*alk] [il];

3
for (k=i;k<=m;k++) alk] [i] *= scale;
3
}
wlil=scale *g;
g=s=scale=0.0;

if Gi <= m && i !=n) {
for (k=1;k<=n;k++) scale += fabs(a[i] [k]);
if (scale) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

68 Chapter 2. _ Solution of Linear Algebraic Equations

for (k=1;k<=n;k++) {
alil(k] /= scale;
s += a[i] [k]*a[i] [k];

3
f=ali] 1;
g = -SIGN(sqrt(s) ,f);
h=f*g-s;
ali] [1]=f-g;
for (k=1;k<=n;k++) rvi{k]=a[i] [k]/h;
for (j=1;j<=m;jt+) {
for (s=0.0,k=1;k<=n;k++) s += alj] [k]*alil [kl];
for (k=1;k<=n;k++) aljl[{k] += s*rvi[k];
3
for (k=1;k<=n;k++) ali] [k] *= scale;
3
}
anorm=FMAX (anorm, (fabs (w[i])+fabs(rvi[i])));
}
for (i=n;i>=1;i--) { Accumulation of right-hand transformations.
if G<n) {
if (2 f{
for (j=1;j<=n; j++) Double division to avoid possible underflow.
vj] (i]=(alil (j]/ali] (1])/g;
for (j=1;j<=n;jt+) {
for (s=0.0,k=1;k<=n;k++) s += a([i] [k]*v[k] [jl];
for (k=1;k<=n;k++) vik] [j] += s*v[k] [il];
3
3
for (j=1;j<=n;j++) vlil [j]l=v[j] [i]=0.0;
}
v(i] [i]=1.0;
vilil;
1=i;
}
for Accumulation of left-hand transformations.
g=wlil;
for (j=1;j<=n;j++) ali] [j]=0.0;
if (g) {
g=1.0/g;
for (j= =n;j++) {
for (s=0.0,k=1;k<=m;k++) s += alk] [i] *al[k] [j];
f=(s/a[il] [i])*g;
for (k=i;k<=m;k++) alk] [j] += f*alk] [i];
}
for (j= ++) aljl fi] * 3
} else for (j=i;j<=m;j++) alj] [i]=0.0;
++a(il C4];
}
for (k=n;k>=1;k--) { Diagonalization of the bidiagonal form: Loop over
for (its=1;its<=30;itst+) { singular values, and over allowed iterations.

Test for splitting.
Note that rvi[1] is always zero.

if ((float) (fabs (rvi[1])+anorm) anorm) {
flag=0;
break;
3
if ((float) (fabs(w[nm])+anorm) == anorm) break;
3
if (flag) {
c=0.0; Cancellation of rvi[1], if 1 > 1.

for (i=1;i<=k;i++) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.6 Singular Value Decomposition 69

f=s*rvi [i];

rvi [i]=c*rvi [i] ;

if ((float)(fabs(f)+anorm) == anorm) break;

g=wlil];

h=pythag (f,g);

wlil=h;

h=1.0/h;

c=g*h;

s = -f*h;

for (j=1;j<=m;j++) {
y=a(j] (nm) ;
z=alj] (il;
alj] nm] =y*c+z*s;
alj] [i]=z*c-y*s;

+

}
3
z=w([k];
if (l==k) { Convergence.

if (z < 0.0) { Singular value is made nonnegative.

j<=n; j++) v0jl 0k] = -vfjl (kl;

}

break;
3
if (its == 30) nrerror("no convergence in 30 svdcmp iterations") ;
x=w([1]; Shift from bottom 2-by-2 minor.
nm=k-1;
y=w[nm] ;
g=rvi [nm] ;
h=rvi [k];

£=((y-z) *(y+z)+(g-h) *(gth))/(2.0*h*y) ;
g=pythag(f,1.0);

£=((x-z) *(x+z) th* ((y/(£+SIGN(g,£)))-h))/x;
c=s=1.0 Next QR transformation:

z=pythag(f ,h) ;
rvil[jl=z;
c=f/z;
s=h/z;
f=x*ctg*s;
& = gkc-x*s;
h=y*s;
y*e cs
for (jj=tsjj<en;jjt+) {
x=v[jj] [jl];
z=v[jj] (il;
v(jj] (j]=xxctz*s;
v[jj] [il=z*c-x*s ;
}
z=pythag(f ,h);
wljl=z; Rotation can be arbitrary if z = 0.
if (2) {
z=1.0/z;
c=f*z;
s=h*z;
3
fecxgtsty;
X=c¥Y-S*g;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

70 Chapter 2. Solution of Linear Algebraic Equations

for (jj=1;jj<=m;jjt+) {
y=aljjl [jl];
z=a[jjl(il;
aljjl] [jl=y*ctz*s;
aljjl] [il=z*c-y*s;
3
3
rvi[1]=0.0;
rvi[k]=f;
wlk]=x;
}
}

free_vector(rvi,1,n);

#include <math.h>
#include "nrutil.h"

float pythag(float a, float b)
Computes (a? + b?)!/2 without destructive underflow or overflow.

{

float absa,absb;

absa=fabs (a) ;

absb=fabs (b) ;

if (absa > absb) return absa*sqrt(1.0+SQR(absb/absa) ) ;

else return (absb == 0.0 ? 0.0 : absb*sqrt(1.0+SQR(absa/absb) )) ;
3

(Double precision versions of svdcmp, svbksb, and pythag, named dsvdcmp,
dsvbksb, and dpythag, are used by the routine rat1sq in §5.13. You can easily
make the conversions, or else get the converted routines from the Numerical Recipes
diskette.)

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), §8.3 and Chapter 12.

Lawson, C.L., and Hanson, R. 1974, Solving Least Squares Problems (Englewood Cliffs, NJ:
Prentice-Hall), Chapter 18.

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), Chapter 9. [1]

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag), Chapter 1.10 by G.H. Golub and C. Reinsch. [2]

Dongarra, J.J., et al. 1979, LINPACK User’s Guide (Philadelphia: S.I.A.M.), Chapter 11. [3]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
86.7. [4]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), §5.2.6. [5]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 71

2.7 Sparse Linear Systems

A system of linear equations is called sparse if only a relatively small number
of its matrix elements a;; are nonzero. It is wasteful to use general methods of
linear algebra on such problems, because most of the O(N °) arithmetic operations
devoted to solving the set of equations or inverting the matrix involve zero operands.
Furthermore, you might wish to work problems so large as to tax your available
memory space, and it is wasteful to reserve storage for unfruitful zero elements.
Note that there are two distinct (and not always compatible) goals for any sparse
matrix method: saving time and/or saving space.

We have already considered one archetypal sparse form in §2.4, the band
diagonal matrix. In the tridiagonal case, e.g., we saw that it was possible to save
both time (order N instead of N°) and space (order N instead of N?). The
method of solution was not different in principle from the general method of LU
decomposition; it was just applied cleverly, and with due attention to the bookkeeping
of zero elements. Many practical schemes for dealing with sparse problems have this
same character. They are fundamentally decomposition schemes, or else elimination
schemes akin to Gauss-Jordan, but carefully optimized so as to minimize the number
of so-called fill-ins, initially zero elements which must become nonzero during the
solution process, and for which storage must be reserved.

Direct methods for solving sparse equations, then, depend crucially on the
precise pattern of sparsity of the matrix. Patterns that occur frequently, or that are
useful as way-stations in the reduction of more general forms, already have special
names and special methods of solution. We do not have space here for any detailed
review of these. References listed at the end of this section will furnish you with an
“in” to the specialized literature, and the following list of buzz words (and Figure
2.7.1) will at least let you hold your own at cocktail parties:

tridiagonal

band diagonal (or banded) with bandwidth 14
band triangular

block diagonal

block tridiagonal

block triangular

cyclic banded

singly (or doubly) bordered block diagonal
singly (or doubly) bordered block triangular
singly (or doubly) bordered band diagonal
singly (or doubly) bordered band triangular
other (!)

You should also be aware of some of the special sparse forms that occur in the
solution of partial differential equations in two or more dimensions. See Chapter 19.

If your particular pattern of sparsity is not a simple one, then you may wish to
try an analyze/factorize/operate package, which automates the procedure of figuring
out how fill-ins are to be minimized. The analyze stage is done once only for each
pattern of sparsity. The factorize stage is done once for each particular matrix that
fits the pattern. The operate stage is performed once for each right-hand side to

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

72 Chapter 2. Solution of Linear Algebraic Equations

zeros

(6D) (k)

Figure 2.7.1. Some standard forms for sparse matrices. (a) Band diagonal; (b) block triangular; (c) block
tridiagonal; (d) singly bordered block diagonal; (e) doubly bordered block diagonal; (f) singly bordered
block triangular; (g) bordered band-triangular; (h) and (i) singly and doubly bordered band diagonal; (j)
and (k) other! (after Tewarson) [1].

be used with the particular matrix. Consult [2,3] for references on this. The NAG
library [4] has an analyze/factorize/operate capability. A substantial collection of
routines for sparse matrix calculation is also available from IMSL [5] as the Yale
Sparse Matrix Package [6].

You should be aware that the special order of interchanges and eliminations,

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 73

prescribed by a sparse matrix method so as to minimize fill-ins and arithmetic
operations, generally acts to decrease the method’s numerical stability as compared
to, e.g., regular LU decomposition with pivoting. Scaling your problem so as to
make its nonzero matrix elements have comparable magnitudes (if you can do it)
will sometimes ameliorate this problem.

In the remainder of this section, we present some concepts which are applicable
to some general classes of sparse matrices, and which do not necessarily depend on
details of the pattern of sparsity.

Sherman-Morrison Formula

Suppose that you have already obtained, by herculean effort, the inverse matrix
A7! of a square matrix A. Now you want to make a “small” change in A, for
example change one element a;;, or a few elements, or one row, or one column.
Is there any way of calculating the corresponding change in A ~! without repeating
your difficult labors? Yes, if your change is of the form

A > (A+u@v) (2.7.1)

for some vectors u and v. If u is a unit vector e;, then (2.7.1) adds the components
of v to the ith row. (Recall that u @ v is a matrix whose i, jth element is the product
of the ith component of u and the jth component of v.) If v is a unit vector e ;, then
(2.7.1) adds the components of u to the jth column. If both u and v are proportional
to unit vectors e; and e; respectively, then a term is added only to the element a ;;.

The Sherman-Morrison formula gives the inverse (A + u @ v) ~+, and is derived
briefly as follows:

(A+u@v)?=(1+A!-u@y)?-At
=(1-A7!-u@v+A7!-u@v-At-u@v-...)- At
=A 1-At-u@v-AT(1-\4?-...)

Atle (At -u) @(v-A™)
1+.
(2.7.2)
where
A\=v-Ateu (2.7.3)

The second line of (2.7.2) is a formal power series expansion. In the third line, the
associativity of outer and inner products is used to factor out the scalars \.

The use of (2.7.2) is this: Given A~! and the vectors u and v, we need only
perform two matrix multiplications and a vector dot product,

z=A}-u w=(A!)?-v NH=V-Z (2.7.4)

(275)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

74 Chapter 2. Solution of Linear Algebraic Equations

The whole procedure requires only 3N’? multiplies and a like number of adds (an
even smaller number if u or v is a unit vector).

The Sherman-Morrison formula can be directly applied to a class of sparse
problems. If you already have a fast way of calculating the inverse of A (e.g., a
tridiagonal matrix, or some other standard sparse form), then (2.7.4)—(2.7.5) allow
you to build up to your related but more complicated form, adding for example a
row or column at a time. Notice that you can apply the Sherman-Morrison formula
more than once successively, using at each stage the most recent update of A~!
(equation 2.7.5). Of course, if you have to modify every row, then you are back to
an N® method. The constant in front of the N° is only a few times worse than the
better direct methods, but you have deprived yourself of the stabilizing advantages
of pivoting — so be careful.

For some other sparse problems, the Sherman-Morrison formula cannot be
directly applied for the simple reason that storage of the whole inverse matrix A ~!
is not feasible. If you want to add only a single correction of the form u ® v,
and solve the linear system

(A+u@v)-x=b (2.7.6)

then you proceed as follows. Using the fast method that is presumed available for
the matrix A, solve the two auxiliary problems

A-y=b A-z=u (2.7.7)
for the vectors y and z. In terms of these,
vey
=y-— |———_ 2.78
xy Pesce ( )

as we see by multiplying (2.7.2) on the right by b.
Cyclic Tridiagonal Systems

So-called cyclic tridiagonal systems occur quite frequently, and are a good
example of how to use the Sherman-Morrison formula in the manner just described.
The equations have the form

bh ca O ::: B ry TL

a2 bz cg ++: © T2
an-1 bn-1 C€N-1 tN-1 TN-1

a vee 0 an bn rN TN

This is a tridiagonal system, except for the matrix elements a and ( in the corners.
Forms like this are typically generated by finite-differencing differential equations
with periodic boundary conditions (§19.4).

We use the Sherman-Morrison formula, treating the system as tridiagonal plus
a correction. In the notation of equation (2.7.6), define vectors u and v to be

Y 1
0 0
u=j|: v=]: (2.7.10)
0 0
a B/y

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

2.7 Sparse Linear Systems 75

Here ¥ is arbitrary for the moment. Then the matrix A is the tridiagonal part of the
matrix in (2.7.9), with two terms modified:

V=b-7, by = by — aB/y (2.7.11)

We now solve equations (2.7.7) with the standard tridiagonal algorithm, and then
get the solution from equation (2.7.8).

The routine cyclic below implements this algorithm. We choose the arbitrary
parameter 7 = —b, to avoid loss of precision by subtraction in the first of equations
(2.7.11). In the unlikely event that this causes loss of precision in the second of
these equations, you can make a different choice.

#include "nrutil.h"

void cyclic(float a[], float b[], float c[], float alpha, float beta,

float r[{], float x[], unsigned long n)
Solves for a vector x[1..n] the “cyclic” set of linear equations given by equation (2.7.9). a,
b, c, and x are input vectors, all dimensioned as [1..n], while alpha and beta are the corner
entries in the matrix. The input is not modified.
{

void tridag(float al], float b[], float c[], float r[], float uf],

unsigned long n);
unsigned long i;
float fact,gamma, *bb,*u,*z;

if (m <= 2) nrerror("n too small in cyclic");

bb=vector(1,n);

u=vector(1,n);

z=vector(1,n);

gamma = -b[1]; Avoid subtraction error in forming bb [1] .

bb [1] =b([1]-gamma; Set up the diagonal of the modified tridi-

bb [n] =b [n] -alpha*beta/gamma; agonal system.

for (i=2;i<n;it++) bb[i]=b[i];

tridag(a,bb,c,r,x,n); Solve A-x =r.

u[1]=gamma; Set up the vector u.

u[n]=alpha;

for (i=2;i<n;it++) uli]=0.0;

tridag(a,bb,c,u,z,n); Solve A-z =u.

fact=(x[1]+beta*x[n] /gamma) / Form v-x/(1+v-z).
(1.0+z[1]+beta*z[n] /gamma) ;

for (i=1;i<=n;i++) x[i] -= fact*z[i]; Now get the solution vector x.

free_vector(z,1,n);

free_vector(u,i,n);

free_vector(bb,1,n);

Woodbury Formula

If you want to add more than a single correction term, then you cannot use (2.7.8)
repeatedly, since without storing a new A~' you will not be able to solve the auxiliary
problems (2.7.7) efficiently after the first step. Instead, you need the Woodbury formula, which
is the block-matrix version of the Sherman-Morrison formula,

(A+U-V")*

_ acl -1 T -1 -1 T -1 (2.7.12)
=A — |A~-U-(1+V)-A~-U)"- VA

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

76 Chapter 2. Solution of Linear Algebraic Equations

Here A is, as usual, an N x N matrix, while U and V are N x P matrices with P < N
and usually P < N. The inner piece of the correction term may become clearer if written
as the tableau,

| ;
|

U -|l+Vv"-At-Ul - vr (2.7.13)

—

where you can see that the matrix whose inverse is needed is only P x P rather than N x N.

The relation between the Woodbury formula and successive applications of the Sherman-
Morrison formula is now clarified by noting that, if U is the matrix formed by columns out of the
P vectors u;,...,up,and V is the matrix formed by columns out of the P vectors v1,..., VP,

then two ways of expressing the same correction to A are

Pp
A+ our @ve | =(A+U-V") (2.7.15)
k=1

(Note that the subscripts on u and v do not denote components, but rather distinguish the
different column vectors.)

Equation (2.7.15) reveals that, if you have A~' in storage, then you can either make the
P corrections in one fell swoop by using (2.7.12), inverting a P x P matrix, or else make
them by applying (2.7.5) P successive times.

If you don’t have storage for A7!, then you must use (2.7.12) in the following way:
To solve the linear equation

P
A+ Sour @ve}-x=b (2.7.16)
k=1
first solve the P auxiliary problems
A-z=uU1
A-Z2 = U2
(2.7.17)
A-zp=up
and construct the matrix Z by columns from the z’s obtained,
Z=|a|--- |zp (2.7.18)

Next, do the P x P matrix inversion
H=(1+V"-z) (2.7.19)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 77

Finally, solve the one further auxiliary problem
A-y=b (2.7.20)
In terms of these quantities, the solution is given by

x=y-Z- [H-(V"-y)| (2.7.21)

Inversion by Partitioning

Once in a while, you will encounter a matrix (not even necessarily sparse)
that can be inverted efficiently by partitioning. Suppose that the N x N matrix
A is partitioned into

A= PQ 2.7.22
rs (2.722)

where P and S are square matrices of size p x p and s x s respectively (p+ s = N).
The matrices Q and R are not necessarily square, and have sizes p x s and s x p,
respectively.
If the inverse of A is partitioned in the same manner,
ata|? 3] (2.7.23)
R S

then P, Q, R, Ss, which have the same sizes as P, Q, R, S, respectively, can be
found by either the formulas

P=(P—Q-S7!-R)"!

Q=-(P-Q-S*-R) 1 (Q-S")

~ (2.7.24)
R=—(S7'-R)-(P—Q-S7!-R)t
S=S-'4(S71-R)-(P—Q-S'-R)1-(Q-871)
or else by the equivalent formulas
P=p-'+(P-!.Q)-(S—R-P!-Q)71- (R-P7?)
Q=-(P7!-Q):(S-R-P7!-Q)!
(2.7.25)

R= —(S—R-P7!-Q)7?-(R- Po)

S=(S—R-P7!.Q)7!

The parentheses in equations (2.7.24) and (2.7.25) highlight repeated factors that
you may wish to compute only once. (Of course, by associativity, you can instead
do the matrix multiplications in any order you like.) The choice between using
equation (2.7.24) and (2.7.25) depends on whether you want P or S to have the
simpler formula; or on whether the repeated expression (S — R - P a. Q)-1 is easier

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

78 Chapter 2. Solution of Linear Algebraic Equations

to calculate than the expression (P — Q- S71 - R)~1; or on the relative sizes of P
and S; or on whether P~! or S~! is already known.

Another sometimes useful formula is for the determinant of the partitioned
matrix,

det A = det P det(S —R-P~'-Q) =detSdet(P—Q-S7'-R) (2.7.26)

Indexed Storage of Sparse Matrices

We have already seen (§2.4) that tri- or band-diagonal matrices can be stored in a compact
format that allocates storage only to elements which can be nonzero, plus perhaps a few wasted
locations to make the bookkeeping easier. What about more general sparse matrices? When a
sparse matrix of dimension N x N contains only a few times N nonzero elements (a typical
case), it is surely inefficient — and often physically impossible — to allocate storage for all
N? elements. Even if one did allocate such storage, it would be inefficient or prohibitive in
machine time to loop over all of it in search of nonzero elements.

Obviously some kind of indexed storage scheme is required, one that stores only nonzero
matrix elements, along with sufficient auxiliary information to determine where an element
logically belongs and how the various elements can be looped over in common matrix
operations. Unfortunately, there is no one standard scheme in general use. Knuth[7] describes
one method. The Yale Sparse Matrix Package [6] and ITPACK [8] describe several other
methods. For most applications, we favor the storage scheme used by PCGPACK [9], which
is almost the same as that described by Bentley [10], and also similar to one of the Yale Sparse
Matrix Package methods. The advantage of this scheme, which can be called row-indexed
sparse storage mode, is that it requires storage of only about two times the number of nonzero
matrix elements. (Other methods can require as much as three or five times.) For simplicity,
we will treat only the case of square matrices, which occurs most frequently in practice.
To represent a matrix A of dimension N x N, the row-indexed scheme sets up two
one-dimensional arrays, call them sa and ija. The first of these stores matrix element values
in single or double precision as desired; the second stores integer values. The storage rules are:

e The first NV locations of sa store A’s diagonal matrix elements, in order. (Note that
diagonal elements are stored even if they are zero; this is at most a slight storage
inefficiency, since diagonal elements are nonzero in most realistic applications.)

e Each of the first NV locations of ija stores the index of the array sa that contains
the first off-diagonal element of the corresponding row of the matrix. (If there are
no off-diagonal elements for that row, it is one greater than the index in sa of the
most recently stored element of a previous row.)

e Location | of ija is always equal to N + 2. (It can be read to determine N.)

e Location N + 1 of ija is one greater than the index in sa of the last off-diagonal
element of the last row. (It can be read to determine the number of nonzero
elements in the matrix, or the number of elements in the arrays sa and ija.)
Location N + 1 of sa is not used and can be set arbitrarily.

e Entries in sa at locations > N + 2 contain A’s off-diagonal values, ordered by
rows and, within each row, ordered by columns.

e Entries in ijaat locations > N +2 contain the column number of the corresponding
element in sa.

While these rules seem arbitrary at first sight, they result in a rather elegant storage

scheme. As an example, consider the matrix

(2.7.27)

nooo

essow
oonrkoO
eousr
eeosss

on

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 79

In row-indexed compact storage, matrix (2.7.27) is represented by the two arrays of length
11, as follows

index k 1 2 3 4 5 6 7) 8 9 | 10 | 11

ijate) |} 7] 8] 8 10 | u fie

w
No
i
on
i

sa[k] 3. | 4. 15. | 0. 5. a ]l. {7 | 9% 2. | 6.

(2.7.28)

Here x is an arbitrary value. Notice that, according to the storage rules, the value of N
(namely 5) is ijal1]-2, and the length of each array is ijalija[1]-1]-1, namely 11.
The diagonal element in row i is sa[i], and the off-diagonal elements in that row are in
sa[k] where k loops from ijali] to ijali+1]-1, if the upper limit is greater or equal to
the lower one (as in C’s for loops).

Here is a routine, sprsin, that converts a matrix from full storage mode into row-indexed
sparse storage mode, throwing away any elements that are less than a specified threshold.
Of course, the principal use of sparse storage mode is for matrices whose full storage mode
won’t fit into your machine at all; then you have to generate them directly into sparse format.
Nevertheless sprsin is useful as a precise algorithmic definition of the storage scheme, for

scale testing of large problems, and for the case where execution time, rather than storage,
shes the impetus to sparse storage.

#include <math.h>

void sprsin(float **a, int n, float thresh, unsigned long nmax, float sa[],
unsigned long ija[])
Converts a square matrix a[1..n][1..n] into row-indexed sparse storage mode. Only ele-
ments of a with magnitude >thresh are retained. Output is in two linear arrays with dimen-
sion nmax (an input parameter): sa[1..] contains array values, indexed by ija[1..]. The
number of elements filled of sa and ija on output are both ijalija[l1]-1]-1 (see text).
{
void nrerror(char error_text[]);
int i,j;
unsigned long k;

for (j=1;j<=n;j++) saljl=aljl] (jl; Store diagonal elements.
ija(1]=n+2; Index to 1st row off-diagonal element, if any.
k=n+1;
for (i=1;i<=n;i++) { Loop over rows.
for (j=1;j<=n;jt+) { Loop over columns.

if (fabs(ali][j]) >= thresh && i != j) {
if (++k > nmax) nrerror("sprsin: nmax too small");

sa[k]=a[i] [j]; Store off-diagonal elements and their columns.
ijalk]=j;
3
}
ijalit+i]=k+1; As each row is completed, store index to
+ next.

The single most important use of a matrix in row-indexed sparse storage mode is to
multiply a vector to its right. In fact, the storage mode is optimized for just this purpose.
The following routine is thus very simple.

void sprsax(float sa[], unsigned long ijal], float x[], float bf],
unsigned long n)
Multiply a matrix in row-index sparse storage arrays Sa and ija by a vector x[1..n], giving
a vector b[1..n].
{

void nrerror(char error_text[]);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

80 Chapter 2. _ Solution of Linear Algebraic Equations

unsigned long i,k;

if (ijaf1] != n+2) nrerror("sprsax: mismatched vector and matrix");
for (i= i++) {

bli]=sali] *x [il ; Start with diagonal term.

for (k=ija[i] ;k<=ija[it+1]-1;k++) Loop over off-diagonal terms.

bli] += salk]*x[ijalkl]];

It is also simple to multiply the transpose of a matrix by a vector to its right. (We will use
this operation later in this section.) Note that the transpose matrix is not actually constructed.

void sprstx(float sa[], unsigned long ijal], float x[], float b[],

unsigned long n)
Multiply the transpose of a matrix in row-index sparse storage arrays Sa and ija by a vector
x[1..n], giving a vector b[1..n].

{
void nrerror(char error_text[]);
unsigned long i,j,k;
if (ijaf1] n+2) nrerror("mismatched vector and matrix in sprstx");
for (i=1;i ;it+) blil=sali]*x[il; Start with diagonal terms.
for (i=1;i<=n;it+) { Loop over off-diagonal terms.
for (k=ijali] ;k<=ijaliti]-1;k++) {
jrijalk];
bj] += salk] *x[il;
}
+
3

(Double precision versions of sprsax and sprstx, named dsprsax and dsprstx, are used
by the routine atimes later in this section. You can easily make the conversion, or else get
the converted routines from the Numerical Recipes diskettes.)

In fact, because the choice of row-indexed storage treats rows and columns quite
differently, it is quite an involved operation to construct the transpose of a matrix, given the
matrix itself in row-indexed sparse storage mode. When the operation cannot be avoided, it
is done as follows: An index of all off-diagonal elements by their columns is constructed
(see §8.4). The elements are then written to the output array in column order. As each
element is written, its row is determined and stored. Finally, the elements in each column
are sorted by row.

void sprstp(float sa[], unsigned long ija[], float sb[], unsigned long ijb[])
Construct the transpose of a sparse square matrix, from row-index sparse storage arrays Sa and
ija into arrays sb and ijb.
{

void iindexx(unsigned long n, long arr[], unsigned long indx[]);

Version of indexx with all float variables changed to long.

unsigned long j,jl,jm,jp,ju,k,m,n2,noff,inc,iv;

float v;
n2=ijal1]; Linear size of matrix plus 2.
for (j=1;j<=n2-2;j++) sb[j]l=saljl]; Diagonal elements.

iindexx(ija{m2-1]-ija[1], (long *)&ija[n2-1] ,&ijb[n2-1]);

Index all off-diagonal elements by their columns.

jp=0;

for (k=ija[1] ;k<=ija[n2-1]-1;k++) { Loop over output off-diagonal elements.

m=ijb([k]+n2-1; Use index table to store by (former) columns.

sb[k]=sal[m] ;
for (j=jpti;j<=ijalm] ;j++) ijbljl=k; Fill in the index to any omitted rows.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 81

jp=ijalm] ; Use bisection to find which row element
jl=1; m is in and put that into ijb[k].
ju=n2-1;

while (ju-jl > 1) {
jm=(jutjl)/2;
if (ijaljm] > m) ju=jm; else jl=jm;

}
ijb[k]=j1;
+
for jj<n2;j++) ijb[j]l=ija[n2-1];
for =n2-2;j++) { Make a final pass to sort each row by
jb(j+1]-ijblj]; Shell sort algorithm.
noff=ijb[j]-1;
inc=1;
do {
inc *= 3;
inc++;
} while (ine <= jl);
do {
inc /= 3;
while (ijb[m-inc] > iv) {
ijb[m]=ijb[m-inc] ;
sb[m]=sb[m-inc] ;
m -= inc;
if (m-noff <= inc) break;
}
ijb(m]=iv;
sb[m]=v;
3
} while (inc > 1);
+

The above routine embeds internally a sorting algorithm from §8.1, but calls the external
routine iindexx to construct the initial column index. This routine is identical to indexx,
as listed in 88.4, except that the latter’s two float declarations should be changed to long.
(The Numerical Recipes diskettes include both indexx and iindexx.) In fact, you can
often use indexx without making these changes, since many computers have the property
that numerical values will sort correctly independently of whether they are interpreted as
floating or integer values.

As final examples of the manipulation of sparse matrices, we give two routines for the
multiplication of two sparse matrices. These are useful for techniques to be described in § 13.10.

In general, the product of two sparse matrices is not itself sparse. One therefore wants
to limit the size of the product matrix in one of two ways: either compute only those elements
of the product that are specified in advance by a known pattern of sparsity, or else compute all
nonzero elements, but store only those whose magnitude exceeds some threshold value. The
former technique, when it can be used, is quite efficient. The pattern of sparsity is specified
by furnishing an index array in row-index sparse storage format (e.g., ija). The program
then constructs a corresponding value array (e.g., sa). The latter technique runs the danger of
excessive compute times and unknown output sizes, so it must be used cautiously.

With row-index storage, it is much more natural to multiply a matrix (on the left) by
the transpose of a matrix (on the right), so that one is crunching rows on rows, rather than
rows on columns. Our routines therefore calculate A - B’, rather than A- B. This means
that you have to run your right-hand matrix through the transpose routine sprstp before
sending it to the matrix multiply routine.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

82 Chapter 2. _ Solution of Linear Algebraic Equations

The two implementing routines, sprspm for “pattern multiply” and sprstm for “threshold
multiply” are quite similar in structure. Both are complicated by the logic of the various
combinations of diagonal or off-diagonal elements for the two input streams and output stream.

void sprspm(float sa[], unsigned long ija[], float sb[], unsigned long ijb[],

float sc[], unsigned long ijc[])
Matrix multiply A.B? where A and B are two sparse matrices in row-index storage mode, and
B? is the transpose of B. Here, sa and ija store the matrix A; sb and ijb store the matrix B.
This routine computes only those components of the matrix product that are pre-specified by the
input index array ijc, which is not modified. On output, the arrays sc and ijc give the product
matrix in row-index storage mode. For sparse matrix multiplication, this routine will often be
preceded by a call to sprstp, so as to construct the transpose of a known matrix into sb, ijb.
{

void nrerror(char error_text[]);

unsigned long i,ijma,ijmb,j,m,ma,mb,mbb,mn;

float sum;

if (ijaf1] != ijbf1] || ijalt] != ijef1])
nrerror("sprspm: sizes do not match");

for ( sije(1]-2;it+) { Loop over rows.
j= Set up so that first pass through loop does the
mn=ijc(lil; diagonal component.
sum=sa[i] *sb[i] ;
for (53) ¢ Main loop over each component to be output.
mb=ijb(j];

for (ma=ija[i] ;ma<=ija[iti]-1;mat+) {
Loop through elements in A’s row. Convoluted logic, following, accounts for the
various combinations of diagonal and off-diagonal elements.
ijma=ijal[ma] ;
if (ijma == j) sum += salma] *sb[j];
else {
while (mb < ijb{j+1]) {
ijmb=ijb [mb] ;
if (ijmb == i) {
sum += sa[i]*sb[mb++] ;
continue;
} else if (ijmb < ijma) {
mb++;
continue;
} else if (ijmb == ijma) {
sum += sa[ma]*sb[mbt+] ;

continue;
}
break;
+
}
}
for (mbb=mb;mbb<=ijb[j+1]-1;mbb++) { Exhaust the remainder of B's row.
if (ijb[mbb] == i) sum += sa[i]*sb[mbb] ;
3
sc[m]=sum;
sum=0.0; Reset indices for next pass through loop.

if (mn >= ijc[it1]) break;
j=ije [m=mn++] ;

#include <math.h>

void sprstm(float sa[], unsigned long ija[], float sb[], unsigned long ijb[],
float thresh, unsigned long nmax, float sc[], unsigned long ijc[])

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 83

Matrix multiply A.B? where A and B are two sparse matrices in row-index storage mode, and
BE is the transpose of B. Here, sa and ija store the matrix A; sb and ijb store the matrix
B. This routine computes all components of the matrix product (which may be non-sparse!),
but stores only those whose magnitude exceeds thresh. On output, the arrays sc and ijc
(whose maximum size is input as nmax) give the product matrix in row-index storage mode.
For sparse matrix multiplication, this routine will often be preceded by a call to sprstp, so as
to construct the transpose of a known matrix into sb, ijb.
{

void nrerror(char error_text[]);

unsigned long i,ijma,ijmb,j,k,ma,mb,mbb;

float sum;

if (ijafi] != ijb[1]) nrerror("sprstm: sizes do not match");
ije[1] ijali];
for ( ji<=ijal1]-2;it+) { Loop over rows of A,
for (j=1;j<=ijb[1]-2;j++) f and rows of B.
if G j) sum=sa[i]*sb[j]; else sum=0.0e0;
mb=ijb(j];
for (ma=ija[i] ;ma<=ija[iti]-1;mat+) {
Loop through elements in A’s row. Convoluted logic, following, accounts for the
various combinations of diagonal and off-diagonal elements.
ijma=ijal[ma] ;
if (ijma == j) sum += sa[ma]*sb[j];
else {
while (mb < ijb[j+1]) {
ijmb=ijb [mb] ;
if (ijmb == i) {
sum sa[i]*sb[mb++] ;
continue;
} else if (ijmb < ijma) {
mb++;
continue;
} else if (ijmb == ijma) {
sum += sa[ma]*sb[mb++] ;
continue;

+

break;

3

3

for (mbb=mb;mbb<
if (ijb{mbb]

== i) sum += sa[i]*sb[mbb];
3
if Gi == j) scli]=sum; Where to put the answer...
else if (fabs(sum) > thresh) {
if (k > nmax) nrerror("sprstm: nmax too small");
sc[k]=sum;
ije[k++]=j;
3
}
aije(iti]=k;

Conjugate Gradient Method for a Sparse System

So-called conjugate gradient methods provide a quite general means for solving the
N x N linear system

A-x=b (2.7.29)

The attractiveness of these methods for large sparse systems is that they reference A only
through its multiplication of a vector, or the multiplication of its transpose and a vector. As

jb(j+1]-1;mbb++) { Exhaust the remainder of B's row.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

84 Chapter 2. Solution of Linear Algebraic Equations

we have seen, these operations can be very efficient for a properly stored sparse matrix. You,
the “owner” of the matrix A, can be asked to provide functions that perform these sparse
matrix multiplications as efficiently as possible. We, the “grand strategists” supply the general
routine, linbcg below, that solves the set of linear equations, (2.7.29), using your functions.
The simplest, “ordinary” conjugate gradient algorithm [11-13] solves (2.7.29) only in the
case that A is symmetric and positive definite. It is based on the idea of minimizing the function

f(x) = 5 Axo bex (2.7.30)
This function is minimized when its gradient
Vf=A-x-b (2.7.31)

is zero, which is equivalent to (2.7.29). The minimization is carried out by generating a
succession of search directions p,, and improved minimizers x;. At each stage a quantity a,
is found that minimizes f (xz + axp,), and xz41 is set equal to the new point xz + xP,.
The p,, and x;, are built up in such a way that x; is also the minimizer of f over the whole
vector space of directions already taken, {p,,p2,...,p,,}. After N iterations you arrive at
the minimizer over the entire vector space, i.e., the solution to (2.7.29).

Later, in §10.6, we will generalize this “ordinary” conjugate gradient algorithm to the
minimization of arbitrary nonlinear functions. Here, where our interest is in solving linear,
but not necessarily positive definite or symmetric, equations, a different generalization is
important, the biconjugate gradient method. This method does not, in general, have a simple
connection with function minimization. It constructs four sequences of vectors, rh, Tk, Pps
p,-& = 1,2,.... You supply the initial vectors ri; and r1, and set p; =r1,p, =T1. Then
you carry out the following recurrence:

—  Tre'lr
Py AP,
Tey1 =Te— OA: py

Ok

< = To.
Troi =Tk —akA +p,

_ (2.7.32)
Be = Piet *Te+1
Te-Tk
Prot =Teti t+ GePp
Proa = Tht + GP,
This sequence of vectors satisfies the biorthogonality condition
Ty-rj =r; -T; =0, j<i (2.7.33)
and the biconjugacy condition
P,-A-pj= j<i (2.7.34)
There is also a mutual orthogonality,
Fi-p; =ri- Pp; =0, j<i (2.7.35)

The proof of these properties proceeds by straightforward induction [14]. As long as the
recurrence does not break down earlier because one of the denominator: zero, it must
terminate after m < N steps with rm41 = Fm+1 = 0. This is basically because after at most
N steps you run out of new orthogonal directions to the vectors you’ve already constructed.

To use the algorithm to solve the system (2.7.29), make an initial guess x: for the
solution. Choose r; to be the residual

m1 =b-A-x, (2.7.36)

and choose Ti = ri. Then form the sequence of improved estimates

Xk+1 = Xk + OKP, (2.7.37)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 85

while carrying out the recurrence (2.7.32). Equation (2.7.37) guarantees that r,41 from the
recurrence is in fact the residual b — A - x~41 corresponding to x41. Since rm41 = 0,
Xm-+1 is the solution to equation (2.7.29).

While there is no guarantee that this whole procedure will not break down or become
unstable for general A, in practice this is rare. More importantly, the exact termination in at
most N iterations occurs only with exact arithmetic. Roundoff error means that you should
regard the process as a genuinely iterative procedure, to be halted when some appropriate
error criterion is met.

The ordinary conjugate gradient algorithm is the special case of the biconjugate gradient
algorithm when A is symmetric, and we choose T; = ri. ThenT;, =r, and p, = p,, for all
k; you can omit computing them and halve the work of the algorithm. This conjugate gradient
version has the interpretation of minimizing equation (2.7.30). If A is positive definite as
well as symmetric, the algorithm cannot break down (in theory!). The routine linbcg below
indeed reduces to the ordinary conjugate gradient method if you input a symmetric A, but
it does all the redundant computations.

Another variant of the general algorithm corresponds to a symmetric but non-positive
definite A, with the choice F; = A-r, instead of Ff) = r,. In this case Fy = A-r, and
Pp, = A-p, for all k. This algorithm is thus equivalent to the ordinary conjugate gradient
algorithm, but with all dot products a - b replaced by a- A -b. It is called the minimum residual
algorithm, because it corresponds to successive minimizations of the function

(x) = 5 ners 5 |A-x—b/? (2.7.38)

where the successive iterates x; minimize ® over the same set of search directions p,, generated
in the conjugate gradient method. This algorithm has been generalized in various ways for
unsymmetric matrices. The generalized minimum residual method (GMRES; see [9,15]) is
probably the most robust of these methods.

Note that equation (2.7.38) gives

VO(x) =A” -(A-x—b) (2.7.39)

For any nonsingular matrix A, A” - A is symmetric and positive definite. You might therefore
be tempted to solve equation (2.7.29) by applying the ordinary conjugate gradient algorithm
to the problem

(AT -A)-x =A™-b (2.7.40)

Don’t! The condition number of the matrix A” - A is the square of the condition number of
A (see §2.6 for definition of condition number). A large condition number both increases the
number of iterations required, and limits the accuracy to which a solution can be obtained. It
is almost always better to apply the biconjugate gradient method to the original matrix A.

So far we have said nothing about the rate of convergence of these methods. The
ordinary conjugate gradient method works well for matrices that are well-conditioned, i.e.,
“close” to the identity matrix. This suggests applying these methods to the preconditioned
form of equation (2.7.29),

(A -A)-x=A +b (2.7.41)

The idea is that you might already be able to solve your linear system easily for some A close
to A, in which case A~'- A © 1, allowing the algorithm to converge in fewer steps. The
matrix A is called a preconditioner [11], and the overall scheme given here is known as the
preconditioned biconjugate gradient method or PBCG.

For efficient implementation, the PBCG algorithm introduces an additional set of vectors
zp and Z; defined by

~ xT

A-Zp=Tk and A -Z=Tr (2.7.42)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

86 Chapter 2. _ Solution of Linear Algebraic Equations

and modifies the definitions of ax, 3, p,,, and p, in equation (2.7.32):

Tk Zp

3B, = 41° Zk41
a T° Zk (2.7.43)

Pri = Zk+1 + BePp
Pri = Ze+1 + GeP_

For linbeg, below, we will ask you to supply routines that solve the auxiliary linear systems
(2.7.42). If you have no idea what to use for the preconditioner A, then use the diagonal part
of A, or even the identity matrix, in which case the burden of convergence will be entirely
on the biconjugate gradient method itself.

The routine linbcg, below, is based on a program originally written by Anne Greenbaum.
(See [13] for a different, less sophisticated, implementation.) There are a few wrinkles you
should know about.

What constitutes “good” convergence is rather application dependent. The routine
linbcg therefore provides for four possibilities, selected by setting the flag itol on input.
If itol=1, iteration stops when the quantity |A - x — b|/|b| is less than the input quantity
tol. If itol=2, the required criterion is

|A | -(A-x—b)|/|A_|-b| < tol (2.7.44)

If itol=3, the routine uses its own estimate of the error in x, and requires its magnitude,
divided by the magnitude of x, to be less than tol. The setting ito1=4 is the same as itol1=3,
except that the largest (in absolute value) component of the error and largest component of x
are used instead of the vector magnitude (that is, the Z,. norm instead of the Lz norm). You
may need to experiment to find which of these convergence criteria is best for your problem.

On output, err is the tolerance actually achieved. If the returned count iter does
not indicate that the maximum number of allowed iterations itmax was exceeded, then err
should be less than tol. If you want to do further iterations, leave all returned quantities as
they are and call the routine again. The routine loses its memory of the spanned conjugate
gradient subspace between calls, however, so you should not force it to return more often
than about every N iterations.

Finally, note that linbcg is furnished in double precision, since it will be usually be
used when N is quite large.

#include <stdio.h>
#include <math.h>

#include "nrutil.h"
#define EPS 1.0e-14

void linbcg(unsigned long n, double b[], double x[], int itol, double tol,
int itmax, int *iter, double *err)

Solves A-x = b for x[1..n], given b[1..n], by the iterative biconjugate gradient method.
On input x[1..n] should be set to an initial guess of the solution (or all zeros); itol is 1,2,3,
or 4, specifying which convergence test is applied (see text); itmax is the maximum number
of allowed iterations; and tol is the desired convergence tolerance. On output, x[1..n] is
reset to the improved solution, iter is the number of iterations actually taken, and err is the
estimated error. The matrix A is referenced only through the user-supplied routines atimes,
which computes the product of either A or its transpose on a vector; and asolve, which solves

A-x=bor At ex = b for some preconditioner matrix A (possibly the trivial diagonal part of A).
t

void asolve(unsigned long n, double b[], double x[], int itrnsp);

void atimes(unsigned long n, double x[], double r[], int itrnsp);

double snrm(unsigned long n, double sx[], int itol);

unsigned long j;

double ak,akden,bk,bkden, bknum, bnrm,dxnrm,xnrm,zminrm,znrm;

double *p,*pp,*r,*rr,*z,*ZZ; Double precision is a good idea in this routine.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.7 Sparse Linear Systems 87

/*

p=dvector(1,n);
pp=dvector(1,n);
r=dvector(1,n);
rr=dvector(1,n);
z=dvector(1,n);
zz=dvector(1,n);

Calculate initial residual.

*iter=0;

atimes(n,x,r,0); Input to atimes is x[1..n], output is r[1..n];
the final 0 indicates that the matrix (not its
transpose) is to be used.

rr(jl=rljl;
+
atimes(n,r,rr,0); */ Uncomment this line to get the “minimum resid-
if (itol == 1) { ual” variant of the algorithm.
bnrm=snrm(n,b,itol) ;
asolve(n,r,z,0); Input to asolve is r[1..n], output is z[1..n];
} the final 0 indicates that the matrix A (not

else if (itol == 2) {
asolve(n,b,z,0);
bnrm=snrm(n,z,itol) ;
asolve(n,r,z,0);

its transpose) is to be used.

+

else if (itol == 3 || itol == 4) {
asolve(n,b,z,0);
bnrm=snrm(n,z,itol) ;
asolve(n,r,z,0);
znrm=snrm(n,z,itol) ;

} else nrerror("illegal itol in linbcg");

while (*iter <= itmax) { Main loop.
++(*iter) ;
asolve(n,rr,zz,1); Final 1 indicates use of transpose matrix A’.
for (bknum=0.0, j= =n;jtt+) bknum += z[j]*rr[j];
Calculate coefficient bk and direction vectors p and pp.

if (*iter
for (j
pljl=z(jl;
ppljl=zz[jl;
}
else {
bk=bknum/bkden;
for (j <=n;jt+) {
plj]=bk*p(jl+z[jl;
pp[j]=bk*pp[j]+zz[jl;
3
}
bkden=bknum; Calculate coefficient ak, new iterate x, and new
atimes(n,p,z,0); residuals r and rr.

for (akden=0.0, j=
ak=bknum/akden ;
atimes(n,pp,zz,1);
for (j=1;j<=n;jt+) {
x({j] += ak*p[j];

<=n;jt++) akden += z[j]*pplj];

r[j] ak*z[j];
rr[j] -= ak*zz[j];
t ~
asolve(n,r,z,0); Solve A-z =r and check stopping criterion.

if (itol == 1)
*err=snrm(n,r,itol)/bnrm;

else if (itol == 2)
*err=snrm(n,z,itol)/bnrm;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

88

Chapter 2. _ Solution of Linear Algebraic Equations

else if (itol == 3 || itol == 4) {
zminrm=znrm;
znrm=snrm(n,z,itol) ;
if (fabs(zminrm-znrm) > EPS*znrm) {
dxnrm=fabs (ak)*snrm(n,p,itol) ;
*err=znrm/fabs(zminrm-znrm) *dxnrm;

} else {
*err=znrm/bnrm; Error may not be accurate, so loop again.
continue;

3

xnrm=snrm(n,x,itol) ;
if (*err <= 0.5*xnrm) *err /= xnrm;

else {
*err=znrm/bnrm; Error may not be accurate, so loop again.
continue;

3

}

printf("iter=/4d err=/,12.6f\n",*iter,*err) ;
if (*err <= tol) break;
+

free_dvector(p,1,n);
free_dvector(pp,1,n);
free_dvector(r,1,n);
free_dvector(rr,1,n);
free_dvector(z,1,n);
free_dvector(zz,1,n);

The routine linbcg uses this short utility for computing vector norms:

#include <math.h>

double snrm(unsigned long n, double sx[], int itol)

Compute one of two norms for a vector sx[1..n], as signaled by itol. Used by linbcg.

{

unsigned long i,isamax;
double ans;

if (itol <= 3) {
ans = 0.0;
for (i=1;i<=n;it++) ans += sx[i]*sx[i]; Vector magnitude norm.
return sqrt(ans) ;
} else {
isamax=1;
for (i=1;i<=n;it+) { Largest component norm.
if (fabs(sx[i]) > fabs(sx[isamax])) isamax=i;
}

return fabs(sx[isamax]);

So that the specifications for the routines atimes and asolve are clear, we list here

simple versions that assume a matrix A stored somewhere in row-index sparse format.

extern unsigned long ijal];
extern double sa[]; The matrix is stored somewhere.

void atimes(unsigned long n, double x[], double r[], int itrnsp)

{

void dsprsax(double sa[], unsigned long ija[], double x[], double b[],
unsigned long n);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

2.7 Sparse Linear Systems 89

void dsprstx(double sa[], unsigned long ija[], double x[], double b[],
unsigned long n);
These are double versions of sprsax and sprstx.

if (itrnsp) dsprstx(sa,ija,x,r,n);
else dsprsax(sa,ija,x,r,n);

extern unsigned long ijal[];
extern double sal]; The matrix is stored somewhere.

void asolve(unsigned long n, double b[], double x[], int itrnsp)
{

unsigned long i;

for(i=1;i<sn;i++) x[i]=(sali] != 0.0 ? bli]/sali] : bfi]);
The matrix A is the diagonal part of A, stored in the first n elements of sa. Since the
transpose matrix has the same diagonal, the flag itrnsp is not used.

CITED REFERENCES AND FURTHER READING:

Tewarson, R.P. 1973, Sparse Matrices (New York: Academic Press). [1]

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic Press),
Chapter 1.3 (by J.K. Reid). [2]

George, A., and Liu, J.W.H. 1981, Computer Solution of Large Sparse Positive Definite Systems
(Englewood Cliffs, NJ: Prentice-Hall). [3]

NAG Fortran Library (Numerical Algorithms Group, 256 Banbury Road, Oxford OX27DE, U.K.).
[4]
IMSL Math/Library Users Manual (IMSL Inc., 2500 CityWest Boulevard, Houston TX 77042). [5]

Eisenstat, S.C., Gursky, M.C., Schultz, M.H., and Sherman, A.H. 1977, Yale Sparse Matrix Pack-
age, Technical Reports 112 and 114 (Yale University Department of Computer Science). [6]

Knuth, D.E. 1968, Fundamental Algorithms, vol. 1 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §2.2.6. [7]

Kincaid, D.R., Respess, J.R., Young, D.M., and Grimes, R.G. 1982, ACM Transactions on Math-
ematical Software, vol. 8, pp. 302-322. [8]

PCGPAK User’s Guide (New Haven: Scientific Computing Associates, Inc.). [9]

Bentley, J. 1986, Programming Pearls (Reading, MA: Addison-Wesley), §9. [10]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), Chapters 4 and 10, particularly §§10.2-10.3. [11]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 8. [12]

Baker, L. 1991, More C Tools for Scientists and Engineers (New York: McGraw-Hill). [13]

Fletcher, R. 1976, in Numerical Analysis Dundee 1975, Lecture Notes in Mathematics, vol. 506,
A. Dold and B Eckmann, eds. (Berlin: Springer-Verlag), pp. 73-89. [14]

Saad, Y., and Schulz, M. 1986, SIAM Journal on Scientific and Statistical Computing, vol. 7,
pp. 856-869. [15]

Bunch, J.R., and Rose, D.J. (eds.) 1976, Sparse Matrix Computations (New York: Academic
Press).

Duff, I.S., and Stewart, G.W. (eds.) 1979, Sparse Matrix Proceedings 1978 (Philadelphia:
S.LA.M.).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

90 Chapter 2. Solution of Linear Algebraic Equations

2.8 Vandermonde Matrices and Toeplitz
Matrices

In §2.4 the case of a tridiagonal matrix was treated specially, because that
particular type of linear system admits a solution in only of order N operations,
rather than of order N? for the general linear problem. When such particular types
exist, it is important to know about them. Your computational savings, should you
ever happen to be working on a problem that involves the right kind of particular
type, can be enormous.

This section treats two special types of matrices that can be solved in of order
N? operations, not as good as tridiagonal, but a lot better than the general case.
(Other than the operations count, these two types having nothing in common.)
Matrices of the first type, termed Vandermonde matrices, occur in some problems
having to do with the fitting of polynomials, the reconstruction of distributions from
their moments, and also other contexts. In this book, for example, a Vandermonde
problem crops up in §3.5. Matrices of the second type, termed Toeplitz matrices,
tend to occur in problems involving deconvolution and signal processing. In this
book, a Toeplitz problem is encountered in §13.7.

These are not the only special types of matrices worth knowing about. The
Hilbert matrices, whose components are of the form aj; = 1/(i+ j — 1), 1,9 =
1,...,.N can be inverted by an exact integer algorithm, and are very difficult to
invert in any other way, since they are notoriously ill-conditioned (see [1] for details).
The Sherman-Morrison and Woodbury formulas, discussed in §2.7, can sometimes
be used to convert new special forms into old ones. Reference [2] gives some other
special forms. We have not found these additional forms to arise as frequently as
the two that we now discuss.

Vandermonde Matrices

A Vandermonde matrix of size N x N is completely determined by N arbitrary
xn, in terms of which its N? components are the integer powers
4 ; N. Evidently there are two possible such forms, depending on whether
we view the 7’s as rows, j’s as columns, or vice versa. In the former case, we get a linear
system of equations that looks like this,

v1 «te ay ci YL
loa. 2% co} _ | y2 (2.8.1)
1 &N wy CN YN

Performing the matrix multiplication, you will see that this equation solves for the unknown
coefficients c; which fit a polynomial to the N pairs of absc' and ordinates (xj, yj).
Precisely this problem will arise in §3.5, and the routine given there will solve (2.8.1) by the
method that we are about to describe.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.8 Vandermonde Matrices and Toeplitz Matrices 91

The alternative identification of rows and columns leads to the set of equations

1 1 see 1 wi n
ry zr2 vee IN w2 q2
N-1 ..N-1 N-1

ry @ os EN wn qN

Write this out and you will see that it relates to the problem of moments: Given the values
of N points x;, find the unknown weights w;, assigned so as to match the given values
q; of the first N moments. (For more on this problem, consult [3].) The routine given in
this section solves (2.8.2).

The method of solution of both (2.8.1) and (2.8.2) is closely related to Lagrange’s
polynomial interpolation formula, which we will not formally meet until §3.1 below. Notwith-
standing, the following derivation should be comprehensible:

Let P;(x) be the polynomial of degree N — 1 defined by

N N
L—-In _
P(x) = TI a = DeAnna! 1 (2.8.3)
aa ©: =
(n#i) :

Here the meaning of the last equality is to define the components of the matrix Aj; as the
coefficients that arise when the product is multiplied out and like terms collected.

The polynomial Pj(x) is a function of x generally. But you will notice that it is
specifically designed so that it takes on a value of zero at all x; with « A j, and has a value
of unity at c = xj. In other words,

wv
Pi(wi) = 63 = YO Aju? (2.84)
k=1

But (2.8.4) says that Aj; is exactly the inverse of the matrix of components ak} , Which
appears in (2.8.2), with the subscript as the column index. Therefore the solution of (2.8.2)
is just that matrix inverse times the right-hand side,

wv
wy = 0 Ajneae (28.5)
k=1

As for the transpose problem (2.8.1), we can use the fact that the inverse of the transpose
is the transpose of the inverse, so

wv
cj = Do Any (28.6)
k=1

The routine in §3.5 implements this.

It remains to find a good way of multiplying out the monomial terms in (2.8.3), in order
to get the components of A;,. This is essentially a bookkeeping problem, and we will let you
read the routine itself to see how it can be solved. One trick is to define a master P(x) by

P(x) = [| (wan) (2.8.7)

work out its coefficients, and then obtain the numerators and denominators of the specific P;’s
via synthetic division by the one supernumerary term. (See §5.3 for more on synthetic division.)
Since each such division is only a process of order N, the total procedure is of order N?.

You should be warned that Vandermonde systems are notoriously ill-conditioned, by
their very nature. (As an aside anticipating §5.8, the reason is the same as that which makes
Chebyshev fitting so impressively accurate: there exist high-order polynomials that are very
good uniform fits to zero. Hence roundoff error can introduce rather substantial coefficients
of the leading terms of these polynomials.) It is a good idea always to compute Vandermonde
problems in double precision.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

92 Chapter 2. Solution of Linear Algebraic Equations

The routine for (2.8.2) which follows is due to G.B. Rybicki.

#include "nrutil.h"

void vander(double x[], double w[], double q[], int n)
Solves the Vandermonde linear system > y ak ly; = qk (k=1,.

1
the vectors x[1..n] and q[1..n]; the vector w[1..n] is output.
{

N). Input consists of

int i,j,k;
double b,s,t,xx;
double *c;

c=dvector(1,n) ;
if (n == 1) wlt]=q[1];

else {
for (i=1;i<=n;i++) c[i]=0.0; Initialize array.
ce[n] = -x[1]; Coefficients of the master polynomial are found
for (i=2;i<=n;it+) { by recursion.
xx = -x[il];
for (j=(nt1-i) ;j<=(m-1); j++) cj] += xx*c[j+1];
c[n] += xx;
}
for (i=1;i<=n;it++) { Each subfactor in turn
xx=x [i] 5
t=b=1.0;
s=q[n];
for (k=n;k>=2;k--) { is synthetically divided,
b=c[k] +xx*b;
s += q[k-1]*b; matrix-multiplied by the right-hand side,
t=xx*t+b;
3
wli]=s/t; and supplied with a denominator.
}
}

free_dvector(c,1,n);

Toeplitz Matrices

An N x N Toeplitz matrix i

specified by giving 2N — 1 numbers R,, k = —N +

1,...,-1,0,1 ,N — 1. Those numbers are then emplaced as matrix elements constant
along the (upper-left to lower-right) diagonals of the matrix:
Ro Ry R-2 +++ Rw-2) R-(n-1)
mh Ro Ra Rows) Roa)
Ra Ri Ro R_(nw-a) R-(n-3) (2.8.8)
Rn-2 Ry-3 Rn-a --- Ro Rit
Rn-1 Rn-2 Rn-3 -:: Ri Ro
The linear Toeplitz problem can thus be written as

where the x;’s, 7 = 1,..., N, are the unknowns to be solved for.
The Toeplitz matrix is symmetric if Ry = R—, for all k. Levinson [4] developed an
algorithm for fast solution of the symmetric Toeplitz problem, by a bordering method, that is,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.8 Vandermonde Matrices and Toeplitz Matrices 93

a recursive procedure that solves the /-dimensional Toeplitz problem

M
So Ri-
j=l

in turn for MZ = 1,2,... until J = N, the desired result, is finally reached. The vector x
is the result at the Mth stage, and becomes the desired answer only when N is reached.
Levinson’s method is well documented in standard texts (e.g.,[5]). The useful fact that
the method generalizes to the nonsymmetric case seems to be less well known. At some risk
of excessive detail, we therefore give a derivation here, due to G.B. Rybicki.
In following a recursion from step M to step M + 1 we find that our developing solution
x) changes in this way:

OO) Sy,  (i=1,...,M) (2.8.10)

(M)

j

M
SORA pe =y i= 1,...,M (2.8.11)
j=l
becomes
tO ER ogy en) sy i= 1,...,M41 (2.8.12)

By eliminating y; we find

M 2) — gM)
So Rij ( mee } = Ri-(u 41) i=1,...,M (2.8.13)
j=l TM41

or by letting i M+1-iand j M+1-j,

M
So RG = (28.14)
j=l

where

(mM) (M+1)
(M) _ ©M+1-j ~ ©M+1-3
Gy; = ae (2.8.15)
M+1
To put this another way,
M M+1)(M -
Greig GP) G1. M (2.8.16)

Thus, if we can use recursion to find the order M quantities a) and G and the single

order M + 1 quantity ot, then all of the other 2(/+))

5 will follow. Fortunately, the

quantity at) follows from equation (2.8.12) with i = M +1,
M
SE Rarer jot? + Roa hh = yar (2.8.17)

j=l

For the unknown order M + 1 quantities at)

quantities in G since

we can substitute the previous order

ol) _ (Mtn
(M) ae
Gyrii—3 = >a (2.8.18)
pavent
The result of this operation is
M (m1)
Y Rusgi-j®y’ — yM4i
ee = Lint at) (2.8.19)

~ yaM M
yar Ruyi-j@uee yj — Ro

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

94 Chapter 2. _ Solution of Linear Algebraic Equations

The only remaining problem is to develop a recursion relation for G. Before we do
that, however, we should point out that there are actually two distinct sets of solutions to the
original linear problem for a nonsymmetric matrix, namely right-hand solutions (which we
have been discussing) and left-hand solutions z;. The formalism for the left-hand solutions
differs only in that we deal with the equations

M
SOR sy 1=1,...,M (2.8.20)
j=l
Then, the same sequence of operations on this set leads to
M
DORA = Ri (2821)
j=l
where
_ (M+)
(mM) M+1-j
H; sy (2.8.22)

(compare with 2.8.14 — 2.8.15). The reason for mentioning the left-hand solutions now is
that, by equation (2.8.21), the H; satisfy exactly the same equation as the x; except for
the substitution y; — R; on the right-hand side. Therefore we can quickly deduce from
equation (2.8.19) that

M M
HOLD = ye, Raj Hh? = Rasa
M+1 ~ Dw M
Vja1 Rury-i@Qre —Ro

By the same token, G satisfies the same equation as z, except for the substitution y; + R_;.
This gives

(2.8.23)

M (M)
Gen ja Ri-m-1Gj* — R-m-1
M+1 M M
Ry HN; — Ro

The same “morphism” also turns equation (2.8.16), and its partner for z, into the final equations

(2.8.24)

M+1 M M-+1) 77(M
Gs y= Gs = aut AM;

(2.8.25)
(M+1) (mM) (M+1) 4(M)
H; =H; ~ Ay Gurtij

Now, starting with the initial values
rt =y/Ro GY =R1/Ro Hf =Ri/Ro (2.8.26)

we can recurse away. At each stage M we use equations (2.8.23) and (2.8.24) to find

HAD ; ae , and then equation (2.8.25) to find the other components of H™MY) Grd |

M+1
From there the vectors x D and/or zt) are easily calculated.
The program below does this. It incorporates the second equation in (2.8.25) in the form
HOD, = HED, — HEDGE (2827)
so that the computation can be done “in place.”

Notice that the above algorithm fails if Ro = 0. In fact, because the bordering method
does not allow pivoting, the algorithm will fail if any of the diagonal principal minors of the
original Toeplitz matrix vanish. (Compare with discussion of the tridiagonal algorithm in
§2.4.) If the algorithm fails, your matrix is not necessarily singular — you might just have
to solve your problem by a slower and more general algorithm such as LU decomposition
with pivoting.

The routine that implements equations (2.8.23)—-(2.8.27) is also due to Rybicki. Note
that the routine’s r[n+j] is equal to Rj above, so that subscripts on the r array vary from
1 to 2N—-1.

(M+

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.8 Vandermonde Matrices and Toeplitz Matrices 95

#include "nrutil.h"
#define FREERETURN {free_vector(h,1,n) ;free_vector(g,1,n) ;return;}

void toeplz(float r[], float x[], float y[], int n)
Solves the Toeplitz system we Rww+i-j)2j = yi @=1,...,N). The Toeplitz matrix need
not be symmetric. y[1..n] and r[1..2*n-1] are input arrays; x[1..n] is the output array.
{

int j,k,m,m1,m2;

float pp,pt1,pt2,qq,qt1,qt2,sd,sgd,sgn,shn,sxn;

float *g,*h;

doa,

if (r{m] == 0.0) nrerror("toeplz-1 singular principal minor") ;
g=vector(1,n);

h=vector(1,n);

x({t]=y([1]/r{n]; Initialize for the recursion.

if (m == 1) FREERETURN

g(1]=r{n-1]/r{n);

h[1]=r[n+1] /r{n];

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

for (m=1;m<=n;m++) { Main loop over the recursion.
mi=m+1;
sxn = -y[m1]; Compute numerator and denominator for x,
sd = -r[n];

for (j=1;j<=m;jt+) {
sxn += r({ntmi-j]*x[j];
sd += r[n+mi-j]*g[m-j+1];

}

if (sd == 0.0) nrerror("toeplz-2 singular principal minor") ;
x[m1]=sxn/sd; whence x.

for (j=1;j<=m;jt++) x{j] -= x[m1]*g[m-j+1];

if (m1 n) FREERETURN

sgn = -r[n-mi]; Compute numerator and denominator for G and H,
shn = -r[ntmi];

sgd = -r[n];

for (j=1;j<=m;jt+) {
sgn += r[n+j-mi]*g[j];

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

shn += r[{n+mi-j]*h[j];
sgd += r[nt+j-m1]*h[m-jt+i];
}
if (sgd == 0.0) nrerror("toeplz-3 singular principal minor");
glm1]=sgn/sgd; whence G and H.
h{m1]=shn/sd;
k=m;
m2=(m+1) >> 1;
pp=g{m1] ;
qq=h [m1] ;
for (j=1;j<=m2;jt+) {
pti=gljl;
pt2=g[k] ;
qti=h[j];
qt (k];
glj]=pti-pp*qt2;
glk] =pt2-pp*qt1;
hlj]=qt1-qq*pt2;
h[k--]=qt2-qq*pt1;
}
+ Back for another recurrence.

nrerror("toeplz - should not arrive here!");

If you are in the business of solving very large Toeplitz systems, you should find out about
so-called “new, fast” algorithms, which require only on the order of N(log N) operations,
compared to N? for Levinson’s method. These methods are too complicated to include here.

96 Chapter 2. Solution of Linear Algebraic Equations

Papers by Bunch [6] and de Hoog [7] will give entry to the literature.

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), Chapter 5 [also treats some other special forms].

Forsythe, G.E., and Moler, C.B. 1967, Computer Solution of Linear Algebraic Systems (Engle-
wood Cliffs, NJ: Prentice-Hall), §19. 1]

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of Linear Equations
(New York: Wiley). [2]

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), pp. 394ff. [3]

Levinson, N., Appendix B of N. Wiener, 1949, Extrapolation, Interpolation and Smoothing of
Stationary Time Series (New York: Wiley). [4]

Robinson, E.A., and Treitel, S. 1980, Geophysical Signal Analysis (Englewood Cliffs, NJ: Prentice-
Hall), pp. 163ff. [5]

Bunch, J.R. 1985, SIAM Journal on Scientific and Statistical Computing, vol. 6, pp. 349-364. [6]
de Hoog, F. 1987, Linear Algebra and Its Applications, vol. 88/89, pp. 123-138. [7]

2.9 Cholesky Decomposition

If a square matrix A happens to be symmetric and positive definite, then it has a

special, more efficient, triangular decomposition. Symmetric means that aj; = aj; for
i,j = 1,...,.N, while positive definite means that
v-A-v>0_ forall vectors v (2.9.1)

(In Chapter 11 we will see that positive definite has the equivalent interpretation that A has
all positive eigenvalues.) While symmetric, positive definite matrices are rather special, they
occur quite frequently in some applications, so their special factorization, called Cholesky
decomposition, is good to know about. When you can use it, Cholesky decomposition is about
a factor of two faster than alternative methods for solving linear equations.

Instead of seeking arbitrary lower and upper triangular factors L and U, Cholesky
decomposition constructs a lower triangular matrix L whose transpose L” can itself serve as
the upper triangular part. In other words we replace equation (2.3.1) by

L-L7=A (2.9.2)

This factorization is sometimes referred to as “taking the square root” of the matrix A. The
components of L7 are of course related to those of L by

Li = Lyi (293)

Writing out equation (2.9.2) in components, one readily obtains the analogs of equations
(2.3.12)-(2.3.13),

i-1 1/2
Lis = (« ~ yr) (2.94)
k=1

and
i-1

1
Li=E (0 -Yerata) j=itlit2,...,N (2.9.5)
“ k=1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.9 Cholesky Decomposition 97

If you apply equations (2.9.4) and (2.9.5) in the order 7 = 1,2,...,.N, you will see
that the L’s that occur on the right-hand side are already determined by the time they are
needed. Also, only components a;; with j > i are referenced. (Since A is symmetric,
these have complete information.) It is convenient, then, to have the factor L overwrite the
subdiagonal (lower triangular but not including the diagonal) part of A, preserving the input
upper triangular values of A. Only one extra vector of length NV is needed to store the diagonal
part of L. The operations count is N?/6 executions of the inner loop (consisting of one
multiply and one subtract), with also N square roots. As already mentioned, this is about a
factor 2 better than LU decomposition of A (where its symmetry would be ignored).

A straightforward implementation is

#include <math.h>

void choldc(float **a, int n, float p[])
Given a positive-definite symmetric matrix a[1..n] [1..n], this routine constructs its Cholesky
decomposition, A = L-L?. On input, only the upper triangle of a need be given; it is not
modified. The Cholesky factor L is returned in the lower triangle of a, except for its diagonal
elements which are returned in p[1..n].
{

void nrerror(char error_text[]);

int i,j,k;

float sum;

for (i
for (j=i;j<=n;jt+) {

for (sum=a[i] [j] ,k=i-1;k>=1;k--) sum -= a[i] [k] *a[j] [k];

if Gis= jf
if (sum <= 0.0) a, with rounding errors, is not positive definite.

nrerror("cholde failed");

plil=sqrt (sum) ;

} else a[j] [i]=sum/p[il;

You might at this point wonder about pivoting. The pleasant answer is that Cholesky
decomposition is extremely stable numerically, without any pivoting at all. Failure of choldc
simply indicates that the matrix A (or, with roundoff error, another very nearby matrix) is
not positive definite. In fact, choldc is an efficient way to test whether a symmetric matrix
is positive definite. (In this application, you will want to replace the call to nrerror with
some less drastic signaling method.)

Once your matrix is decomposed, the triangular factor can be used to solve a linear
equation by backsubstitution. The straightforward implementation of this is

void cholsl(float **a, int n, float p[], float b[], float x[])
Solves the set of n linear equations A - x = b, where a is a positive-definite symmetric matrix.
af1..n][1..n] and p[1..n] are input as the output of the routine choldc. Only the lower
subdiagonal portion of a is accessed. b[1..n] is input as the right-hand side vector. The
solution vector is returned in x[1..n]. a, n, and Pp are not modified and can be left in place
for successive calls with different right-hand sides b. b is not modified unless you identify b and
x in the calling sequence, which is allowed.
t

int i,k;

float sum;

for (i=1;i<=n;i++) { Solve L - y = b, storing y in x.
for (sum=b[i] ,k=i-1;k>=1;k--) sum -= a[i] [k] *x[k];
x[i]=sum/p [i] ;

}

for (i=n;i>=1;i--) { Solve LT - x
for (sum=x[i] ,k=i+1;k<=n;k++) sum

y-
= alk] [i] *x[k];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

98 Chapter 2. Solution of Linear Algebraic Equations

x[i]=sum/p[il;

A typical use of choldc and cho1s1 is in the inversion of covariance matrices describing
the fit of data to a model; see, e.g., §15.6. In this, and many other applications, one often needs
L~!. The lower triangle of this matrix can be efficiently found from the output of choldc:

for (j=it1;j<=n;jt+) {
sum=0.0;

k<j;k++) sum -= a[j] [k] *al[k] [i];
alj] [il=sum/p[j];

CITED REFERENCES AND FURTHER READING:

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag), Chapter 1/1.

Gill, P.E., Murray, W., and Wright, M.H. 1991, Numerical Linear Algebra and Optimization, vol. 1
(Redwood City, CA: Addison-Wesley), §4.9.2.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§5.3.5.

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), $4.2.

2.10 QR Decomposition

There is another matrix factorization that is sometimes very useful, the so-called QR
decomposition,

A=Q-R (2.10.1)
Here R is upper triangular, while Q is orthogonal, that is,
Q7-Q=1 (2.10.2)

where Q” is the transpose matrix of Q. Although the decomposition exists for a general
rectangular matrix, we shall restrict our treatment to the case when all the matrices are square,
with dimensions N x N.

Like the other matrix factorizations we have met (LU, SVD, Cholesky), Q.R decompo-
sition can be used to solve systems of linear equations. To solve

A-x=b (2.10.3)
first form Q7 - b and then solve
R-x=Q"-b (2.10.4)

by backsubstitution. Since QR decomposition involves about twice as many operations as
LU decomposition, it is not used for typical systems of linear equations. However, we will
meet special cases where QR is the method of choice.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.10 QR Decomposition 99

The standard algorithm for the QR decomposition involves successive Householder
transformations (to be discussed later in §11.2). We write a Householder matrix in the form
1 —u @u/c where c = du -u. An appropriate Householder matrix applied to a given matrix
can zero all elements in a column of the matrix situated below a chosen element. Thus we
arrange for the first Householder matrix Q, to zero all elements in the first column of A below
the first element. Similarly yet zeroes all elements in the second column below the second
element, and so on up to Q,,_,. Thus

R=Q1-Q-A (2.10.5)
Since the Householder matrices are orthogonal,

Q = Qua Qi)? = Q + Quaa (2.10.6)

In most applications we don’t need to form Q explicitly; we instead store it in the factored
form (2.10.6). Pivoting is not usually necessary unless the matrix A is very close to singular.
A general QR algorithm for rectangular matrices including pivoting is given in[1]. For square
matrices, an implementation is the following:

#include <math.h>
#include "nrutil.h"

void qrdcmp(float **a, int n, float *c, float *d, int *sing)

Constructs the QR decomposition of a[1..n][1..n]. The upper triangular matrix R is re-
turned in the upper triangle of a, except for the diagonal elements of R which are returned in
da[1..n]. The orthogonal matrix Q is represented as a product of n — 1 Householder matrices
Qi -+-Qn—1, where Q; = 1—uj @uj/cj. The ith component of uj is zero fori =1,...,7 -—1
while the nonzero components are returned in ali] [j] for i = j,...,n. sing returns as
true (1) if singularity is encountered during the decomposition, but the decomposition is still
completed in this case; otherwise it returns false (0).

{

int i,j,k;
float scale,sigma,sum,tau;

*sing=0;
for (k=1;k<n;k++) {
scale=0.0;
for (i=k;i<=n;it++) scale=FMAX(scale,fabs(a[i] [k]));
if (scale 0.0) { Singular case.
*sing:
c[k]=d[k]=0.0;
} else { Form Q;, and Q;- A
for (i=k;i<=n;i++) ali][k] /= scale;
for (sum=0.0,i=k;i<=n;i++) sum += SQR(aLi] [k]);
sigma=SIGN (sqrt (sum) ,a[k] [k]);
alk] [k] += sigma;
c[k]=sigma*a[k] [k] ;
a[k] = -scale*sigma;
for (j=k+1;j<=n;j++) {
for (sum=0.0,i=k;i<=n;it++) sum += a[i] [k]*ali][j];
tau=sum/c[k] ;
for (i=k;i<=n;i++) ali][j] -= tau*a[i] [k];

+
+
d{n]=a[n] [n] ;
if (d{n] == 0.0) *sing=1;

The next routine, qgrsolv, is used to solve linear systems. In many applications only the
part (2.10.4) of the algorithm is needed, so we separate it off into its own routine rsolv.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

100 Chapter 2. Solution of Linear Algebraic Equations

void qrsolv(float **a, int n, float c[], float d[], float b[])
Solves the set of n linear equations A-x = b. a[1..n][1..n], c[1..n], and d[1..n] are
input as the output of the routine qrdcmp and are not modified. b[1..n] is input as the
right-hand side vector, and is overwritten with the solution vector on output.
{

void rsolv(float **a, int n, float d[], float b[]);

int i,j;

float sum,tau;

for (j=1;j<n;j++) { Form Q7 - b.
for (sum=0.0,i=j;i<=n;it++) sum += ali] [j]*b[il;
tau=sum/c[j];
for (i=j;i<=n;it+) bli] -= tau*ali] [jl];

+

rsolv(a,n,d,b); Solve R-x = Q? -b.

void rsolv(float **a, int n, float d[], float b[])

Solves the set of n linear equations R - x = b, where R is an upper triangular matrix stored in
aandd. a[i1..n][1..n] and d[1..n] are input as the output of the routine qrdcmp and
are not modified. b[1..n] is input as the right-hand side vector, and is overwritten with the
solution vector on output.

{
int i,j;
float sum;
bn] /= d{n];
for (i=n-1;i>=1;i--) {
for (sum=0.0,j=it1;j<=n; j++) sum += ali] [j]*b[j];
bli]=(b[i]-sum) /d[i] ;
+
3

See [2] for details on how to use QR decomposition for constructing orthogonal bases,
and for solving least-squares problems. (We prefer to use SVD, §2.6, for these purposes,
because of its greater diagnostic capability in pathological cases.)

Updating a QR decomposition

Some numerical algorithms involve solving a succession of linear systems each of which
differs only slightly from its predecessor. Instead of doing O(N") operations each time
to solve the equations from scratch, one can often update a matrix factorization in O(N?)
operations and use the new factorization to solve the next set of linear equations. The LU
decomposition is complicated to update because of pivoting. However, QR turns out to be
quite simple for a very common kind of update,

A-A+t+s@t (2.10.7)
(compare equation 2.7.1). In practice it is more convenient to work with the equivalent form
A=Q:R A’ =Q’-R’=Q-(R+u®@v) (2.10.8)

One can go back and forth between equations (2.10.7) and (2.10.8) using the fact that Q
is orthogonal, giving

t=v andeithr s=Q-u or u=Q7-s (2.10.9)

The algorithm [2] has two phases. In the first we apply N — 1 Jacobi rotations (§11.1) to
reduce R + u ® v to upper Hessenberg form. Another N — 1 Jacobi rotations transform this
upper Hessenberg matrix to the new upper triangular matrix R’. The matrix Q’ is simply the
product of Q with the 2(.NV — 1) Jacobi rotations. In applications we usually want Q*, and
the algorithm can easily be rearranged to work with this matrix instead of with Q.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.10 QR Decomposition 101

#include <math.h>
#include "nrutil.h"

void qrupdt(float **r, float **qt, int n, float ul], float v[])

Given the QR decomposition of some n x n matrix, calculates the QR decomposition of the
matrix Q:(R-+u@v). The quantities are dimensioned as r[1..n] [1..n], qt[1..n][1..n],
ufi1..n], and v[1..n]. Note that Q7 is input and returned in qt.

{
void rotate(float **r, float **qt, int n, int i, float a, float b);
int i,j,k;
for (k=n;k>=1;k--) { Find largest k such that u[k] ¥ 0.
if (u[k]) break;
+
if (k < 1) k=1;
for (i=k-1;i>=1;i--) { Transform R + u @ v to upper Hessenberg.
rotate(r,qt,n,i,u[i] ,-u[i+1]);
if (ufi] 0.0) uli]=fabs(u[it+1]);
else if (fabs(u[i]) > fabs(u[it1]))
uli]=fabs(u[i])*sqrt (1.0+SQR(uL[it1] /ulil]));
else u[i]=fabs(u[it+1])*sqrt(1.0+SQR(uli] /ulit1]));
}
for (j=1;j<=n;j++) rf1][j] += ult]*v[jl;
for (i=1;i<k;i++) Transform upper Hessenberg matrix to upper tri-
rotate(r,qt,n,i,r(i] [i] ,-r[i+1] [i]); angular.
3

#include <math.h>
#include "nrutil.h"

void rotate(float **r, float **qt, int n, int i, float a, float b)
Given matrices r[1..n][1..n] and qt[1..n][1..n], carry out a Jacobi rotation on rows
i and i+ 1 of each matrix. a and b are the parameters of the rotation: cos @ = a/\/a? + b?,
sind = b/Va2 +b?.
{

int j;

float c,fact,s,w,y;

if (a == 0.0) { Avoid unnecessary overflow or underflow.
c=0.0;
s=(b >= 0.0 7 1.0 : -1.0);

} else if (fabs(a) > fabs(b)) {
fact=b/a;
c=SIGN(1.0/sqrt (1.0+(fact*fact)),a);
s=fact*c;

} else {
fact=a/b;
s=SIGN(1.0/sqrt (1.0+(fact*fact)),b);
c=fact*s;

for

j<=n;jt+) { Premultiply r by Jacobi rotation.
y=r [il [5];

wer [i+1] [jl];

xr [i] [j]=c*y-s*w;

r(itt] [j]=s*y+c*w;

for (j=1;j<=n; j++) { Premultiply qt by Jacobi rotation.
y=qt [il [j];
w=qt [it+1] [5];
qt i] (j]=c*y-s+w;
qt [it1] [j]=s*ytc+w;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

102 Chapter 2. Solution of Linear Algebraic Equations

We will make use of QR decomposition, and its updating, in §9.7.

CITED REFERENCES AND FURTHER READING:

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag), Chapter 1/8. [1]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), §§5.2, 5.3, 12.6. [2]

2.11 Is Matrix Inversion an N3 Process?

We close this chapter with a little entertainment, a bit of algorithmic prestidig-
itation which probes more deeply into the subject of matrix inversion. We start
with a seemingly simple question:

How many individual multiplications does it take to perform the matrix mul-
tiplication of two 2 x 2 matrices,

a1 a2 bir big C1 C12
: = 2.11.1
(c a22 ) ( boo ) ( Cq1 C22 ) ( )
Eight, right? Here they are written explicitly:

C11 = ai X by + aig X bar

C12 = ai X big + aig X ba2

(2.11.2)

€21 = Gai X by + 22 X bar

€22 = G21 X big + a2 X bo2

Do you think that one can write formulas for the c’s that involve only seven
multiplications? (Try it yourself, before reading on.)
Such a set of formulas was, in fact, discovered by Strassen [1]. The formulas are:
Q1 = (ai + G22) X (b11 + b22)
Qe = (aa1 + a22) x dit
Q3 = a1 X (biz — b22)
Qa = a22 x (—bi1 + ba1) (2.11.3)
Qs = (ai + a2) X bog
Q6 = (—a11 + G21) x (b11 + biz)
Q7z = (a12 — 422) X (b21 + b22)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

2.11 Is Matrix Inversion an N° Process? 103

in terms of which

en = Q1+Q1-Q5+Q7
cr = Q2 + Qa
c12 = Q3 + Qs
22 = Q1 + Q3 — Q2 + V6

(2.11.4)

What’s the use of this? There is one fewer multiplication than in equation
(2.11.2), but many more additions and subtractions. It is not clear that anything
has been gained. But notice that in (2.11.3) the a’s and b’s are never commuted.
Therefore (2.11.3) and (2.11.4) are valid when the a’s and b’s are themselves matrices.
The problem of multiplying two very large matrices (of order N = 2” for some
integer m) can now be broken down recursively by partitioning the matrices into
quarters, sixteenths, etc. And note the key point: The savings is not just a factor
“7/8”; it is that factor at each hierarchical level of the recursion. In total it reduces
the process of matrix multiplication to order N!°£27 instead of N°.

What about all the extra additions in (2.11.3)-(2.11.4)? Don’t they outweigh
the advantage of the fewer multiplications? For large N, it turns out that there are
six times as many additions as multiplications implied by (2.11.3)-(2.11.4). But,
if N is very large, this constant factor is no match for the change in the exponent
from N® to Nie827,

With this “fast” matrix multiplication, Strassen also obtained a surprising result
for matrix inversion [1]. Suppose that the matrices

(c ae) and (“ 2) (2.115)
a2, 422 C21 C22
are inverses of each other. Then the c’s can be obtained from the a’s by the following

operations (compare equations 2.7.22 and 2.7.25):

Ry, = Inverse(a11)

Ry = a1 X Ry
R3 = Ry X ay
Rg = a2 x Rg
Rs = Ra — an2
Re = Inverse(Rs) (2.11.6)
cig = R3 x Re
C21 = Re x Ro
Rz = R3 X c21
cu = Ri — Bz

C22 = —Re

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

104 Chapter 2. Solution of Linear Algebraic Equations

In (2.11.6) the “inverse” operator occurs just twice. It is to be interpreted as the
reciprocal if the a’s and c’s are scalars, but as matrix inversion if the a’s and c’s are
themselves submatrices. Imagine doing the inversion of a very large matrix, of order
N = 2™, recursively by partitions in half. At each step, halving the order doubles
the number of inverse operations. But this means that there are only N divisions in
all! So divisions don’t dominate in the recursive use of (2.11.6). Equation (2.11.6)
is dominated, in fact, by its 6 multiplications. Since these can be done by an N !°827
algorithm, so can the matrix inversion!

This is fun, but let’s look at practicalities: If you estimate how large N has to be
before the difference between exponent 3 and exponent log 2 7 = 2.807 is substantial
enough to outweigh the bookkeeping overhead, arising from the complicated nature
of the recursive Strassen algorithm, you will find that LU decomposition is in no
immediate danger of becoming obsolete.

If, on the other hand, you like this kind of fun, then try these: (1) Can you
multiply the complex numbers (a+b) and (c+ id) in only three real multiplications?
[Answer: see §5.4.] (2) Can you evaluate a general fourth-degree polynomial in
x for many different values of x with only three multiplications per evaluation?
[Answer: see §5.3.]

CITED REFERENCES AND FURTHER READING:

Strassen, V. 1969, Numerische Mathematik, vol. 13, pp. 354-356. [1]

Kronsjé, L. 1987, Algorithms: Their Complexity and Efficiency, 2nd ed. (New York: Wiley).

Winograd, S. 1971, Linear Algebra and Its Applications, vol. 4, pp. 381-388.

Pan, V. Ya. 1980, SIAM Journal on Computing, vol. 9, pp. 321-342.

Pan, V. 1984, How to Multiply Matrices Faster, Lecture Notes in Computer Science, vol. 179
(New York: Springer-Verlag)

Pan, V. 1984, SIAM Review, vol. 26, pp. 393-415. [More recent results that show that an exponent
of 2.496 can be achieved — theoretically!]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 3. Interpolation and
Extrapolation

3.0 Introduction

We sometimes know the value of a function f(z) ata set of points 71, 72,...,0N
(say, with x1 <... < xy), but we don’t have an analytic expression for f (x) that lets
us calculate its value at an arbitrary point. For example, the f(x ;)’s might result from
some physical measurement or from long numerical calculation that cannot be cast
into a simple functional form. Often the x ;’s are equally spaced, but not necessarily.

The task now is to estimate f(x) for arbitrary x by, in some sense, drawing a
smooth curve through (and perhaps beyond) the x ;. If the desired x is in between the
largest and smallest of the x;’s, the problem is called interpolation; if x is outside
that range, it is called extrapolation, which is considerably more hazardous (as many
former stock-market analysts can attest).

Interpolation and extrapolation schemes must model the function, between or
beyond the known points, by some plausible functional form. The form should
be sufficiently general so as to be able to approximate large classes of functions
which might arise in practice. By far most common among the functional forms
used are polynomials (§3.1). Rational functions (quotients of polynomials) also turn
out to be extremely useful (§3.2). Trigonometric functions, sines and cosines, give
rise to trigonometric interpolation and related Fourier methods, which we defer to
Chapters 12 and 13.

There is an extensive mathematical literature devoted to theorems about what
sort of functions can be well approximated by which interpolating functions. These
theorems are, alas, almost completely useless in day-to-day work: If we know
enough about our function to apply a theorem of any power, we are usually not in
the pitiful state of having to interpolate on a table of its values!

Interpolation is related to, but distinct from, function approximation. That task
consists of finding an approximate (but easily computable) function to use in place
of a more complicated one. In the case of interpolation, you are given the function f
at points not of your own choosing. For the case of function approximation, you are
allowed to compute the function f at any desired points for the purpose of developing
your approximation. We deal with function approximation in Chapter 5.

One can easily find pathological functions that make a mockery of any interpo-
lation scheme. Consider, for example, the function

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

106 Chapter 3. —_ Interpolation and Extrapolation

which is well-behaved everywhere except at x = 7, very mildly singular at x = 7,
and otherwise takes on all positive and negative values. Any interpolation based on
the values x = 3.13, 3.14, 3.15, 3.16, will assuredly get a very wrong answer for
the value « = 3.1416, even though a graph plotting those five points looks really
quite smooth! (Try it on your calculator.)

Because pathologies can lurk anywhere, it is highly desirable that an interpo-
lation and extrapolation routine should provide an estimate of its own error. Such
an error estimate can never be foolproof, of course. We could have a function that,
for reasons known only to its maker, takes off wildly and unexpectedly between
two tabulated points. Interpolation always presumes some degree of smoothness
for the function interpolated, but within this framework of presumption, deviations
from smoothness can be detected.

Conceptually, the interpolation process has two stages: (1) Fit an interpolating
function to the data points provided. (2) Evaluate that interpolating function at
the target point x.

However, this two-stage method is generally not the best way to proceed in
practice. Typically it is computationally less efficient, and more susceptible to
roundoff error, than methods which construct a functional estimate f(x) directly
from the N tabulated values every time one is desired. Most practical schemes start
at a nearby point f(;), then add a sequence of (hopefully) decreasing corrections,
as information from other f(x;)’s is incorporated. The procedure typically takes
O(N?) operations. If everything is well behaved, the last correction will be the
smallest, and it can be used as an informal (though not rigorous) bound on the error.

In the case of polynomial interpolation, it sometimes does happen that the
coefficients of the interpolating polynomial are of interest, even though their use
in evaluating the interpolating function should be frowned on. We deal with this
eventuality in §3.5.

Local interpolation, using a finite number of “nearest-neighbor” points, gives
interpolated values f(x) that do not, in general, have continuous first or higher
derivatives. That happens because, as x crosses the tabulated values x;, the
interpolation scheme switches which tabulated points are the “local” ones. (If such
a switch is allowed to occur anywhere else, then there will be a discontinuity in the
interpolated function itself at that point. Bad idea!)

In situations where continuity of derivatives is a concern, one must use
the “stiffer” interpolation provided by a so-called spline function. A spline is
a polynomial between each pair of table points, but one whose coefficients are
determined “slightly” nonlocally. The nonlocality is designed to guarantee global
smoothness in the interpolated function up to some order of derivative. Cubic splines
(83.3) are the most popular. They produce an interpolated function that is continuous
through the second derivative. Splines tend to be stabler than polynomials, with less
possibility of wild oscillation between the tabulated points.

The number of points (minus one) used in an interpolation scheme is called
the order of the interpolation. Increasing the order does not necessarily increase
the accuracy, especially in polynomial interpolation. If the added points are distant
from the point of interest x, the resulting higher-order polynomial, with its additional
constrained points, tends to oscillate wildly between the tabulated values. This
oscillation may have no relation at all to the behavior of the “true” function (see
Figure 3.0.1). Of course, adding points close to the desired point usually does help,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.0 Introduction 107

Figure 3.0.1. (a) A smooth function (solid line) is more accurately interpolated by a high-order
polynomial (shown schematically as dotted line) than by a low-order polynomial (shown as a piecewise
linear dashed line). (b) A function with sharp corners or rapidly changing higher derivatives is less
accurately approximated by a high-order polynomial (dotted line), which is too “stiff,” than by a low-order
polynomial (dashed lines). Even some smooth functions, such as exponentials or rational functions, can
be badly approximated by high-order polynomials.

but a finer mesh implies a larger table of values, not always available.

Unless there is solid evidence that the interpolating function is close in form to
the true function f, it is a good idea to be cautious about high-order interpolation.
We enthusiastically endorse interpolations with 3 or 4 points, we are perhaps tolerant
of 5 or 6; but we rarely go higher than that unless there is quite rigorous monitoring
of estimated errors.

When your table of values contains many more points than the desirable order
of interpolation, you must begin each interpolation with a search for the right “local”
place in the table. While not strictly a part of the subject of interpolation, this task is
important enough (and often enough botched) that we devote §3.4 to its discussion.

The routines given for interpolation are also routines for extrapolation. An
important application, in Chapter 16, is their use in the integration of ordinary
differential equations. There, considerable care is taken with the monitoring of
errors. Otherwise, the dangers of extrapolation cannot be overemphasized: An
interpolating function, which is perforce an extrapolating function, will typically go
berserk when the argument x is outside the range of tabulated values by more than
the typical spacing of tabulated points.

Interpolation can be done in more than one dimension, e.g., for a function

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

108 Chapter 3. —_ Interpolation and Extrapolation

f(x,y, 2). Multidimensional interpolation is often accomplished by a sequence of
one-dimensional interpolations. We discuss this in §3.6.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.2.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 2.

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 3.

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software (Englewood Cliffs,
NJ: Prentice Hall), Chapter 4.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), Chapter 5.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), Chapter 3.

Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York: Wiley), Chapter 6.

3.1 Polynomial Interpolation and Extrapolation

Through any two points there is a unique line. Through any three points, a
unique quadratic. Et cetera. The interpolating polynomial of degree N — 1 through
the N points y1 = f(r1),y2 = f(2),---,yn = f(xw) is given explicitly by
Lagrange’s classical formula,

(en —@i)(en — @2)...(@n —@N-1)

(3.1.1)
There are N terms, each a polynomial of degree NV — 1 and each constructed to be
zero at all of the x; except one, at which it is constructed to be y;.

It is not terribly wrong to implement the Lagrange formula straightforwardly,
but it is not terribly right either. The resulting algorithm gives no error estimate, and
it is also somewhat awkward to program. A much better algorithm (for constructing
the same, unique, interpolating polynomial) is Neville’s algorithm, closely related to
and sometimes confused with Aitken’s algorithm, the latter now considered obsolete.

Let P; be the value at a of the unique polynomial of degree zero (ie.,
a constant) passing through the point (r1,y1); so P; = y;. Likewise define
Po, P3,..., Pn. Now let Piz be the value at x of the unique polynomial of
degree one passing through both (71, y1) and (x2, y2). Likewise P23, P34,...,
Pww-yn- Similarly, for higher-order polynomials, up to P123....7, which is the value
of the unique interpolating polynomial through all N points, i.e., the desired answer.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.1 Polynomial Interpolation and Extrapolation 109

The various P’s form a “tableau” with “ancestors” on the left leading to a single
“descendant” at the extreme right. For example, with N = 4,

vy: yw=P
Pro
XQ: yo = P2 Prog
Pog Pi234 (3.1.2)
v3: Ys = Ps Po34
P34
v4: ya = Pa

Neville’s algorithm is a recursive way of filling in the numbers in the tableau
a column at a time, from left to right. It is based on the relationship between a
“daughter” P and its two “parents,”

— itm) Piiet)...c¢m—1) + (ti — ©) PEi41)(-42)...(i+m)

Py, . =
i(i+1)...(i+m) tam

(3.1.3)

This recurrence works because the two parents already agree at points 7,41...
Vitm-1-

An improvement on the recurrence (3.1.3) is to keep track of the small
differences between parents and daughters, namely to define (for m = 1,2,...,
N-),

Cm,i = Pi..(itm) — Pi...(itm—1)

Dini = Pi, im) — Poisay...(it¢m): (3.1.4)

Then one can easily derive from (3.1.3) the relations

(titm4i — £)(Cmi+1 — Dm,i)

Dinsi,i
m-+1, Xi — Litm+l (3.15)

(ai — &)(Cmi+1 — Dm,i)

Uj — Litmti

Cm+1,i

At each level m, the C’s and D’s are the corrections that make the interpolation one
order higher. The final answer P;,..1 is equal to the sum of any y; plus a set of C’s
and/or D’s that form a path through the family tree to the rightmost daughter.

Here is a routine for polynomial interpolation or extrapolation from N input
points. Note that the input arrays are assumed to be unit-offset. If you have
zero-offset arrays, remember to subtract 1 (see §1.2):

#include <math.h>
#include "nrutil.h"

void polint(float xa[], float ya[], int n, float x, float *y, float *dy)
Given arrays xa[1..n] and ya[1..n], and given a value x, this routine returns a value y, and
an error estimate dy. If P(x) is the polynomial of degree N — 1 such that P(xa;) = ya;,i =
1,...,m, then the returned value y = P(x).
{

int i,m,ns=1;

float den,dif,dift,ho,hp,w;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

110 Chapter 3. —_ Interpolation and Extrapolation

float *c,*d;

dif=fabs (x-xa[1]);
c=vector(1,n);
d=vector(1,n);

for (i=1;i<=n;it+) { Here we find the index ns of the closest table entry,
if ( (dift=fabs(x-xal[i])) < dif) {
ns=i;
dif=dift;
}
c([il=yali]; and initialize the tableau of c’s and d's.
dl[il=yalil;
+
*y=ya[ns--] ; This is the initial approximation to y.
for (m=1;m<n;m++) { For each column of the tableau,
for (i=1;i<=n-m;it+) { we loop over the current c's and d’s and update
ho=xa[i]-x; them.

hp=xa[i+m] -x;
w=c(it1]-d[i];

if ( (den=ho-hp) == 0.0) nrerror("Error in routine polint");
This error can occur only if two input xa's are (to within roundoff) identical.
den=w/den;

d[il=hp*den; Here the c’s and d's are updated.

c[i]=ho*den;

}
ay += (*dy=(24ns < (n-m) ? c[nst+i] : d[ms--]));
After each column in the tableau is completed, we decide which correction, c or d,
we want to add to our accumulating value of y, i.e., which path to take through the
tableau—forking up or down. We do this in such a way as to take the most “straight
line” route through the tableau to its apex, updating ns accordingly to keep track of
where we are. This route keeps the partial approximations centered (insofar as possible)
on the target x. The last dy added is thus the error indication.

}

free_vector(d,1,n);

free_vector(c,1,n);

Quite often you will want to call polint with the dummy arguments xa
and ya replaced by actual arrays with offsets. For example, the construction
polint (&xx [14] ,&yy [14] ,4,x,y,dy) performs 4-point interpolation on the tab-
ulated values xx[15..18], yy[15..18]. For more on this, see the end of §3.4.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.2.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§2.1.

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), 86.1.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.2 Rational Function Interpolation and Extrapolation 111

3.2 Rational Function Interpolation and
Extrapolation

Some functions are not well approximated by polynomials, but are well
approximated by rational functions, that is quotients of polynomials. We de-
note by Ri(i41)..(i+m) a rational function passing through the m + 1 points
(i, Yi)---(Vitms Yitm). More explicitly, suppose

Ru; . Pilz) — pot pie +--+ + pytt
i(i+1)...(i +m) Q.(2) tnt + qe”

(3.2.1)

Since there are + v + 1 unknown p’s and q’s (qo being arbitrary), we must have
m+l=ptve4+l (3.2.2)

In specifying a rational function interpolating function, you must give the desired
order of both the numerator and the denominator.

Rational functions are sometimes superior to polynomials, roughly speaking,
because of their ability to model functions with poles, that is, zeros of the denominator
of equation (3.2.1). These poles might occur for real values of x, if the function
to be interpolated itself has poles. More often, the function f() is finite for all
finite real x, but has an analytic continuation with poles in the complex x-plane.
Such poles can themselves ruin a polynomial approximation, even one restricted to
real values of x, just as they can ruin the convergence of an infinite power series
in x. If you draw a circle in the complex plane around your m tabulated points,
then you should not expect polynomial interpolation to be good unless the nearest
pole is rather far outside the circle. A rational function approximation, by contrast,
will stay “good” as long as it has enough powers of x in its denominator to account
for (cancel) any nearby poles.

For the interpolation problem, a rational function is constructed so as to go
through a chosen set of tabulated functional values. However, we should also
mention in passing that rational function approximations can be used in analytic
work. One sometimes constructs a rational function approximation by the criterion
that the rational function of equation (3.2.1) itself have a power series expansion
that agrees with the first m + 1 terms of the power series expansion of the desired
function f(a). This is called Padé approximation, and is discussed in §5.12.

Bulirsch and Stoer found an algorithm of the Neville type which performs
rational function extrapolation on tabulated data. A tableau like that of equation
(3.1.2) is constructed column by column, leading to a result and an error estimate.
The Bulirsch-Stoer algorithm produces the so-called diagonal rational function, with
the degrees of numerator and denominator equal (if m is even) or with the degree
of the denominator larger by one (if m is odd, cf. equation 3.2.2 above). For the
derivation of the algorithm, refer to [1]. The algorithm is summarized by a recurrence

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

112 Chapter 3. Interpolation and Extrapolation

relation exactly analogous to equation (3.1.3) for polynomial approximation:

Riisr)...i¢m) = Roi41)...(i+m)
Risty...G+m) — Bi

a

f (i+m-1)

t
( C— 254 ) (a — Ret)... itm) — Ri... (itm—1) ) -1
@—Litm RG41)...G+m) —RG41)...G+m—=1)

(3.2.3)

This recurrence generates the rational functions through m + 1 points from the ones
through m and (the term R(j41)...(i+m—1) inequation 3.2.3) m—1 points. It is started
with
Ri=y (3.2.4)
and with
R= [Riisi)..G+m) with m=-—1)=0 (3.2.5)

Now, exactly as in equations (3.1.4) and (3.1.5) above, we can convert the
recurrence (3.2.3) to one involving only the small differences

Cm,i = Ri..(itm) — Ri...(i+m—1)

(3.2.6)
Dini = Ri...-¢m) — Rei41)...(i+m)
Note that these satisfy the relation
C41 — D+, = Cm iti — Dini (3.2.7)
which is useful in proving the recurrences
D Cm it1(Cm,it1 — Dm,i)
m+1,i
(5) Dini — Cm,it1
(3.2.8)

a] Dyni(Cmi+1 — Di)
rts) Dini — Cmit1

This recurrence is implemented in the following function, whose use is analogous
in every way to polint in §3.1. Note again that unit-offset input arrays are
assumed (§1.2).

#include <math.h>

#include "nrutil.h"

#define TINY 1.0e-25 A small number.

#define FREERETURN {free_vector(d,1,n) ;free_vector(c,1,n);return;}

void ratint(float xa[], float ya[], int n, float x, float *y, float *dy)
Given arrays xa[1..n] and ya[1..n], and given a value of x, this routine returns a value of
y and an accuracy estimate dy. The value returned is that of the diagonal rational function,
evaluated at x, which passes through the n points (xa;,ya,), i = 1...n.
{

int m,i,ns=1;

float w,t,hh,h,dd,*c,*d;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.3 Cubic Spline Interpolation 113

c=vector(1,n);
d=vector(1,n);
hh=fabs(x-xa[1]);
for (i=1;i<=n;i++) {
h=fabs(x-xa[i]);
if (h 0.0) {
*y=ya lil;
*dy=0.0;
FREERETURN

The TINY part is needed to prevent a rare zero-over-zero

d[i]=yali]+TINy;
+ condition.
*y=ya[ns--] ;
for (m=1;m<n;mt++) {

for (i=1;i<=n-m;it+) {
w=c[iti]-d([i] ;
h=xa [itm] -x; h will never be zero, since this was tested in the initial-
t=(xa[i]-x)*d[i]/h; izing loop.
dd=t-c [iti];
if (dd == 0.0) nrerror("Error in routine ratint");

This error condition indicates that the interpolating function has a pole at the
requested value of x.

dd=w/dd;
da(ij=c[i+1] *dd;
c[i]=t*dd;
+
4y += (*dy=(2ns < (n-m) ? c[nst+i] : d[ms--]));
+
FREERETURN

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§2.2. [1]

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), §6.2.

Cuyt, A., and Wuytack, L. 1987, Nonlinear Methods in Numerical Analysis (Amsterdam: North-
Holland), Chapter 3.

3.3 Cubic Spline Interpolation

Given a tabulated function y; = y(;), 1 = 1...N, focus attention on one
particular interval, between «; and «;41. Linear interpolation in that interval gives
the interpolation formula

y = Ays + Byjyr (3.3.1)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

114 Chapter 3. —_ Interpolation and Extrapolation

where

Uji — x UX;

A=

B=1-A

Tj41 — Xj Tjp1 — Bj

(3.3.2)

Equations (3.3.1) and (3.3.2) are a special case of the general Lagrange interpolation
formula (3.1.1).

Since it is (piecewise) linear, equation (3.3.1) has zero second derivative in
the interior of each interval, and an undefined, or infinite, second derivative at the
abscissas x;. The goal of cubic spline interpolation is to get an interpolation formula
that is smooth in the first derivative, and continuous in the second derivative, both
within an interval and at its boundaries.

Suppose, contrary to fact, that in addition to the tabulated values of y;, we
also have tabulated values for the function’s second derivatives, y’’, that is, a set
of numbers y/’. Then, within each interval, we can add to the right-hand side of
equation (3.3.1) a cubic polynomial whose second derivative varies linearly from a
value y// on the left to a value y‘, , on the right. Doing so, we will have the desired
continuous second derivative. If we also construct the cubic polynomial to have
zero values at x; and 7j+1, then adding it in will not spoil the agreement with the
tabulated functional values y; and yj; at the endpoints 7; and 7j+1.

A little side calculation shows that there is only one way to arrange this
construction, namely replacing (3.3.1) by

y = Ayj + Byjyi + Cyj + Dj (3.3.3)
where A and B are defined in (3.3.2) and

1
C= gi" — A)(xj41 — xj)? D

Notice that the dependence on the independent variable x in equations (3.3.3) and
(3.3.4) is entirely through the linear x-dependence of A and B, and (through A and
B) the cubic a-dependence of C and D.

We can readily check that y” is in fact the second derivative of the new
interpolating polynomial. We take derivatives of equation (3.3.3) with respect to x,
using the definitions of A, B, C, D to compute dA/dx,dB/dx,dC/dx, and dD/dx.
The result is

3B?
6

dy _ yj+i1—yj  3A?—1

ita — 25 )yy + ti41— 2 s)¥j41 (3.35)

dx &j414 — 2; 6

for the first derivative, and

@y

eo Ay + By/,, (3.3.6)
for the second derivative. Since A = 1 at x;, A = 0 at xj41, while B is just the
other way around, (3.3.6) shows that y”’ is just the tabulated second derivative, and
also that the second derivative will be continuous across (e.g.) the boundary between
the two intervals (aj~1,%;) and (”;,2j41).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.3 Cubic Spline Interpolation 115

The only problem now is that we supposed the y/’’s to be known, when, actually,

they are not. However, we have not yet required that the first derivative, computed
from equation (3.3.5), be continuous across the boundary between two intervals. The
key idea of a cubic spline is to require this continuity and to use it to get equations
for the second derivatives y/’.

The required equations are obtained by setting equation (3.3.5) evaluated for
x = x, in the interval (2 ;_1, x;) equal to the same equation evaluated for z = x ; but

in the interval (2:;, 7; 41). With some rearrangement, this gives (for j = 2,..., N—1)
U5 a Finh yl , Vil Pj-l | Bjt1 Thy! Yjtt 7 U5 Yi Yj-1
6 3 J 6 j+d Ljip1— Ly jp — Lj-1
(3.3.7)

These are N — 2 linear equations in the N unknowns y/’,i = 1,..., N. Therefore

there is a two-parameter family of possible solutions.
For a unique solution, we need to specify two further conditions, typically taken
as boundary conditions at 71 and x. The most common ways of doing this are either

e set one or both of y// and yX, equal to zero, giving the so-called natural
cubic spline, which has zero second derivative on one or both of its
boundaries, or

e set either of y// and yy, to values calculated from equation (3.3.5) so as
to make the first derivative of the interpolating function have a specified
value on either or both boundaries.

One reason that cubic splines are especially practical is that the set of equations
(3.3.7), along with the two additional boundary conditions, are not only linear, but
also tridiagonal. Each uy is coupled only to its nearest neighbors at 7 + 1. Therefore,
the equations can be solved in O(N) operations by the tridiagonal algorithm (§2.4).
That algorithm is concise enough to build right into the spline calculational routine.
This makes the routine not completely transparent as an implementation of (3.3.7),
so we encourage you to study it carefully, comparing with tridag (§2.4). Arrays
are assumed to be unit-offset. If you have zero-offset arrays, see §1.2.

#include "nrutil.h"

void spline(float x[], float y[], int n, float yp1, float ypn, float y2[])
Given arrays x[1..n] and y[1..n] containing a tabulated function, i.e., y,; = f (xi), with
X1 <X2<...< xy, and given values yp1 and ypn for the first derivative of the interpolating
function at points 1 and n, respectively, this routine returns an array y2[1..n] that contains
the second derivatives of the interpolating function at the tabulated points x;. If yp1 and/or
ypn are equal to 1 x 10°° or larger, the routine is signaled to set the corresponding boundary
condition for a natural spline, with zero second derivative on that boundary.
t

int i,k;

float p,qn,sig,un,*u;

u=vector(1,n-1) ;

if (yp1 > 0.99e30) The lower boundary condition is set either to be “nat-
y2[1]=u[1]=0.0; ural”

else { or else to have a specified first derivative.
y2[1] = -0.5;

ul1J=(3.0/(x[2]-x[1]))*((y [2] -y [1])/(«x[2]-x[1])-yp1) ;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

116 Chapter 3. —_ Interpolation and Extrapolation

for (i=2;i<=n-1;i++) { This is the decomposition loop of the tridiagonal al-
sig=(x[i]-x[i-1])/(«[i+1]-x[i-1]); gorithm. y2 and u are used for tem-
p=sig*y2[i-1]+2.0; porary storage of the decomposed
y2[il=(sig-1.0)/p; factors.
uli]=(ylit+1]-y[i])/(xlit1]-xfi]) - (ylil-yli-1])/(«fi]-xfi-1]);
uli]=(6.0*u[i] /(x[i+1]-x [i-1])-sig*uli-1])/p;

+

if (ypn > 0.99e30) The upper boundary condition is set either to be
qn=un=0.0; “natural”

else { or else to have a specified first derivative.
qn=0.5;
un=(3.0/(x[n]-x[n-1]))*(ypn-(y[n] -y [n-1])/(x[n]-x[n-1]));

+

y2 [n] =(un-qn*u [n-1] ) /(qn*y2 [n-1]+1.0) ;

for (k=n-1;k>=1;k--) This is the backsubstitution loop of the tridiagonal
y2(k] =y2[k] *y2[k+1] +u[k] ; algorithm.

free_vector(u,1,n-1);

It is important to understand that the program spline is called only once to
process an entire tabulated function in arrays x; and y;. Once this has been done,
values of the interpolated function for any value of x are obtained by calls (as many
as desired) to a separate routine splint (for “spline interpolation”):

void splint(float xa[], float ya[], float y2a[], int n, float x, float *y)

Given the arrays xa[1..n] and ya[1..n], which tabulate a function (with the xaj's in order),
and given the array y2a[1..n], which is the output from spline above, and given a value of
x, this routine returns a cubic-spline interpolated value y.

{
void nrerror(char error_text[]);
int klo,khi,k;
float h,b,a;
3 We will find the right place in the table by means of
i=n; bisection. This is optimal if sequential calls to this
while (khi-klo > 1) { routine are at random values of x. If sequential calls
k=(khi+klo) >> 1; are in order, and closely spaced, one would do better
if (xa[k] > x) khi=k; to store previous values of klo and khi and test if
else klo=k; they remain appropriate on the next call.
} klo and khi now bracket the input value of x.
h=xa[khi] -xa[klo] ;
if (h == 0.0) nrerror("Bad xa input to routine splint"); The xa's must be dis-
a=(xa[khi]-x)/h; tinct.
b=(x-xa[klo])/h; Cubic spline polynomial is now evaluated.
¥y=a*ya[klo]+b*ya[khi] + ((a*a*a-a) *y2a[klo] +(b*b*b-b) *y2a [khi] )*(h*h)/6.0;
3

CITED REFERENCES AND FURTHER READING:

De Boor, C. 1978, A Practical Guide to Splines (New York: Springer-Verlag).

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), §§4.4-4.5.

aay and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),

2.4.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:

McGraw-Hill), §3.8.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.4 How to Search an Ordered Table 117

3.4 How to Search an Ordered Table

Suppose that you have decided to use some particular interpolation scheme,
such as fourth-order polynomial interpolation, to compute a function f(x) from a
set of tabulated x;’s and f;’s. Then you will need a fast way of finding your place
in the table of x;’s, given some particular value x at which the function evaluation
is desired. This problem is not properly one of numerical analysis, but it occurs so
often in practice that it would be negligent of us to ignore it.

Formally, the problem is this: Given an array of abscissas xx[j],j=1,2,...,n,
with the elements either monotonically increasing or monotonically decreasing, and
given a number x, find an integer j such that x lies between xx[j] and xx[j+1].
For this task, let us define fictitious array elements xx[0] and xx[n+1] equal to
plus or minus infinity (in whichever order is consistent with the monotonicity of the
table). Then j will always be between 0 and n, inclusive; a value of 0 indicates
“off-scale” at one end of the table, n indicates off-scale at the other end.

In most cases, when all is said and done, it is hard to do better than bisection,
which will find the right place in the table in about logon tries. We already did use
bisection in the spline evaluation routine splint of the preceding section, so you
might glance back at that. Standing by itself, a bisection routine looks like this:

void locate(float xx[], unsigned long n, float x, unsigned long *j)

Given an array xx[1..n], and given a value x, returns a value j such that x is between xx[j]
and xx[j+1]. xx must be monotonic, either increasing or decreasing. j=0 or j=n is returned
to indicate that x is out of range.

{
unsigned long ju,jm,jl;
int ascnd;
ji=0; Initialize lower
ju=nt1; and upper limits.
ascnd=(xx[n] >= xx[1]);
while (ju-jl > 1) f If we are not yet done,
jm=(jutjl) >> 1; compute a midpoint,
if (x >= xx[jm] == ascnd)
jl=jm; and replace either the lower limit
else
ju=jm; or the upper limit, as appropriate.
} Repeat until the test condition is satisfied.
if (x == xx[1]) *j=1; Then set the output
else if(x == xx[n]) *j=n-1;
else *j=jl;
3 and return.

A unit-offset array xx is assumed. To use locate with a zero-offset array,
remember to subtract 1 from the address of xx, and also from the returned value j.

Search with Correlated Values

Sometimes you will be in the situation of searching a large table many times,
and with nearly identical abscissas on consecutive searches. For example, you
may be generating a function that is used on the right-hand side of a differential
equation: Most differential-equation integrators, as we shall see in Chapter 16, call

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

118 Chapter 3. Interpolation and Extrapolation

8
J hunt phase

aa
. *0—¢ * * ° °
1 7 10 14 22 7738
(b) bisection phase
Figure 3.4.1. (a) The routine locate finds a table entry by bisection. Shown here is the sequence

of steps that converge to element 51 in a table of length 64. (b) The routine hunt searches from a
previous known position in the table by increasing steps, then converges by bisection. Shown here is a
particularly unfavorable example, converging to element 32 from element 7. A favorable example would
be convergence to an element near 7, such as 9, which would require just three “hops.”

for right-hand side evaluations at points that hop back and forth a bit, but whose
trend moves slowly in the direction of the integration.

In such cases it is wasteful to do a full bisection, ab initio, on each call. The
following routine instead starts with a guessed position in the table. It first “hunts,”
either up or down, in increments of 1, then 2, then 4, etc., until the desired value is
bracketed. Second, it then bisects in the bracketed interval. At worst, this routine is
about a factor of 2 slower than locate above (if the hunt phase expands to include
the whole table). At best, it can be a factor of logan faster than locate, if the desired
point is usually quite close to the input guess. Figure 3.4.1 compares the two routines.

void hunt(float xx[], unsigned long n, float x, unsigned long *jlo)
Given an array xx[1..n], and given a value x, returns a value jlo such that x is between
xx[jlo] and xx[jlo+1]. xx[1..n] must be monotonic, either increasing or decreasing.
jlo=0 or jlo=n is returned to indicate that x is out of range. jlo on input is taken as the
initial guess for jlo on output.
t

unsigned long jm, jhi,inc;

int ascnd;

ascnd=(xx[n] >= xx[1]); True if ascending order of table, false otherwise.

if (*jlo <= 0 || *jlo > n) { Input guess not useful. Go immediately to bisec-
*j10=0; tion.
jhi=n+1;
} else {
inc=1; Set the hunting increment.
if (x >= xx[*jlo] == ascnd) { Hunt up:
if (*jlo == n) return;
jhi=(*jlo)+1;
while (x >= xx[jhi] == ascnd) { Not done hunting,
*jlo=jhi;
inc += inc; so double the increment
jhi=(*jlo)tinc;
if (jhi >n) { Done hunting, since off end of table.
jhisn+1;
break;

} Try again.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.4 How to Search an Ordered Table 119

} Done hunting, value bracketed.
} else { Hunt down:
if (*jlo == 1) {
*j10=0;
return;
3
jhi=(*jlo)--;
while (x < xx[*jlo] == ascnd) { Not done hunting,
jhi=(*jlo);
inc <<= 1; so double the increment
if (inc >= jhi) { Done hunting, since off end of table.
#j10=0;
break;
}
else *jlo=jhi-inc;
} and try again.
} Done hunting, value bracketed.
} Hunt is done, so begin the final bisection phase:

while (jhi-(*jlo) != 1) {
jm=(jhi+(#jlo)) >> 1;
if (x >= xx[jm] == ascnd)
*jlo=jm;
else
jhi=jm;
+
if (x == xx[n]) *jlo=n-1;
aif (x == xx[1]) *jlo=1;

If your array xx is zero-offset, read the comment following locate, above.
After the Hunt

The problem: Routines locate and hunt return an index j such that your
desired value lies between table entries xx [j] and xx[j+1], where xx[1. .n] is the
full length of the table. But, to obtain an m-point interpolated value using a routine
like polint (§3.1) or ratint (83.2), you need to supply much shorter xx and yy
arrays, of length m. How do you make the connection?

The solution: Calculate

k = IMIN(IMAX(j-(m-1)/2,1) ,n+1-m)

(The macros IMIN and IMAX give the minimum and maximum of two integer
arguments; see §1.2 and Appendix B.) This expression produces the index of the
leftmost member of an m-point set of points centered (insofar as possible) between
j and j+1, but bounded by 1 at the left and n at the right. C then lets you call the
interpolation routine with array addresses offset by k, e.g.,

polint (&xx[k-1] ,&yy[k-1] ,m,...)

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §6.2.1.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

120 Chapter 3. Interpolation and Extrapolation

3.5 Coefficients of the Interpolating Polynomial

Occasionally you may wish to know not the value of the interpolating polynomial
that passes through a (small!) number of points, but the coefficients of that poly-
nomial. A valid use of the coefficients might be, for example, to compute
simultaneous interpolated values of the function and of several of its derivatives (see
§5.3), or to convolve a segment of the tabulated function with some other function,
where the moments of that other function (i.e., its convolution with powers of x)
are known analytically.

However, please be certain that the coefficients are what you need. Generally the
coefficients of the interpolating polynomial can be determined much less accurately
than its value at a desired abscissa. Therefore it is not a good idea to determine the
coefficients only for use in calculating interpolating values. Values thus calculated
will not pass exactly through the tabulated points, for example, while values computed
by the routines in §3.1-§3.3 will pass exactly through such points.

Also, you should not mistake the interpolating polynomial (and its coefficients)
for its cousin, the best fit polynomial through a data set. Fitting is a smoothing
process, since the number of fitted coefficients is typically much less than the
number of data points. Therefore, fitted coefficients can be accurately and stably
determined even in the presence of statistical errors in the tabulated values. (See
§14.8.) Interpolation, where the number of coefficients and number of tabulated
points are equal, takes the tabulated values as perfect. If they in fact contain statistical
errors, these can be magnified into oscillations of the interpolating polynomial in
between the tabulated points.

As before, we take the tabulated points to be y; = y(z;). If the interpolating
polynomial is written as

y=co tee + cor? +---+ ena (3.5.1)
then the c;’s are required to satisfy the linear equation
Lom ato aM) fal_|m (3.5.2)

This is a Vandermonde matrix, as described in §2.8. One could in principle solve
equation (3.5.2) by standard techniques for linear equations generally (§2.3); however
the special method that was derived in §2.8 is more efficient by a large factor, of
order N, so it is much better.

Remember that Vandermonde systems can be quite ill-conditioned. In such a
case, no numerical method is going to give a very accurate answer. Such cases do
not, please note, imply any difficulty in finding interpolated values by the methods
of §3.1, but only difficulty in finding coefficients.

Like the routine in §2.8, the following is due to G.B. Rybicki. Note that the
arrays are all assumed to be zero-offset.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.5 Coefficients of the Interpolating Polynomial 121

#include "nrutil.h"

void polcoe(float x[], float y[], int n, float cof[])
Given arrays x[0..n] and y[0..n] containing a tabulated function y, = f(x;), this routine
returns an array of coefficients cof [0..n], such that y,; = )), cof ;x/.
{
int k,j,i;
float phi,ff,b,*s;

s=vector(0,n) ;
for (i=0;i<=n;i++) s[i]=cof[i]=0.0;

Coefficients s; of the master polynomial P(x) are
found by recurrence.

s(j] -= xfil*s(j+1];

s[n] -= x[il;
}
for (j=0;j<=n;j++) {
phi=n+1;
for (k=n;k>=1;k--) The quantity phi = |];4,,(j; — 2%) is found as a
phi=k+s [k]+x(j] *phi; derivative of P(x;).
ff=y[j]/phi;
b=1.0; Coefficients of polynomials in each term of the La-
for (k=n;k>=0;k--) { grange formula are found by synthetic division of
cof[k] += b+ff; P(x) by (a — aj). The solution cy is accumu-
b=s[k]+x[j] *b; lated.
}
}

free_vector(s,0,n);

Another Method

Another technique is to make use of the function value interpolation routine
already given (polint §3.1). If we interpolate (or extrapolate) to find the value of
the interpolating polynomial at ~ = 0, then this value will evidently be cg. Now
we can subtract co from the y;’s and divide each by its corresponding x ;. Throwing
out one point (the one with smallest x; is a good candidate), we can repeat the
procedure to find c;, and so on.

It is not instantly obvious that this procedure is stable, but we have generally
found it to be somewhat more stable than the routine immediately preceding. This
method is of order N°, while the preceding one was of order N?. You will
find, however, that neither works very well for large N, because of the intrinsic
ill-condition of the Vandermonde problem. In single precision, N up to 8 or 10 is
satisfactory; about double this in double precision.

#include <math.h>
#include "nrutil.h"

void polcof(float xa[], float ya[], int n, float cof[])
Given arrays xa[0..n] and ya[0..n] containing a tabulated function ya; =

f(xa;), this
routine returns an array of coefficients cof [0..n] such that ya; = }); cof;xat.
t

void polint(float xa[], float ya[], int n, float x, float *y, float *dy);

int k,j,i;

float xmin,dy,*x,*y;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

122 Chapter 3. —_ Interpolation and Extrapolation

x=vector(0,n);

y=vector(0,n);

for (j=0;j<=n;j++) {
x[jl]=xalj];
yljl=yaljl;

for (j=0;j<=n;j++) {
polint(x-1,y-1,n+1-j,0.0,&cof[j] ,&dy);
Subtract 1 from the pointers to x and y because polint uses dimensions [1..n]. We
extrapolate to z = 0.

xmin=1.0e38;
k=-1
for (i=O0;i<=n-j;it+) { Find the remaining x; of smallest
if (fabs(x[i]) < xmin) { absolute value,
xmin=fabs(x[i]);
k=i;
3
if (xfi]) ylil=(ylil-cof[j])/x(il; (meanwhile reducing all the terms)
}
for (i=kt+1;i<=n-j;it+) { and eliminate it.
y(i-1]=y (il;
x({i-1]=x (fil;
}

+
free_vector(y,0,n);
free_vector(x,0,n);

If the point 2 = 0 is not in (or at least close to) the range of the tabulated <x ;’s,
then the coefficients of the interpolating polynomial will in general become very large.
However, the real “information content” of the coefficients is in small differences
from the “translation-induced” large values. This is one cause of ill-conditioning,
resulting in loss of significance and poorly determined coefficients. You should
consider redefining the origin of the problem, to put a = 0 in a sensible place.

Another pathology is that, if too high a degree of interpolation is attempted on
a smooth function, the interpolating polynomial will attempt to use its high-degree
coefficients, in combinations with large and almost precisely canceling combinations,
to match the tabulated values down to the last possible epsilon of accuracy. This
effect is the same as the intrinsic tendency of the interpolating polynomial values to
oscillate (wildly) between its constrained points, and would be present even if the
machine’s floating precision were infinitely good. The above routines polcoe and
polcof have slightly different sensitivities to the pathologies that can occur.

Are you still quite certain that using the coefficients is a good idea?

CITED REFERENCES AND FURTHER READING:
Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York: Wiley), §5.2.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.6 Interpolation in Two or More Dimensions 123

3.6 Interpolation in Two or More Dimensions

In multidimensional interpolation, we seek an estimate of y(%1,22,...,2n)
from an n-dimensional grid of tabulated values y and n one-dimensional vec-
tors giving the tabulated values of each of the independent variables 71, 2x2,...,
x,,. We will not here consider the problem of interpolating on a mesh that is not
Cartesian, i.e., has tabulated function values at “random” points in n-dimensional
space rather than at the vertices of a rectangular array. For clarity, we will consider
explicitly only the case of two dimensions, the cases of three or more dimensions
being analogous in every way.

In two dimensions, we imagine that we are given a matrix of functional values
ya(1..m] [1..n]. We are also given an array x1a[1..m],and an array x2a[1..n].
The relation of these input quantities to an underlying function y(x 1, x2) is

yaljJ] Uk] = y(xtal[j],x2a[k]) (3.6.1)

We want to estimate, by interpolation, the function y at some untabulated point

An important concept is that of the grid square in which the point (x1, x2)
falls, that is, the four tabulated points that surround the desired interior point. For
convenience, we will number these points from 1 to 4, counterclockwise starting
from the lower left (see Figure 3.6.1). More precisely, if

xtalj] <2 <xtalj+1]

(3.6.2)
x2a[k] < rq < x2a[k+1]
defines j and k, then
yi = yaljl [k)
yo = yalj+1] [k]
(3.6.3)

yg = yalj+1] [k+1]
ya = yaljl (k+1]

The simplest interpolation in two dimensions is bilinear interpolation on the
grid square. Its formulas are:

t = (a, — xta[j])/(xtalj+1] — xtalj])

u = (x2 — x2a[k])/(x2a[k+1] — x2a[k]) (3.64)

(so that t and u each lie between 0 and 1), and

y(v,2%2) = (1—t)(1—u)yr +t(1 — w)yo + tuys + (1 — thuys (3.6.5)

Bilinear interpolation is frequently “close enough for government work.” As
the interpolating point wanders from grid square to grid square, the interpolated

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

124 Chapter 3. Interpolation and Extrapolation

pt. number
desired pt. y
vo (X1,X2) bop
3) 1,X2 am
ay/ OX, & ww
sa
S
dy/ dxy w
&y/ OX 0X2
(a) (b)
Figure 3.6.1. (a) Labeling of points used in the two-dimensional interpolation routines bcuint and

beucof. (b) For each of the four points in (a), the user supplies one function value, two first derivatives,
and one cross-derivative, a total of 16 numbers.

function value changes continuously. However, the gradient of the interpolated
function changes discontinuously at the boundaries of each grid square.

There are two distinctly different directions that one can take in going beyond
bilinear interpolation to higher-order methods: One can use higher order to obtain
increased accuracy for the interpolated function (for sufficiently smooth functions!),
without necessarily trying to fix up the continuity of the gradient and higher
derivatives. Or, one can make use of higher order to enforce smoothness of some of
these derivatives as the interpolating point crosses grid-square boundaries. We will
now consider each of these two directions in turn.

Higher Order for Accuracy

The basic idea is to break up the problem into a succession of one-dimensional
interpolations. If we want to do m-1 order interpolation in the x ; direction, and n-1
order in the x2 direction, we first locate an m x n sub-block of the tabulated function
matrix that contains our desired point (x1,22). We then do m one-dimensional
interpolations in the x2 direction, i.e., on the rows of the sub-block, to get function
values at the points (x1a[j],x2), j = 1,...,m. Finally, we do a last interpolation
in the x, direction to get the answer. If we use the polynomial interpolation routine
polint of §3.1, and a sub-block which is presumed to be already located (and
addressed through the pointer float **ya, see $1.2), the procedure looks like this:

#include "nrutil.h"

void polin2(float x1a[], float x2a[], float *+*ya, int m, int n, float x1,

float x2, float *y, float *dy)
Given arrays x1a[1..m] and x2a[1..n] of independent variables, and a submatrix of function
values ya[1..m] [1..n], tabulated at the grid points defined by x1a and x2a; and given values
x1 and x2 of the independent variables; this routine returns an interpolated function value y,
and an accuracy indication dy (based only on the interpolation in the x1 direction, however).
{

void polint(float xa[], float ya[], int n, float x, float *y, float *dy);

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.6 Interpolation in Two or More Dimensions 125

int j;
float *ymtmp;

ymtmp=vector(1,m);

for (j=1;j<=m;j++) { Loop over rows.

polint (x2a,ya[j] ,n,x2,&ymtmp[j] ,dy) ; Interpolate answer into temporary stor-
} age.
polint(x1a,ymtmp,m,x1,y,dy); Do the final interpolation.

free_vector (ymtmp,1,m) ;

Higher Order for Smoothness: Bicubic Interpolation

We will give two methods that are in common use, and which are themselves
not unrelated. The first is usually called bicubic interpolation.

Bicubic interpolation requires the user to specify at each grid point not just
the function (21,22), but also the gradients 0y/Ox1 = yi, Oy/Ox2 = y» and
the cross derivative 0?y/0210x2 = y12. Then an interpolating function that is
cubic in the scaled coordinates t and u (equation 3.6.4) can be found, with the
following properties: (i) The values of the function and the specified derivatives
are reproduced exactly on the grid points, and (ii) the values of the function and
the specified derivatives change continuously as the interpolating point crosses from
one grid square to another.

It is important to understand that nothing in the equations of bicubic interpolation
requires you to specify the extra derivatives correctly! The smoothness properties are
tautologically “forced,” and have nothing to do with the “accuracy” of the specified
derivatives. It is a separate problem for you to decide how to obtain the values that
are specified. The better you do, the more accurate the interpolation will be. But
it will be smooth no matter what you do.

Best of all is to know the derivatives analytically, or to be able to compute them
accurately by numerical means, at the grid points. Next best is to determine them by
numerical differencing from the functional values already tabulated on the grid. The
relevant code would be something like this (using centered differencing):

yialjl] [k]=(yalj+1] [k]-yalj-1] [k])/(xtalj+1]-x1alj-1]);

y2a[j] [k]=(yalj] [k+1]-yalj] [k-1])/ («2a [k+1]-x2a[k-1]);

yi12a[j] (k]=(yaljt+1] [k+1] -ya[j+1] [k-1] -yalj-1] [k+1]+ya[j-1] [k-1])
/((xtalj+1]-x1alj-1]) *(x2a[k+1]-x2a[k-1]));

To do a bicubic interpolation within a grid square, given the function y and the
derivatives y1, y2, y12 at each of the four corners of the square, there are two steps:
First obtain the sixteen quantities cjj, i,j = 1,...,4 using the routine bcucof
below. (The formulas that obtain the c’s from the function and derivative values
are just a complicated linear transformation, with coefficients which, having been
determined once in the mists of numerical history, can be tabulated and forgotten.)
Next, substitute the c’s into any or all of the following bicubic formulas for function
and derivatives, as desired:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

126 Chapter 3. —_ Interpolation and Extrapolation

4 4
y(@41,%2) = Ss Ss Ciy tityJ-t

i=1 j=1

4 4
y(t, %2) = Ss WG = ljeijt'?w (dt /dx1)

i=1 j=1

ya(er,72) = > SOG — Dei

i=1 j=1

44
Y2(@1, 22) = WC = 1) — L)eijt* 2 u~? (dt /dxr) (du/dx2)

i=1 j=1

(3.6.6)

‘ly? (du/dx2)

where ¢ and wu are again given by equation (3.6.4).

void beucof(float y[], float yi[], float y2[], float y12[], float di, float d2,
float **c)
Given arrays y[1..4], yif1..4], y2[1..4], and y12[1..4], containing the function, gra-
dients, and cross derivative at the four grid points of a rectangular grid cell (numbered coun-
terclockwise from the lower left), and given d1 and d2, the length of the grid cell in the 1- and
2-directions, this routine returns the table c[1..4][1..4] that is used by routine bcuint
for bicubic interpolation.
{
static int wt[16] [16
{ 1,0,0,0,0,0,0,0,

int 1,k,j,i;
float xx,d1d2,cl[16] ,x[16];

did2=d1*d2;
for (i=1;i<=4;i++) { Pack a temporary vector x.

x[i+3]=y1 [i] *d1;
x [i+7]=y2[i] *42;
x[iti11]=y12[i] *a1d2;

}

for <=15;it++) { Matrix multiply by the stored table.
jk<=15;k++) xx += wt [i] [k] *x[k];

+

1=0;

for (i=1;i<=4;i++) Unpack the result into the output table.

for (j=1;j<=4;jt++) cli] [j]=c1[1++];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3.6 Interpolation in Two or More Dimensions 127

The implementation of equation (3.6.6), which performs a bicubic interpolation,
gives back the interpolated function value and the two gradient values, and uses the
above routine bcucof, is simply:

#include "nrutil.h"

void bcuint(float y[], float yi[], float y2[], float y12[], float x11,

float xlu, float x21, float x2u, float x1, float x2, float *ansy,

float *ansy1, float *ansy2)
Bicubic interpolation within a grid square. Input quantities are y,y1,y2,y12 (as described in
beucof); x11 and x1u, the lower and upper coordinates of the grid square in the 1-direction;
X21 and x2u likewise for the 2-direction; and x1,x2, the coordinates of the desired point for
the interpolation. The interpolated function value is returned as ansy, and the interpolated
gradient values as ansy1 and ansy2. This routine calls bcucof.

{

void beucof(float y[], float yi[], float y2[], float y12[], float di,
float d2, float **c);

int i;

float t,u,d1,d2,**c;

c=matrix(1,4,1,4);

di=x1u-x11;

d2=x2u-x21;

beucof (y,y1,y2,y12,d1,d2,c); Get the c's.

if (xlu == x11 || x2u == x21) nrerror("Bad input in routine bcuint");

t=(x1-x11)/d1; Equation (3.6.4).

u=(x2-x21)/d2;

*ansy=(*ansy2)=(*ansy1)=0.0;

for (i=4;i>=1;i--) { Equation (3.6.6).
*ansy=t* (*ansy)+((c[i] [4] *ut+c [i] [3] )*utc [i] [2])*u+c [i] [1];
xansy2=t* (xansy2)+(3.0*c [i] [4] *u+2.0*c [i] [3] ) xutc [i] [2];
xansy1=u* (xansy1)+(3.0*c [4] [i] #t+2.0*c [3] [i] ) *t+c [2] [i];

+

*ansy1 /= dil;

*ansy2 /= d2;

free_matrix(c,1,4,1,4);

3

Higher Order for Smoothness: Bicubic Spline

The other common technique for obtaining smoothness in two-dimensional
interpolation is the bicubic spline. Actually, this is equivalent to a special case
of bicubic interpolation: The interpolating function is of the same functional form
as equation (3.6.6); the values of the derivatives at the grid points are, however,
determined “globally” by one-dimensional splines. However, bicubic splines are
usually implemented in a form that looks rather different from the above bicubic
interpolation routines, instead looking much closer in form to the routine polin2
above: To interpolate one functional value, one performs m one-dimensional splines
across the rows of the table, followed by one additional one-dimensional spline
down the newly created column. It is a matter of taste (and trade-off between time
and memory) as to how much of this process one wants to precompute and store.
Instead of precomputing and storing all the derivative information (as in bicubic
interpolation), spline users typically precompute and store only one auxiliary table,
of second derivatives in one direction only. Then one need only do spline evaluations
(not constructions) for the m row splines; one must still do a construction and an

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

128 Chapter 3. —_ Interpolation and Extrapolation

evaluation for the final column spline. (Recall that a spline construction is a process
of order N, while a spline evaluation is only of order log N — and that is just to
find the place in the table!)

Here is a routine to precompute the auxiliary second-derivative table:

void splie2(float x1a[], float x2a[], float **ya, int m, int n, float **y2a)
Given an m by n tabulated function ya[1..m][1..n], and tabulated independent variables
x2a[1..n], this routine constructs one-dimensional natural cubic splines of the rows of ya
and returns the second-derivatives in the array y2a[1..m] [1..n]. (The array x1a[1..m] is
included in the argument list merely for consistency with routine splin2.)
{

void spline(float x[], float y[], int n, float yp1, float ypn, float y2[]);

int j;

for (j=1;j<=m; j++)

spline(x2a,ya[j] ,n,1.0e30,1.0e30,y2a[j]); Values 1 x 109° signal a nat-
} ural spline.

(If you want to interpolate on a sub-block of a bigger matrix, see §1.2.)
After the above routine has been executed once, any number of bicubic spline
interpolations can be performed by successive calls of the following routine:

#include "nrutil.h"

void splin2(float x1a[], float x2a[], float **ya, float **y2a, int m, int n,
float x1, float x2, float *y)

Given xia, x2a, ya, m, n as described in splie2 and y2a as produced by that routine; and

given a desired interpolating point x1,x2; this routine returns an interpolated function value y

by bicubic spline interpolation.

{
void spline(float x[], float y[], int n, float yp1, float ypn, float y2[]);
void splint(float xa[], float ya[], float y2a[], int n, float x, float *y);
int j;
float *ytmp,*yytmp;
ytmp=vector(1,m);
yytmp=vector(1,m); Perform m evaluations of the row splines constructed by
for (j=1;j<=m; j++) splie2, using the one-dimensional spline evaluator

splint (x2a,yalj] ,y2a[j] ,n,x2,&yytmp[j]); splint.

spline(x1a,yytmp,m,1.0e30,1.0e30,ytmp) ; Construct the one-dimensional col-
splint (x1la,yytmp,ytmp,m,x1,y); umn spline and evaluate it.
free_vector(yytmp,1,m) ;
free_vector(ytmp,1,m);

3

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.2.

Kinahan, B.F., and Harm, R. 1975, Astrophysical Journal, vol. 200, pp. 330-335.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §5.2.7.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
87.7.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 4. Integration of Functions

4.0 Introduction

Numerical integration, which is also called quadrature, has a history extending
back to the invention of calculus and before. The fact that integrals of elementary
functions could not, in general, be computed analytically, while derivatives could
be, served to give the field a certain panache, and to set it a cut above the arithmetic
drudgery of numerical analysis during the whole of the 18th and 19th centuries.

With the invention of automatic computing, quadrature became just one numer-
ical task among many, and not a very interesting one at that. Automatic computing,
even the most primitive sort involving desk calculators and rooms full of “computers”
(that were, until the 1950s, people rather than machines), opened to feasibility the
much richer field of numerical integration of differential equations. Quadrature is
merely the simplest special case: The evaluation of the integral

i-[ f(a)dx (4.0.1)

is precisely equivalent to solving for the value J = y(b) the differential equation

dy _

F=f) (4.0.2)

with the boundary condition
y(a) =0 (4.0.3)

Chapter 16 of this book deals with the numerical integration of differential
equations. In that chapter, much emphasis is given to the concept of “variable” or
“adaptive” choices of stepsize. We will not, therefore, develop that material here.
If the function that you propose to integrate is sharply concentrated in one or more
peaks, or if its shape is not readily characterized by a single length-scale, then it
is likely that you should cast the problem in the form of (4.0.2)-(4.0.3) and use
the methods of Chapter 16.

The quadrature methods in this chapter are based, in one way or another, on the
obvious device of adding up the value of the integrand at a sequence of abscissas
within the range of integration. The game is to obtain the integral as accurately
as possible with the smallest number of function evaluations of the integrand. Just
as in the case of interpolation (Chapter 3), one has the freedom to choose methods

129

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

130 Chapter 4. _ Integration of Functions

of various orders, with higher order sometimes, but not always, giving higher
accuracy. “Romberg integration,” which is discussed in §4.3, is a general formalism
for making use of integration methods of a variety of different orders, and we
recommend it highly.

Apart from the methods of this chapter and of Chapter 16, there are yet
other methods for obtaining integrals. One important class is based on function
approximation. We discuss explicitly the integration of functions by Chebyshev
approximation (“Clenshaw-Curtis” quadrature) in §5.9. Although not explicitly
discussed here, you ought to be able to figure out how to do cubic spline quadrature
using the output of the routine spline in §3.3. (Hint: Integrate equation 3.3.3
over x analytically. See [1].)

Some integrals related to Fourier transforms can be calculated using the fast
Fourier transform (FFT) algorithm. This is discussed in §13.9.

Multidimensional integrals are another whole multidimensional bag of worms.
Section 4.6 is an introductory discussion in this chapter; the important technique of
Monte-Carlo integration is treated in Chapter 7.

CITED REFERENCES AND FURTHER READING:

Carnahan, B., Luther, H.A., and Wilkes, J.O. 1969, Applied Numerical Methods (New York:
Wiley), Chapter 2.

Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York: Wiley), Chapter 7.

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 4.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 3.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), Chapter 4.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
87.4.

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software (Englewood Cliffs,
NJ: Prentice Hall), Chapter 5.

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), §5.2, p. 89. [1]

Davis, P., and Rabinowitz, P. 1984, Methods of Numerical Integration, 2nd ed. (Orlando, FL:
Academic Press).

4.1 Classical Formulas for Equally Spaced
Abscissas

Where would any book on numerical analysis be without Mr. Simpson and his
“rule”? The classical formulas for integrating a function whose value is known at
equally spaced steps have a certain elegance about them, and they are redolent with
historical association. Through them, the modern numerical analyst communes with
the spirits of his or her predecessors back across the centuries, as far as the time
of Newton, if not farther. Alas, times do change; with the exception of two of the
most modest formulas (“extended trapezoidal rule,” equation 4.1.11, and “extended

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.1 Classical Formulas for Equally Spaced Abscissas 131

Xo Xx] XQ oo XN OXNGI

open formulas use these points

closed formulas use these points

Figure 4.1.1. Quadrature formulas with equally spaced abscissas compute the integral of a function
between xo and 2,41. Closed formulas evaluate the function on the boundary points, while open
formulas refrain from doing so (useful if the evaluation algorithm breaks down on the boundary points).

midpoint rule,” equation 4.1.19, see §4.2), the classical formulas are almost entirely
useless. They are museum pieces, but beautiful ones.

Some notation: We have a sequence of abscissas, denoted x0, 21,...,
«Nn+1 Which are spaced apart by a constant step h,

aj=a+tih i=0,1,...,.N4+1 (4.1.1)
A function f(z) has known values at the x;’s,

f(a) = fi (4.1.2)

We want to integrate the function f(x) between a lower limit a and an upper limit
b, where a and b are each equal to one or the other of the z;’s. An integration
ormula that uses the value of the function at the endpoints, f(a) or f(b), is called
a closed formula. Occasionally, we want to integrate a function whose value at one
or both endpoints is difficult to compute (e.g., the computation of f goes to a limit
of zero over zero there, or worse yet has an integrable singularity there). In this
case we want an open formula, which estimates the integral using only x ;’s strictly
between a and b (see Figure 4.1.1).

The basic building blocks of the classical formulas are rules for integrating a
unction over a small number of intervals. As that number increases, we can find
tules that are exact for polynomials of increasingly high order. (Keep in mind that
higher order does not always imply higher accuracy in real cases.) A sequence of
such closed formulas is now given.

Closed Newton-Cotes Formulas
Trapezoidal rule:
| ~ f(x)dx = als fit ; f| + O(n? f”) (4.13)

Here the error term O( ) signifies that the true answer differs from the estimate by
an amount that is the product of some numerical coefficient times /? times the value

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

132 Chapter 4. _ Integration of Functions

of the function’s second derivative somewhere in the interval of integration. The
coefficient is knowable, and it can be found in all the standard references on this
subject. The point at which the second derivative is to be evaluated is, however,
unknowable. If we knew it, we could evaluate the function there and have a higher-
order method! Since the product of a knowable and an unknowable is unknowable,
we will streamline our formulas and write only O( ), instead of the coefficient.
Equation (4.1.3) is a two-point formula (x; and x2). It is exact for polynomials
up to and including degree 1, ie., f(z) = x. One anticipates that there is a
three-point formula exact up to polynomials of degree 2. This is true; moreover, by a
cancellation of coefficients due to left-right symmetry of the formula, the three-point

formula is exact for polynomials up to and including degree 3, i.e., f(a) = 2°:

Simpson’s rule:

I * F(a)aer nfo se AD + O(n® fF) (4.14)

Here f‘) means the fourth derivative of the function f evaluated at an unknown
place in the interval. Note also that the formula gives the integral over an interval
of size 2h, so the coefficients add up to 2.
There is no lucky cancellation in the four-point formula, so it is also exact for
polynomials up to and including degree 3.
3

Simpson’s 5 rule:

[terte=n[E na one ens sn] +001) 415)
_, eae glit get gist gis t te
The five-point formula again benefits from a cancellation:

Bode’s rule:

af t OF hs t ah + O(n" f) (4.1.6)

3 a5 4" a5

This is exact for polynomials up to and including degree 5.
At this point the formulas stop being named after famous personages, so we
will not go any further. Consult [1] for additional formulas in the sequence.

Extrapolative Formulas for a Single Interval

We are going to depart from historical practice for a moment. Many texts
would give, at this point, a sequence of “Newton-Cotes Formulas of Open Type.”
Here is an example:

ts 55,5 5, BB 5 (a)
r)dx = | f+ fo 4 t t °
ne f(x)dx [Eh at? mf aft O(n? f™)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

4.1 Classical Formulas for Equally Spaced Abscissas 133

Notice that the integral from a = xo to b = sz is estimated, using only the interior
points x1, %2,23,x4. In our opinion, formulas of this type are not useful for the
reasons that (i) they cannot usefully be strung together to get “extended” rules, as we
are about to do with the closed formulas, and (ii) for all other possible uses they are
dominated by the Gaussian integration formulas which we will introduce in §4.5.

Instead of the Newton-Cotes open formulas, let us set out the formulas for
estimating the integral in the single interval from xo to x1, using values of the
function f at 71,22,.... These will be useful building blocks for the “extended”
open formulas.

[fae = nin] +0087) (4.17)
a 3, 1 3 a

| f(x)dx = non - sh + O(h? f”) (4.1.8)

| * f(a)dex = nl Eh - fe ah + O(h4 f)) (4.1.9)

an 55, 59, | 87, 9 | as pay
[ f(x)dx nh ag f2 + ay f3 ma + O(h? f&) (4.1.10)
Perhaps a word here would be in order about how formulas like the above can
be derived. There are elegant ways, but the most straightforward is to write down the
basic form of the formula, replacing the numerical coefficients with unknowns, say
P,q,7, 8. Without loss of generality take xo = 0 and x, = 1,80 h = 1. Substitute in
turn for f(a) (and for f1, f2, fz, fa) the functions f(x) = 1, f(x) = a, f(x) = x?,
and f(x) = x°. Doing the integral in each case reduces the left-hand side to a
number, and the right-hand side to a linear equation for the unknowns p, q,r, s.
Solving the four equations produced in this way gives the coefficients.

Extended Formulas (Closed)

If we use equation (4.1.3) N — 1 times, to do the integration in the intervals
(x1, 2), (2, 73),...,(@N—1, ZN), and then add the results, we obtain an “extended”
or “composite” formula for the integral from x; to ry.

Extended trapezoidal rule:

“a (4.1.11)

“+ fn-it sh ; o(i ar)

Here we have written the error estimate in terms of the interval b — a and the number
of points N instead of in terms of h. This is clearer, since one is usually holding
a and 6 fixed and wanting to know (e.g.) how much the error will be decreased

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

134 Chapter 4. _ Integration of Functions

by taking twice as many steps (in this case, it is by a factor of 4). In subsequent
equations we will show only the scaling of the error term with the number of steps.
For reasons that will not become clear until $4.2, equation (4.1.11) is in fact
the most important equation in this section, the basis for most practical quadrature
schemes.
The extended formula of order 1/N? is:

[ f(x)dx neh fe + fs + fat

(4.1.12)

5

-+ fn-24 Sie t ain] t 0(x)

(We will see in a moment where this comes from.)
If we apply equation (4.1.4) to successive, nonoverlapping pairs of intervals,
we get the extended Simpson’s rule:

4.1.13
9 1 (4.1.13)

4 3in-2 giN-1 sf 0(x)

Notice that the 2/3, 4/3 alternation continues throughout the interior of the evalu-
ation. Many people believe that the wobbling alternation somehow contains deep
information about the integral of their function that is not apparent to mortal eyes.
In fact, the alternation is an artifact of using the building block (4.1.4). Another
extended formula with the same order as Simpson’s rule is

23,

[ fa)dr nen Ltt Bist fat Sot

23 7 3.
st fyiat fn—34 ag iN-2 t giv t gfx (4.1.14)

This equation is constructed by fitting cubic polynomials through successive groups
of four points; we defer details to §18.3, where a similar technique is used in the
solution of integral equations. We can, however, tell you where equation (4.1.12)
came from. It is Simpson’s extended rule, averaged with a modified version of
itself in which the first and last step are done with the trapezoidal rule (4.1.3). The
trapezoidal step is two orders lower than Simpson’s rule; however, its contribution to
the integral goes down as an additional power of N (since it is used only twice, not
N times). This makes the resulting formula of degree one less than Simpson.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.1 Classical Formulas for Equally Spaced Abscissas 135

Extended Formulas (Open and Semi-open)

We can construct open and semi-open extended formulas by adding the closed
formulas (4.1.11)-(4.1.14), evaluated for the second and subsequent steps, to the
extrapolative open formulas for the first step, (4.1.7)-(4.1.10). As discussed
immediately above, it is consistent to use an end step that is of one order lower
than the (repeated) interior step. The resulting formulas for an interval open at
both ends are as follows:

Equations (4.1.7) and (4.1.11) give

| f (x)dx n [be t fg+fat---+fn—24 fy] +O (x2) (4.1.15)
@

Equations (4.1.8) and (4.1.12) give

[see nf Fh ott tet fs

12 12
++ fy—3 + Glv-2 + ate] (4.1.16)
+0 (x)
Equations (4.1.9) and (4.1.13) give
i f(x)dx nh +04 ia : fot
“4 sf + Sines + 04 aT tN (4.1.17)

The interior points alternate 4/3 and 2/3. If we want to avoid this alternation,
we can combine equations (4.1.9) and (4.1.14), giving

; bola t fot fot fet

. 11 1, 55.
++ fuyis + fn-a+ >fn-3— a fn-2 + 5 fn-1

8
1
+0(52)

We should mention in passing another extended open formula, for use where
the limits of integration are located halfway between tabulated abscissas. This one is
known as the extended midpoint rule, and is accurate to the same order as (4.1.15):

(4.1.18)

[ f(x)dx = hl fsy2 + fy2 + frjat
“" (4.1.19)

s+ fyesyet+ fyi] 4 0(xz)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

136 Chapter 4. Integration of Functions

N=1
-—.———o£_ icy cx§qeri 2
PR 20 3
eee» 4

o—____»____»___» __» __» __» _e —__e (total after N= 4)

Figure 4.2.1. Sequential calls to the routine trapzd incorporate the information from previous calls and
evaluate the integrand only at those new points necessary to refine the grid. The bottom line shows the
totality of function evaluations after the fourth call. The routine qsimp, by weighting the intermediate
results, transforms the trapezoid rule into Simpson’s rule with essentially no additional overhead.

There are also formulas of higher order for this situation, but we will refrain from
giving them.

The semi-open formulas are just the obvious combinations of equations (4.1.11)-
(4.1.14) with (4.1.15)-(4.1.18), respectively. At the closed end of the integration,
use the weights from the former equations; at the open end use the weights from
the latter equations. One example should give the idea, the formula with error term
decreasing as 1/N® which is closed on the right and open on the left:

[- f(«)dx nF h + ifs + fa + fot
l (4.1.20)

5

-+ frn-24 Shiva { ahs] o(x)

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.4. 1]

Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York: Wiley), 87.1.

4.2 Elementary Algorithms

Our starting point is equation (4.1.11), the extended trapezoidal rule. There are
two facts about the trapezoidal rule which make it the starting point for a variety of
algorithms. One fact is rather obvious, while the second is rather “deep.”

The obvious fact is that, for a fixed function f(z) to be integrated between fixed
limits a and b, one can double the number of intervals in the extended trapezoidal
rule without losing the benefit of previous work. The coarsest implementation of
the trapezoidal rule is to average the function at its endpoints a and b. The first
stage of refinement is to add to this average the value of the function at the halfway
point. The second stage of refinement is to add the values at the 1/4 and 3/4 points.
And so on (see Figure 4.2.1).

Without further ado we can write a routine with this kind of logic to it:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.2 Elementary Algorithms 137

#define FUNC(x) ((*func) (x))

float trapzd(float (*func) (float), float a, float b, int n)

This routine computes the nth stage of refinement of an extended trapezoidal rule. func is input
as a pointer to the function to be integrated between limits a and b, also input. When called with
n=1, the routine returns the crudest estimate of rp f(x)dx. Subsequent calls with n=2,3,...

(in that sequential order) will improve the accuracy by adding 8-2 additional interior points.
{

float x,tnm,sum,del;

static float s;

int it,j;

if (nm == 1) {
return (s=0.5*(b-a)*(FUNC(a)+FUNC(b))) 5

} else {
for (it=1,j=1;j<n-1;jt++) it <<= 1;
tnm=it;
del=(b-a) /tnm; This is the spacing of the points to be added.
x=at0.5*del;
for (sum=0.0,j=1;j<=it;j++,x+=del) sum += FUNC(x);
s=0.5*(s+(b-a) *sum/tnm) ; This replaces s by its refined value.
return s;
+

The above routine (trapzd) is a workhorse that can be harnessed in several
ways. The simplest and crudest is to integrate a function by the extended trapezoidal
rule where you know in advance (we can’t imagine how!) the number of steps you
want. If you want 2” + 1, you can accomplish this by the fragment

for (j=1; j<=mt1; j++) s=trapzd(func,a,b,j);

with the answer returned as s.
Much better, of course, is to refine the trapezoidal rule until some specified
degree of accuracy has been achieved:

#include <math.h>
#define EPS 1.0e-5
#define JMAX 20

float qtrap(float (*func) (float), float a, float b)
Returns the integral of the function func from a to b. The parameters EPS can be set to the
desired fractional accuracy and JMAX so that 2 to the power JMAX~-1 is the maximum allowed
number of steps. Integration is performed by the trapezoidal rule.
{

float trapzd(float (*func) (float), float a, float b, int n);

void nrerror(char error_text[]);

int j;

float s,olds=0.0; Initial value of olds is arbitrary.

for (j=1;j<=JMAX;j++) {
rapzd(func,a,b,j);
if (j > 5) Avoid spurious early convergence.
if (fabs(s-olds) < EPS*fabs(olds) ||
(s == 0.0 && olds == 0.0)) return s;

olds=s;
+
nrerror("Too many steps in routine qtrap");
return 0.0; Never get here.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

138 Chapter 4. _ Integration of Functions

Unsophisticated as it is, routine qtrap is in fact a fairly robust way of doing
integrals of functions that are not very smooth. Increased sophistication will usually
translate into a higher-order method whose efficiency will be greater only for
sufficiently smooth integrands. qtrap is the method of choice, e.g., for an integrand
which is a function of a variable that is linearly interpolated between measured data
points. Be sure that you do not require too stringent an EPS, however: If qtrap takes
too many steps in trying to achieve your required accuracy, accumulated roundoff
errors may start increasing, and the routine may never converge. A value 10 ~°
is just on the edge of trouble for most 32-bit machines; it is achievable when the
convergence is moderately rapid, but not otherwise.

We come now to the “deep” fact about the extended trapezoidal rule, equation
(4.1.11). It is this: The error of the approximation, which begins with a term of
order 1/N?, is in fact entirely even when expressed in powers of 1/N. This follows
directly from the Euler-Maclaurin Summation Formula,

[PO teoiae =n] Sh fet fat fat Fi

Byh? |, / Bogh?® | (2k-1) — p(2k—1)
~ 7 Of — Da = Gp (hy TWh yao
Here Bo; is a Bernoulli number, defined by the generating function
t co t™
ea= Ss Ba (4.2.2)
n=0
with the first few even values (odd values vanish except for By = —1/2)
1 1 1
Bo=1 Bo== By Be
é 42
1 : 5 ° 691 (423)
5
B B B
S30 1 66 2730

Equation (4.2.1) is not a convergent expansion, but rather only an asymptotic
expansion whose error when truncated at any point is always less than twice the
magnitude of the first neglected term. The reason that it is not convergent is that
the Bernoulli numbers become very large, e.g.,

495057205241079648212477525
66

Bso

The key point is that only even powers of h occur in the error series of (4.2.1).
This fact is not, in general, shared by the higher-order quadrature rules in §4.1.
For example, equation (4.1.12) has an error series beginning with O(1/N *), but
continuing with all subsequent powers of N: 1/N+, 1/N®, etc.

Suppose we evaluate (4.1.11) with NV steps, getting a result Sy, and then again
with 2N steps, getting a result Sx. (This is done by any two consecutive calls of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.2 Elementary Algorithms 139

trapzd.) The leading error term in the second evaluation will be 1/4 the size of the
error in the first evaluation. Therefore the combination

yi 4g 1
S= 352n — 35n (4.2.4)

will cancel out the leading order error term. But there is no error term of order 1/N 3,
by (4.2.1). The surviving error is of order 1/N +, the same as Simpson’s rule. In fact,
it should not take long for you to see that (4.2.4) is exactly Simpson’s rule (4.1.13),
alternating 2/3’s, 4/3’s, and all. This is the preferred method for evaluating that rule,
and we can write it as a routine exactly analogous to qtrap above:

#include <math.h>
#define EPS 1.0e-6
#define JMAX 20

float qsimp(float (*func) (float), float a, float b)
Returns the integral of the function func from a to b. The parameters EPS can be set to the
desired fractional accuracy and JMAX so that 2 to the power JMAX~-1 is the maximum allowed
number of steps. Integration is performed by Simpson's rule.

{
float trapzd(float (*func) (float), float a, float b, int n);
void nrerror(char error_text[]);
int j;
float s,st,ost=0.0,os=0.0;
for (j=1;j<=JMAX;j++) {
st=trapzd(func,a,b,j);
s=(4.0*st-ost)/3.0; Compare equation (4.2.4), above.
if (j > 5) Avoid spurious early convergence.
if (fabs(s-os) < EPS*fabs(os) ||
(s == 0.0 && os == 0.0)) return s;
os=s;
ost=st;
+
nrerror("Too many steps in routine qsimp");
return 0.0; Never get here.
3

The routine qsimp will in general be more efficient than qtrap (i.e., require
fewer function evaluations) when the function to be integrated has a finite 4th
derivative (i.e., a continuous 3rd derivative). The combination of qsimp and its
necessary workhorse trapzd is a good one for light-duty work.

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§3.3.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§87.4.1-7.4.2.

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), §5.3.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

140 Chapter 4. _ Integration of Functions

4.3 Romberg Integration

We can view Romberg’s method as the natural generalization of the routine
qsimp in the last section to integration schemes that are of higher order than
Simpson’s rule. The basic idea is to use the results from k successive refinements
of the extended trapezoidal rule (implemented in trapzd) to remove all terms in
the error series up to but not including O(1/N?*). The routine qsimp is the case
of k = 2. This is one example of a very general idea that goes by the name of
Richardson’s deferred approach to the limit: Perform some numerical algorithm for
various values of a parameter h, and then extrapolate the result to the continuum
limit h = 0.

Equation (4.2.4), which subtracts off the leading error term, is a special case of
polynomial extrapolation. In the more general Romberg case, we can use Neville’s
algorithm (see §3.1) to extrapolate the successive refinements to zero stepsize.
Neville’s algorithm can in fact be coded very concisely within a Romberg integration
routine. For clarity of the program, however, it seems better to do the extrapolation
by function call to polint, already given in §3.1.

#include <math.h>

#define EPS 1.0e-6

#define JMAX 20

#tdefine JMAXP (JMAX+1)

#define K 5

Here EPS is the fractional accuracy desired, as determined by the extrapolation error estimate;
JMAX limits the total number of steps; K is the number of points used in the extrapolation.

float gromb(float (*func) (float), float a, float b)
Returns the integral of the function func from a to b. Integration is performed by Romberg’s
method of order 2K, where, e.g., K=2 is Simpson's rule.

t
void polint(float xa[], float ya[], int n, float x, float *y, float *dy);
float trapzd(float (*func) (float), float a, float b, int n);
void nrerror(char error_text[]);
float ss,dss;
float s[JMAXP] ,h[JMAXP+1] ; These store the successive trapezoidal approxi-
int j; mations and their relative stepsizes.
h[1]=1.0;
for (j=1;j<=JMAX;j++) {
s[j]=trapzd(func,a,b,j);
if (j >= kK) ¢
polint (&h[j-K] ,&s[j-K] ,K,0.0,&ss,&dss) ;
if (fabs(dss) <= EPS*fabs(ss)) return ss;
}
h[j+1]=0.25+*h[j];
This is a key step: The factor is 0.25 even though the stepsize is decreased by only
0.5. This makes the extrapolation a polynomial in h? as allowed by equation (4.2.1),
not just a polynomial in h.
+
nrerror("Too many steps in routine qromb") ;
return 0.0; Never get here.
3

The routine qromb, along with its required trapzd and polint, is quite
powerful for sufficiently smooth (e.g., analytic) integrands, integrated over intervals

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.4 Improper Integrals 141

which contain no singularities, and where the endpoints are also nonsingular. qromb,
in such circumstances, takes many, many fewer function evaluations than either of
the routines in §4.2. For example, the integral

2
i x log(x + V2? + Idx

0

converges (with parameters as shown above) on the very first extrapolation, after
just 5 calls to trapzd, while qsimp requires 8 calls (8 times as many evaluations of
the integrand) and qtrap requires 13 calls (making 256 times as many evaluations
of the integrand).

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§§3.4-3.5.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§87.4.1-7.4.2.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §4.10-2.

4.4 Improper Integrals

For our present purposes, an integral will be “improper” if it has any of the
following problems:

e its integrand goes to a finite limiting value at finite upper and lower limits,

but cannot be evaluated right on one of those limits (e.g., sin z/x at x = 0)

e its upper limit is co , or its lower limit is —o0

¢ it has an integrable singularity at either limit (e.g.,«~1/? at a = 0)

e it has an integrable singularity at a known place between its upper and

lower limits

e it has an integrable singularity at an unknown place between its upper

and lower limits

If an integral is infinite (e.g., SP x~'dx), or does not exist in a limiting sense
(e.g., fe cos zdx), we do not call it improper; we call it impossible. No amount of
clever algorithmics will return a meaningful answer to an ill-posed problem.

In this section we will generalize the techniques of the preceding two sections
to cover the first four problems on the above list. A more advanced discussion of
quadrature with integrable singularities occurs in Chapter 18, notably §18.3. The
fifth problem, singularity at unknown location, can really only be handled by the
use of a variable stepsize differential equation integration routine, as will be given
in Chapter 16.

We need a workhorse like the extended trapezoidal rule (equation 4.1.11), but
one which is an open formula in the sense of §4.1, i.e., does not require the integrand
to be evaluated at the endpoints. Equation (4.1.19), the extended midpoint rule, is the
best choice. The reason is that (4.1.19) shares with (4.1.11) the “deep” property of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

142 Chapter 4. _ Integration of Functions

having an error series that is entirely even in h. Indeed there is a formula, not as well
known as it ought to be, called the Second Euler-Maclaurin summation formula,

[ 7 f(x)dx = hl fsy2 + foyo + frye +++ + fr—aj2 + frn—12]
2
+ Rt (4.4.1)
Boxh?* —2k+1)( p(2k—-1) _ p(2k—1)
+ opr to? Y(fy ft )te

This equation can be derived by writing out (4.2.1) with stepsize h, then writing it
out again with stepsize h/2, then subtracting the first from twice the second.

It is not possible to double the number of steps in the extended midpoint rule
and still have the benefit of previous function evaluations (try it!). However, it is
possible to triple the number of steps and do so. Shall we do this, or double and
accept the loss? On the average, tripling does a factor 3 of unnecessary work,
since the “right” number of steps for a desired accuracy criterion may in fact fall
anywhere in the logarithmic interval implied by tripling. For doubling, the factor
is only 2, but we lose an extra factor of 2 in being unable to use all the previous
evaluations. Since 1.732 < 2 x 1.414, it is better to triple.

Here is the resulting routine, which is directly comparable to trapzd.

#define FUNC(x) ((*func) (x))

float midpnt(float (*func) (float), float a, float b, int n)
This routine computes the nth stage of refinement of an extended midpoint rule. func is input
as a pointer to the function to be integrated between limits a and b, also input. When called with
n=1, the routine returns the crudest estimate of rp f(x)dx. Subsequent calls with n=2,3,...
(in that sequential order) will improve the accuracy of s by adding (2/3) x 38-1 additional
interior points. s should not be modified between sequential calls.
{

float x,tnm,sum,del,ddel;

static float s;

int it,j;

if (nm == 1) {
return (s=(b-a)*FUNC(0.5*(atb)));
} else {
for(it=1,j=1;j<n-1; j++) it *= 3;
tnm=it;
del=(b-a)/(3.0*tnm) ;
ddel=delt+del; The added points alternate in spacing between
x=at0.5*del; del and ddel.
sum=0.0;
for (j=1;j<sit;jt+) {
sum += FUNC(x) ;
x += ddel;
sum += FUNC(x) ;
x += del;
}
s=(s+(b-a) *sum/tnm)/3.0; The new sum is combined with the old integral
return s; to give a refined integral.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.4 Improper Integrals 143

The routine midpnt can exactly replace trapzd in a driver routine like qtrap
(84.2); one simply changes trapzd(func,a,b,j) tomidpnt(func,a,b, j),and
perhaps also decreases the parameter JMAX since 3 JMAX-1 (from step tripling) is a
much larger number than gJMAX—1 (step doubling).

The open formula implementation analogous to Simpson’s rule (qsimp in §4.2)
substitutes midpnt for trapzd and decreases JMAX as above, but now also changes
the extrapolation step to be

s=(9.0*st-ost)/8.0;

since, when the number of steps is tripled, the error decreases to 1/9th its size, not
1/4th as with step doubling.

Either the modified qtrap or the modified qsimp will fix the first problem
on the list at the beginning of this section. Yet more sophisticated is to generalize
Romberg integration in like manner:

#include <math.h>
#define EPS 1.0e-6
#define JMAX 14
#tdefine JMAXP (JMAX+1)
#define K 5

float gromo(float (*func) (float), float a, float b,

float (*choose) (float (*) (float), float, float, int))
Romberg integration on an open interval. Returns the integral of the function func from a to b,
using any specified integrating function choose and Romberg’s method. Normally choose will
be an open formula, not evaluating the function at the endpoints. It is assumed that choose
triples the number of steps on each call, and that its error series contains only even powers of
the number of steps. The routines midpnt, midinf, midsql, midsqu, midexp, are possible
choices for choose. The parameters have the same meaning as in qromb.
{

void polint(float xa[], float ya[], int n, float x, float *y, float *dy);

void nrerror(char error_text[]);

int j;

float ss,dss,h[JMAXP+1] ,s[JMAXP] ;

*choose) (func,a,b,j);

if (Gj >=) {
polint (&h[j-K] ,&s[j-K] ,K,0.0,&ss,&dss) ;
if (fabs(dss) <= EPS*fabs(ss)) return ss;

}

h[j+1]=h[j]/9.0; This is where the assumption of step tripling and an even
+ error series is used.
nrerror("Too many steps in routing qromo");
return 0.0; Never get here.

Don’t be put off by qgromo’s complicated ANSI declaration. A typical invocation
(integrating the Bessel function Yo(x) from 0 to 2) is simply

#include "nr.h"
float answer;

answer=qromo(bessy0,0.0,2.0,midpnt) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

144 Chapter 4. _ Integration of Functions

The differences between gromo and qromb (§4.3) are so slight that it is perhaps
gratuitous to list qromo in full. It, however, is an excellent driver routine for solving
all the other problems of improper integrals in our first list (except the intractable
fifth), as we shall now see.

The basic trick for improper integrals is to make a change of variables to
eliminate the singularity, or to map an infinite range of integration to a finite one.
For example, the identity

b ay 74
f(a)dx = | z!(z) dt ab>0 (4.4.2)
a jot t

can be used with either b — oo and a positive, or with a — —oo and b negative, and
works for any function which decreases towards infinity faster than 1/2”.

You can make the change of variable implied by (4.4.2) either analytically and
then use (e.g.) qromo and midpnt to do the numerical evaluation, or you can let
the numerical algorithm make the change of variable for you. We prefer the latter
method as being more transparent to the user. To implement equation (4.4.2) we
simply write a modified version of midpnt, called midinf, which allows b to be
infinite (or, more precisely, a very large number on your particular machine, such
as 1 x 10°°), or a to be negative and infinite.

#define FUNC(x) ((#funk) (1.0/(x))/((x)*(x))) Effects the change of variable.

float midinf(float (*funk) (float), float aa, float bb, int n)
This routine is an exact replacement for midpnt, i.e., returns the nth stage of refinement of
the integral of funk from aa to bb, except that the function is evaluated at evenly spaced
points in 1/a rather than in x. This allows the upper limit bb to be as large and positive as
the computer allows, or the lower limit aa to be as large and negative, but not both. aa and
bb must have the same sign.
t

float x,tnm,sum,del,ddel,b,a;

static float s;

int it,j;
b=1.0/aa; These two statements change the limits of integration.
a=1.0/bb;
if (n == 1) { From this point on, the routine is identical to midpnt.
return (s=(b-a)*FUNC(0.5*(atb)));
} else {
for(it=1,j=1;j<n-1; j++) it *= 3;
tnm=it;

del=(b-a)/(3.0*tnm) ;
ddel=del+del ;

x=at0.5*del;
sum=0.0;
for (j=1;j<sit;jt+) {
sum FUNC (x) ;
x += ddel;
sum += FUNC(x) ;
x += del;

+

return (s=(s+(b-a)*sum/tnm) /3.0) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.4 Improper Integrals 145

If you need to integrate from a negative lower limit to positive infinity, you do
this by breaking the integral into two pieces at some positive value, for example,

answer=qromo (funk ,-5.0,2.0,midpnt) +qromo(funk,2.0,1.0e30,midinf) ;

Where should you choose the breakpoint? At a sufficiently large positive value so
that the function funk is at least beginning to approach its asymptotic decrease to
zero value at infinity. The polynomial extrapolation implicit in the second call to
qromo deals with a polynomial in 1/z, not in x.

To deal with an integral that has an integrable power-law singularity at its lower
limit, one also makes a change of variable. If the integrand diverges as (x — a) ~7,
0<y < 1, near x = a, use the identity

b 1 (b-a)!~7

If the singularity is at the upper limit, use the identity

(b>a) (443)

b 1 (ay 1
| f(x)dz = —— t™7 f(b —th7 )dt (b> a) (4.4.4)
a 1-7 Jo
If there is a singularity at both limits, divide the integral at an interior breakpoint
as in the example above.
Equations (4.4.3) and (4.4.4) are particularly simple in the case of inverse
square-root singularities, a case that occurs frequently in practice:

b b-—a
f(a)dx = [ Qtf(a+P)dt  (b>a) (4.4.5)
a 0

for a singularity at a, and

, f(x)dx = [ - 2tf(b—t)dt (b>a) (4.4.6)

for a singularity at b. Once again, we can implement these changes of variable
transparently to the user by defining substitute routines for midpnt which make the
change of variable automatically:

#include <math.h>
#define FUNC(x) (2.0*(x)*(*funk) (aat(x) *(x)))

float midsql(float (*funk) (float), float aa, float bb, int n)
This routine is an exact replacement for midpnt, except that it allows for an inverse square-root
singularity in the integrand at the lower limit aa.

float x,tnm,sum,del,ddel,a,b;
static float s;
int it,j;

b=sqrt (bb-aa) ;
a=0.0;
if (@ == 1) {
The rest of the routine is exactly like midpnt and is omitted.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

146 Chapter 4. _ Integration of Functions

Similarly,

#include <math.h>
#define FUNC(x) (2.0*(x)*(*funk) (bb- (x) *(x)))

float midsqu(float (*funk) (float), float aa, float bb, int n)
This routine is an exact replacement for midpnt, except that it allows for an inverse square-root
singularity in the integrand at the upper limit bb.

float x,tnm,sum,del,ddel,a,b;
static float s;
int it,j;

b=sqrt (bb-aa) ;
a=0.0;
if (@ == 1) {
The rest of the routine is exactly like midpnt and is omitted.

One last example should suffice to show how these formulas are derived in
general. Suppose the upper limit of integration is infinite, and the integrand falls off
exponentially. Then we want a change of variable that maps e ~“dx into (-£)dt (with
the sign chosen to keep the upper limit of the new variable larger than the lower
limit). Doing the integration gives by inspection

or xz =—logt (4.4.7)

so that

(4.4.8)

The user-transparent implementation would be

#include <math.h>
#define FUNC(x) ((*funk) (-log(x))/(x))

float midexp(float (*funk) (float), float aa, float bb, int n)

This routine is an exact replacement for midpnt, except that bb is assumed to be infinite
(value passed not actually used). It is assumed that the function funk decreases exponentially
rapidly at infinity.

t

float x,tnm,sum,del,ddel,a,b;
static float s;

int it,j;

b=exp(-aa) ;

a=0.0;

if (@@ == 1) {
The rest of the routine is exactly like midpnt and is omitted.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 4.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 147

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
87.4.3, p. 294.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§3.7, p. 152.

4.5 Gaussian Quadratures and Orthogonal
Polynomials

In the formulas of $4.1, the integral of a function was approximated by the sum
of its functional values at a set of equally spaced points, multiplied by certain aptly
chosen weighting coefficients. We saw that as we allowed ourselves more freedom
in choosing the coefficients, we could achieve integration formulas of higher and
higher order. The idea of Gaussian quadratures is to give ourselves the freedom to
choose not only the weighting coefficients, but also the location of the abscissas at
which the function is to be evaluated: They will no longer be equally spaced. Thus,
we will have twice the number of degrees of freedom at our disposal; it will turn out
that we can achieve Gaussian quadrature formulas whose order is, essentially, twice
that of the Newton-Cotes formula with the same number of function evaluations.

Does this sound too good to be true? Well, in a sense it is. The catch is a
familiar one, which cannot be overemphasized: High order is not the same as high
accuracy. High order translates to high accuracy only when the integrand is very
smooth, in the sense of being “well-approximated by a polynomial.”

There is, however, one additional feature of Gaussian quadrature formulas that
adds to their usefulness: We can arrange the choice of weights and abscissas to make
the integral exact for a class of integrands “polynomials times some known function
W(a)” rather than for the usual class of integrands “polynomials.” The function
W (2) can then be chosen to remove integrable singularities from the desired integral.
Given W(x), in other words, and given an integer NV, we can find a set of weights
w; and abscissas x; such that the approximation

N

b

j=l

is exact if f(x) is a polynomial. For example, to do the integral

(4.5.2)

(not a very natural looking integral, it must be admitted), we might well be interested
in a Gaussian quadrature formula based on the choice

1

W(2) = (4.5.3)

in the interval (—1, 1). (This particular choice is called Gauss-Chebyshev integration,
for reasons that will become clear shortly.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

148 Chapter 4. _ Integration of Functions

Notice that the integration formula (4.5.1) can also be written with the weight
function W (x) not overtly visible: Define g(2) = W(x) f(x) and vj; = w;/W(a;).
Then (4.5.1) becomes

N

j=l

Where did the function W(x) go? It is lurking there, ready to give high-order
accuracy to integrands of the form polynomials times W (zx), and ready to deny high-
order accuracy to integrands that are otherwise perfectly smooth and well-behaved.
When you find tabulations of the weights and abscissas for a given W (x), you have
to determine carefully whether they are to be used with a formula in the form of
(4.5.1), or like (4.5.4).

Here is an example of a quadrature routine that contains the tabulated abscissas
and weights for the case W(a) = 1 and N = 10. Since the weights and abscissas
are, in this case, symmetric around the midpoint of the range of integration, there
are actually only five distinct values of each:

float qgaus(float (*func) (float), float a, float b)
Returns the integral of the function func between a and b, by ten-point Gauss-Legendre inte-
gration: the function is evaluated exactly ten times at interior points in the range of integration.

{

int j;

float xr,xm,dx,s;

static float x[]={0.0,0.1488743389,0.4333953941, The abscissas and weights.
0.6794095682 , 0. 8650633666 ,0.9739065285} ; First value of each array

static float w[]={0.0,0.2955242247 ,0.2692667193, not used.

0.2190863625 ,0. 1494513491 ,0.0666713443} ;

xm=0.5*(bta) ;

xr=0.5*(b-a) ;
Will be twice the average value of the function, since the
for (j=1;j<=5;jt++) { ten weights (five numbers above each used twice)
dx=xr*x([j]; sum to 2.

s t= wlj]*((*func) (xm+dx)+(*func) (xm-dx)) ;
+

return s *= xr; Scale the answer to the range of integration.

The above routine illustrates that one can use Gaussian quadratures without
necessarily understanding the theory behind them: One just locates tabulated weights
and abscissas in a book (e.g., [1] or [2]). However, the theory is very pretty, and it
will come in handy if you ever need to construct your own tabulation of weights and
abscissas for an unusual choice of W (2). We will therefore give, without any proofs,
some useful results that will enable you to do this. Several of the results assume that
W (a) does not change sign inside (a,b), which is usually the case in practice.

The theory behind Gaussian quadratures goes back to Gauss in 1814, who
used continued fractions to develop the subject. In 1826 Jacobi rederived Gauss’s
results by means of orthogonal polynomials. The systematic treatment of arbitrary
weight functions W (2) using orthogonal polynomials is largely due to Christoffel in
1877. To introduce these orthogonal polynomials, let us fix the interval of interest
to be (a,b). We can define the “scalar product of two functions f and g over a

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 149

weight function W” as

b
(tla) = [ Wop lealwrae (45.5)

The scalar product is a number, not a function of x. Two functions are said to be
orthogonal if their scalar product is zero. A function is said to be normalized if its
scalar product with itself is unity. A set of functions that are all mutually orthogonal
and also all individually normalized is called an orthonormal set.

We can find a set of polynomials (i) that includes exactly one polynomial of
order j, called p;(x), for each j = 0,1,2,..., and (ii) all of which are mutually
orthogonal over the specified weight function W(x). A constructive procedure for
finding such a set is the recurrence relation

po(z) = 1 (4.5.6)
pj+i(@) = (w — a;)p;(@) — bjpj-i(v) — § = 0,1,2,...

where
Ps vt ; (45.7)
5, — wile) pa,
(pj—-1|Pj-1)

The coefficient bo is arbitrary; we can take it to be zero.

The polynomials defined by (4.5.6) are monic, i.e., the coefficient of their
leading term [2/ for p;(x)] is unity. If we divide each p;(x) by the constant
{(p;|p;)]'/2 we can render the set of polynomials orthonormal. One also encounters
orthogonal polynomials with various other normalizations. You can convert from
a given normalization to monic polynomials if you know that the coefficient of
zx! in p; is \j, say; then the monic polynomials are obtained by dividing each p ;
by A;. Note that the coefficients in the recurrence relation (4.5.6) depend on the
adopted normalization.

The polynomial p;(x) can be shown to have exactly j distinct roots in the
interval (a,b). Moreover, it can be shown that the roots of p;(2) “interleave” the
j — 1 roots of pj—1 (x), ie., there is exactly one root of the former in between each
two adjacent roots of the latter. This fact comes in handy if you need to find all the
roots: You can start with the one root of pi (x) and then, in turn, bracket the roots
of each higher 7, pinning them down at each stage more precisely by Newton’s rule
or some other root-finding scheme (see Chapter 9).

Why would you ever want to find all the roots of an orthogonal polynomial
p;(«)? Because the abscissas of the N-point Gaussian quadrature formulas (4.5.1)
and (4.5.4) with weighting function W (zx) in the interval (a, b) are precisely the roots
of the orthogonal polynomial py (x) for the same interval and weighting function.
This is the fundamental theorem of Gaussian quadratures, and lets you find the
abscissas for any particular case.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

150 Chapter 4. _ Integration of Functions

Once you know the abscissas 71,...,2,, you need to find the weights w;,
j =1,...,N. One way to do this (not the most efficient) is to solve the set of
linear equations

po(@1) «+. po(tn) 7 Pwr 2 W(2)po(x)dx
ray (71) ve pry) we] ° (458)

Equation (4.5.8) simply solves for those weights such that the quadrature (4.5.1)
gives the correct answer for the integral of the first N orthogonal polynomials. Note
that the zeros on the right-hand side of (4.5.8) appear because p1(x),...,pn—1(x)
are all orthogonal to po(x), which is a constant. It can be shown that, with those
weights, the integral of the next V — 1 polynomials is also exact, so that the quadrature
is exact for all polynomials of degree 2/V — 1 or less. Another way to evaluate the
weights (though one whose proof is beyond our scope) is by the formula

(pn—1|Ppn—1)

pn—1(23)Dy (23)

(4.5.9)

Wi =

where p’y (x;) is the derivative of the orthogonal polynomial at its zero x ;.

The computation of Gaussian quadrature rules thus involves two distinct phases:
(i) the generation of the orthogonal polynomials po,..., py, i.e., the computation of
the coefficients a;, b; in (4.5.6); (ii) the determination of the zeros of py (a), and
the computation of the associated weights. For the case of the “classical” orthogonal
polynomials, the coefficients a; and b; are explicitly known (equations 4.5.10 —
4.5.14 below) and phase (i) can be omitted. However, if you are confronted with a
“nonclassical” weight function W (2), and you don’t know the coefficients a ; and
b;, the construction of the associated set of orthogonal polynomials is not trivial.
We discuss it at the end of this section.

Computation of the Abscissas and Weights

This task can range from easy to difficult, depending on how much you already
know about your weight function and its associated polynomials. In the case of
classical, well-studied, orthogonal polynomials, practically everything is known,
including good approximations for their zeros. These can be used as starting guesses,
enabling Newton’s method (to be discussed in §9.4) to converge very rapidly.
Newton’s method requires the derivative p/y (x), which is evaluated by standard
relations in terms of py and py_1. The weights are then conveniently evaluated by
equation (4.5.9). For the following named cases, this direct root-finding is faster,
by a factor of 3 to 5, than any other method.

Here are the weight functions, intervals, and recurrence relations that generate
the most commonly used orthogonal polynomials and their corresponding Gaussian
quadrature formulas.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 151

Gauss-Legendre:

W(a)=1 -l<a<l
(i + U)Phaa = (2) + I2Pj — §Pj1 (4.5.10)
Gauss-Chebyshev:
W(x) = (1-27)? -l<a<l
Ty41 = 2aT; — Ty-1 (4.5.11)

Gauss-Laguerre:

W(a) =2%e* 0<a%<0o

(G+ VL 51 = (-e@ + 29 +at LF -(G+a)Li_y (4.5.12)

Gauss-Hermite:
W(2) = em -—0<4%<oo
Ayj41 = 2vH; — 29Hj-1 (4.5.13)
Gauss-Jacobi:
W(x) =(1—2)*(1+2)% -l<a<l
a, B B
oj Poy? = (dj + ejx)P\ ” FPP (4.5.14)

where the coefficients c;,d;,e;, and f; are given by
co = 297 +1I)G+at+84+1)2j)+a+ 8)
dj = (23 +0+8+1)(0? — 6?)
ej = (27 + at B\(Q7+a+B+1)(2Qj+a+ fh +2)
fj = 25 + a)(j + B)(27 +a + B+ 2)

We now give individual routines that calculate the abscissas and weights for
these cases. First comes the most common set of abscissas and weights, those of
Gauss-Legendre. The routine, due to G.B. Rybicki, uses equation (4.5.9) in the
special form for the Gauss-Legendre case,

(4.5.15)

2
(1 = 25)[Px (es)?
The routine also scales the range of integration from (zx 1, x2) to (—1, 1), and provides
abscissas 2; and weights w,; for the Gaussian formula

(4.5.16)

Wj =

N

j=l

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

152 Chapter 4. Integration of Functions

#include <math.h>
#define EPS 3.0e-11 EPS is the relative precision.

void gauleg(float x1, float x2, float x[], float wf], int n)

Given the lower and upper limits of integration x1 and x2, and given n, this routine returns
arrays x[1..n] and w[1..n] of length n, containing the abscissas and weights of the Gauss-
Legendre n-point quadrature formula.

{
int m,j,i;
double z1,z,xm,xl,pp,p3,p2,p1; High precision is a good idea for this rou-
tine.
m=(n+1)/2; The roots are symmetric in the interval, so
xm=0.5* (x2+x1) ; we only have to find half of them.
x1=0.5*(x2-x1);
for (i=1;i<=m;i++) { Loop over the desired roots.
z=cos(3.141592654*(i-0.25)/(n+0.5));
Starting with the above approximation to the ith root, we enter the main loop of
refinement by Newton’s method.
do {
Loop up the recurrence relation to get the
p3=p2; Legendre polynomial evaluated at z.
p2=p1;
p1=((2.0*j-1.0) *z*p2-(j-1.0) *p3)/j;
3
pi is now the desired Legendre polynomial. We next compute pp, its derivative,
by a standard relation involving also p2, the polynomial of one lower order.
pp=n* (z*p1-p2) /(z*z-1.0) ;
zi=z;
z=z1-p1/pp; Newton's method.
} while (fabs(z-z1) > EPS);
x[i]=xm-x1*z; Scale the root to the desired interval,
x[nt+1-i] =xm+x1l*z; and put in its symmetric counterpart.
wli]=2.0*x1/((1.0-z*z) *pp*pp) ; Compute the weight
w{nt1-i]=w[i]; and its symmetric counterpart.
+
3

Next we give three routines that use initial approximations for the roots given
by Stroud and Secrest [2]. The first is for Gauss-Laguerre abscissas and weights, to
be used with the integration formula

N

oo
[ a%e* f(a)dx = w; f(a) (4.5.18)
0 j=l

#include <math.h>

#define EPS 3.0e-14 Increase EPS if you don’t have this preci-

#define MAXIT 10 sion.

void gaulag(float x[], float w[], int n, float alf)
Given alf, the parameter a of the Laguerre polynomials, this routine returns arrays x[1. .n]
and w[1..n] containing the abscissas and weights of the n-point Gauss-Laguerre quadrature
formula. The smallest abscissa is returned in x[1], the largest in x[n].
{

float gammin(float xx);

void nrerror(char error_text[]);

int i,its,j;

float ai;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 153

double p1,p2,p3,pp,z,z1; High precision is a good idea for this rou-

tine.
for (i=1;i<=n;i++) { Loop over the desired roots.
if Gi 1) { Initial guess for the smallest root.
z=(1.0t+alf)*(3.0+0.92*alf)/(1.0+2.4*n+1.8*alf) ;
}else if (i == 2) { Initial guess for the second root.
z += (15.0+6.25*alf)/(1.0+0.9*alf+2.5*n) ;
} else { Initial guess for the other roots.

-2;

z ((1.0+2.55*ai)/(1.9*ai)+1.26*ai*alf/
(1.0+3.5¥ai))*(z-x[i-2])/(1.0+0.3*alf) ;
}
for (its=1;its<=MAXIT;its++) { Refinement by Newton's method.
Loop up the recurrence relation to get the
Laguerre polynomial evaluated at z.
p2=pi;
pi=((2*j-1+alf-z) *p2-(j-1+alf) *p3)/j;
3
pi is now the desired Laguerre polynomial. We next compute pp, its derivative,
by a standard relation involving also p2, the polynomial of one lower order.
pp=(n*pi-(n+alf)*p2)/z;
zi=z;
z=z1-p1/pp; Newton's formula.
if (fabs(z-z1) <= EPS) break;
}
if (its > MAXIT) nrerror("too many iterations in gaulag");
x[il=z; Store the root and the weight.

wli] = -exp(gamm1n(alf+n)-gammln((float)n))/(pp*n*p2) ;

Next is a routine for Gauss-Hermite abscissas and weights. If we use the
“standard” normalization of these functions, as given in equation (4.5.13), we find
that the computations overflow for large N because of various factorials that occur.
We can avoid this by using instead the orthonormal set of polynomials H;. They
are generated by the recurrence

~ ~ 1 ~
H4=0, Ho=—7. Fyy. nl - i (4.5.19)

The formula for the weights becomes

(4.5.20)

while the formula for the derivative with this normalization is

= (2jHj-1 (4.5.21)
The abscissas and weights returned by gauher are used with the integration formula
N

[oe nome Som

—oo

(4.5.22)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

154 Chapter 4. Integration of Functions

#include <math.h>

#define EPS 3.0e-14 Relative precision.
#define PIM4 0.7511255444649425 1/ni/4,
#define MAXIT 10 Maximum iterations.

void gauher(float x[], float w[], int n)

Given n, this routine returns arrays x[1..n] and w[1..n] containing the abscissas and weights
of the n-point Gauss-Hermite quadrature formula. The largest abscissa is returned in x[1], the
most negative in x[n].

{
void nrerror(char error_text[]);
int i,its,j,m;
double p1,p2,p3,pp,z,z1; High precision is a good idea for this rou-
tine.
m=(n+1)/2;
The roots are symmetric about the origin, so we have to find only half of them.
for (i= pitt) { Loop over the desired roots.
if Gi 1) { Initial guess for the largest root.
z=sqrt ( (double) (2*n+1))-1.85575*pow( (double) (2*n+1) ,-0. 16667) ;
}else if (i == 2) { Initial guess for the second largest root.
z 1.14*pow((double)n,0.426)/z;
}else if (i == 3) { Initial guess for the third largest root.
z=1.86*z-0.86*x([1] ;
}else if Gi == 4) { Initial guess for the fourth largest root.
z=1.91*z-0.91*x([2] ;
} else { Initial guess for the other roots.
z=2.0*z-x[i-2];
}
for (its=1;its<=MAXIT;its++) { Refinement by Newton's method.
pi=PIM4;
p2=0.
for ( n;jtt+) { Loop up the recurrence relation to get
p3=p2; the Hermite polynomial evaluated at
p2=p1; Zz.
pi=z*sqrt (2.0/j)*p2-sqrt (( (double) (j-1))/j) *p3;
}
pi is now the desired Hermite polynomial. We next compute pp, its derivative, by
the relation (4.5.21) using p2, the polynomial of one lower order.
pp=sqrt ( (double) 2*n) *p2;
zi=z;
z=z1-p1/pp; Newton's formula.
if (fabs(z-z1) <= EPS) break;
}
if (its > MAXIT) nrerror("too many iterations in gauher");
x[il=z; Store the root
x{nt1-i] = -z; and its symmetric counterpart.
wli]=2.0/(pp*pp) ; Compute the weight
w{nt1-i]=w[i]; and its symmetric counterpart.
+
}

Finally, here is a routine for Gauss-Jacobi abscissas and weights, which
implement the integration formula

N

1

1 j=l

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 155

#include <math.h>
#define EPS 3.0e-14 Increase EPS if you don’t have this preci-
#define MAXIT 10 sion.

void gaujac(float x[], float w[], int n, float alf, float bet)
Given alf and bet, the parameters a and (3 of the Jacobi polynomials, this routine returns
arrays x[1..n] and w[1..n] containing the abscissas and weights of the n-point Gauss-Jacobi
quadrature formula. The largest abscissa is returned in x[1], the smallest in x[n].
{

float gammin(float xx);

void nrerror(char error_text[]);

int i,its,j;

float alfbet,an,bn,ri,r2,r3;

double a,b,c,p1,p2,p3,pp,temp,z,z1; High precision is a good idea for this rou-

tine.
for (i=1;i<=n;i++) { Loop over the desired roots.
if Gi 1) { Initial guess for the largest root.
an=alf/n;
bn=bet/n;

1.0+alf)*(2.78/(4.0+n*n) +0.768*an/n) ;

r2=1.0+1.48*an+0.96*bn+0. 452*an*an+0 .83*an*bn;

z=1.0-r1/r'

}else if (i == 2) { Initial guess for the second largest root.
ri=(4.1+alf)/((1.0+alf)*(1.0+0.156*alf)) ;
r2=1.0+0.06*(n-8.0)*(1.0+0.12*alf)/n;
r3=1.0+0.012*bet*(1.0+0.25*fabs(alf))/n;

z -= (1.0-z) *r1+*r2*r3;

}else if (i == 3) { Initial guess for the third largest root.
ri=(1.67+0.28*alf)/(1.0+0.37*alf) ;

-0+0.22*(n-8.0)/n;

0+8.0*bet/((6.28+bet) *n*n) ;

(x[1]-z) #r1*r2*r3;

} else if (i == n-1) { Initial guess for the second smallest root.
r1=(1.0+0.235*bet) /(0.766+0.119*bet) ;
r2=1.0/(1.0+0.639*(n-4.0)/(1.0+0.71*(n-4.0)));
13=1.0/(1.0+20.0*alf/((7.5+alf) *n*n)) ;

z t= (z-x[n-3])*r1i+*r2*r3;

}else if (i == n) { Initial guess for the smallest root.
ri=(1.0+0.37*bet) /(1.67+0.28*bet) ;
r2=1.0/(1.0+0.22*(n-8.0)/n) ;
r3=1.0/(1.0+8.0*alf/((6.28+alf) *n*n)) ;

z t= (z-x[n-2])*ri*r2*r3;

} else { Initial guess for the other roots.

z=3.0*x(i-1]-3.0*x[i-2] +x [i-3] ;

}

alfbet=alftbet;

for (its=1;its<=MAXIT;its++) { Refinement by Newton's method.
temp=2.0+alfbet; Start the recurrence with Po and P; to avoid
pi=(alf-bet+temp*z) /2.0; a division by zero when a + 3 = 0 or

-1.
Loop up the recurrence relation to get the
Jacobi polynomial evaluated at z.

temp=2*jtalfbet;

a=2*j*(jt+alfbet) *(temp-2.0);
temp-1.0)*(alf*alf-bet*bet+temp* (temp-2.0)*z) ;
.0*(j-1+alf) *(j-1+bet)*temp;

pi=(b*p2-c*p3) /a;

3

pp=(n*(alf-bet-temp*z) *p1+2.0* (ntalf)*(n+bet) *p2) / (temp*(1.0-z*z)) ;
pi is now the desired Jacobi polynomial. We next compute pp, its derivative, by
a standard relation involving also p2, the polynomial of one lower order.

zi=z;

z=z1-p1/pp; Newton's formula.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

156 Chapter 4. Integration of Functions

if (fabs(z-z1) <= EPS) break;

}
if (its > MAXIT) nrerror("too many iterations in gaujac");
x[il=z; Store the root and the weight.

wli]=exp(gamm1n (alf+n)+gamm1n(bet+n)-gamm1n(n+1.0)-
gammln(ntalfbet+1.0))*temp*pow(2.0,alfbet) /(pp*p2) ;

Legendre polynomials are special cases of Jacobi polynomials with a = 3 = 0,
but it is worth having the separate routine for them, gauleg, given above. Chebyshev
polynomials correspond to a = 3 = —1/2 (see §5.8). They have analytic abscissas
and weights:

wy =

2/8

Case of Known Recurrences

Turn now to the case where you do not know good initial guesses for the zeros of your
orthogonal polynomials, but you do have available the coefficients a; and b; that generate
them. As we have seen, the zeros of py() are the abscissas for the N-point Gaussian
quadrature formula. The most useful computational formula for the weights is equation
(4.5.9) above, since the derivative p'y can be efficiently computed by the derivative of (4.5.6)
in the general case, or by special relations for the classical polynomials. Note that (4.5.9) is
valid as written only for monic polynomials; for other normalizations, there is an extra factor
of An /ANn—1, where An is the coefficient of a in PN.

Except in those special cases already discussed, the best way to find the absci

s is not

to use a root-finding method like Newton’s method on py(x). Rather, it is generally faster
to use the Golub-Welsch [3] algorithm, which is based on a result of Wilf [4]. This algorithm
notes that if you bring the term xp; to the left-hand side of (4.5.6) and the term p;+1 to the
right-hand side, the recurrence relation can be written in matrix form as
Po fao 1 Po 07
Pl bi ai il Pl 0
x = 4]:
pN-2 bn-2 Gn-2 1 pN-2 0
LPN-1 L bn-1 @n-1 PN-1 PN J
or
zp =T-p+pnen-1 (4.5.25)
Here T is a tridiagonal matrix, p is a column vector of po, p1,--.,PN—1, and en—1 is a unit
vector with a 1 in the (NV — 1)st (last) position and zeros elsewhere. The matrix T can be

symmetrized by a diagonal similarity transformation D to give
ao Vor
Vb a vb2
J=DTD'= : : (4.5.26)
Vbn-2 an-2 Von
Von @N-1

The matrix J is called the Jacobi matrix (not to be confused with other matrices named
after Jacobi that arise in completely different problems!). Now we see from (4.5.25) that

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 157

pn(xj) = 0 is equivalent to x; being an eigenvalue of T. Since eigenvalues are preserved
by a similarity transformation, x; is an eigenvalue of the symmetric tridiagonal matrix J .
Moreover, Wilf [4] shows that if vj is the eigenvector corresponding to the eigenvalue «;,
normalized so that v- v = 1, then

wy = pov; a (4.5.27)

where
b
Lo = [ W(x) dx (4.5.28)

and where vj;,1 is the first component of v. As we shall see in Chapter 11, finding all
eigenvalues and eigenvectors of a symmetric tridiagonal matrix is a relatively efficient and
well-conditioned procedure. We accordingly give a routine, gaucof, for finding the abscissas
and weights, given the coefficients a; and b;. Remember that if you know the recurrence
relation for orthogonal polynomials that are not normalized to be monic, you can easily convert
it to monic form by means of the quantities \,;.

#include <math.h>
#include "nrutil.h"

void gaucof(int n, float a[], float b[], float amu0, float x[], float w[])
Computes the abscissas and weights for a Gaussian quadrature formula from the Jacobi matrix.
On input, a[1..n] and b[1..n] are the coefficients of the recurrence relation for the set of
monic orthogonal polynomials. The quantity o = rp W (a) dx is input as amuO. The abscissas
x[1..n] are returned in descending order, with the corresponding weights in w[1..n]. The
arrays a and b are modified. Execution can be speeded up by modifying tqli and eigsrt to
compute only the first component of each eigenvector.

{
void eigsrt(float d[], float **v, int n);
void tqli(float d[], float e[], int n, float **z);
int i,j;
float **z;
z=matrix(1,n,1,n);
for (i=1;i<=n;it+) {
if Gi != 1) blil=sqrt(b[i]); Set up superdiagonal of Jacobi matrix.
for (j=1;j<=n;j++) zli][j]=(float)(i == 5);
Set up identity matrix for tqli to compute eigenvectors.
+
tqli(a,b,n,z);
eigsrt(a,z,n); Sort eigenvalues into descending order.
w[i]=amu0*z [1] [i] *z[1] [i]; Equation (4.5.27).
}
free_matrix(z,1,n,1,n);
3

Orthogonal Polynomials with Nonclassical Weights

This somewhat specialized subsection will tell you what to do if your weight function
is not one of the classical ones dealt with above and you do not know the a;’s and b;’s
of the recurrence relation (4.5.6) to use in gaucof. Then, a method of finding the a,;’s
and b;’s is needed.

The procedure of Stieltjes is to compute ao from (4.5.7), then pi(x) from (4.5.6).
Knowing po and p;, we can compute a; and b; from (4.5.7), and so on. But how are we
to compute the inner products in (4.5.7)?

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

158 Chapter 4. Integration of Functions

The textbook approach is to represent each p;() explicitly as a polynomial in x and
to compute the inner products by multiplying out term by term. This will be feasible if we
know the first 2N moments of the weight function,

b
w=/ wW(a)de j=0,1,...,2N—1 (4.5.29)

However, the solution of the resulting set of algebraic equations for the coefficients a; and b;
in terms of the moments jz; is in general extremely ill-conditioned. Even in double precision,
it is not unusual to lose all accuracy by the time N = 12. We thus reject any procedure
based on the moments (4.5.29).

Sack and Donovan [5] discovered that the numerical stability is greatly improved if,
instead of using powers of « as a set of basis functions to represent the p;’s, one uses some
other known set of orthogonal polynomials 7;(a), say. Roughly speaking, the improved
stability occurs because the polynomial basis “samples” the interval (a,b) better than the
power basis when the inner product integrals are evaluated, especially if its weight function
resembles W(x).

So assume that we know the modified moments

b
y= i 13 (x)W (x)da j=0,1,...,2N-1 (4.5.30)
where the 7;’s satisfy a recurrence relation analogous to (4.5.6),
T1(%) =0
mo(x) = 1 (4.5.31)

mj41(x) = (% — a5) 75 (x) — Bjmj-1(x) j=0,1,2,...

and the coefficients a;, 3; are known explicitly. Then Wheeler [6] has given an efficient
O(N?) algorithm equivalent to that of Sack and Donovan for finding a; and b; via a set
of intermediate quantities

ont = (pe|m) k,l>-1 (4.5.32)
Initialize

o-11=0 1=1,2,...,2N—-—2

ool = 1=0,1,...,2N-1
4.5.33
ao = ao + ma ( )

Vo
bo = 0

Then, for k = 1,2,...,N — 1, compute
Ok = Ok-1 41 — (Ak—-1 — On)OR—-1,1 — bR-1 K-21 + BioK—1,1-1

l=k,k+1,...,2AN-k-1
Tk-1,k 4 ChAT

Gp = AR —
OK-1,k-1 CK k
by = — Zh
Ok-1,k-1

(4.5.34)

Note that the normalization factors can also easily be computed if needed:

(po|po) = Yo

(4.5.35)

(p;lpi) = bj (pj-alpj-1) FG = 1,2...

You can find a derivation of the above algorithm in Ref.[7].
Wheeler’s algorithm requires that the modified moments (4.5.30) be accurately computed.
In practical cases there is often a closed form, or else recurrence relations can be used. The

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.5 Gaussian Quadratures and Orthogonal Polynomials 159

algorithm is extremely successful for finite intervals (a, b). For infinite intervals, the algorithm
does not completely remove the ill-conditioning. In this case, Gautschi [8,9] recommends
reducing the interval to a finite interval by a change of variable, and then using a suitable
discretization procedure to compute the inner products. You will have to consult the
references for details.

We give the routine orthog for generating the coefficients a; and b; by Wheeler’s
algorithm, given the coefficients a; and (;, and the modified moments v;. For consistency
with gaucof, the vectors a, 3, a and b are 1-based. Correspondingly, we increase the indices
of the o matrix by 2, ie., sigfk,1] = o,~2,1-2.

#include "nrutil.h"

void orthog(int n, float anu[], float alpha[], float beta[], float al],

float b[])
Computes the coefficients aj and b;, j = 0,...N — 1, of the recurrence relation for monic
orthogonal polynomials with weight function W (a) by Wheeler's algorithm. On input, the arrays
alpha[1..2*n-1] and beta[1..2*n-1] are the coefficients aj and 3;, 7 = 0 2N —2, of
the recurrence relation for the chosen basis of orthogonal polynomials. The modified moments
v; are input in anu[1..2*n]. The first n coefficients are returned in a[1..n] and b[1..n].

int k,1;
float **sig;
int looptmp;

sig=matrix(1,2*n+1,1,2*n+1);
looptmp=2*n ;
for (1=3;1<=looptmp;1++) sig[1] [1]=0.0; Initialization, Equation (4.5.33).
looptmp++;
for (1=2;1<=looptmp;1++) sig[2] [1]=anu[1-1];
a[1]=alpha[1]+anu[2] /anu[1] ;
b[1]=0.0;
for (k=3;k<=nt1i;k++) { Equation (4.5.34).
Llooptmp=2*n-k+3;
for (1=k;1<=looptmp;1+t+) {
sig(k] (1]=sig[k-1] (1+1]+(alpha[1-1] -a[k-2]) *sig[k-1] [1]-
b(k-2]*sig[k-2] [1] +beta[1-1] *sig(k-1] [1-1];
}
a[k-1]=alpha[k-1]+sig[k] [k+1]/sig[k] [k] -sig[k-1] [k]/sig[k-1] [k-1];
b[k-1]=sig[k] [k]/sig[k-1] [k-1];

+

free_matrix(sig,1,2*n+1,1,2*n+1);

As an example of the use of orthog, consider the problem [7] of generating orthogonal
polynomials with the weight function W («) = — log x on the interval (0,1). A suitable set

of 7;’s is the shifted Legendre polynomials
_ Gil)?
T= (gyi 2 —1) (4.5.36)

The factor in front of P; makes the polynomials monic. The coefficients in the recurrence
relation (4.5.31) are

1
a= 5 j=0,1,...
1 (4.5.37)
1 ~ W475) j=1,2,...
while the modified moments are
1 j=0
yay
vy = 4 (-1) i) al (4.5.38)

HG + 1)(29)!

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

160 Chapter 4. Integration of Functions

A call to orthog with this input allows one to generate the required polynomials to machine
accuracy for very large N, and hence do Gaussian quadrature with this weight function. Before
Sack and Donovan’s observation, this seemingly simple problem was essentially intractable.

Extensions of Gaussian Quadrature

There are many different ways in which the ideas of Gaussian quadrature have
been extended. One important extension is the case of preassigned nodes: Some
points are required to be included in the set of abscissas, and the problem is to choose
the weights and the remaining abscissas to maximize the degree of exactness of the
the quadrature rule. The most common cases are Gauss-Radau quadrature, where
one of the nodes is an endpoint of the interval, either a or b, and Gauss-Lobatto
quadrature, where both a and b are nodes. Golub [10] has given an algorithm similar
to gaucof for these cases.

The second important extension is the Gauss-Kronrod formulas. For ordinary
Gaussian quadrature formulas, as V increases the sets of abscissas have no points
in common. This means that if you compare results with increasing N as a way of
estimating the quadrature error, you cannot reuse the previous function evaluations.
Kronrod [11] posed the problem of searching for optimal sequences of rules, each
of which reuses all abscissas of its predecessor. If one starts with N = m, say,
and then adds n new points, one has 2n + m free parameters: the n new abscissas
and weights, and m new weights for the fixed previous abscissas. The maximum
degree of exactness one would expect to achieve would therefore be 2n + m — 1.
The question is whether this maximum degree of exactness can actually be achieved
in practice, when the abscissas are required to all lie inside (a,b). The answer to
this question is not known in general.

Kronrod showed that if you choose n = m+ 1, an optimal extension can
be found for Gauss-Legendre quadrature. Patterson [12] showed how to compute
continued extensions of this kind. Sequences such as N = 10, 21, 43,87,... are
popular in automatic quadrature routines [13] that attempt to integrate a function until
some specified accuracy has been achieved.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.4. 1]

Stroud, A.H., and Secrest, D. 1966, Gaussian Quadrature Formulas (Englewood Cliffs, Nu:
Prentice-Hall). [2]

Golub, G.H., and Welsch, J.H. 1969, Mathematics of Computation, vol. 23, pp. 221-230 and
A1-A10. [3]

Wilf, H.S. 1962, Mathematics for the Physical Sciences (New York: Wiley), Problem 9, p. 80. [4]

Sack, R.A., and Donovan, A.F. 1971/72, Numerische Mathematik, vol. 18, pp. 465-478. [5]

Wheeler, J.C. 1974, Rocky Mountain Journal of Mathematics, vol. 4, pp. 287-296. [6]

Gautschi, W. 1978, in Recent Advances in Numerical Analysis, C. de Boor and G.H. Golub, eds.
(New York: Academic Press), pp. 45-72. [7]

Gautschi, W. 1981, in E.B. Christoffel, P.L. Butzer and F. Fehér, eds. (Basel: Birkhauser Verlag),
pp. 72-147. [8]

Gautschi, W. 1990, in Orthogonal Polynomials, P. Nevai, ed. (Dordrecht: Kluwer Academic Pub-
lishers), pp. 181-216. [9]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.6 Multidimensional Integrals 161

Golub, G.H. 1973, SIAM Review, vol. 15, pp. 318-334. [10]
Kronrod, A.S. 1964, Doklady Akademii Nauk SSSR, vol. 154, pp. 283-286 (in Russian). [11]

Patterson, T.N.L. 1968, Mathematics of Computation, vol. 22, pp. 847-856 and C1—-C11; 1969,
op. cit., vol. 23, p. 892. [12]

Piessens, R., de Doncker, E., Uberhuber, C.W., and Kahaner, D.K. 1983, QUADPACK: A Sub-
routine Package for Automatic Integration (New York: Springer-Verlag). [13]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),

§3.6.
Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §6.5.

Carnahan, B., Luther, H.A., and Wilkes, J.O. 1969, Applied Numerical Methods (New York:
Wiley), §§2.9-2.10.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §§4.4-4.8.

4.6 Multidimensional Integrals

Integrals of functions of several variables, over regions with dimension greater
than one, are not easy. There are two reasons for this. First, the number of function
evaluations needed to sample an V-dimensional space increases as the Nth power
of the number needed to do a one-dimensional integral. If you need 30 function
evaluations to do a one-dimensional integral crudely, then you will likely need on
the order of 30000 evaluations to reach the same crude level for a three-dimensional
integral. Second, the region of integration in N-dimensional space is defined by
an N — 1 dimensional boundary which can itself be terribly complicated: It need
not be convex or simply connected, for example. By contrast, the boundary of a
one-dimensional integral consists of two numbers, its upper and lower limits.

The first question to be asked, when faced with a multidimensional integral,
is, “can it be reduced analytically to a lower dimensionality?” For example,
so-called iterated integrals of a function of one variable f(t) can be reduced to
one-dimensional integrals by the formula

Pah Leh ta
- | ¢ — t)" 1 f(t)dt

Alternatively, the function may have some special symmetry in the way it depends
on its independent variables. If the boundary also has this symmetry, then the
dimension can be reduced. In three dimensions, for example, the integration of a
spherically symmetric function over a spherical region reduces, in polar coordinates,
to a one-dimensional integral.

The next questions to be asked will guide your choice between two entirely
different approaches to doing the problem. The questions are: Is the shape of the
boundary of the region of integration simple or complicated? Inside the region, is
the integrand smooth and simple, or complicated, or locally strongly peaked? Does

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

162 Chapter 4. _ Integration of Functions

the problem require high accuracy, or does it require an answer accurate only to
a percent, or a few percent?

If your answers are that the boundary is complicated, the integrand is not
strongly peaked in very small regions, and relatively low accuracy is tolerable, then
your problem is a good candidate for Monte Carlo integration. This method is very
straightforward to program, in its cruder forms. One needs only to know a region
with simple boundaries that includes the complicated region of integration, plus a
method of determining whether a random point is inside or outside the region of
integration. Monte Carlo integration evaluates the function at a random sample of
points, and estimates its integral based on that random sample. We will discuss it in
more detail, and with more sophistication, in Chapter 7.

If the boundary is simple, and the function is very smooth, then the remaining
approaches, breaking up the problem into repeated one-dimensional integrals, or
multidimensional Gaussian quadratures, will be effective and relatively fast [1]. If
you require high accuracy, these approaches are in any case the only ones available
to you, since Monte Carlo methods are by nature asymptotically slow to converge.

For low accuracy, use repeated one-dimensional integration or multidimensional
Gaussian quadratures when the integrand is slowly varying and smooth in the region
of integration, Monte Carlo when the integrand is oscillatory or discontinuous, but
not strongly peaked in small regions.

If the integrand is strongly peaked in small regions, and you know where those
regions are, break the integral up into several regions so that the integrand is smooth
in each, and do each separately. If you don’t know where the strongly peaked regions
are, you might as well (at the level of sophistication of this book) quit: It is hopeless
to expect an integration routine to search out unknown pockets of large contribution
in a huge N-dimensional space. (But see §7.8.)

If, on the basis of the above guidelines, you decide to pursue the repeated one-
dimensional integration approach, here is how it works. For definiteness, we will
consider the case of a three-dimensional integral in x, y, z-space. Two dimensions,
or more than three dimensions, are entirely analogous.

The first step is to specify the region of integration by (i) its lower and upper
limits in x, which we will denote x; and 29; (ii) its lower and upper limits in y at
a specified value of x, denoted y;(x) and y2(x); and (iii) its lower and upper limits
in z at specified x and y, denoted z;(x,y) and z(x,y). In other words, find the
numbers x; and x2, and the functions y; (2), y2(x), z1(x, y), and z2(zx, y) such that

r= [ [| [ecdyacteee)

£2 y2(x) z2(2x,y)
x1 ya(x) z1(@,y)

For example, a two-dimensional integral over a circle of radius one centered on
the origin becomes

(4.6.2)

1 Vi-a?
fief wv few (4.63)
-1 -Vi-#

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

4.6 Multidimensional Integrals 163

inner integration
2 2 2
s
&
3S
y eb
2
B= |
=
2
3
te)
>
x
Figure 4.6.1. Function evaluations for a two-dimensional integral over an irregular region, shown

schematically. The outer integration routine, in y, requests values of the inner, a, integral at locations
along the y axis of its own choosing. The inner integration routine then evaluates the function at
« locations suitable to it. This is more accurate in general than, e.g., evaluating the function on a
Cartesian mesh of points.

Now we can define a function G(«, y) that does the innermost integral,

22(a,y)

a(a.y)

and a function H(2) that does the integral of G(x, y),

y2(a)
ya (a)

and finally our answer as an integral over H(x)

x2

In an implementation of equations (4.6.4)-(4.6.6), some basic one-dimensional
integration routine (e.g., qgaus in the program following) gets called recursively:
once to evaluate the outer integral J, then many times to evaluate the middle integral
#, then even more times to evaluate the inner integral G (see Figure 4.6.1). Current
values of x and y, and the pointer to your function func, are passed “over the head”
of the intermediate calls through static top-level variables.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

164 Chapter 4. _ Integration of Functions

static float xsav,ysav;
static float (*nrfunc) (float ,float,float) ;

float quad3d(float (*func) (float, float, float), float x1, float x2)
Returns the integral of a user-supplied function func over a three-dimensional region specified
by the limits x1, x2, and by the user-supplied functions yy1, yy2, z1, and z2, as defined in
(4.6.2). (The functions yi and yz are here called yy1 and yy2 to avoid conflict with the names
of Bessel functions in some C libraries). Integration is performed by calling qgaus recursively.
{

float qgaus(float (*func)(float), float a, float b);

float f1(float x);

nrfunc=func;
return qgaus(f1,x1,x2);

3
float f1(float x) This is H of eq. (4.6.5).
t
float qgaus(float (*func) (float), float a, float b);
float f2(float y);
float yy1(float) ,yy2(float) ;
xSaVv=x}
return qgaus(f2,yy1(x) ,yy2(x));
}
float f2(float y) This is G of eq. (4.6.4).
t
float qgaus(float (*func) (float), float a, float b);
float f£3(float z);
float z1(float,float) ,z2(float, float) ;
ysav=y;
return qgaus(f3,z1(xsav,y) ,z2(xsav,y));
}
float f£3(float z) The integrand f(x,y, z) evaluated at fixed a and y.
t
return (*nrfunc) (xsav,ysav,z) ;
}

The necessary user-supplied functions have the following prototypes:

float func(float x,float y,float z); The 3-dimensional function to be inte-
float yy1(float x); grated.

float yy2(float x);

float z1(float x,float y);

float z2(float x,float y);

CITED REFERENCES AND FURTHER READING:

Stroud, A.H. 1971, Approximate Calculation of Multiple Integrals (Englewood Cliffs, NJ: Prentice-
Hall). [1]

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
87.7, p. 318.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §6.2.5, p. 307.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), equations 25.4.58ff.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 5. Evaluation of Functions

5.0 Introduction

The purpose of this chapter is to acquaint you with a selection of the techniques
that are frequently used in evaluating functions. In Chapter 6, we will apply and
illustrate these techniques by giving routines for a variety of specific functions.
The purposes of this chapter and the next are thus mostly in harmony, but there
is nevertheless some tension between them: Routines that are clearest and most
illustrative of the general techniques of this chapter are not always the methods of
choice for a particular special function. By comparing this chapter to the next one,
you should get some idea of the balance between “general” and “special” methods
that occurs in practice.

Insofar as that balance favors general methods, this chapter should give you
ideas about how to write your own routine for the evaluation of a function which,
while “special” to you, is not so special as to be included in Chapter 6 or the
standard program libraries.

CITED REFERENCES AND FURTHER READING:

Fike, C.T. 1968, Computer Evaluation of Mathematical Functions (Englewood Cliffs, NU: Prentice-
Hall).

Lanczos, C. 1956, Applied Analysis; reprinted 1988 (New York: Dover), Chapter 7.

5.1 Series and Their Convergence

Everybody knows that an analytic function can be expanded in the neighborhood
of a point xo in a power series,

°°

f(x) = Yo ax(x = 20)* (5.1.1)

k=0

Such series are straightforward to evaluate. You don’t, of course, evaluate the kth
power of x — x9 ab initio for each term; rather you keep the k — 1st power and update
it with a multiply. Similarly, the form of the coefficients a is often such as to make
use of previous work: Terms like k! or (2k)! can be updated in a multiply or two.

165

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

166 Chapter 5. Evaluation of Functions

How do you know when you have summed enough terms? In practice, the
terms had better be getting small fast, otherwise the series is not a good technique
to use in the first place. While not mathematically rigorous in all cases, standard
practice is to quit when the term you have just added is smaller in magnitude than
some small € times the magnitude of the sum thus far accumulated. (But watch out
if isolated instances of a, = 0 are possible!).

A weakness of a power series representation is that it is guaranteed not to
converge farther than that distance from xo at which a singularity is encountered
in the complex plane. This catastrophe is not usually unexpected: When you find
a power series in a book (or when you work one out yourself), you will generally
also know the radius of convergence. An insidious problem occurs with series that
converge everywhere (in the mathematical sense), but almost nowhere fast enough
to be useful in a numerical method. Two familiar examples are the sine function
and the Bessel function of the first kind,

sing = 3 (=F 2k (5.1.2)
2 Qk +1!
ayn . (—4a?)*
In(a) = (5) > Ek +n (5.13)

Both of these series converge for all x. But both don’t even start to converge
until k >> |x|; before this, their terms are increasing. This makes these series
useless for large x.

Accelerating the Convergence of Series

There are several tricks for accelerating the rate of convergence of a series (or,
equivalently, of a sequence of partial sums). These tricks will not generally help in
cases like (5.1.2) or (5.1.3) while the size of the terms is still increasing. For series
with terms of decreasing magnitude, however, some accelerating methods can be
startlingly good. Aitken’s 6?-process is simply a formula for extrapolating the partial
sums of a series whose convergence is approximately geometric. If 5-1, Sn, Sn4i
are three successive partial sums, then an improved estimate is

4 (Sn41— Sn)?
Sn = Sra Sn4i— 28n + Sn—1 G14)
You can also use (5.1.4) with n + 1 and n — 1 replaced by n + p and n — p
respectively, for any integer p. If you form the sequence of $/’s, you can apply
(5.1.4) a second time to that sequence, and so on. (In practice, this iteration will
only rarely do much for you after the first stage.) Note that equation (5.1.4) should
be computed as written; there exist algebraically equivalent forms that are much
more susceptible to roundoff error.

For alternating series (where the terms in the sum alternate in sign), Euler’s
transformation can be a powerful tool. Generally it is advisable to do a small

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.1 Series and Their Convergence 167

number of terms directly, through term n — 1 say, then apply the transformation to
the rest of the series beginning with term n. The formula (for n even) is

= = (-1)°
i 1)*us = Uo — U1 + ug... — Un-1 4 Ss el [A®un| (5.1.5)
s=0 s=0

Here A is the forward difference operator, i.e.,

Aun = Un41 — Un
2
A*Un = Uns2 — 2Un41 + Un (5.1.6)

Mun = Unsg — 3Un4z2 + 3Un41 — Un etc.

Of course you don’t actually do the infinite sum on the right-hand side of (5.1.5),
but only the first, say, p terms, thus requiring the first p differences (5.1.6) obtained
from the terms starting at up.

Euler’s transformation can be applied not only to convergent series. In some
cases it will produce accurate answers from the first terms of a series that is formally
divergent. It is widely used in the summation of asymptotic series. In this case
it is generally wise not to sum farther than where the terms start increasing in
magnitude; and you should devise some independent numerical check that the results
are meaningful.

There is an elegant and subtle implementation of Euler’s transformation due
to van Wijngaarden [1]: It incorporates the terms of the original alternating series
one at a time, in order. For each incorporation it either increases p by 1, equivalent
to computing one further difference (5.1.6), or else retroactively increases n by 1,
without having to redo all the difference calculations based on the old n value! The
decision as to which to increase, n or p, is taken in such a way as to make the
convergence most rapid. Van Wijngaarden’s technique requires only one vector of
saved partial differences. Here is the algorithm:

#include <math.h>

void eulsum(float *sum, float term, int jterm, float wksp[])
Incorporates into sum the jterm'th term, with value term, of an alternating series. sum is
input as the previous partial sum, and is output as the new partial sum. The first call to this
routine, with the first term in the series, should be with jterm=1. On the second call, term
should be set to the second term of the series, with sign opposite to that of the first call, and
jterm should be 2. And so on. wksp is a workspace array provided by the calling program,
dimensioned at least as large as the maximum number of terms to be incorporated.
{

int j;

static int nterm;

float tmp,dum;

if (jterm == 1) { Initialize:
nterm=1; Number of saved differences in wksp.
*sum=0 .5* (wksp[1]=term) ; Return first estimate.

} else {

tmp=wksp[1] ;

wksp[1]=term;

jjsenterm-1; j++) { Update saved quantities by van Wijn-
dum=wksp[j+1] ; gaarden’s algorithm.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

168 Chapter 5. Evaluation of Functions

wksp[j+1]=0.5*(wksp[j]+tmp) ;

tmp=dum ;

3

wksp [nterm+1] =0.5*(wksp[nterm]+tmp) ;

if (fabs(wksp[nterm+1]) <= fabs(wksp[nterm])) Favorable to increase p,
*sum += (0.5*wksp[++nterm]) ; and the table becomes longer.

else Favorable to increase n,
*sum += wksp[nterm+1] ; the table doesn’t become longer.

The powerful Euler technique is not directly applicable to a series of positive
terms. Occasionally it is useful to convert a series of positive terms into an alternating
series, just so that the Euler transformation can be used! Van Wijngaarden has given
a transformation for accomplishing this [1]:

oo oo
Sou = S°(-1)""'w, (5.1.7)
r=1 r=1

where

Wr = Up + Qvap + Avay + BUgr + °° + (5.1.8)

Equations (5.1.7) and (5.1.8) replace a simple sum by a two-dimensional sum, each
term in (5.1.7) being itself an infinite sum (5.1.8). This may seem a strange way to
save on work! Since, however, the indices in (5.1.8) increase tremendously rapidly,
as powers of 2, it often requires only a few terms to converge (5.1.8) to extraordinary
accuracy. You do, however, need to be able to compute the v,.’s efficiently for
“random” values r. The standard “updating” tricks for sequential r’s, mentioned
above following equation (5.1.1), can’t be used.

Actually, Euler’s transformation is a special case of a more general transforma-
tion of power series. Suppose that some known function g(z) has the series

9(2) = Yo bnz” (5.1.9)

n=0
and that you want to sum the new, unknown, series

oo

f(a = Ss Cnbnz” (5.1.10)
n=0
Then it is not hard to show (see [2]) that equation (5.1.10) can be written as

oo

f(ia= Sara) (5.1.11)

n=0

which often converges much more rapidly. Here A‘) co is the nth finite-difference
operator (equation 5.1.6), with A( cg = co, and g\” is the nth derivative of g(z).
The usual Euler transformation (equation 5.1.5 with n = 0) can be obtained, for
example, by substituting

g(z) 1 liz+2—234.-- (5.1.12)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.2 Evaluation of Continued Fractions 169

into equation (5.1.11), and then setting z = 1.

Sometimes you will want to compute a function from a series representation
even when the computation is not efficient. For example, you may be using the values
obtained to fit the function to an approximating form that you will use subsequently
(cf. §5.8). If you are summing very large numbers of slowly convergent terms, pay
attention to roundoff errors! In floating-point representation it is more accurate to
sum a list of numbers in the order starting with the smallest one, rather than starting
with the largest one. It is even better to group terms pairwise, then in pairs of pairs,
etc., so that all additions involve operands of comparable magnitude.

CITED REFERENCES AND FURTHER READING:

Goodwin, E.T. (ed.) 1961, Modern Computing Methods, 2nd ed. (New York: Philosophical Li-
brary), Chapter 13 [van Wijngaarden’s transformations]. [1]

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
Chapter 3.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §3.6.

Mathews, J., and Walker, R.L. 1970, Mathematical Methods of Physics, 2nd ed. (Reading, MA:
W.A. Benjamin/Addison-Wesley), §2.3. [2]

5.2 Evaluation of Continued Fractions

Continued fractions are often powerful ways of evaluating functions that occur
in scientific applications. A continued fraction looks like this:

ay

f(x) = bo 4
by +

(5.2.1)

a2

bot

Printers prefer to write this as

ay a2 a3 a4 a5
x bo 4 oo 5.2.2
f(x) ° by + bo + bg+ ba+ b5 4 ( )

In either (5.2.1) or (5.2.2), the a’s and b’s can themselves be functions of x, usually
linear or quadratic monomials at worst (i.e., constants times 2 or times 27). For
example, the continued fraction representation of the tangent function is

tang = — — - 5.2.3
1-— 3- 5- 7- ( )

Continued fractions frequently converge much more rapidly than power series
expansions, and in a much larger domain in the complex plane (not necessarily
including the domain of convergence of the series, however). Sometimes the
continued fraction converges best where the series does worst, although this is not

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

170 Chapter 5. Evaluation of Functions

a general rule. Blanch [1] gives a good review of the most useful convergence tests
for continued fractions.

There are standard techniques, including the important quotient-difference algo-
rithm, for going back and forth between continued fraction approximations, power
series approximations, and rational function approximations. Consult Acton [2] for
an introduction to this subject, and Fike [3] for further details and references.

How do you tell how far to go when evaluating a continued fraction? Unlike
a series, you can’t just evaluate equation (5.2.1) from left to right, stopping when
the change is small. Written in the form of (5.2.1), the only way to evaluate the
continued fraction is from right to left, first (blindly!) guessing how far out to
start. This is not the right way.

The right way is to use a result that relates continued fractions to rational
approximations, and that gives a means of evaluating (5.2.1) or (5.2.2) from left
to right. Let f, denote the result of evaluating (5.2.2) with coefficients through
a, and b,. Then

An
fn = B, (5.2.4)

where A,, and B,, are given by the following recurrence:
A_,=1 B_,=0
Ao = bo Bo=1

Aj = bj Aj-1 + ajAj_2 B; = bj Bj-1 + aj Bj_2 j =1,2,...,0
(5.2.5)

This method was invented by J. Wallis in 1655 (!), and is discussed in his Arithmetica
Infinitorum [4]. You can easily prove it by induction.

In practice, this algorithm has some unattractive features: The recurrence (5.2.5)
frequently generates very large or very small values for the partial numerators and
denominators A; and B;. There is thus the danger of overflow or underflow of the
floating-point representation. However, the recurrence (5.2.5) is linear in the A’s and
B’s. At any point you can rescale the currently saved two levels of the recurrence,
e.g., divide Aj, B;,Aj;-1, and B;_, all by B;. This incidentally makes A; = f;
and is convenient for testing whether you have gone far enough: See if f ; and f;—1
from the last iteration are as close as you would like them to be. (If B; happens to
be zero, which can happen, just skip the renormalization for this cycle. A fancier
level of optimization is to renormalize only when an overflow is imminent, saving
the unnecessary divides. All this complicates the program logic.)

Two newer algorithms have been proposed for evaluating continued fractions.
Steed’s method does not use A; and B; explicitly, but only the ratio D; = B;_1/B;.
One calculates D; and Af; = f; — fj;-1 recursively using

Dj =1/(b; + ajD;-1) (5.2.6)
Af = (0)D5 — DAFi-1 (5.2.7)

Steed’s method (see, e.g., [5]) avoids the need for rescaling of intermediate results.
However, for certain continued fractions you can occasionally run into a situation

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.2 Evaluation of Continued Fractions 171

where the denominator in (5.2.6) approaches zero, so that D; and Af; are very
large. The next Af;,1 will typically cancel this large change, but with loss of
accuracy in the numerical running sum of the f ;’s. It is awkward to program around
this, so Steed’s method can be recommended only for cases where you know in
advance that no denominator can vanish. We will use it for a special purpose in
the routine bessik (86.7).

The best general method for evaluating continued fractions seems to be the
modified Lentz’s method [6]. The need for rescaling intermediate results is avoided
by using both the ratios

Cj = Aj/Aj-1, Dj = Bj-1/B; (5.2.8)

and calculating f; by

fj = fy-1 CGD; (5.2.9)
From equation (5.2.5), one easily shows that the ratios satisfy the recurrence relations
D; =1/(b; + a;D;-1), Cj = b; +a; /Cj-1 (5.2.10)

In this algorithm there is the danger that the denominator in the expression for D ;,
or the quantity C’; itself, might approach zero. Either of these conditions invalidates
(5.2.10). However, Thompson and Barnett [5] show how to modify Lentz’s algorithm
to fix this: Just shift the offending term by a small amount, e.g., 10~°°. If you
work through a cycle of the algorithm with this prescription, you will see that f ;+1
is accurately calculated.

In detail, the modified Lentz’s algorithm is this:

e Set fo = bo; if bo = 0 set fo = tiny.

e Set Co = fo.
e Set Do = 0.
e For j = 1,

Set Dj = bj + ajDj-1.

If Dj; = 0, set Dj = tiny.

Set Cj = bj + aj/Cj-1.

If Cj = 0 set Cj = tiny.

Set Dj = 1/D;.

Set Aj = CjD;.

Set fj = fj)-1A;-

If |A; — 1| < eps then exit.
Here eps is your floating-point precision, say 10-7 or 10-5. The parameter tiny
should be less than typical values of eps|b;|, say 10~°°.

The above algorithm assumes that you can terminate the evaluation of the
continued fraction when |f; — f;—1| is sufficiently small. This is usually the case,
but by no means guaranteed. Jones [7] gives a list of theorems that can be used to
justify this termination criterion for various kinds of continued fractions.

There is at present no rigorous analysis of error propagation in Lentz’s algorithm.
However, empirical tests suggest that it is at least as good as other methods.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

172 Chapter 5. Evaluation of Functions

Manipulating Continued Fractions

Several important properties of continued fractions can be used to rewrite them
in forms that can speed up numerical computation. An equivalence transformation

Qn > An, bn 4 Abn, Anz1 > An 41 (5.2.11)

leaves the value of a continued fraction unchanged. By a suitable choice of the scale
factor A you can often simplify the form of the a’s and the b’s. Of course, you
can carry out successive equivalence transformations, possibly with different \’s, on
successive terms of the continued fraction.

The even and odd parts of a continued fraction are continued fractions whose
successive convergents are fo, and f2n+1, respectively. Their main use is that they
converge twice as fast as the original continued fraction, and so if their terms are not
much more complicated than the terms in the original there can be a big savings in
computation. The formula for the even part of (5.2.2) is

C1 c2

ven = dg + —— —— --- 5.2.12
feven = do hi be ( )
where in terms of intermediate variables
ay
ay=—
bi
an (5.2.13)
Qn = ; n>2
bn bn-1
we have
dg=bo, cr =a, di =1+ay2
(5.2.14)
Cn = —A2n-102n-2, dn =1+ A2n-1 + O2n, n>2

You can find the similar formula for the odd part in the review by Blanch [1]. Often
a combination of the transformations (5.2.14) and (5.2.11) is used to get the best
form for numerical work.

We will make frequent use of continued fractions in the next chapter.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §3.10.

Blanch, G. 1964, SIAM Review, vol. 6, pp. 383-421. [1]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 11. [2]

Cuyt, A., and Wuytack, L. 1987, Nonlinear Methods in Numerical Analysis (Amsterdam: North-
Holland), Chapter 1.

Fike, C.T. 1968, Computer Evaluation of Mathematical Functions (Englewood Cliffs, NU: Prentice-
Hall), §§8.2, 10.4, and 10.5. [3]

Wallis, J. 1695, in Opera Mathematica, vol. 1, p. 355, Oxoniae e Theatro Shedoniano. Reprinted
by Georg Olms Verlag, Hildeshein, New York (1972). [4]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.3 Polynomials and Rational Functions 173

Thompson, I.J., and Barnett, A.R. 1986, Journal of Computational Physics, vol. 64, pp. 490-509.
[5]
Lentz, W.J. 1976, Applied Optics, vol. 15, pp. 668-671. [6]

Jones, W.B. 1973, in Padé Approximants and Their Applications, P.R. Graves-Morris, ed. (Lon-
don: Academic Press), p. 125. [7]

5.3 Polynomials and Rational Functions

A polynomial of degree N is represented numerically as a stored array of
coefficients, c[j] with j= 0,...,.N. We will always take c [0] to be the constant
term in the polynomial, c [N] the coefficient of «Y ; but of course other conventions
are possible. There are two kinds of manipulations that you can do with a polynomial:
numerical manipulations (such as evaluation), where you are given the numerical
value of its argument, or algebraic manipulations, where you want to transform
the coefficient array in some way without choosing any particular argument. Let’s
start with the numerical.

We assume that you know enough never to evaluate a polynomial this way:

p=c[0]+c [1] *x+c [2] *x*xt+c [3] *x*x*x+c [4] *x¥x*x*x;

or (even worse!),

p=c[0]+c [1] *x+c [2] *pow(x,2.0)+c[3] *pow(x,3.0)+c[4] *pow(x,4.0);

Come the (computer) revolution, all persons found guilty of such criminal
behavior will be summarily executed, and their programs won’t be! It is a matter
of taste, however, whether to write

p=c [0] +x*(c [1] +x* (c [2] +x*(c [3] +x*c[4])));

or
p=(((c[4] *x+c [3] )*x+c[2])*x+c[1])*x+c[0] ;

If the number of coefficients c[0..n] is large, one writes
p=c[n] ;
for(j=n-1;j>=0;j--) p=p*xt+c[j];

or

p=c[j=n] ;
while (j>0) p=p*x+c[--j];

Another useful trick is for evaluating a polynomial P(x) and its derivative
dP(x)/dx simultaneously:

p=c[n] ;
dp=0.0;
for(j=n-1;j>=0;j--) {dp=dp*x+p; p=p*x+c[j];}

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

174 Chapter 5. Evaluation of Functions

or

while (j>0) {dp=dp*xtp; p=p*xtc[--j];}

which yields the polynomial as p and its derivative as dp.
The above trick, which is basically synthetic division [1,2], generalizes to the
evaluation of the polynomial and nd of its derivatives simultaneously:

void ddpoly(float c[], int nc, float x, float pd[], int nd)
Given the nc+1 coefficients of a polynomial of degree nc as an array c[0..nc] with c[0]
being the constant term, and given a value x, and given a value nd>1, this routine returns the
polynomial evaluated at x as pd[0] and nd derivatives as pd[1..nd].
{

int nnd,j,i;

float cnst=1.0;

pd[0]=c[nc] ;
for (j=1;j<=nd;j++) pd[j]=0.0;
for (i=nc-1;i>=0;i--) {
nnd=(nd < (nc-i) ? nd : nc-i);
for (j=nnd;j>=1;j--)
pda[jJ=pd[j]*x+pda[j-1];
pd[0]=pd[0] *x+c [i] ;

for (i=2;i<=nd;it+) { After the first derivative, factorial constants come in.
cnst

pd[i] *= cnst;

As a curiosity, you might be interested to know that polynomials of degree
n > 3 can be evaluated in fewer than n multiplications, at least if you are willing
to precompute some auxiliary coefficients and, in some cases, do an extra addition.
For example, the polynomial

P(x) = ao + ayx + ayn? + agar? + aga* (5.3.1)

where a4 > 0, can be evaluated with 3 multiplications and 5 additions as follows:

P(a) = [(Av + B)? + Av + C][(Av + B)? + D) +E (5.3.2)

where A, B,C, D, and E are to be precomputed by

A= (a4)'/4
_ a3 — As
B= TAB
D =3B?+8p%4 U4 ue (5.3.3)
ag >}
c= 4 ~2B-6B*-D

E =a) — B*-— B?(C + D)-CD

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.3 Polynomials and Rational Functions 175

Fifth degree polynomials can be evaluated in 4 multiplies and 5 adds; sixth degree
polynomials can be evaluated in 4 multiplies and 7 adds; if any of this strikes
you as interesting, consult references [3-5]. The subject has something of the same
entertaining, if impractical, flavor as that of fast matrix multiplication, discussed
in §2.11.

Turn now to algebraic manipulations. You multiply a polynomial of degree
n — 1 (array of range [0..n-1]) by a monomial factor x — a by a bit of code
like the following,

¢e[n]=c[n-1];
-1;j>=1;j--) clj]=clj-1]-cljl]*a;
= (a);

Likewise, you divide a polynomial of degree n by a monomial factor x — a
(synthetic division again) using

rem=c[n] ;

c[n]=0.0;

for(i=n-1;i>=0;i--) {
swap=c [i];
c{i]=rem;
rem=swap+rem*a;

which leaves you with a new polynomial array and a numerical remainder rem.

Multiplication of two general polynomials involves straightforward summing
of the products, each involving one coefficient from each polynomial. Division of
two general polynomials, while it can be done awkwardly in the fashion taught using
pencil and paper, is susceptible to a good deal of streamlining. Witness the following
routine based on the algorithm in [3].

void poldiv(float u[], int n, float v[], int nv, float q[], float r[])

Given the n+1 coefficients of a polynomial of degree n in u[0..n], and the nv+1 coefficients
of another polynomial of degree nv in v[0..nv], divide the polynomial u by the polynomial
v (“u"/“v") giving a quotient polynomial whose coefficients are returned in q(0..n], and a
remainder polynomial whose coefficients are returned in r[0..n]. The elements r[nv. .n]
and q[n-nv+1..n] are returned as zero.

{

for (k=n-nv;k>=0;k--) {

q([k] =r (nv+k] /v [nv] ;

for (j=nv+k-1;j>=k;j--) rfj] -= q{k]*v[j-k];
}
for (j=nv;j<=n;j++) r[j]=0.0;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

176 Chapter 5. Evaluation of Functions

Rational Functions
You evaluate a rational function like

P(x) — pot pit +---+ pyar

R(a
(2) Quiz) go tua +--+ qe"

(5.3.4)

in the obvious way, namely as two separate polynomials followed by a divide. As
a matter of convention one usually chooses go = 1, obtained by dividing numerator
and denominator by any other go. It is often convenient to have both sets of
coefficients stored in a single array, and to have a standard function available for
doing the evaluation:

double ratval(double x, double cof[], int mm, int kk)
Given mm, kk, and cof [0..mm+kk], evaluate and return the rational function (cof [0] +

cof [1]x +--+ + cof [mm] x™)/(1 + cof [mm+i]x +--- + cof [mm+kk] x**),

{
int j;
double sumd,sumn; Note precision! Change to float if desired.
for (sumn=cof [mm] , j=mm-1;j>=0;j--) sumn=sumn*x+cof [j];

for (sumd=0.0, j=mm+kk;j>=mm+1;j--) sumd=(sumd+cof [j])*x;
return sumn/(1.0+sumd) ;

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 183, 190. [1]

Mathews, J., and Walker, R.L. 1970, Mathematical Methods of Physics, 2nd ed. (Reading, MA:
W.A. Benjamin/Addison-Wesley), pp. 361-363. [2]

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), $4.6. [3]

Fike, C.T. 1968, Computer Evaluation of Mathematical Functions (Englewood Cliffs, NU: Prentice-
Hall), Chapter 4.

Winograd, S. 1970, Communications on Pure and Applied Mathematics, vol. 23, pp. 165-179. [4]
Kronsjé, L. 1987, Algorithms: Their Complexity and Efficiency, 2nd ed. (New York: Wiley). [5]

5.4 Complex Arithmetic

As we mentioned in §1.2, the lack of built-in complex arithmetic in C is a
nuisance for numerical work. Even in languages like FORTRAN that have complex
data types, it is disconcertingly common to encounter complex operations that
produce overflows or underflows when both the complex operands and the complex
result are perfectly representable. This occurs, we think, because software companies
assign inexperienced programmers to what they believe to be the perfectly trivial
task of implementing complex arithmetic.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.4 Complex Arithmetic 177

Actually, complex arithmetic is not quite trivial. Addition and subtraction
are done in the obvious way, performing the operation separately on the real and
imaginary parts of the operands. Multiplication can also be done in the obvious way,
with 4 multiplications, one addition, and one subtraction,

(a + ib)(c + id) = (ac — bd) + i(be + ad) (5.4.1)

(the addition before the i doesn’t count; it just separates the real and imaginary parts
notationally). But it is sometimes faster to multiply via

(a + ib)(c + id) = (ac — bd) + i{(a + b)(c + d) — ac — bd] (5.4.2)

which has only three multiplications (ac, bd, (a + b)(c + d)), plus two additions and
three subtractions. The total operations count is higher by two, but multiplication
is a slow operation on some machines.

While it is true that intermediate results in equations (5.4.1) and (5.4.2) can
overflow even when the final result is representable, this happens only when the final
answer is on the edge of representability. Not so for the complex modulus, if you
are misguided enough to compute it as

lat ib] = Vaz +0? (bad!) (5.4.3)

whose intermediate result will overflow if either a or b is as large as the square
root of the largest representable number (e.g., 10 '® as compared to 103%). The right
way to do the calculation is

saa) _flelVLEOLE lal >
Ja + | {peer lal < [a 644)

Complex division should use a similar trick to prevent avoidable overflows,
underflow, or loss of precision,

[a + b(d/c)] + é[b— a(d/e)]

atib_ e+ d(d/e) lel 2 lal (545)
ct+id [a(c/d) + b] + i[b(c/d) — a] lel < lal _
c(c/d) +d °

Of course you should calculate repeated subexpressions, like c/d or d/c, only once.

Complex square root is even more complicated, since we must both guard
intermediate results, and also enforce a chosen branch cut (here taken to be the
negative real axis). To take the square root of c + id, first compute

0 c=d=0

Vit VEE? ig >
Cc VJ c/d)?
vay ela VEEP |c| < |

(5.4.6)

w=

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

178 Chapter 5. Evaluation of Functions

Then the answer is

0 w=0
wti d #0,c>0
U a w sc>
Vetid= 547
crt iy w#0,c<0,d>0 ( )
2w
ld)
iw wH#0,c<0,d<0
2w

Routines implementing these algorithms are listed in Appendix C.

CITED REFERENCES AND FURTHER READING:
Midy, P., and Yakovlev, Y. 1991, Mathematics and Computers in Simulation, vol. 33, pp. 33-49.

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley) [see solutions to exercises 4.2.1.16 and 4.6.4.41].

5.5 Recurrence Relations and Clenshaw’s
Recurrence Formula

Many useful functions satisfy recurrence relations, e.g.,

(n+ 1)Pr4i(x) = (2n + 1)rP, (x) — nPp-i (2) (5.5.1)
2n

In41(2) = —In() ~ In—1(2) (5.5.2)

nEn+1(x) =e” — cE, (2) (5.5.3)

cosn6 = 2cos@ cos(n — 1)6 — cos(n — 2)4 (5.5.4)

sinné = 2cos@sin(n — 1)@ — sin(n — 2)0 (5.5.5)

where the first three functions are Legendre polynomials, Bessel functions of the first
kind, and exponential integrals, respectively. (For notation see [1].) These relations
are useful for extending computational methods from two successive values of n to
other values, either larger or smaller.

Equations (5.5.4) and (5.5.5) motivate us to say a few words about trigonometric
functions. If your program’s running time is dominated by evaluating trigonometric
functions, you are probably doing something wrong. Trig functions whose arguments
form a linear sequence 0 = 09 + nd, n = 0,1,2,..., are efficiently calculated by
the following recurrence,

cos(9 + 5) = cos@ — [acosé + J sin 6]
(5.5.6)

sin(@ + 6) = sin@ — [asin 6 — Bcos 6]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.5 Recurrence Relations and Clenshaw’s Recurrence Formula 179

where a and 3 are the precomputed coefficients
3 afd _.
a = 2sin 5 B=sind (5.5.7)

The reason for doing things this way, rather than with the standard (and equivalent)
identities for sums of angles, is that here a and @ do not lose significance if the
incremental 6 is small. Likewise, the adds in equation (5.5.6) should be done in
the order indicated by square brackets. We will use (5.5.6) repeatedly in Chapter
12, when we deal with Fourier transforms.

Another trick, occasionally useful, is to note that both sin and cos @ can be
calculated via a single call to tan:

0 1-? 2t
t=tan | - cos 0 sin? = (5.5.8)
2 14+ 14+

The cost of getting both sin and cos, if you need them, is thus the cost of tan plus
2 multiplies, 2 divides, and 2 adds. On machines with slow trig functions, this can
be a savings. However, note that special treatment is required if 9 + +7. And also
note that many modern machines have very fast trig functions; so you should not
assume that equation (5.5.8) is faster without testing.

Stability of Recurrences

You need to be aware that recurrence relations are not necessarily stable
against roundoff error in the direction that you propose to go (either increasing n or
decreasing n). A three-term linear recurrence relation

Yn+1 + AnYn + bnYn—1 = 0, n=1,2,... (5.5.9)

has two linearly independent solutions, f;, and g,, say. Only one of these corresponds
to the sequence of functions f;, that you are trying to generate. The other one gn,
may be exponentially growing in the direction that you want to go, or exponentially
damped, or exponentially neutral (growing or dying as some power law, for example).
If it is exponentially growing, then the recurrence relation is of little or no practical
use in that direction. This is the case, e.g., for (5.5.2) in the direction of increasing
n, when a < mn. You cannot generate Bessel functions of high n by forward
recurrence on (5.5.2).
To state things a bit more formally, if

fr/gn 70 as n—-0o (5.5.10)

then f,, is called the minimal solution of the recurrence relation (5.5.9). Nonminimal
solutions like g,, are called dominant solutions. The minimal solution is unique, if it
exists, but dominant solutions are not — you can add an arbitrary multiple of f,, to
a given g,. You can evaluate any dominant solution by forward recurrence, but not
the minimal solution. (Unfortunately it is sometimes the one you want.)
Abramowitz and Stegun (in their Introduction) [1] give a list of recurrences that
are stable in the increasing or decreasing directions. That list does not contain all

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

180 Chapter 5. Evaluation of Functions

possible formulas, of course. Given a recurrence relation for some function f ,, (2)
you can test it yourself with about five minutes of (human) labor: For a fixed x
in your range of interest, start the recurrence not with true values of f ;(a) and
fj+1(a), but (first) with the values 1 and 0, respectively, and then (second) with
0 and 1, respectively. Generate 10 or 20 terms of the recursive sequences in the
direction that you want to go (increasing or decreasing from 7), for each of the two
starting conditions. Look at the difference between the corresponding members of
the two sequences. If the differences stay of order unity (absolute value less than
10, say), then the recurrence is stable. If they increase slowly, then the recurrence
may be mildly unstable but quite tolerably so. If they increase catastrophically, then
there is an exponentially growing solution of the recurrence. If you know that the
function that you want actually corresponds to the growing solution, then you can
keep the recurrence formula anyway e.g., the case of the Bessel function Y;, (x) for
increasing n, see §6.5; if you don’t know which solution your function corresponds
to, you must at this point reject the recurrence formula. Notice that you can do this
test before you go to the trouble of finding a numerical method for computing the
two starting functions f;(x) and f;+1(2): stability is a property of the recurrence,
not of the starting values.

An alternative heuristic procedure for testing stability is to replace the recur-
rence relation by a similar one that is linear with constant coefficients. For example,
the relation (5.5.2) becomes

Yn+1 — 2VYn + Yn—1 = 0 (5.5.11)

where y = n/x is treated as a constant. You solve such recurrence relations
by trying solutions of the form yn = a”. Substituting into the above recur-
rence gives

a’? —2ya+1=0 o a=ytV7-1 (5.5.12)

The recurrence is stable if |a| < 1 for all solutions a. This holds (as you can verify)
if |y| < Lorn < x. The recurrence (5.5.2) thus cannot be used, starting with Jo (x)
and J;(x), to compute J,,(x) for large n.

Possibly you would at this point like the security of some real theorems on
this subject (although we ourselves always follow one of the heuristic procedures).
Here are two theorems, due to Perron [2]:

TheoremA. If in (5.5.9) an ~ an®, by ~ bn® as n > 00, and 3 < 2a, then

gn4i/9n~ —an®, fail fn ~ —(b/a)n®-% (5.5.13)

and f,, is the minimal solution to (5.5.9).
Theorem B. Under the same conditions as Theorem A, but with G = 2a,
consider the characteristic polynomial

P+at+b=0 (5.5.14)
If the roots t; and tz of (5.5.14) have distinct moduli, |t;| > |t2| say, then

In+i/9n ~ tin, fr4i/fn ~ ten™ (5.5.15)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.5 Recurrence Relations and Clenshaw’s Recurrence Formula 181

and f, is again the minimal solution to (5.5.9). Cases other than those in these
two theorems are inconclusive for the existence of minimal solutions. (For more
on the stability of recurrences, see [3].)

How do you proceed if the solution that you desire is the minimal solution? The
answer lies in that old aphorism, that every cloud has a silver lining: If a recurrence
relation is catastrophically unstable in one direction, then that (undesired) solution
will decrease very rapidly in the reverse direction. This means that you can start
with any seed values for the consecutive fj; and f;,1 and (when you have gone
enough steps in the stable direction) you will converge to the sequence of functions
that you want, times an unknown normalization factor. If there is some other way
to normalize the sequence (e.g., by a formula for the sum of the f,,’s), then this
can be a practical means of function evaluation. The method is called Miller’s
algorithm. An example often given [1,4] uses equation (5.5.2) in just this way, along
with the normalization formula

1 = Jo(a) + 2Jo(a) + 24(x) + 26 (x) ++ (5.5.16)

Incidentally, there is an important relation between three-term recurrence
relations and continued fractions. Rewrite the recurrence relation (5.5.9) as

b
Yn =n (5.5.17)
Yn-1 An + Yn+1/Yn
Iterating this equation, starting with n, gives
Yn bn bn (5.5.18)
Yn-1 an — Anti —

Pincherle’s Theorem [2] tells us that (5.5.18) converges if and only if (5.5.9) has a
minimal solution f,,, in which case it converges to f/ fn—1. This result, usually for
the case n = 1 and combined with some way to determine jf, underlies many of the
practical methods for computing special functions that we give in the next chapter.

Clenshaw’s Recurrence Formula

Clenshaw’s recurrence formula [5] is an elegant and efficient way to evaluate a
sum of coefficients times functions that obey a recurrence formula, e.g.,

N N

£0) = So cxecoskO or f(x) = So ce Pe (2)

k=0 k=0
Here is how it works: Suppose that the desired sum is

N

= Vo Fe(x) (5.5.19)

k=0

f(s

and that F}, obeys the recurrence relation

Fy4i(2) = a(n, x) Fy (x) + B(n, x) Fr—1 (2) (5.5.20)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

182 Chapter 5. Evaluation of Functions

for some functions a(n,x) and G(n,x). Now define the quantities y, (k =
N,N —1,...,1) by the following recurrence:

YN+2 = ynq1 = 0

Ye = Ok, ©)yps1 + B(K+1,2)yeso+cen (k= N,N —-1,...,1)

If you solve equation (5.5.21) for cz on the left, and then write out explicitly the
sum (5.5.19), it will look (in part) like this:

f(x) =:
Ys — a(8,2)y9 — 2(9, 2) y10] F(x)
yr — a(7,x)ys — B(8, x)yo]F7(x)
yo — a(6,x)y7 — B(7, x) ys] Fo(x)
ys — a(5,2)ys — B(6, )y7]F5(x) (5.5.22)

yo — a(2, x)y3 — B(3, x)ya] Fo (x)
yi — a(1,x)y2 — B(2, x) ys] Fi (a)
co + B(1, 2)y2 — B(1, 2) y2]Fo(x)

+ 4+ t+ + + + + 4+

Notice that we have added and subtracted (1, )y2 in the last line. If you examine
the terms containing a factor of yg in (5.5.22), you will find that they sum to zero as
a consequence of the recurrence relation (5.5.20); similarly all the other y ;,’s down
through yz. The only surviving terms in (5.5.22) are

f(x) = BC, x) Fo(«)y2 + Fi(x)yi + Fo(x)co (5.5.23)

Equations (5.5.21) and (5.5.23) are Clenshaw’s recurrence formula for doing the
sum (5.5.19): You make one pass down through the y;’s using (5.5.21); when you
have reached y2 and y; you apply (5.5.23) to get the desired answer.

Clenshaw’s recurrence as written above incorporates the coefficients c, in a
downward order, with k decreasing. At each stage, the effect of all previous c;,’s
is “remembered” as two coefficients which multiply the functions F,4; and Fy,
(ultimately Fo and F). If the functions F;, are small when k is large, and if the
coefficients c;, are small when k is small, then the sum can be dominated by small
Fj,’s. In this case the remembered coefficients will involve a delicate cancellation
and there can be a catastrophic loss of significance. An example would be to sum
the trivial series

Jis(1) =0 x Jo(1) $0 x (1) +... 40% Aa $1 x Jis(1) (5.5.24)

Here J5, which is tiny, ends up represented as a canceling linear combination of
Jo and J;, which are of order unity.

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

5.6 Quadratic and Cubic Equations 183

The solution in such cases is to use an alternative Clenshaw recurrence that
incorporates c;,’s in an upward direction. The relevant equations are

y2 =y-1=0 (5.5.25)
Uk = aaa —a(k,x)yr—1 — cr];

(k =0,1,...,N—1) (5.5.26)

f(x) = w(x) — B(N,a2)Fw—i(x)yn-1 — Fr (x)yn-2 (5.5.27)

The rare case where equations (5.5.25)-(5.5.27) should be used instead of
equations (5.5.21) and (5.5.23) can be detected automatically by testing whether
the operands in the first sum in (5.5.23) are opposite in sign and nearly equal in
magnitude. Other than in this special case, Clenshaw’s recurrence is always stable,
independent of whether the recurrence for the functions F', is stable in the upward
or downward direction.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), pp. xiii, 697. [1]

Gautschi, W. 1967, SIAM Review, vol. 9, pp. 24-82. [2]

Lakshmikantham, V., and Trigiante, D. 1988, Theory of Difference Equations: Numerical Methods
and Applications (San Diego: Academic Press). [3]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 20ff. [4]

Clenshaw, C.W. 1962, Mathematical Tables, vol. 5, National Physical Laboratory (London: H.M.
Stationery Office). [5]

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§4.4.3, p. 111.

Goodwin, E.T. (ed.) 1961, Modern Computing Methods, 2nd ed. (New York: Philosophical Li-
brary), p. 76.

5.6 Quadratic and Cubic Equations

The roots of simple algebraic equations can be viewed as being functions of the
equations’ coefficients. We are taught these functions in elementary algebra. Yet,
surprisingly many people don’t know the right way to solve a quadratic equation
with two real roots, or to obtain the roots of a cubic equation.

There are two ways to write the solution of the quadratic equation
ax? +ba+c=0 (5.6.1)

with real coefficients a, b,c, namely

Vb? — dac
2a

(5.6.2)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

184 Chapter 5. Evaluation of Functions

and 9
iC
¢ = ——_____. 5.6.3
* —b+ Vb? — 4ac ( )

If you use either (5.6.2) or (5.6.3) to get the two roots, you are asking for trouble:
If either a or c (or both) are small, then one of the roots will involve the subtraction
of b from a very nearly equal quantity (the discriminant); you will get that root very
inaccurately. The correct way to compute the roots is

1
g=-5 [e + sgn(b) /02 — 4ac| (5.64)
Then the two roots are
a=! and =F (5.6.5)
q

If the coefficients a, b, c, are complex rather than real, then the above formulas
still hold, except that in equation (5.6.4) the sign of the square root should be
chosen so as to make

Re(b*\/b? — 4ac) > 0 (5.6.6)
where Re denotes the real part and asterisk denotes complex conjugation.
Apropos of quadratic equations, this seems a convenient place to recall that

the inverse hyperbolic functions sinh! and cosh™* are in fact just logarithms of
solutions to such equations,

sinh7!(z) = In(a + V2? +1) (5.6.7)
cosh™! (x) = +In(x + V2? — 1) (5.6.8)

Equation (5.6.7) is numerically robust for « > 0. For negative x, use the symmetry
sinh! (-2) =—- sinh”! (2). Equation (5.6.8) is of course valid only for x > 1.

For the cubic equation
xe +ax? +br+c=0 (5.6.9)
with real or complex coefficients a, b,c, first compute

a? — 3b 2a® — 9ab + 27¢
Q= = and R —sar

(5.6.10)

If Q and R are real (always true when a, b,c are real) and R? < Q°, then the cubic
equation has three real roots. Find them by computing

0 = arecos(R/1/Q?) (5.6.11)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.6 Quadratic and Cubic Equations 185

in terms of which the three roots are

n= -2y/Qeos(2) - $
3 3
2 = —2 Geos (AP) 8 (5.6.12)
3 3
0-2
rz = —-2 Qeos 3) -$

(This equation first appears in Chapter VI of Francois Viéte’s treatise “De emen-
datione,’ published in 1615!)
Otherwise, compute

1/3
A=-|R+ VP —@| (5.6.13)
where the sign of the square root is chosen to make

Re(R*/R? — Q3) > 0 (5.6.14)

(asterisk again denoting complex conjugation). If Q and R are both real, equations
(5.6.13)-(5.6.14) are equivalent to

A =~sen(R) [\R| + VP —]” (5.6.15)

where the positive square root is assumed. Next compute

_fQ/A (A#0)
B= {i (A=0) (5.6.16)
in terms of which the three roots are
at =(A+B)-5 (5.6.17)
(the single real root when a, b,c are real) and
1 q
a =--(A+B)— "4 BA B)
: 3 A (5.6.18)
a Bs
v3 =—-=(A+ B)—-=-i—(A-B
w3 = —5(A + B)— 3 —i( )

(in that same case, a complex conjugate pair). Equations (5.6.13)-(5.6.16) are
arranged both to minimize roundoff error, and also (as pointed out by A.J. Glassman)
to ensure that no choice of branch for the complex cube root can result in the
spurious loss of a distinct root.

If you need to solve many cubic equations with only slightly different coeffi-
cients, it is more efficient to use Newton’s method (§9.4).

CITED REFERENCES AND FURTHER READING:

Weast, R.C. (ed.) 1967, Handbook of Tables for Mathematics, 3rd ed. (Cleveland: The Chemical
Rubber Co.), pp. 130-133.

Pachner, J. 1983, Handbook of Numerical Analysis Applications (New York: McGraw-Hill), §6.1.

McKelvey, J.P. 1984, American Journal of Physics, vol. 52, pp. 269-270; see also vol. 53, p. 775,
and vol. 55, pp. 374-375.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

186 Chapter 5. Evaluation of Functions

5.7 Numerical Derivatives

Imagine that you have a procedure which computes a function f(x), and now
you want to compute its derivative f’(a). Easy, right? The definition of the
derivative, the limit as h — 0 of

Pa) ~ L@tW = FO) 6571)
h
practically suggests the program: Pick a small value h; evaluate f(a +h); you
probably have f(x) already evaluated, but if not, do it too; finally apply equation
(5.7.1). What more needs to be said?

Quite a lot, actually. Applied uncritically, the above procedure is almost
guaranteed to produce inaccurate results. Applied properly, it can be the right way
to compute a derivative only when the function f is fiercely expensive to compute,
when you already have invested in computing f(x), and when, therefore, you want
to get the derivative in no more than a single additional function evaluation. In such
a situation, the remaining issue is to choose h properly, an issue we now discuss:

There are two sources of error in equation (5.7.1), truncation error and roundoff
error. The truncation error comes from higher terms in the Taylor series expansion,

fle +h) = fla) +hy"(a) + Sh P"a) + EHMa)t-— —(5:72)
whence
fle+h) =f) — Lp any
1 ft shh" + (5.73)

The roundoff error has various contributions. First there is roundoff error in h:
Suppose, by way of an example, that you are at a point x = 10.3 and you blindly
choose h = 0.0001. Neither x = 10.3 nor 2 +h = 10.30001 is a number with
an exact representation in binary; each is therefore represented with some fractional
error characteristic of the machine’s floating-point format, € ,,, whose value in single
precision may be ~ 10~*. The error in the effective value of h, namely the difference
between x + h and x as represented in the machine, is therefore on the order of € m2,
which implies a fractional error in h of order ~ €,2/h ~ 107?! By equation (5.7.1)
this immediately implies at least the same large fractional error in the derivative.
We arrive at Lesson 1: Always choose h so that x + A and x differ by an exactly
representable number. This can usually be accomplished by the program steps

temp=xr+h

(5.7.4)
h=temp—2x

Some optimizing compilers, and some computers whose floating-point chips have
higher internal accuracy than is stored externally, can foil this trick; if so, it is
usually enough to declare temp as volatile, or else to call a dummy function
donothing (temp) between the two equations (5.7.4). This forces temp into and
out of addressable memory.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.7 Numerical Derivatives 187

With fh an “exact” number, the roundoff error in equation (5.7.1) is e, ~
es|f(a)/h|. Here ey is the fractional accuracy with which f is computed; for a
simple function this may be comparable to the machine accuracy, € ¢ © €m, but for a
complicated calculation with additional sources of inaccuracy it may be larger. The
truncation error in equation (5.7.3) is on the order of e, ~ |hf’’(x)|. Varying h to
minimize the sum e, + e; gives the optimal choice of h,

h~,| a wm fEfite (5.75)

where x, = (f/f’)'/? is the “curvature scale” of the function f, or “characteristic
scale” over which it changes. In the absence of any other information, one often
assumes a, = x (except near = 0 where some other estimate of the typical x
scale should be used).

With the choice of equation (5.7.5), the fractional accuracy of the computed
derivative is

(er ben/IFL~ VRP PY? ~ VE (5.76)

Here the last order-of-magnitude equality assumes that f, f’, and f” all share
the same characteristic length scale, usually the case. One sees that the simple
finite-difference equation (5.7.1) gives at best only the square root of the machine
accuracy €m.

If you can afford two function evaluations for each derivative calculation, then
it is significantly better to use the symmetrized form

fla +h) ~ f(e—h)

(a) © 5.77
f(x) = (5.7)
In this case, by equation (5.7.2), the truncation error is e, ~ h?f’”. The roundoff
error e€,. is about the same as before. The optimal choice of h, by a short calculation

analogous to the one above, is now

. enf aa cag
h (+) (ep) '/3.x, (5.78)

and the fractional error is

(er + en)/IP 1 ~ (en POG YE /F ~ (EP? (5.7.9)

which will typically be an order of magnitude (single precision) or two orders of
magnitude (double precision) better than equation (5.7.6). We have arrived at Lesson
2: Choose h to be the correct power of € ¢ or €m times a characteristic scale x.

You can easily derive the correct powers for other cases [1]. For a function of
two dimensions, for example, and the mixed derivative formula

of [f(@th,y+h)—f(c+h,y—h)] -[f(e@—h,y+h) — f(x@—h,y—h)|

OxdOy 4h?
(5.7.10)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

188 Chapter 5. Evaluation of Functions

the correct scaling is h ~ ef! te.

It is disappointing, certainly, that no simple finite-difference formula like
equation (5.7.1) or (5.7.7) gives an accuracy comparable to the machine accuracy
€m, or even the lower accuracy to which f is evaluated, «r. Are there no better
methods?

Yes, there are. All, however, involve exploration of the function’s behavior over
scales comparable to x,., plus some assumption of smoothness, or analyticity, so that
the high-order terms in a Taylor expansion like equation (5.7.2) have some meaning.
Such methods also involve multiple evaluations of the function f, so their increased
accuracy must be weighed against increased cost.

The general idea of “Richardson’s deferred approach to the limit” is particularly
attractive. For numerical integrals, that idea leads to so-called Romberg integration
(for review, see $4.3). For derivatives, one seeks to extrapolate, to h — 0, the result
of finite-difference calculations with smaller and smaller finite values of h. By the
use of Neville’s algorithm (§3.1), one uses each new finite-difference calculation to
produce both an extrapolation of higher order, and also extrapolations of previous,
lower, orders but with smaller scales h. Ridders [2] has given a nice implementation
of this idea; the following program, dfridr, is based on his algorithm, modified by
an improved termination criterion. Input to the routine is a function f (called func),
a position x, and a largest stepsize h (more analogous to what we have called x.
above than to what we have called h). Output is the returned value of the derivative,
and an estimate of its error, err.

#include <math.h>

#include "nrutil.h"

#define CON 1.4 Stepsize is decreased by CON at each iteration.
#tdefine CON2 (CON*CON)

#define BIG 1.0e30

#define NTAB 10 Sets maximum size of tableau.
#define SAFE 2.0 Return when error is SAFE worse than the best so
far.

float dfridr(float (*func) (float), float x, float h, float *err)
Returns the derivative of a function func at a point x by Ridders’ method of polynomial
extrapolation. The value h is input as an estimated initial stepsize; it need not be small, but
rather should be an increment in x over which func changes substantially. An estimate of the
error in the derivative is returned as err.
t

int i,j;

float errt,fac,hh,**a,ans;

if (h == 0.0) nrerror("h must be nonzero in dfridr.");
a=matrix(1,NTAB,1,NTAB);

hh=h;

a(1] (1]=((*func) (x+hh)-(*func) (x-hh) )/(2.0*hh) ;

*err=BIG;

for (i=2;i<=NTAB;i++) {

Successive columns in the Neville tableau will go to smaller stepsizes and higher orders of
extrapolation.

hh /= CON;

a1] [i] =((*func) (x+hh) -(*func) (x-hh))/(2.0*hh) ; Try new, smaller step-

fac=CON2; size.

for (j=2;j<-isj++) { Compute extrapolations of various orders, requiring
aljJ] (iJ=(alj-1] [i] *fac-a[j-1] [i-1])/(fac-1.0); no new function eval-
fac=CON2*fac; uations.

errt=FMAX (fabs (a[j] [i]-alj-1] [i]) , fabs (alj] [i]-alj-1] [i-1]));

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.7 Numerical Derivatives 189

The error strategy is to compare each new extrapolation to one order lower, both
at the present stepsize and the previous one.
if (errt <= *err) { If error is decreased, save the improved answer.
*err=errt ;
ans=a[j] [il];
3
}
if (fabs(ali] [i]-ali-1] [i-1]) >= SAFE*(*err)) break;
If higher order is worse by a significant factor SAFE, then quit early.
}
free_matrix(a,1,NTAB,1,NTAB) ;
return ans;

In dfridr, the number of evaluations of func is typically 6 to 12, but is allowed
to be as great as 2xNTAB. As a function of input h, it is typical for the accuracy
to get better as h is made larger, until a sudden point is reached where nonsensical
extrapolation produces early return with a large error. You should therefore choose
a fairly large value for h, but monitor the returned value err, decreasing h if it is
not small. For functions whose characteristic x scale is of order unity, we typically
take h to be a few tenths.

Besides Ridders’ method, there are other possible techniques. If your function
is fairly smooth, and you know that you will want to evaluate its derivative many
times at arbitrary points in some interval, then it makes sense to construct a
Chebyshev polynomial approximation to the function in that interval, and to evaluate
the derivative directly from the resulting Chebyshev coefficients. This method is
described in §§5.8-5.9, following.

Another technique applies when the function consists of data that is tabulated
at equally spaced intervals, and perhaps also noisy. One might then want, at each
point, to least-squares fit a polynomial of some degree M, using an additional
number n,, of points to the left and some number 7 pz of points to the right of each
desired x value. The estimated derivative is then the derivative of the resulting
fitted polynomial. A very efficient way to do this construction is via Savitzky-Golay
smoothing filters, which will be discussed later, in §14.8. There we will give a
routine for getting filter coefficients that not only construct the fitting polynomial but,
in the accumulation of a single sum of data points times filter coefficients, evaluate
it as well. In fact, the routine given, savgol, has an argument 1d that determines
which derivative of the fitted polynomial is evaluated. For the first derivative, the
appropriate setting is 1d=1, and the value of the derivative is the accumulated sum
divided by the sampling interval h.

CITED REFERENCES AND FURTHER READING:

Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Englewood Cliffs, NJ: Prentice-Hall), §§5.4-5.6. [1]

Ridders, C.J.F. 1982, Advances in Engineering Software, vol. 4, no. 2, pp. 75-76. [2]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

190 Chapter 5. Evaluation of Functions

5.8 Chebyshev Approximation

The Chebyshev polynomial of degree n is denoted T',(z), and is given by
the explicit formula

T,,(x) = cos(n arccos 2) (5.8.1)

This may look trigonometric at first glance (and there is in fact a close relation
between the Chebyshev polynomials and the discrete Fourier transform); however
(5.8.1) can be combined with trigonometric identities to yield explicit expressions
for T,,(a) (see Figure 5.8.1),

= 4x? — 32 (5.8.2)

Tn4i(@) = 2aTp(x) —Tra(x) nl.

(There also exist inverse formulas for the powers of x in terms of the T’,,’s — see
equations 5.11.2-5.11.3.)

The Chebyshev polynomials are orthogonal in the interval [—1, 1] over a weight
(1 — a?)~1/?. In particular,

TAT), fo, 5*4
I, OE ae = {a ining (5.8.3)

The polynomial T,,(a) has n zeros in the interval [—1, 1], and they are located
at the points

n

k-4
v= cos (7 21) k=1,2,...,n (5.8.4)
In this same interval there are n + 1 extrema (maxima and minima), located at
k
n

At all of the maxima T;,(z) = 1, while at all of the minima T,,(x) = —1;
it is precisely this property that makes the Chebyshev polynomials so useful in
polynomial approximation of functions.

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

5.8 Chebyshev Approximation 19

Chebyshev polynomials

Figure 5.8.1. Chebyshev polynomials To (a) through Tg (a). Note that Tj has j roots in the interval
(—1, 1) and that all the polynomials are bounded between +1.

The Chebyshev polynomials satisfy a discrete orthogonality relation as well as
the continuous one (5.8.3): If x, (k = 1,...,m) are the m zeros of T,,(x) given
by (5.8.4), and if 1,7 < m, then

m 0 iAj
SO Ti (xe )D; (we) = {me i=j#0 (5.8.6)
k=1

m i=j=0

It is not too difficult to combine equations (5.8.1), (5.8.4), and (5.8.6) to prove
the following theorem: If f(x) is an arbitrary function in the interval [—1, 1], and if
N coefficients c;,7 = 0,...,.N — 1, are defined by

a &
6) = 2D Fle )T (ee)
a 2) 4) (58.7)
~25 |) cos mk — 3) cos milk = 3)
vei (eee)
then the approximation formula
N-1 1
k=0

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD
(S-801€b-1ZS-0 NES!) DNILAdNOO OISILNSIOS 4O LHW SHL !O NI SAdIO4Y TWOIMSWNN Woy aBed ejdwes

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

192 Chapter 5. Evaluation of Functions

is exact for x equal to all of the N zeros of T'n(z).

For a fixed N, equation (5.8.8) is a polynomial in x which approximates the
function f(a) in the interval [—1, 1] (where all the zeros of Ty (x) are located). Why
is this particular approximating polynomial better than any other one, exact on some
other set of N points? The answer is not that (5.8.8) is necessarily more accurate
than some other approximating polynomial of the same order N (for some specified
definition of “accurate’’), but rather that (5.8.8) can be truncated to a polynomial of
lower degree m < N ina very graceful way, one that does yield the “most accurate”
approximation of degree m (in a sense that can be made precise). Suppose N is
so large that (5.8.8) is virtually a perfect approximation of f(z). Now consider
the truncated approximation

m-1
1

‘(a) & Cel (x)| — =c0 5.8.9

fle) =| Yo exi(a)] — 5 (58.9)
with the same c;’s, computed from (5.8.7). Since the T,(a)’s are all bounded
between +1, the difference between (5.8.9) and (5.8.8) can be no larger than the
sum of the neglected c,’s (k = m,...,N — 1). In fact, if the c;,,’s are rapidly
decreasing (which is the typical case), then the error is dominated by CmTn(x),
an oscillatory function with m+ 1 equal extrema distributed smoothly over the
interval [—1, 1]. This smooth spreading out of the error is a very important property:
The Chebyshev approximation (5.8.9) is very nearly the same polynomial as that
holy grail of approximating polynomials the minimax polynomial, which (among all
polynomials of the same degree) has the smallest maximum deviation from the true
function f(a). The minimax polynomial is very difficult to find; the Chebyshev
approximating polynomial is almost identical and is very easy to compute!

So, given some (perhaps difficult) means of computing the function f(x), we
now need algorithms for implementing (5.8.7) and (after inspection of the resulting
cx’s and choice of a truncating value m) evaluating (5.8.9). The latter equation then
becomes an easy way of computing f(x) for all subsequent time.

The first of these tasks is straightforward. A generalization of equation (5.8.7)
that is here implemented is to allow the range of approximation to be between two
arbitrary limits a and b, instead of just —1 to 1. This is effected by a change of variable

—(b+a
y= tS (5.8.10)

and by the approximation of f(z) by a Chebyshev polynomial in y.

#include <math.h>
#include "nrutil.h"
#define PI 3.141592653589793

void chebft(float a, float b, float c[], int n, float (*func) (float))
Chebyshev fit: Given a function func, lower and upper limits of the interval [a,b], and a
maximum degree n, this routine computes the n coefficients c[0..n-1] such that func(x) +

(enc CpTx(y)] — co/2, where y and « are related by (5.8.10). This routine is to be used with
moderately large n (e.g., 30 or 50), the array of c’s subsequently to be truncated at the smaller
value m such that Cm and subsequent elements are negligible.
{

int k,j;

float fac,bpa,bma,*f;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.8 Chebyshev Approximation 193

f=vector(0,n-1);
bma=0.5*(b-a) ;
bpa=0.5*(b+a) ;

for (k=0;k<n;k++) { We evaluate the function at the n points required
float y=cos(PI*(k+0.5)/n); by (5.8.7).
f [k] =(*func) (y*bma+bpa) ;
}
fac=2.0/n;
for (j=0;j<n;j++) {
double sum=0.0; We will accumulate the sum in double precision,
for (k=0;k<n;k++) a nicety that you can ignore.
sum += f£[k]*cos(PI*j*(k+0.5)/n);
c[j]=fac*sum;
}

free_vector(f,0,n-1);

(If you find that the execution time of chebft is dominated by the calculation of
N? cosines, rather than by the NV evaluations of your function, then you should look
ahead to §12.3, especially equation 12.3.22, which shows how fast cosine transform
methods can be used to evaluate equation 5.8.7.)

Now that we have the Chebyshev coefficients, how do we evaluate the approxi-
mation? One could use the recurrence relation of equation (5.8.2) to generate values
for T;,(x) from Tp = 1,7, = 2, while also accumulating the sum of (5.8.9). It
is better to use Clenshaw’s recurrence formula (85.5), effecting the two processes
simultaneously. Applied to the Chebyshev series (5.8.9), the recurrence is

Amy = dm =

dj = 2xdj41 — djy2 + cj j=m-—1,m—2,...,1 (5.8.11)

1
f(x) = do = xd, — dy + 3

float chebev(float a, float b, float c[], int m, float x)
Chebyshev evaluation: All arguments are input. c[0..m-1] is an array of Chebyshev coeffi-
cients, the first m elements of c output from chebft (which must have been called with the
same a and b). The Chebyshev polynomial ym cy Th (y) — Co/2 is evaluated at a point
y = [x — (b + a)/2]/[(b — a)/2], and the result is returned as the function value.
{

void nrerror(char error_text[]);

float d=0.0,dd=0.0,sv,y,y2;

int j;

if ((x-a)*(x-b) > 0.0) nrerror("x not in range in routine chebev");

y2=2.0*(y=(2.0*x-a-b)/(b-a)) ; Change of variable.
for (j=m-1;j>=1;j--) { Clenshaw's recurrence.
sv=d;
d=y2*d-ddtc[j];
dd=sv;
+
return y+d-dd+0.5*c[0]; Last step is different.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

194 Chapter 5. Evaluation of Functions

If we are approximating an even function on the interval [—1, 1], its expansion
will involve only even Chebyshev polynomials. It is wasteful to call chebev with
all the odd coefficients zero [1]. Instead, using the half-angle identity for the cosine
in equation (5.8.1), we get the relation

(5.8.12)

Thus we can evaluate a series of even Chebyshev polynomials by calling chebev
with the even coefficients stored consecutively in the array c, but with the argument
x replaced by 2x? — 1.

An odd function will have an expansion involving only odd Chebyshev poly-
nomials. It is best to rewrite it as an expansion for the function f(x)/x, which
involves only even Chebyshev polynomials. This will give accurate values for
f(«)/a near x = 0. The coefficients c/, for f(a)/a can be found from those for
f(x) by recurrence:

yo
Cyy1 = 0

, . (5.8.13)
1 = 2€n — Ch yas n=N-1,N-83,...

J
Cn-

Equation (5.8.13) follows from the recurrence relation in equation (5.8.2).

If you insist on evaluating an odd Chebyshev series, the efficient way is to once
again use chebev with x replaced by y = 2”? — 1, and with the odd coefficients
stored consecutively in the array c. Now, however, you must also change the last
formula in equation (5.8.11) to be

f(x) = 2[(2y — 1)d, — dz + e9] (5.8.14)

and change the corresponding line in chebev.

CITED REFERENCES AND FURTHER READING:

Clenshaw, C.W. 1962, Mathematical Tables, vol. 5, National Physical Laboratory, (London: H.M.
Stationery Office). [1]

Goodwin, E.T. (ed.) 1961, Modern Computing Methods, 2nd ed. (New York: Philosophical Li-
brary), Chapter 8.

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
§4.4.1, p. 104.

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §6.5.2, p. 334.

Carnahan, B., Luther, H.A., and Wilkes, J.O. 1969, Applied Numerical Methods (New York:
Wiley), $1.10, p. 39.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.9 Derivatives or Integrals of a Chebyshev-approximated Function 195

5.9 Derivatives or Integrals of a
Chebyshev-approximated Function

If you have obtained the Chebyshev coefficients that approximate a function in
a certain range (e.g., from chebft in §5.8), then it is a simple matter to transform
them to Chebyshev coefficients corresponding to the derivative or integral of the
function. Having done this, you can evaluate the derivative or integral just as if it
were a function that you had Chebyshev-fitted ab initio.

The relevant formulas are these: If c;, 1 = 0,...,m — 1 are the coefficients
that approximate a function f in equation (5.8.9), C; are the coefficients that
approximate the indefinite integral of f, and c’ are the coefficients that approximate
the derivative of f, then

(i> 0) (5.9.1)
G1 = Chay t+ 2ic; (i =m-—1,m-—2,...,1) (5.9.2)

Equation (5.9.1) is augmented by an arbitrary choice of Co, corresponding to an
arbitrary constant of integration. Equation (5.9.2), which is a recurrence, is started
with the values c},, = c/,,_, = 0, corresponding to no information about the m + 1st

“Mm. “mM —

Chebyshev coefficient of the original function f.
Here are routines for implementing equations (5.9.1) and (5.9.2).

void chder(float a, float b, float c[], float cder[], int n)
Given a,b,c[0..n-1], as output from routine chebft §5.8, and given n, the desired degree
of approximation (length of ¢ to be used), this routine returns the array cder[0..n-1], the
Chebyshev coefficients of the derivative of the function whose coefficients are c.
{

int j;

float con;

eder [n-1]=0.0; n-1 and n-2 are special cases.
cder [n-2]=2* (n-1)*c [n-1] ;
for (j=n-3;j>=0;j--)
cder [j]=cder [j+2]+2*(j+1)*c[j+1]; Equation (5.9.2).
con=2.0/(b-a) ;
for (j=0;j<n; j++)
cder[j] *= con;

Normalize to the interval b-a.

void chint(float a, float b, float c[], float cint[], int n)
Given a,b,c[0..n-1], as output from routine chebft §5.8, and given n, the desired degree
of approximation (length of c to be used), this routine returns the array cint[0..n-1], the
Chebyshev coefficients of the integral of the function whose coefficients are c. The constant of
integration is set so that the integral vanishes at a.
{

int j;

float sum=0.0,fac=1.0,con;

con=0.25*(b-a) ; Factor that normalizes to the interval b-a.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

196 Chapter 5. Evaluation of Functions

2; j++) {
cint [j]=con*(c[j-1]-clj+1])/j; Equation (5.9.1).
sum += factcint[j]; Accumulates the constant of integration.
fac = -fac; Will equal +1.

+

cint [n-1]=con*c[n-2] /(n-1) ;
sum += fac*cint[n-1];

cint [0]=2.0*sum; Set the constant of integration.

Special case of (5.9.1) for n-1.

Clenshaw-Curtis Quadrature

Since a smooth function’s Chebyshev coefficients c; decrease rapidly, generally expo-
nentially, equation (5.9.1) is often quite efficient as the basis for a quadrature scheme. The
routines chebft and chint, used in that order, can be followed by repeated calls to chebev
if {° f(«)da is required for many different values of « in the range a < x < b.

If only the single definite integral pr f(«)dz is required, then chint and chebev are
replaced by the simpler formula, derived from equation (5.9.1),

1
“~ Qk + D2k—1)

b
| f(x)dx = (b—a) 50-2 on Cok —

(5.9.3)
where the c;’s are as returned by chebft. The series can be truncated when c; becomes
negligible, and the first neglected term gives an error estimate.

This scheme is known as Clenshaw-Curtis quadrature [1]. It is often combined with an
adaptive choice of N, the number of Chebyshev coefficients calculated via equation (5.8.7),
which is also the number of function evaluations of f(a). If a modest choice of N does
not give a sufficiently small cz, in equation (5.9.3), then a larger value is tried. In this
adaptive case, it is even better to replace equation (5.8.7) by the so-called “trapezoidal” or
Gauss-Lobatto (§4.5) variant,

=25"5 cos mk cos nok j =0 N-1 (5.9.4)
on_d 8 | 5 8 | ar j=0,..., 9:

where (N.B.!) the two primes signify that the first and last terms in the sum are to be
multiplied by 1/2. If N is doubled in equation (5.9.4), then half of the new function
evaluation points are identical to the old ones, allowing the previous function evaluations to be
reused. This feature, plus the analytic weights and abscissas (cosine functions in 5.9.4), give
Clenshaw-Curtis quadrature an edge over high-order adaptive Gaussian quadrature (cf. §4.5),
which the method otherwise resembles.

If your problem forces you to large values of N, you should be aware that equation (5.9.4)
can be evaluated rapidly, and simultaneously for all the values of j, by a fast cosine transform.
(See §12.3, especially equation 12.3.17.) (We already remarked that the nontrapezoidal form
(5.8.7) can also be done by fast cosine methods, cf. equation 12.3.22.)

CITED REFERENCES AND FURTHER READING:

Goodwin, E.T. (ed.) 1961, Modern Computing Methods, 2nd ed. (New York: Philosophical Li-
brary), pp. 78-79.

Clenshaw, C.W., and Curtis, A.R. 1960, Numerische Mathematik, vol. 2, pp. 197-205. [1]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.10 Polynomial Approximation from Chebyshev Coefficients 197

5.10 Polynomial Approximation from
Chebyshev Coefficients

You may well ask after reading the preceding two sections, “Must I store and
evaluate my Chebyshev approximation as an array of Chebyshev coefficients for a
transformed variable y? Can’t I convert the c;,’s into actual polynomial coefficients
in the original variable x and have an approximation of the following form?”

m1

k=0

Yes, you can do this (and we will give you the algorithm to do it), but we
caution you against it: Evaluating equation (5.10.1), where the coefficient g’s reflect
an underlying Chebyshev approximation, usually requires more significant figures
than evaluation of the Chebyshev sum directly (as by chebev). This is because
the Chebyshev polynomials themselves exhibit a rather delicate cancellation: The
leading coefficient of T;,(2), for example, is 2”~'; other coefficients of T;,(«) are
even bigger; yet they all manage to combine into a polynomial that lies between +1.
Only when m is no larger than 7 or 8 should you contemplate writing a Chebyshev
fit as a direct polynomial, and even in those cases you should be willing to tolerate
two or so significant figures less accuracy than the roundoff limit of your machine.

You get the g’s in equation (5.10.1) from the c’s output from chebft (suitably
truncated at a modest value of m) by calling in sequence the following two procedures:

#include "nrutil.h"

void chebpc(float c[], float d[], int n)
Chebyshev polynomial coefficients. Given a coefficient array c[0. .n-1], this routine generates
a coefficient array d[0. .n-1] such that yn ay® = yr cx. Tk (y) — Co/2. The method
is Clenshaw's recurrence (5.8.11), but now applied algebraically rather than arithmetically.
{

int k,j;

float sv,*dd;

dd=vector(0,n-1);
for (j=0;j<n;j++) d[j]=dd[j]=0.0;
d[0]=c[n-1];
for (j=n-2;j>=1;j--) {
for (k=n-j;k>=1;k--) {
sv=d[k];
dk] =2.0*d[k-1]-dd [k] ;
dd[k]=sv;

}

sv=d[0] ;

da[0] = -dd[0]+clj];
dd[0]=sv;

~ 15 j>=155--)
(j-1]-aalj];
a0] = -dd[0]+0.5*c[0];
free_vector(dd,0,n-1);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

198 Chapter 5. Evaluation of Functions

void peshft(float a, float b, float d[], int n)

Polynomial coefficient shift. Given a coefficient array a0; wil, this routine generates a
coefficient array gl0.. n-1] such that ve ayy” =ynt 3 9ne", where x and y are related
by (5.8.10), ie., the interval —1 < y < 1 is mapped to the interval a < a < b. The array
g is returned in d.

t
int k,j;
float fac,cnst;
censt=2.0/(b-a) ;
fac=cnst;
for (j=1;j<n;j++) f First we rescale by the factor const...
d{j] *= fac;
fac *= cnst;
+
cnst=0.5*(atb) ; ...which is then redefined as the desired shift.
for (j=0;j<=n-2;j++) We accomplish the shift by synthetic division. Synthetic
for (k=n-2;k>=j;k--) division is a miracle of high-school algebra. If you
d{k] -= cnst*d[k+1] ; never learned it, go do so. You won't be sorry.
3

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 59, 182-183 [synthetic division].

5.11 Economization of Power Series

One particular application of Chebyshev methods, the economization of power series, is
an occasionally useful technique, with a flavor of getting something for nothing.

Suppose that you are already computing a function by the use of a convergent power
series, for example

_ x 2 2

f@sl-gtay- mare (5.11.1)
(This function is actually sin(/x)/,/x, but pretend you don’t know that.) You might be
doing a Er eee that requires evaluating the series many times in some particular interval, say
(0, (27)*]. Everything is fine, except that the series requires a large number of terms before
its error (approximated by the first neglected term, say) is tolerable. In our example, with
x = (27), the first term smaller than 10~” is '3/(27!). This then approximates the error
of the finite series whose last term is 2’? /(25!).

Notice that because of the large exponent in zx! the error is much smaller than 1077
everywhere in the interval except at the very largest values of a. This is the feature that allows
“economization”: if we are willing to let the error elsewhere in the interval rise to about the
same value that the first neglected term has at the extreme end of the interval, then we can
replace the 13-term series by one that is significantly shorter.

Here are the steps for doing so:

1. Change variables from x to y, as in equation (5.8.10), to map the x interval into

-l1<y<l.

2. Find the coefficients of the Chebyshev sum (like equation 5.8.8) that exactly equals your
truncated power series (the one with enough terms for accuracy).

3. Truncate this Chebyshev series to a smaller number of terms, using the coefficient of the
first neglected Chebyshev polynomial as an estimate of the error.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.11 Economization of Power Series 199

4. Convert back to a polynomial in y.
5. Change variables back to x.

All of these steps can be done numerically, given the coefficients of the original power
series expansion. The first step is exactly the inverse of the routine peshft (§5.10), which
mapped a polynomial from y (in the interval [—1, 1]) to x (in the interval {a, b]). But since
equation (5.8.10) is a linear relation between x and y, one can also use peshft for the
inverse. The inverse of

peshft(a,b,d,n)

turns out to be (you can check this)

—2— —b-a
hft | —_——— ,__—_,d,
Pes ( b-a b-a »)

The second step requires the inverse operation to that done by the routine chebpc (which
took Chebyshev coefficients into polynomial coefficients). The following routine, pccheb,
accomplishes this, using the formula [1]

where the last term depends on whether k is even or odd,

Qr-1

k 1/{k
+ (u- nya) BO) (k odd), 4 3(jf,) (Keven). (5.11.3)

void pccheb(float d[], float c[], int n)
Inverse of routine chebpc: given an array of polynomial coefficients d[0..n-1], returns an
equivalent array of Chebyshev coefficients c[0..n-1].

{
int j,jm,jp.k:
float fac,pow;
pow=1.0; Will be powers of 2.
c[0]=2.0*d[0] ;
for (k=1;k<n;k++) { Loop over orders of a in the polynomial.
c[k]=0.0; Zero corresponding order of Chebyshev.
fac=d[k] /pow;
jm=k;
jp=1;
for (j=k;j>=0;j-=2,jm--,jpt+) {
Increment this and lower orders of Chebyshev with the combinatorial coefficent times
d[k]; see text for formula.
clj] += fac;
fac *= ((float) jm)/((float) jp);
}
pow += pow;
+
}

The fourth and fifth steps are accomplished by the routines chebpe and pcshft,
respectively. Here is how the procedure looks all together:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

200 Chapter 5. Evaluation of Functions

#define NFEW ..
#define NMANY ..

float *c,*d,*e,a,b;
Economize NMANY power series coefficients e[0..NMANY-1] in the range (a,b) into NFEW
coefficients d[0..NFEW-1].

c=vector (0, NMANY-1) ;

d=vector(0,NFEW-1) ;

e=vector(0,NMANY-1) ;

peshft ((-2.0-b-a) /(b-a) , (2.0-b-a) / (b-a) ,e ,NMANY) ;
pccheb(e,c,NMANY) ;

Here one would normally examine the Chebyshev coefficients c[0..NMANY-1] to decide
how small NFEW can be.

chebpc(c,d,NFEW) ;

peshft (a,b,d,NFEW) ;

In our example, by the way, the 8th through 10th Chebyshev coefficients turn out to
be on the order of —7 x 107°, 3 x 1077, and —9 x 107°, so reasonable truncations (for
single precision calculations) are somewhere in this range, yielding a polynomial with 8 —
10 terms instead of the original 13.

Replacing a 13-term polynomial with a (say) 10-term polynomial without any loss of
accuracy — that does seem to be getting something for nothing. Is there some magic in
this technique? Not really. The 13-term polynomial defined a function f(x). Equivalent to
economizing the series, we could instead have evaluated f(x) at enough points to construct
its Chebyshev approximation in the interval of interest, by the methods of §5.8. We would
have obtained just the same lower-order polynomial. The principal lesson is that the rate
of convergence of Chebyshev coefficients has nothing to do with the rate of convergence of
power series coefficients; and it is the former that dictates the number of terms needed in a
polynomial approximation. A function might have a divergent power series in some region
of interest, but if the function itself is well-behaved, it will have perfectly good polynomial
approximations. These can be found by the methods of §5.8, but not by economization of
series. There is slightly less to economization of series than meets the eye.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 12.

Arfken, G. 1970, Mathematical Methods for Physicists, 2nd ed. (New York: Academic Press),
p. 631. [1]

5.12 Padé Approximants

A Padé approximant, so called, is that rational function (of a specified order) whose
power series expansion agrees with a given power series to the highest possible order. If
the rational function is

(5.12.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.12 Padé Approximants 201

then R(x) is said to be a Padé approximant to the series

f(a) = > cpa® (5.12.2)
k=0
if
R(0) = f(0) (5.12.3)
and also

k=1,2,...,M+N (5.12.4)

Equations (5.12.3) and (5.12.4) furnish M + N + 1 equations for the unknowns ao,..., a
and b,,...,bn. The easiest way to see what these equations are is to equate (5.12.1) and
(5.12.2), multiply both by the denominator of equation (5.12.1), and equate all powers of
a that have either a’s or b’s in their coefficients. If we consider only the special case of
a diagonal rational approximation, M = N (cf. §3.2), then we have ap = co, with the
remaining a’s and b’s satisfying

N
> bmen—m+k = —CN+k, (5.12.5)
m=1
k
SF bmck—-m = ae, N (5.12.6)
m=0

(note, in equation 5.12.1, that b) = 1). To solve these, start with equations (5.12.5), which
are a set of linear equations for all the unknown b’s. Although the set is in the form of a
Toeplitz matrix (compare equation 2.8.8), experience shows that the equations are frequently
close to singular, so that one should not solve them by the methods of §2.8, but rather by
full LU decomposition. Additionally, it is a good idea to refine the solution by iterative
improvement (routine mprove in §2.5) [1].

Once the b’s are known, then equation (5.12.6) gives an explicit formula for the unknown
a’s, completing the solution.

Padé approximants are typically used when there is some unknown underlying function
f(x). We suppose that you are able somehow to compute, perhaps by laborious analytic
expansions, the values of f(a) and a few of its derivatives at x = 0: f(0), f’(0), f’” (0),
and so on. These are of course the first few coefficients in the power series expansion of
f(x); but they are not necessarily getting small, and you have no idea where (or whether)
the power series is convergent.

By contrast with techniques like Chebyshev approximation (§5.8) or economization
of power series (§5.11) that only condense the information that you already know about a
function, Padé approximants can give you genuinely new information about your function’s
values. It is sometimes quite mysterious how well this can work. (Like other mysteries in
mathematics, it relates to analyticity.) An example will illustrate.

Imagine that, by extraordinary labors, you have ground out the first five terms in the
power series expansion of an unknown function f(x),

yo td, Loa 49 3, 17
SQ) S24 gat ae — are + Tergy”

(It is not really necessary that you know the coefficients in exact rational form — numerical
values are just as good. We here write them as rationals to give you the impression that
they derive from some side analytic calculation.) Equation (5.12.7) is plotted as the curve
labeled “power series” in Figure 5.12.1. One sees that for x X 4 it is dominated by its
largest, quartic, term.

We now take the five coefficients in equation (5.12.7) and run them through the routine
pade listed below. It returns five rational coefficients, three a’s and two b’s, for use in equation
(5.12.1) with M = N = 2. The curve in the figure labeled “Padé” plots the resulting rational
function. Note that both solid curves derive from the same five original coefficient values.

bee (5.12.7)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

202 Chapter 5. Evaluation of Functions

10-711 perry etry TTT
gle f(x) = [7+ (1 + x)4/3]13 _|
6 ;— power series (5 terms) mY om
SB oo- a
4te —
Padé (5 coefficients)
2
git L1 L1 L1 L1
0 2 4 6 8 10

Figure 5.12.1. The five-term power series expansion and the derived five-coefficient Paé approximant
for a sample function f(x). The full power series converges only for  < 1. Note that the Padé
approximant maintains accuracy far outside the radius of convergence of the series.

To evaluate the results, we need Deus ex machina (a useful fellow, when he is available)
to tell us that equation (5.12.7) is in fact the power series expansion of the function

f(x) = [7+ A+ a)*/98 (5.12.8)

which is plotted as the dotted curve in the figure. This function has a branch point at 2 = —1,
so its power series is convergent only in the range —1 < a < 1. In most of the range
shown in the figure, the series is divergent, and the value of its truncation to five terms is
rather meaningless. Nevertheless, those five terms, converted to a Padé approximant, give a
remarkably good representation of the function up to at least x ~ 10.

Why does this work? Are there not other functions with the same first five terms in
their power series, but completely different behavior in the range (say) 2 < 2 < 10? Indeed
there are. Padé approximation has the uncanny knack of picking the function you had in
mind from among all the possibilities. Except when it doesn’t! That is the downside of
Padé approximation: it is uncontrolled. There is, in general, no way to tell how accurate
it is, or how far out in x it can usefully be extended. It is a powerful, but in the end still
mysterious, technique.

Here is the routine that gets a’s and b’s from your c’s. Note that the routine is specialized
to the case MM = N, and also that, on output, the rational coefficients are arranged in a format
for use with the evaluation routine ratval (§5.3). (Also for consistency with that routine,
the array of c’s is passed in double precision.)

#include <math.h>
#include "nrutil.h"
#define BIG 1.0e30

void pade(double cof[], int n, float *resid)

Given cof [0..2*n], the leading terms in the power series expansion of a function, solve the
linear Padé equations to return the coefficients of a diagonal rational function approximation to
the same function, namely (cof [0] + cof [i]a+---+cof[nJa%)/(1+cof[nti]a+---+

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.12 Padé Approximants 203

cof [2*n]xz). The value resid is the norm of the residual vector; a small value indicates a
well-converged solution. Note that cof is double precision for consistency with ratval.

{

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

void mprove(float **a, float **alud, int n, int indx[], float bf],
float x{[]);

int j,k,*indx;

float d,rr,rrold,sum, **q,**qlu, *x,*y,¥*Z}

indx=ivector(1,n);

q=matrix(1,n,1,n);

qlu=matrix(1,n,1,n);

x=vector(1,n);

y=vector(1,n);

z=vector(1,n);

3i++) { Set up matrix for solving.

(jl=cof [n+j];

for (k=1;k<=n;k++) {
ql] (k]=cof [j-k+n] ;
qlu[j] (k]=q[j] [kl];

+

ludcemp(qlu,n,indx,&d) ; Solve by LU decomposition and backsubstitu-
lubksb(qlu,n, indx,x) ; tion.

rr=BIG;

do { Important to use iterative improvement, since

rrold=rr; the Padé equations tend to be ill-conditioned.
for (j=1;j<=n;j++) zljl=x(jl;
mprove(q,qlu,n,indx,y,x);
for (rr=0.0,j=1;j<=n; j++) Calculate residual.
rr += SQR(z[j]-x[j]);
} while (rr < rrold);
*resid=sqrt (rrold) ;
for (k=1;k<=n;k++) { Calculate the remaining coefficients.
for (sum=cof [k] ,j=1;j<=k;j++) sum -= z[j]*cof[k-j];
y[k]=sum;
} Copy answers to output.
for (j=1;j<=n;j++) {
cof [j]=y (jl;
cof[jtn] = -z[j];

If it is no longer improving, call it quits.

}

free_vector(z,1,n);
free_vector(y,1,n);
free_vector(x,1,n);
free_matrix(qlu,i1,n,1,n);
free_matrix(q,1,n,1,n);
free_ivector(indx,1,n);

CITED REFERENCES AND FURTHER READING:
Ralston, A. and Wilf, H.S. 1960, Mathematical Methods for Digital Computers (New York: Wiley),

p. 14.

Cuyt, A., and Wuytack, L. 1987, Nonlinear Methods in Numerical Analysis (Amsterdam: North-

Holland), Chapter 2.

Graves-Morris, P.R. 1979, in Padé Approximation and Its Applications, Lecture Notes in Mathe-

matics, vol. 765, L. Wuytack, ed. (Berlin: Springer-Verlag). [1]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

204 Chapter 5. Evaluation of Functions

5.13 Rational Chebyshev Approximation

In §5.8 and §5.10 we learned how to find good polynomial approximations to a given
function f(x) in a given interval a < x < b. Here, we want to generalize the task to find
good approximations that are rational functions (see §5.3). The reason for doing so is that,
for some functions and some intervals, the optimal rational function approximation is able
to achieve substantially higher accuracy than the optimal polynomial approximation with the
same number of coefficients. This must be weighed against the fact that finding a rational
function approximation is not as straightforward as finding a polynomial approximation,
which, as we saw, could be done elegantly via Chebyshev polynomials.

Let the desired rational function R(x) have numerator of degree m and denominator
of degree k. Then we have

— Potpiets-+pme™
R@) = net tat © I) fora<a<b (5.13.1)
The unknown quantities that we need to find are po,...,pm and qi,...,qx,thatis,sm+k+1
quantities in all. Let r(a) denote the deviation of R(x) from f(a), and let r denote its

maximum absolute value,

The ideal minimax solution would be that choice of p’s and q’s that minimizes r. Obviously
there is some minimax solution, since r is bounded below by zero. How can we find it, or
a reasonable approximation to it?

A first hint is furnished by the following fundamental theorem: If R(x) is nondegenerate
(has no common polynomial factors in numerator and denominator), then there is a unique
choice of p’s and q’s that minimizes r; for this choice, r(x) has m + k + 2 extrema in
a << b, all of magnitude r and with alternating sign. (We have omitted some technical
assumptions in this theorem. See Ralston [1] for a precise statement.) We thus learn that the
situation with rational functions is quite analogous to that for minimax polynomials: In §5.8
we saw that the error term of an nth order approximation, with n + 1 Chebyshev coefficients,
was generally dominated by the first neglected Chebyshev term, namely T;,+1, which itself
has n + 2 extrema of equal magnitude and alternating sign. So, here, the number of rational
coefficients, m + k + 1, plays the same role of the number of polynomial coefficients, n + 1.

A different way to see why r(x) should have m + k + 2 extrema is to note that R(x)
can be made exactly equal to f(a) at any m + k + 1 points x;. Multiplying equation (5.13.1)
by its denominator gives the equations

po+pitit-++ + pmal” = f (ai) t+ qari +--+ +qnr*)

(5.13.3)
i=1, sm+tk+1

This is a set of m +k + 1 linear equations for the unknown p’s and q’s, which can be
solved by standard methods (e.g., LU decomposition). If we choose the 2;’s to all be in
the interval (a, b), then there will generically be an extremum between each chosen 2; and
xi+1, plus also extrema where the function goes out of the interval at a and 5, for a total
of m +k +2 extrema. For arbitrary x;’s, the extrema will not have the same magnitude.
The theorem says that, for one particular choice of «;’s, the magnitudes can be beaten down
to the identical, minimal, value of r.

Instead of making f(2;) and R(2;) equal at the points x;, one can instead force the
residual r(a;) to any desired values y; by solving the linear equations

pot pieit:+++pmat” = [f(ai) — ysl + aa +--- + quv®)

(5.13.4)
i=1,2,....m+k+1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.13 Rational Chebyshev Approximation 205

In fact, if the x;’s are chosen to be the extrema (not the zeros) of the minimax solution,
then the equations satisfied will be

pot piti test pmol” = [f (ai) r+ qt ++ + an?)

5.13.5
i=1, (5.13.5)

.m+k+2

where the + alternates for the alternating extrema. Notice that equation (5.13.5) is satisfied at
m + k + 2 extrema, while equation (5.13.4) was satisfied only at m + k + 1 arbitrary points.
How can this be? The answer is that r in equation (5.13.5) is an additional unknown, so that
the number of both equations and unknowns is m + k + 2. True, the set is mildly nonlinear
(in r), but in general it is still perfectly soluble by methods that we will develop in Chapter 9.

We thus see that, given only the locations of the extrema of the minimax rational
function, we can solve for its coefficients and maximum deviation. Additional theorems,
leading up to the so-called Remes algorithms [1], tell how to converge to these locations by
an iterative process. For example, here is a (slightly simplified) statement of Remes’ Second
Algorithm: (1) Find an initial rational function with m + k + 2 extrema x; (not having equal
deviation). (2) Solve equation (5.13.5) for new rational coefficients and r. (3) Evaluate the
resulting R(a) to find its actual extrema (which will not be the same as the guessed values).
(4) Replace each guessed value with the nearest actual extremum of the same sign. (5) Go
back to step 2 and iterate to convergence. Under a broad set of assumptions, this method will
converge. Ralston [1] fills in the necessary details, including how to find the initial set of x;’s.

Up to this point, our discussion has been textbook-standard. We now reveal ourselves
as heretics. We don’t much like the elegant Remes algorithm. Its two nested iterations (on
r in the nonlinear set 5.13.5, and on the new sets of 2;’s) are finicky and require a lot of
special logic for degenerate cases. Even more heretical, we doubt that compulsive searching
for the exactly best, equal deviation, approximation is worth the effort — except perhaps for
those few people in the world whose business it is to find optimal approximations that get
built into compilers and microchips.

When we use rational function approximation, the goal is usually much more pragmatic:
Inside some inner loop we are evaluating some function a zillion times, and we want to
speed up its evaluation. Almost never do we need this function to the last bit of machine
accuracy. Suppose (heresy!) we use an approximation whose error has m + k + 2 extrema
whose deviations differ by a factor of 2. The theorems on which the Remes algorithms
are based guarantee that the perfect minimax solution will have extrema somewhere within
this factor of 2 range — forcing down the higher extrema will cause the lower ones to rise,
until all are equal. So our “sloppy” approximation is in fact within a fraction of a least
significant bit of the minimax one.

That is good enough for us, especially when we have available a very robust method
for finding the so-called “sloppy” approximation. Such a method is the least-squares solution
of overdetermined linear equations by singular value decomposition (§2.6 and §15.4). We
proceed as follows: First, solve (in the least-squares sense) equation (5.13.3), not just for
m-+k-+1 values of x;, but for a significantly larger number of x;’s, spaced approximately
like the zeros of a high-order Chebyshev polynomial. This gives an initial guess for R(x).
Second, tabulate the resulting deviations, find the mean absolute deviation, call it 7, and then
solve (again in the least-squares sense) equation (5.13.5) with r fixed and the + chosen to be
the sign of the observed deviation at each point 2;. Third, repeat the second step a few times.

You can spot some Remes orthodoxy lurking in our algorithm: The equations we solve
are trying to bring the deviations not to zero, but rather to plus-or-minus some consistent
value. However, we dispense with keeping track of actual extrema; and we solve only linear
equations at each stage. One additional trick is to solve a weighted least-squares problem,
where the weights are chosen to beat down the largest deviations fastest.

Here is a program implementing these ideas. Notice that the only calls to the function
fn occur in the initial filling of the table fs. You could easily modify the code to do this filling
outside of the routine. It is not even necessary that your abscissas xs be exactly the ones that we
use, though the quality of the fit will deteriorate if you do not have several abscissas between
each extremum of the (underlying) minimax solution. Notice that the rational coefficients are
output in a format suitable for evaluation by the routine ratval in §5.3.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

206 Chapter 5. Evaluation of Functions

TTT It
c m=k=4 7
2x 1076 | £(x) = cos(x)/(1 + €*)
cr O<x<a
107° =
2 L
2 L
\ E
e oF
x L
-1x 10-6
-2x 10-6
Citbivritriiirtiiii tii tiiritiiiitisd
0 5 1 1.5 2 2.5 3
x
Figure 5.13.1. Solid curves show deviations r(x) for five successive iterations of the routine ratlsq

for an arbitrary test problem. The algorithm does not converge to exactly the minimax solution (shown
as the dotted curve). But, after one iteration, the discrepancy is a small fraction of the last significant
bit of accuracy.

#include <stdio.h>

#include <math.h>

#include "nrutil.h"

#define NPFAC 8

#define MAXIT 5

#define PIO2 (3.141592653589793/2.0)
#define BIG 1.0e30

void ratlsq(double (*fn)(double), double a, double b, int mm, int kk,

double cof[], double *dev)
Returns in cof [0. .mm+kk] the coefficients of a rational function approximation to the function
fn in the interval (a,b). Input quantities mm and kk specify the order of the numerator and
denominator, respectively. The maximum absolute deviation of the approximation (insofar as
is known) is returned as dev.
t

double ratval(double x, double cof[], int mm, int kk);

void dsvbksb(double *#u, double w[], double **v, int m, int n, double b[],

double x[]);

void dsvdcemp(double **a, int m, int n, double w[], double **v);

These are double versions of svdcmp, svbksb.

int i,it,j,ncof,npt;

double devmax,e,hth,power,sum,*bb,*coff,*ee,*fs,**u, **v, *w, wt , *xS;

ncof=mm+kk+1;

npt=NPFAC*ncof ; Number of points where function is evaluated,
bb=dvector (1,npt) ; i.e., fineness of the mesh.
coff=dvector(0,ncof-1) ;

ee=dvector(1,npt) ;

fs=dvector(1,npt) ;

u=dmatrix(1,npt,1,ncof) ;

v=dmatrix(1,ncof,1,ncof) ;

w=dvector(1,ncof) ;

wt=dvector(1,npt) ;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

5.13 Rational Chebyshev Approximation 207

xs=dvector(1,npt) ;

*dev=BIG;
for (i=1;i<=npt;it+) { Fill arrays with mesh abscissas and function val-
if (i < npt/2) { ues.
hth=PI02*(i-1)/(npt-1.0); At each end, use formula that minimizes round-
xs [i] =at(b-a) *DSQR(sin(hth)) ; off sensitivity.
} else {
hth=P102+*(npt-i)/(npt-1.0);
xs [i]=b-(b-a) *DSQR(sin(hth) ) ;
}
fs[i]=(*fn) (xs[i]);
wt [i]=1.0; In later iterations we will adjust these weights to
ee[i]=1.0; combat the largest deviations.
+
e=0.0;
for (it=1;it<=MAXIT;it++) { Loop over iterations.
for (i=1;i<=npt;it+) { Set up the “design matrix” for the least-squares
power=wt [i]; fit.
bb [i]=power* (fs [i]+SIGN(e,ee[i]));
Key idea here: Fit to fn(a) +e where the deviation is positive, to £n(2) — e where
it is negative. Then e is supposed to become an approximation to the equal-ripple
deviation.
for (j= mm+1;j++) {
uli] [j]=power ;
power *= xs[i];
}
power = -bb[il;
for (j=mm+2;j<=ncof;j++) {
power *= xs[i];
uli] [j]=power ;
}
}
dsvdemp(u,npt ,ncof ,w,v); Singular Value Decomposition.
In especially singular or difficult cases, one might here edit the singular values w[1..ncof] ,
replacing small values by zero. Note that dsvbksb works with one-based arrays, so we
must subtract 1 when we pass it the zero-based array coff.
dsvbksb(u,w,v,npt ,ncof ,bb, coff-1) ;
devmax=sum=0.0;
for (j=1;j<=npt; j++) { Tabulate the deviations and revise the weights.
ee[j]=ratval (xs [j] ,coff,mm,kk)-fs[j];
Use weighting to emphasize most deviant points.
if (wt[j] > devmax) devmax=wt[j];
}
e=sum/npt ; Update e to be the mean absolute deviation.
if (devmax xdev) { Save only the best coefficient set found.
for (j=0;j<ncof;j++) cof[jl]=coff [jl];
*dev=devmax;
}
printf(" ratlsq iteration= 42d max error= %10.3e\n",it,devmax) ;
}

free_dvector(xs,1,npt) ;
free_dvector(wt,1,npt) ;
free_dvector(w,1,ncof) ;
free_dmatrix(v,1,ncof,1,ncof);
free_dmatrix(u,1,npt,1,ncof);
free_dvector(fs,1,npt);
free_dvector(ee,1,npt) ;
free_dvector(coff,0,ncof-1) ;
free_dvector(bb,1,npt) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

208 Chapter 5. Evaluation of Functions

Figure 5.13.1 shows the discrepancies for the first five iterations of ratlsq when it is
applied to find the m = k = 4 rational fit to the function f(x) = cosz/(1+ e”) in the
interval (0,7). One sees that after the first iteration, the results are virtually as good as the
minimax solution. The iterations do not converge in the order that the figure suggests: In
fact, it is the second iteration that is best (has smallest maximum deviation). The routine
rat1sq accordingly returns the best of its iterations, not necessarily the last one; there is no
advantage in doing more than five iterations.

CITED REFERENCES AND FURTHER READING:

Ralston, A. and Wilf, H.S. 1960, Mathematical Methods for Digital Computers (New York: Wiley),
Chapter 13. [1]

5.14 Evaluation of Functions by Path
Integration

In computer programming, the technique of choice is not necessarily the most
efficient, or elegant, or fastest executing one. Instead, it may be the one that is quick
to implement, general, and easy to check.

One sometimes needs only a few, or a few thousand, evaluations of a special
unction, perhaps a complex valued function of a complex variable, that has many
different parameters, or asymptotic regimes, or both. Use of the usual tricks (series,
continued fractions, rational function approximations, recurrence relations, and so
orth) may result in a patchwork program with tests and branches to different
ormulas. While such a program may be highly efficient in execution, it is often not
the shortest way to the answer from a standing start.

A different technique of considerable generality is direct integration of a
unction’s defining differential equation — an ab initio integration for each desired
unction value — along a path in the complex plane if necessary. While this may at
first seem like swatting a fly with a golden brick, it turns out that when you already
have the brick, and the fly is asleep right under it, all you have to do is let it fall!

As a specific example, let us consider the complex hypergeometric func-
tion 2F\(a, b,c; z), which is defined as the analytic continuation of the so-called
hypergeometric series,

ab z . a(a+1)jb(b+1) 2

2F\(a,b,¢;2) = 1

el’ ce(c +1) 2!
a(a $1)... @+j-Wbb+1)-.- (b+ 5-H
e(e+1)...(e+j-1) je

(5.14.1)
The series converges only within the unit circle |z| < 1 (see [1]), but one’s interest
in the function is often not confined to this region.
The hypergeometric function 2 F; is a solution (in fact the solution that is regular
at the origin) of the hypergeometric differential equation, which we can write as

2(1—2)F" = abF - [c—(a+b4+1)z|F" (5.14.2)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

5.14 Evaluation of Functions by Path Integration 209

Here prime denotes d/dz. One can see that the equation has regular singular points
at z = 0,1, and oo. Since the desired solution is regular at z = 0, the values 1 and
oo will in general be branch points. If we want 2F} to be a single valued function,
we must have a branch cut connecting these two points. A conventional position for
this cut is along the positive real axis from 1 to oo, though we may wish to keep
open the possibility of altering this choice for some applications.

Our golden brick consists of a collection of routines for the integration of sets
of ordinary differential equations, which we will develop in detail later, in Chapter
16. For now, we need only a high-level, “black-box” routine that integrates such
a set from initial conditions at one value of a (real) independent variable to final
conditions at some other value of the independent variable, while automatically
adjusting its internal stepsize to maintain some specified accuracy. That routine is
called odeint and, in one particular invocation, calculates its individual steps with
a sophisticated Bulirsch-Stoer technique.

Suppose that we know values for F and its derivative F’’ at some value zo, and
that we want to find F' at some other point z in the complex plane. The straight-line
path connecting these two points is parametrized by

2(s) = 20 + 8(z1 — 20) (5.14.3)

with s a real parameter. The differential equation (5.14.2) can now be written as
a set of two first-order equations,

dF
ds
dF" abF — [c—(a+b+1)z]F"

T= (a )( z(1— 2) )

= (21 — 20) F"
(5.14.4)

to be integrated from s = 0 to s = 1. Here F and F’ are to be viewed as two
independent complex variables. The fact that prime means d/dz can be ignored; it
will emerge as a consequence of the first equation in (5.14.4). Moreover, the real and
imaginary parts of equation (5.14.4) define a set of four real differential equations,
with independent variable s. The complex arithmetic on the right-hand side can be
viewed as mere shorthand for how the four components are to be coupled. It is
precisely this point of view that gets passed to the routine odeint, since it knows
nothing of either complex functions or complex independent variables.

It remains only to decide where to start, and what path to take in the complex
plane, to get to an arbitrary point z. This is where consideration of the function’s
singularities, and the adopted branch cut, enter. Figure 5.14.1 shows the strategy
that we adopt. For |z| < 1/2, the series in equation (5.14.1) will in general converge
rapidly, and it makes sense to use it directly. Otherwise, we integrate along a straight
line path from one of the starting points (+1/2, 0) or (0, 41/2). The former choices
are natural for 0 < Re(z) < 1 and Re(z) < 0, respectively. The latter choices are
used for Re(z) > 1, above and below the branch cut; the purpose of starting away
from the real axis in these cases is to avoid passing too close to the singularity at
z = 1 (see Figure 5.14.1). The location of the branch cut is defined by the fact that
our adopted strategy never integrates across the real axis for Re (z) > 1.

An implementation of this algorithm is given in §6.12 as the routine hypgeo.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

210 Chapter 5. Evaluation of Functions

Im

e

branch cut
Re
trey
Figure 5.14.1. Complex plane showing the singular points of the hypergeometric function, its branch
cut, and some integration paths from the circle |z| = 1/2 (where the power series converges rapidly)

to other points in the plane.

A number of variants on the procedure described thus far are possible, and easy
to program. If successively called values of z are close together (with identical values
of a, b, and c), then you can save the state vector (F, F'’) and the corresponding value
of z on each call, and use these as starting values for the next call. The incremental
integration may then take only one or two steps. Avoid integrating across the branch
cut unintentionally: the function value will be “correct,” but not the one you want.

Alternatively, you may wish to integrate to some position z by a dog-leg path
that does cross the real axis Re z > 1, as a means of moving the branch cut. For
example, in some cases you might want to integrate from (0, 1/2) to (3/2, 1/2),
and go from there to any point with Re z > 1 — with either sign of Im z. (If
you are, for example, finding roots of a function by an iterative method, you do
not want the integration for nearby values to take different paths around a branch
point. If it does, your root-finder will see discontinuous function values, and will
likely not converge correctly!)

In any case, be aware that a loss of numerical accuracy can result if you integrate
through a region of large function value on your way to a final answer where the
function value is small. (For the hypergeometric function, a particular case of this is
when a and b are both large and positive, with c and x % 1.) In such cases, you'll
need to find a better dog-leg path.

The general technique of evaluating a function by integrating its differential
equation in the complex plane can also be applied to other special functions. For

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

v
g
3.
a
8

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

3
Fy
Q
3
om
a
8
>
2
=
Ss
5

a
Pa
Fa
8
$s
2
é
5

2
Pa
8
2
3
3
8
3

3
S
gs
=
ga
2

<

z
3
2
g
s
2
4
3
8
a
g
Zz
z
3
3
2
2
D
Fa
2.

3
3
&
3
=
fa
g
ce}
iw]
yD
fe)
=

#
=.
2
<
3
&
2
s

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

5.14 Evaluation of Functions by Path Integration 211

example, the complex Bessel function, Airy function, Coulomb wave function, and
Weber function are all special cases of the confluent hypergeometric function, with a
differential equation similar to the one used above (see, e.g., [1] §13.6, for a table of
special cases). The confluent hypergeometric function has no singularities at finite z:
That makes it easy to integrate. However, its essential singularity at infinity means
that it can have, along some paths and for some parameters, highly oscillatory or
exponentially decreasing behavior: That makes it hard to integrate. Some case by
case judgment (or experimentation) is therefore required.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York). [1]

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 6. Special Functions

6.0 Introduction

There is nothing particularly special about a special function, except that
some person in authority or textbook writer (not the same thing!) has decided to
bestow the moniker. Special functions are sometimes called higher transcendental
functions (higher than what?) or functions of mathematical physics (but they occur in
other fields also) or functions that satisfy certain frequently occurring second-order
differential equations (but not all special functions do). One might simply call them
“useful functions” and let it go at that; it is surely only a matter of taste which
functions we have chosen to include in this chapter.

Good commercially available program libraries, such as NAG or IMSL, contain
routines for a number of special functions. These routines are intended for users who
will have no idea what goes on inside them. Such state of the art “black boxes” are
often very messy things, full of branches to completely different methods depending
on the value of the calling arguments. Black boxes have, or should have, careful
control of accuracy, to some stated uniform precision in all regimes.

We will not be quite so fastidious in our examples, in part because we want
to illustrate techniques from Chapter 5, and in part because we want you to
understand what goes on in the routines presented. Some of our routines have an
accuracy parameter that can be made as small as desired, while others (especially
those involving polynomial fits) give only a certain accuracy, one that we believe
serviceable (typically six significant figures or more). We do not certify that the
routines are perfect black boxes. We do hope that, if you ever encounter trouble
in a routine, you will be able to diagnose and correct the problem on the basis of
the information that we have given.

In short, the special function routines of this chapter are meant to be used —
we use them all the time — but we also want you to be prepared to understand
their inner workings.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York) [full of useful numerical approximations to a great variety
of functions].

IMSL Sfun/Library Users Manual (IMSL Inc., 2500 CityWest Boulevard, Houston TX 77042).

NAG Fortran Library (Numerical Algorithms Group, 256 Banbury Road, Oxford OX27DE, U.K.),
Chapter S.

212

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

6.1 Gamma, Beta, and Related Functions 213

Hart, J.F., et al. 1968, Computer Approximations (New York: Wiley).

Hastings, C. 1955, Approximations for Digital Computers (Princeton: Princeton University Press).
Luke, Y.L. 1975, Mathematical Functions and Their Approximations (New York: Academic Press).

6.1 Gamma Function, Beta Function, Factorials,
Binomial Coefficients

The gamma function is defined by the integral

T(z) = [ tte-'dt (6.1.1)

When the argument z is an integer, the gamma function is just the familiar factorial
function, but offset by one,

n! =T(n+1) (6.1.2)
The gamma function satisfies the recurrence relation
T(z +1) = 2T(z) (6.1.3)

If the function is known for arguments z > 1 or, more generally, in the half complex
plane Re(z) > 1 it can be obtained for z < 1 or Re (z) < 1 by the reflection formula

Tv TZ

T(l-z)= P@)sin(zz) PO +2)sin(nz)

(6.14)

Notice that ['(z) has a pole at z = 0, and at all negative integer values of z.
There are a variety of methods in use for calculating the function T'(z)

numerically, but none is quite as neat as the approximation derived by Lanczos [1].

This scheme is entirely specific to the gamma function, seemingly plucked from
thin air. We will not attempt to derive the approximation, but only state the
resulting formula: For certain integer choices of y and NV, and for certain coefficients
€1,C2,...,¢n, the gamma function is given by

P(z+l = (zt yt byte ets)
(6.1.5)

C1 c2 CN
x Vor | eo 4 fee 4 2>0
TON TET eee zen t | ©?

You can see that this is a sort of take-off on Stirling’s approximation, but with a
series of corrections that take into account the first few poles in the left complex
plane. The constant co is very nearly equal to 1. The error term is parametrized by e.

For y = 5, N = 6, and a certain set of c’s, the error is smaller than |e| < 2 x 1071,

Impressed? If not, then perhaps you will be impressed by the fact that (with these
same parameters) the formula (6.1.5) and bound on ¢ apply for the complex gamma
function, everywhere in the half complex plane Re z > 0.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

214 Chapter 6. Special Functions

It is better to implement InI'(x) than I'(z), since the latter will overflow many
computers’ floating-point representation at quite modest values of x. Often the
gamma function is used in calculations where the large values of I'(x) are divided by
other large numbers, with the result being a perfectly ordinary value. Such operations
would normally be coded as subtraction of logarithms. With (6.1.5) in hand, we can
compute the logarithm of the gamma function with two calls to a logarithm and 25
or so arithmetic operations. This makes it not much more difficult than other built-in
functions that we take for granted, such as sin x or e”:

#include <math.h>

float gammln(float xx)
Returns the value In[I'(xx)] for xx > 0.

{
Internal arithmetic will be done in double precision, a nicety that you can omit if five-figure
accuracy is good enough.
double x,y,tmp,ser;
static double cof [6]={76.18009172947146 , -86 .50532032941677,
24.01409824083091 ,-1.231739572450155,
0.1208650973866179e-2, -0 .5395239384953e-5} ;
int j;
Y=X=XX}
tmp=x+5.5;
tmp -= (x+0.5)*log(tmp) ;
ser=1.000000000190015;
for (j=0;j<=5;j++) ser += cof[j]/tty;
return -tmp+log(2.5066282746310005*ser/x) ;
3

How shall we write a routine for the factorial function n!? Generally the
factorial function will be called for small integer values (for large values it will
overflow anyway!), and in most applications the same integer value will be called for
many times. It is a profligate waste of computer time to call exp(gamm1n(n+1.0))
for each required factorial. Better to go back to basics, holding gamm1n in reserve
for unlikely calls:

#include <math.h>

float factrl(int n)
Returns the value n! as a floating-point number.
t
float gammin(float xx);
void nrerror(char error_text[]);
static int ntop=4;
static float a[33]={1.0,1.0,2.0,6.0,24.0}; Fill in table only as required.
int j;

if (n < 0) nrerror("Negative factorial in routine factrl");

if (m > 32) return exp(gammin(n+1.0));

Larger value than size of table is required. Actually, this big a value is going to overflow

on many computers, but no harm in trying.

while (ntop<n) { Fill in table up to desired value.
j=ntopt+;
alntop]=a[j] *ntop;

+

return a[n];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.1 Gamma, Beta, and Related Functions 215

A useful point is that factr1 will be exact for the smaller values of n, since
floating-point multiplies on small integers are exact on all computers. This exactness
will not hold if we turn to the logarithm of the factorials. For binomial coefficients,
however, we must do exactly this, since the individual factorials in a binomial
coefficient will overflow long before the coefficient itself will.

The binomial coefficient is defined by

= ——— 0<k<n 6.1.6
(;) iin-m OSRS (6.1.6)

#include <math.h>

float bico(int n, int k)
Returns the binomial coefficient (2) as a floating-point number.

{

float factln(int n);

return floor(0.5+exp(fact1n(n)-fact1n(k)-fact1n(n-k)));

The floor function cleans up roundoff error for smaller values of n and k.
3
which uses

float factln(int n)
Returns In(n!).

float gammln(float xx);
void nrerror(char error_text[]);
static float a[101]; A static array is automatically initialized to zero.

if (n < 0) nrerror("Negative factorial in routine factln");
if (n <= 1) return 0.0;

if (m <= 100) return a[n] ? a[n] : (aln]=gammin(n+1.0)); In range of table.
else return gammln(nt1.0); Out of range of table.

If your problem requires a series of related binomial coefficients, a good idea
is to use recurrence relations, for example

("r= aee)-(@)*(e"))

Finally, turning away from the combinatorial functions with integer valued
arguments, we come to the beta function,

(6.1.7)

1
B(z,w) = Biw,z) = | et. —t)’-ldt (6.1.8)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

216 Chapter 6. Special Functions

which is related to the gamma function by

T(z)T'(w)

B(z,w) = T(z+w)

(6.19)

hence

#include <math.h>
float beta(float z, float w)
Returns the value of the beta function B(z, w).
t
float gammin(float xx);

return exp(gamm1n(z)+gamml1n(w)-gamm1n(z+w) ) ;

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 6.

Lanczos, C. 1964, SIAM Journal on Numerical Analysis, ser. B, vol. 1, pp. 86-96. [1]

6.2 Incomplete Gamma Function, Error
Function, Chi-Square Probability Function,
Cumulative Poisson Function

The incomplete gamma function is defined by

(dt  (a>0) (6.2.1)

It has the limiting values
P(a,0) =0 and P(a,oo) =1 (6.2.2)

The incomplete gamma function P(a, x) is monotonic and (for a greater than one or
so) rises from “near-zero” to “near-unity” in a range of x centered on about a — 1,
and of width about \/a (see Figure 6.2.1).

The complement of P(a,x) is also confusingly called an incomplete gamma
function,

T(a,z)_ 1
Ta) ~ Pa)

Q(a,x) =1-Pla,z) = [ et? dt (a>0) (6.2.3)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.2 Incomplete Gamma Function 217

incomplete gamma function P (a,x)

Figure 6.2.1. The incomplete gamma function P(a, x) for four values of a.
It has the limiting values
Q(a,0) =1 and Q(a,co) =0 (6.2.4)

The notations P(a,x),y(a,x), and I'(a, x) are standard; the notation Q(a, x) is
specific to this book.
There is a series development for 7(a, x) as follows:

oy TO on
“ > T(ia+1+ mn) (62.5)

One does not actually need to compute a new ['(a + 1 + 2) for each n; one rather
uses equation (6.1.3) and the previous coefficient.
A continued fraction development for I'(a, x) is

(x>0) (6.2.6)

T(a,2) ete ( 1 1l-a 1 2-a 2 )

a+ lt 2+ 1+ et

It is computationally better to use the even part of (6.2.6), which converges twice
as fast (see §5.2):

1 1-(l-a) 2-(2-a)
c+l-a t+3—-a w+5—-a

(x > 0)
(6.2.7)

It turns out that (6.2.5) converges rapidly for x less than about a + 1, while
(6.2.6) or (6.2.7) converges rapidly for x greater than about a + 1. In these respective

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

218 Chapter 6. Special Functions

regimes each requires at most a few times \/a terms to converge, and this many
only near x = a, where the incomplete gamma functions are varying most rapidly.
Thus (6.2.5) and (6.2.7) together allow evaluation of the function for all positive
a and x. An extra dividend is that we never need compute a function value near
zero by subtracting two nearly equal numbers. The higher-level functions that return
P(a,x) and Q(a,x) are

float gammp(float a, float x)
Returns the incomplete gamma function P(a, 2).

{
void gcf(float *gammcf, float a, float x, float *gln);
void gser(float *gamser, float a, float x, float *gln);
void nrerror(char error_text[]);
float gamser,gammcf,gin;
if (x < 0.0 || a <= 0.0) nrerror("Invalid arguments in routine gammp");
if (x < (at1.0)) { Use the series representation.
gser (&gamser,a,x,&gln) ;
return gamser;
} else { Use the continued fraction representation
gcf (&gammcf ,a,x,&gln) ;
return 1.0-gammcf; and take its complement.
+
3

float gammq(float a, float x)
Returns the incomplete gamma function Q(a,x) = 1— P(a,«).

t
void gcf(float *gammcf, float a, float x, float *gln);
void gser(float *gamser, float a, float x, float *gln);
void nrerror(char error_text[]);
float gamser,gammcf,gin;
if (x < 0.0 || a <= 0.0) nrerror("Invalid arguments in routine gammq");
if (x < (at1.0)) { Use the series representation
gser (&gamser,a,x,&gln) ;
return 1.0-gamser; and take its complement.
} else { Use the continued fraction representation.
gcf (&gammcf ,a,x,&gln) ;
return gammcf;
+
}

The argument gln is set by both the series and continued fraction procedures
to the value InI'(a); the reason for this is so that it is available to you if you want to
modify the above two procedures to give y(a, x) and I'(a, x), in addition to P(a, x)
and Q(a, x) (cf. equations 6.2.1 and 6.2.3).

The functions gser and gcf which implement (6.2.5) and (6.2.7) are

#include <math.h>
#define ITMAX 100
#define EPS 3.0e-7

void gser(float *gamser, float a, float x, float *gln)

Returns the incomplete gamma function P(a, x) evaluated by its series representation as gamser.

Also returns InI(a) as gln.
{
float gammin(float xx);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.2 Incomplete Gamma Function 219

void nrerror(char error_text[]);

int

n;

float sum,del,ap;

*gln=gamm1n(a) ;

if (

x <= 0.0) {

if (x < 0.0) nrerror("x less than 0 in routine gser");
*gamser=0.0;

return;

} else {

ap=a;
del=sum=1.0/a;
for (n=1;n<=ITMAX;n++) {
+tap;
del *= x/ap;
sum += del;
if (fabs(del) < fabs(sum)*EPS) {
*gamser=sum*exp (-x+a*log(x)-(*gln));
return;
3
}
nrerror("a too large, ITMAX too small in routine gser");
return;

#include <math.h>

#define ITMAX 100
#define EPS 3.0e-7
#define FPMIN 1.0e-30

Maximum allowed number of iterations.

Relative accuracy.

Number near the smallest representable
floating-point number.

void gcf(float *gammcf, float a, float x, float *gln)
Returns the incomplete gamma function Q(a,«) evaluated by its continued fraction represen-
tation as gammcf. Also returns InI'(a) as gln.

{

float gammln(float xx);
void nrerror(char error_text[]);

int

a;

float an,b,c,d,del,h;

*gln=gamm1n(a) ;

b=x+1.0-a;
c=1.0/FPMIN;
d=1.0/b;
h=d;
for (i=1;i<=ITMAX;i++) {
an = -i*(i-a);
b += 2.0;
d=an*dt+b;
if (fabs(d) < FPMIN) d=FPMIN;
c=btan/c;
if (fabs(c) < FPMIN) c=FPMIN;
d=1.0/d;
del=d*c;
h *= del;

if (fabs(del-1.0) < EPS) break;

+

Set up for evaluating continued fraction
by modified Lentz'’s method (§5.2)
with bo = 0.

Iterate to convergence.

if (i > ITMAX) nrerror("a too large, ITMAX too small in gcf");

*gammcf=exp(-x+a*log(x)-(*gln))*h;

Put factors in front.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

220 Chapter 6. Special Functions

Error Function

The error function and complementary error function are special cases of the
incomplete gamma function, and are obtained moderately efficiently by the above
procedures. Their definitions are

2 x

erf(z) = <= | eat 628
=| (628)
and
(6.2.9)
The functions have the following limiting values and symmetries:
erf(0) = 0 erf(co) =1 erf(—a) = —erf(x) (6.2.10)

erfc(0) = 1 erfc(oo) = 0 erfc(—a) = 2 — erfe(x) (6.2.11)

They are related to the incomplete gamma functions by

erf(x) = P(5.0°) (x > 0) (6.2.12)
and
erfe(x) = Q (5 ”) (x > 0) (6.2.13)

We’ll put an extra “‘f” into our routine names to avoid conflicts with names already
in some C libraries:

float erff(float x)
Returns the error function erf(x).
{
float gammp(float a, float x);

return x < 0.0 ? -gammp(0.5,x*x) : gammp(0.5,x*x);

float erffc(float x)
Returns the complementary error function erfc(x).

t

float gammp(float a, float x);

float gammq(float a, float x);

return x < 0.0 ? 1.0+gammp(0.5,x*x) : gammq(0.5,x*x) ;
}

If you care to do so, you can easily remedy the minor inefficiency in erff and
erffc, namely that (0.5) = \/z is computed unnecessarily when gammp or gammq
is called. Before you do that, however, you might wish to consider the following
routine, based on Chebyshev fitting to an inspired guess as to the functional form:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.2 Incomplete Gamma Function 221

#include <math.h>

float erfcc(float x)
Returns the complementary error function erfc(x) with fractional error everywhere less than
1.2 x 10-7.

{
float t,z,ans;
z=fabs(x);
t=1.0/(1.0+0.5*z) ;
ans=t*exp (-z*z-1.26551223+t*(1.00002368+t*(0.37409196+t* (0 .09678418+
t*(-0.18628806+t* (0. 27886807+t* (-1.13520398+t* (1.48851587+
t*(-0.82215223+t*0.17087277)))))))))3
return x >= 0.0 ? ans : 2.0-ans;
3

There are also some functions of two variables that are special cases of the
incomplete gamma function:

Cumulative Poisson Probability Function
P,(< k), for positive x and integer k > 1, denotes the cumulative Poisson
probability function. It is defined as the probability that the number of Poisson
random events occurring will be between 0 and k — 1 inclusive, if the expected mean
number is x. It has the limiting values
P,(<1l)=e* P,(< oc) =1 (6.2.14)

Its relation to the incomplete gamma function is simply

Chi-Square Probability Function

P(x?|v) is defined as the probability that the observed chi-square for a correct
model should be less than a value x7. (We will discuss the use of this function in
Chapter 15.) Its complement Q(x?|v) is the probability that the observed chi-square
will exceed the value y? by chance even for a correct model. In both cases v is an
integer, the number of degrees of freedom. The functions have the limiting values

P(O|v) =0 P(oolv) = 1 (6.2.16)
QOlv)=1 — Q(colv) =0 (6.2.17)

and the following relation to the incomplete gamma functions,
Poe) = P(%,X) = yx (6.2.18
MSN ag) = SAmP\ DD 2.18)

QO? |r) = als <) = ganna (5 <) (6.2.19)

22 22

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

222 Chapter 6. Special Functions

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapters 6, 7, and 26.

Pearson, K. (ed.) 1951, Tables of the Incomplete Gamma Function (Cambridge: Cambridge
University Press).

6.3 Exponential Integrals

The standard definition of the exponential integral is

oo et
E,(x) -| at, x>0, n=0,1,... (6.3.1)
1

The function defined by the principal value of the integral

Ei(z) = — 7 dt = 7 dt, z>0 (6.3.2)

-2 oo

is also called an exponential integral. Note that Ei(—2) is related to —E\(x) by
analytic continuation.
The function E,, (2) is a special case of the incomplete gamma function

E,(a) = 2" 'P(1—n,2) (6.3.3)

We can therefore use a similar strategy for evaluating it. The continued fraction —
just equation (6.2.6) rewritten — converges for all x > 0:

E,(x) = e7* ( (6.3.4)

1 on 1 n+l 2
a+ 1+ a+ 1+ e+

We use it in its more rapidly converging even form,

1-
c+n—- x+n+2—-— x+n+4—-

The continued fraction only really converges fast enough to be useful for x 2 1.
For 0 < x < 1, we can use the series representation

n= oo _,ym
E,(a) = 4 - [-ne + (n)] — yy es (636)
mzn—1

The quantity a(n) here is the digamma function, given for integer arguments by

(6.3.7)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.3 Exponential Integrals 223

where 7 = 0.5772156649 . .. is Euler’s constant. We evaluate the expression (6.3.6)
in order of ascending powers of x:

1 a ed 4 (—a)"~?
Bute) =— [gy ey t Pat aay
; (- n—1 - (—a)n+t
"Dy! [Ine + WM) ~ | +a pay |

(6.3.8)

The first square bracket is omitted when n = 1. This method of evaluation has the
advantage that for large n the series converges before reaching the term containing
w(n). Accordingly, one needs an algorithm for evaluating 7(n) only for small n,
n S 20-40. We use equation (6.3.7), although a table look-up would improve
efficiency slightly.

Amos [1] presents a careful discussion of the truncation error in evaluating
equation (6.3.8), and gives a fairly elaborate termination criterion. We have found
that simply stopping when the last term added is smaller than the required tolerance
works about as well.

Two special cases have to be handled separately:

Eo(:

(63.9)

En(0) = 7) n>1

The routine expint allows fast evaluation of E(x) to any accuracy EPS
within the reach of your machine’s word length for floating-point numbers. The
only modification required for increased accuracy is to supply Euler’s constant with
enough significant digits. Wrench [2] can provide you with the first 328 digits if
necessary!

#include <math.h>

#define MAXIT 100 Maximum allowed number of iterations.

#define EULER 0.5772156649 Euler’s constant +.

#define FPMIN 1.0e-30 Close to smallest representable floating-point number.

#define EPS 1.0e-7 Desired relative error, not smaller than the machine pre-
cision.

float expint(int n, float x)
Evaluates the exponential integral E,, (x).
t

void nrerror(char error_text[]);

int i,ii,nm1;

float a,b,c,d,del,fact,h,psi,ans;

nmi=n-1;
if (n< 0 || x < 0.0 || (x==0.0 && (m==0 || n==1)))
nrerror("bad arguments in expint");
else {
if (m == 0) ans=exp(-x)/x; Special case.
else {
if (x == 0.0) ans=1.0/nm1; Another special case.

else {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

224 Chapter 6. Special Functions

if (x > 1.0) { Lentz’s algorithm (§5.2).
b=x+n;
c=1.0/FPMIN;
d=1.0/b;
h=d;
for (i=1;i<=MAXIT;i++) {
a = -i*(nmiti);
b += 2.0;

.0/ (a*d+tb) ; Denominators cannot be zero.

h *= del;

if (fabs(del-1.0) < EPS) {
ans=h*exp(-x) ;
return ans;

}
+
nrerror("continued fraction failed in expint");
} else { Evaluate series.
ans = (nmi!=0 ? 1.0/nm1 : -log(x)-EULER) ; Set first term.
fact=1.0;
for (i=1;i<=MAXIT;i++) {
fact -x/i;
if (i != nm1) del = -fact/(i-nm1);
else {
psi = -EULER; Compute ~(n).
for (ii=1;ii<=nm1;iit++) psi += 1.0/ii;
del=fact*(-log(x)+psi) ;
}
ans += del;
if (fabs(del) < fabs(ans)*EPS) return ans;
+

nrerror("series failed in expint");

}
+

return ans;

A good algorithm for evaluating Ei is to use the power series for small x and
the asymptotic series for large . The power series is

2
t { bree 6.3.10
1-1! 2-2! ( )
where 7 is Euler’s constant. The asymptotic expansion is
Ei ui 63.11
i(x) ~ T4o424... 3.

The lower limit for the use of the asymptotic expansion is approximately | In EPS|,
where EPS is the required relative error.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.3 Exponential Integrals 225

#include <math.h>

#define EULER 0.57721566 Euler’s constant +.

#define MAXIT 100 Maximum number of iterations allowed.

#define FPMIN 1.0e-30 Close to smallest representable floating-point number.

#define EPS 6.0e-8 Relative error, or absolute error near the zero of Ei at
a = 0.3725.

float ei(float x)
Computes the exponential integral Ei(x) for « > 0.
{

void nrerror(char error_text[]);

int k;

float fact,prev,sum,term;

if (x <= 0.0) nrerror("Bad argument in ei");
if (x < FPMIN) return log(x)+EULER; Special case: avoid failure of convergence
if (x <= -log(EPS)) { test because of underflow.
sum=0. Use power series.
fact=1.0;
for (k=1;k<=MAXIT;kt+) {
fact *= x/k;
term=fact/k;
sum += term;
if (term < EPS*sum) break;

}
if (k > MAXIT) nrerror("Series failed in ei");
return sum+log(x)+EULER;
} else { Use asymptotic series.
sum=0.0; Start with second term.
term=1.0;
for (k=1;k<=MAXIT;kt+) {
prev=term;
term *= k/x;
if (term < EPS) break;
Since final sum is greater than one, term itself approximates the relative error.
if (term < prev) sum += term; Still converging: add new term.
else {
sum -= prev; Diverging: subtract previous term and
break; exit.
3
}

return exp(x)*(1.0+sum)/x;

CITED REFERENCES AND FURTHER READING:

Stegun, |.A., and Zucker, R. 1974, Journal of Research of the National Bureau of Standards,
vol. 78B, pp. 199-216; 1976, op. cit., vol. 80B, pp. 291-311.

Amos D.E. 1980, ACM Transactions on Mathematical Software, vol. 6, pp. 365-377 [1]; also
vol. 6, pp. 420-428.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 5.

Wrench J.W. 1952, Mathematical Tables and Other Aids to Computation, vol. 6, p. 255. [2]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

226 Chapter 6. Special Functions

a a OO
re 4
r (05,50) vee
a7 7
L “7 , Fa
. i
gh /“ (8.0,10.0) a
~~ a :
S L : 14
3 L 1
~% t
fi
< r aoe
£ 6e i 4
oO 1
s 1 4
& 0.5.0.5) i]
g t
2 i
g4 / 4
a 4
I /
6 ’ 4
§ :
& “ 4
2 a a
ae (5.0,0.5)
0
Coed |
0 2 4 6 8 1

x
Figure 6.4.1. The incomplete beta function J; (a, b) for five different pairs of (a,b). Notice that the
pairs (0.5, 5.0) and (5.0, 0.5) are symmetrically related as indicated in equation (6.4.3).
6.4 Incomplete Beta Function, Student’s
Distribution, F-Distribution, Cumulative
Binomial Distribution

The incomplete beta function is defined by

Le(a.b) = eer Tn ee (a,b>0) (64.1)

It has the limiting values
Ip(a,b) =0 I,(a,b) =1 (6.4.2)

and the symmetry relation
I, (a,b) = 1— T~2(b, a) (6.4.3)
If a and 0 are both rather greater than one, then J,,(a,b) rises from “near-zero” to

“near-unity” quite sharply at about z = a/(a + 6). Figure 6.4.1 plots the function
for several pairs (a,b).

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD
(S-801€b-1ZS-0 NES!) DNILAdNOO OISILNSIOS 4O LHW SHL !O NI SAdIO4Y TWOIMSWNN Woy aBed ejdwes

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

6.4 Incomplete Beta Function 227

The incomplete beta function has a series expansion

x*(1—2)  Biatlnt) anys
I, (a,b 14 ptt) 644
(0) = Bab) > Blaatbn+1) |’ (6.44)

but this does not prove to be very useful in its numerical evaluation. (Note, however,
that the beta functions in the coefficients can be evaluated for each value of n with
just the previous value and a few multiplies, using equations 6.1.9 and 6.1.3.)

The continued fraction representation proves to be much more useful,

e(1—2)[ 1 dy dp
To(a,0) = “Ba b) ls I+1+ (645)
where
d (a+m)(a+b+m)ax
aml (a + 2m)(a + 2m + 1) 646
dom =

(a + 2m —1)(a + 2m)

This continued fraction converges rapidly for x < (a + 1)/(a + b + 2), taking in
the worst case O(,/max(a, b)) iterations. But for 7 > (a + 1)/(a +b + 2) we can
just use the symmetry relation (6.4.3) to obtain an equivalent computation where the
continued fraction will also converge rapidly. Hence we have

#include <math.h>

float betai(float a, float b, float x)
Returns the incomplete beta function Ix(a,b).

{
float betacf(float a, float b, float x);
float gammin(float xx);
void nrerror(char error_text[]);
float bt;
if (x < 0.0 || x > 1.0) nrerror("Bad x in routine betai");
if (x == 0.0 || x == 1.0) bt=0.0;
else Factors in front of the continued fraction.
bt=exp(gamm1n(atb) -gamm1n(a) -gamm1n(b) +a*log (x) +b*log(1.0-x)) ;
if (x < (at1.0)/(atb+2.0)) Use continued fraction directly.
return bt*betacf(a,b,x)/a;
else Use continued fraction after making the sym-
return 1.0-bt*betacf(b,a,1.0-x)/b; metry transformation.
3

which utilizes the continued fraction evaluation routine

#include <math.h>
#define MAXIT 100
#define EPS 3.0e-7
#define FPMIN 1.0e-30

float betacf(float a, float b, float x)
Used by betai: Evaluates continued fraction for incomplete beta function by modified Lentz’s

method (§5.2).
{

void nrerror(char error_text[]);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

228 Chapter 6. Special Functions

int m,m2;
float aa,c,d,del,h,qab,qam,qap;

qab=atb; These q's will be used in factors that occur
qap=at1.0; in the coefficients (6.4.6).

-1.0;

.0;

.0-qab*x/qap;

(fabs(d) < FPMIN) d=FPMIN;
-0/d;

First step of Lentz’s method.

for (m=1;m<=MAXIT;m++) {

m2=2+m;
aa=m* (b-m) *x/((qam+m2) *(atm2)) ;
d=1.0+aaxd; One step (the even one) of the recurrence.

if (fabs(d) < FPMIN) d=FPMIN;
c=1.0+aa/c;
if (fabs(c) < FPMIN) c=FPMIN;

d=1.0/d;

h *= d*c;

aa = -(atm)*(qab+m)*x/((a+m2)*(qap+m2)) ;

d=1.0+aaxd; Next step of the recurrence (the odd one).

if (fabs(d) < FPMIN) d=FPMIN;
c=1.0+aa/c;
if (fabs(c) < FPMIN) c=FPMIN;

d=1.0/d;

del=d*c;

h *= del;

if (fabs(del-1.0) < EPS) break; Are we done?
+
if (m > MAXIT) nrerror("a or b too big, or MAXIT too small in betacf");
return h;

Student’s Distribution Probability Function

Student’s distribution, denoted A(t|v), is useful in several statistical contexts,
notably in the test of whether two observed distributions have the same mean. A(t|v)
is the probability, for 1 degrees of freedom, that a certain statistic t (measuring
the observed difference of means) would be smaller than the observed value if the
means were in fact the same. (See Chapter 14 for further details.) Two means are
significantly different if, e.g., A(t|v) > 0.99. In other words, 1 — A(¢|v) is the
significance level at which the hypothesis that the means are equal is disproved.

The mathematical definition of the function is

vt

A(t\v) 1 (i a) we (6.42)

Limiting values are
A(O|v) = 0 A(oolv) = 1 (6.4.8)

A(t|v) is related to the incomplete beta function I, (a, b) by

A(t\y) =1- Te, (5 7) (6.49)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.4 Incomplete Beta Function 229

So, you can use (6.4.9) and the above routine betai to evaluate the function.

F-Distribution Probability Function

This function occurs in the statistical test of whether two observed samples
have the same variance. A certain statistic F’, essentially the ratio of the observed
dispersion of the first sample to that of the second one, is calculated. (For further
details, see Chapter 14.) The probability that F’ would be as large as it is if the first
sample’s underlying distribution actually has smaller variance than the second’s is
denoted Q(F|v1, 2), where v; and v2 are the number of degrees of freedom in the
first and second samples, respectively. In other words, Q(F'|v;, 2) is the significance
level at which the hypothesis “1 has smaller variance than 2” can be rejected. A
small numerical value implies a very significant rejection, in turn implying high
confidence in the hypothesis “1 has variance greater or equal to 2.”

Q(F|1,v2) has the limiting values

Q(0|r1, v2) = 1 Q(co|1, 72) = 0 (6.4.10)

Its relation to the incomplete beta function I,,(a, b) as evaluated by betai above is

Q(Flv1,12) =I» (3 +) (6.4.11)

Cumulative Binomial Probability Distribution

Suppose an event occurs with probability p per trial. Then the probability P of
its occurring k or more times in 7 trials is termed a cumulative binomial probability,
and is related to the incomplete beta function J, (a, b) as follows:

ray ("\e'a-p =1,(k,n—k+1) (6.4.12)

For n larger than a dozen or so, betai is a much better way to evaluate the sum in
(6.4.12) than would be the straightforward sum with concurrent computation of the
binomial coefficients. (For n smaller than a dozen, either method is acceptable.)

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapters 6 and 26.

Pearson, E., and Johnson, N. 1968, Tables of the Incomplete Beta Function (Cambridge: Cam-
bridge University Press).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

230 Chapter 6. Special Functions

6.5 Bessel Functions of Integer Order

This section and the next one present practical algorithms for computing various
kinds of Bessel functions of integer order. In §6.7 we deal with fractional order. In
fact, the more complicated routines for fractional order work fine for integer order
too. For integer order, however, the routines in this section (and §6.6) are simpler
and faster. Their only drawback is that they are limited by the precision of the
underlying rational approximations. For full double precision, it is best to work with
the routines for fractional order in §6.7.

For any real v, the Bessel function J,(a) can be defined by the series
representation

J(z) = (5 “> we (65.1)

The series converges for all x, but it is not computationally very useful for x >> 1.
For v not an integer the Bessel function Y,(x) is given by

J, (x) cos(vm) — J_,(x)

Y, (2) (6.5.2)

cae

The right-hand side goes to the correct limiting value Y,, (x) as v goes to some integer
n, but this is also not computationally useful.

For arguments « < v, both Bessel functions look qualitatively like simple
power laws, with the asymptotic forms for0 <«# <v

v(x) ~ no (3) v>0

Yi(z) 7 (5) aso

For x > v, both Bessel functions look qualitatively like sine or cosine waves whose
amplitude decays as x~!/?. The asymptotic forms for a >> v are

2 1 1
Jy(a) ~ 4/ = cos(« ~ 5um— i")

2 1 1
Y,(x) ~  ain(e— Son — 3)

In the transition region where x ~ v, the typical amplitudes of the Bessel functions
are on the order

(6.54)

gs 4 0.4473
32/37 (2) pi/3 pi/3

Qs 1 __ 0.7748
3/67 (3) pi/3 yi/3

Jv) ~
(6.55)
Yi(v) ~ -

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

6.5 Bessel Functions of Integer Order 231

g 0 a

& a

bs L |

Q L |

é sl, ‘ ’ 4

3 E 1 Yo y, “ J

Z Ly ly 4
o ca , Y,

[aa rt U ’ 2 |

lL, y 4

-1 1 / |

im Uy 1 7

proof Ul al

riot t al

for ot 1

“lotr q

ji’ ot 4

fb! 4 4

fb ! 4 4

btw 1 | 4 | L | | ou 1
0 2 4 6 8 10

xX
Figure 6.5.1. Bessel functions Jo(x) through J3(a) and Yo(a) through Y2(a).
which holds asymptotically for large v. Figure 6.5.1 plots the first few Bessel

functions of each kind.
The Bessel functions satisfy the recurrence relations

Inar(a) = 7, (0) ~ In a(x) (6.56)
and »
Yuta) = ¥n(a) = Yn-u (a) (65.7)

As already mentioned in §5.5, only the second of these (6.5.7) is stable in the
direction of increasing n for x <n. The reason that (6.5.6) is unstable in the
direction of increasing n is simply that it is the same recurrence as (6.5.7): A small
amount of “polluting” Y,, introduced by roundoff error will quickly come to swamp
the desired J,,, according to equation (6.5.3).

A practical strategy for computing the Bessel functions of integer order divides
into two tasks: first, how to compute Jo, J1, Yo, and Y;, and second, how to use the
recurrence relations stably to find other J’s and Y’s. We treat the first task first:

For x between zero and some arbitrary value (we will use the value 8),
approximate Jo(z) and J; (zx) by rational functions in x. Likewise approximate by
rational functions the “regular part” of Yo(x) and Yj (zx), defined as

Yo(a) — 7 Jo(x) n(a) and via) = 2 |(2)Im(e) ~ 4] (6.5.8)

For 8 < a < oo, use the approximating forms (n = 0, 1)

Jn(e) = 4 2 [Ps (=) cos(Xn) — Qn (=) sin(Xn) (659)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

232 Chapter 6. Special Functions

Y,(x) = = [r.(5) sin(X,) + Qn( =) cos(n)| (6.5.10)

where

2n+1

Xp, =x r

T (6.5.11)
and where Po, Pi,Qo, and Q are each polynomials in their arguments, for 0 <
8/x < 1. The P’s are even polynomials, the Q’s odd.

Coefficients of the various rational functions and polynomials are given by
Hart [1], for various levels of desired accuracy. A straightforward implementation is

#include <math.h>

float bessjO(float x)
Returns the Bessel function Jo(x) for any real x.
{
float ax,z;
double xx,y,ans,ansi1,ans2; Accumulate polynomials in double precision.

if ((ax=fabs(x)) < 8.0) f Direct rational function fit.
YHKEX;
ans1=57568490574. O+y* (-13362590354. O+y* (651619640.7
ty*(-11214424.18+y* (77392 .33017+y* (-184.9052456)))));
ans2=57568490411 . 0+y* (1029532985. O+y* (9494680 .718
t+y* (59272. 64853+y* (267 .8532712+y*1.0))));
ans=ansi/ans2;
} else { Fitting function (6.5.9).
z=8.0/ax;
y=Z¥z;
xx=ax-0.785398164;
ans1=1.0+y*(-0.1098628627e-2t+y* (0.2734510407e-4
+y*(-0.2073370639e-5t+y*0 . 2093887211e-6) )) ;
ans2 = -0.1562499995e-1+y*(0.1430488765e-3
t+y*(-0.6911147651e-5t+y*(0.7621095161e-6
-y*0.934945152e-7)));
ans=sqrt (0.636619772/ax) * (cos (xx) *ans1-z*sin (xx) *ans2) ;
+

return ans;

#include <math.h>

float bessy0(float x)
Returns the Bessel function Yo(x) for positive x.

{
float bessjO(float x);
float z;
double xx,y,ans,ansi1,ans2; Accumulate polynomials in double precision.
if (x < 8.0) { Rational function approximation of (6.5.8).
YHKEX;

ansi = -2957821389.0+y* (7062834065. O+y* (-512359803.6
+y* (10879881 . 29+y* (-86327 .92757+y*228 . 4622733) ))) ;
ans2=40076544269 . O+y* (745249964. 8+y* (7189466 . 438
t+y* (47447 . 26470+y* (226. 1030244+y*1.0))));
ans=(ans1/ans2)+0.636619772*bessj0 (x) *log(x) ;
} else { Fitting function (6.5.10).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.5 Bessel Functions of Integer Order 233

yr2ez;

xx=x-0.785398164;

ans1=1.0+y*(-0.1098628627e-2t+y* (0.2734510407e-4
+y*(-0.2073370639e-5t+y*0 . 2093887211e-6) )) ;

ans2 = -0.1562499995e-1+y*(0.1430488765e-3
t+y*(-0.6911147651e-5t+y*(0.7621095161e-6
t+y*(-0.934945152e-7))));

ans=sqrt (0.636619772/x) * (sin(xx) *ans1+z*cos (xx) *ans2) ;

+

return ans;

#include <math.h>

float bessji(float x)
Returns the Bessel function J; (x) for any real x.
{
float ax,z;
double xx,y,ans,ansi1,ans2; Accumulate polynomials in double precision.

if ((ax=fabs(x)) < 8.0) { Direct rational approximation.
YHKEX;
ans1=x* (72362614232. 0+y* (-7895059235 . O+y* (242396853. 1
t+y*(-2972611.439+y* (15704. 48260+y*(-30. 16036606) )))));
ans2=144725228442. 0+y* (2300535178. 0+y* (18583304.74
t+y* (99447 .43394+y* (376.9991397+y*1.0))));
ans=ansi/ans2;
} else { Fitting function (6.5.9).
z=8.0/ax;
y=Z¥z;
xx=ax-2.356194491;
ans1=1.0+y*(0.183105e-2+y*(-0.3516396496e-4
t+y*(0.2457520174e-5t+y*(-0.240337019e-6) ))) ;
ans2=0 .04687499995+y* (-0 . 2002690873e-3
t+y*(0.8449199096e-5t+y* (-0 .88228987e-6
+y*0.105787412e-6) )) ;
ans=sqrt (0.636619772/ax) * (cos (xx) *ans1-z*sin (xx) *ans2) ;
if (x < 0.0) ans = -ans;
+

return ans;

#include <math.h>

float bessy1(float x)
Returns the Bessel function Yi(x) for positive x.

{
float bessji(float x);
float z;
double xx,y,ans,ansi1,ans2; Accumulate polynomials in double precision.
if (x < 8.0) { Rational function approximation of (6.5.8).
YHKEX;

ans1=x*(-0.4900604943e13+y*(0.1275274390e13
t+y*(-0.5153438139e11+y*(0.7349264551e9
t+y*(-0.4237922726e7+y*0 .8511937935e4)))));

ans2=0 .2499580570e14+y*(0.4244419664e12
t+y*(0.3733650367e10+y* (0. 2245904002e8
t+y*(0.1020426050e6+y* (0. 3549632885e3+y) )))) 3

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

234 Chapter 6. Special Functions

ans=(ans1/ans2)+0.636619772* (bessj1(x)*log(x)-1.0/x);
} else { Fitting function (6.5.10).
z=8.0/x;
y=Z¥z;
xx=x-2.356194491;
ans1=1.0+y*(0.183105e-2+y*(-0.3516396496e-4
t+y*(0.2457520174e-5t+y*(-0.240337019e-6) ))) ;
ans2=0 .04687499995+y* (-0 . 2002690873e-3
+y*(0.8449199096e-5t+y* (-0 .88228987e-6
+y*0.105787412e-6) )) ;
ans=sqrt (0.636619772/x) * (sin(xx) *ans1+z*cos (xx) *ans2) ;
+

return ans;

We now turn to the second task, namely how to use the recurrence formulas
(6.5.6) and (6.5.7) to get the Bessel functions J,,(x) and Y;,(a) for n > 2. The latter
of these is straightforward, since its upward recurrence is always stable:

float bessy(int n, float x)
Returns the Bessel function Yn(x) for positive x and n > 2.
{

float bessy0(float x);

float bessy1(float x);

void nrerror(char error_text[]);

int j;

float by,bym,byp,tox;

if (mn < 2) nrerror("Index n less than 2 in bessy");
tox=2.0/x;
by=bessy1 (x) ; Starting values for the recurrence.
bym=bessy0 (x) ;
for (j=1;j<n;jt++) f Recurrence (6.5.7).
byp=j*tox*by-bym;
bym=by ;
by=byp;

return by;

The cost of this algorithm is the call to bessy1 and bessy0 (which generate a
call to each of bessj1 and bessj0), plus O(n) operations in the recurrence.

As for Jn(), things are a bit more complicated. We can start the recurrence
upward on n from Jo and Jj, but it will remain stable only while n does not exceed
x. This is, however, just fine for calls with large x and small n, a case which
occurs frequently in practice.

The harder case to provide for is that with « < n. The best thing to do here
is to use Miller’s algorithm (see discussion preceding equation 5.5.16), applying
the recurrence downward from some arbitrary starting value and making use of the
upward-unstable nature of the recurrence to put us onto the correct solution. When
we finally arrive at Jo or J; we are able to normalize the solution with the sum
(5.5.16) accumulated along the way.

The only subtlety is in deciding at how large an n we need start the downward
recurrence so as to obtain a desired accuracy by the time we reach the n that we
really want. If you play with the asymptotic forms (6.5.3) and (6.5.5), you should
be able to convince yourself that the answer is to start larger than the desired n by

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.5 Bessel Functions of Integer Order 235

an additive amount of order [constant x n]1/?, where the square root of the constant

is, very roughly, the number of significant figures of accuracy.
The above considerations lead to the following function.

#include <math.h>

#define ACC 40.0 Make larger to increase accuracy.
#define BIGNO 1.0e10

#define BIGNI 1.0e-10

float bessj(int n, float x)
Returns the Bessel function Jn(x) for any real x and n > 2.
{

float bessjO(float x);

float bessji(float x);

void nrerror(char error_text[]);

int j,jsum,m;

float ax,bj,bjm,bjp,sum,tox,ans;

if (n < 2) nrerror("Index n less than 2 in bessj");
ax=fabs (x);
if (ax == 0.0)
return 0.0;
else if (ax > (float) n) { Upwards recurrence from Jo and J.
tox=2.0/ax;
bjm=bessj0(ax) ;
bj=bessj1(ax);
for (j=1;j<n;jt+) {
bjp=j*tox*bj-bjm;

bj i
bj=bjp;
ans=bj;
} else { Downwards recurrence from an even m here com-
tox=2.0/ax; puted.
m=2*((n+(int) sqrt (ACC*n))/2);
jsum=0; jsum will alternate between 0 and 1; when it is
bjp=ans=sum=0.0; 1, we accumulate in sum the even terms in

(5.5.16).
=m;j>0;j--) { The downward recurrence.
*tox*bj-bjp;
bj=bjm;
if (fabs(bj) > BIGNO) { Renormalize to prevent overflows.
bj *= BIGNI;
bjp *= BIGNI;
ans *= BIGNI;
sum *= BIGNI;
3
if (jsum) sum += bj; Accumulate the sum.
jsum=!jsum; Change 0 to 1 or vice versa.
if (j == n) ans=bjp; Save the unnormalized answer.
}
sum=2.0*sum-bj; Compute (5.5.16)
ans /= sum; and use it to normalize the answer.

+

return x < 0.0 && (n & 1) ? -ans : ans;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

236 Chapter 6. Special Functions

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 9.

Hart, J.F., et al. 1968, Computer Approximations (New York: Wiley), §6.8, p. 141. [1]

6.6 Modified Bessel Functions of Integer Order

The modified Bessel functions J,,(x) and K,,(x) are equivalent to the usual
Bessel functions J, and Y,, evaluated for purely imaginary arguments. In detail,
the relationship is

In(x) = (-#)" In (iz)
T (6.6.1)

K,,(x) = zi aliz) + iY, (ix)

The particular choice of prefactor and of the linear combination of J,, and Y,, to form
XX, are simply choices that make the functions real-valued for real arguments «x.

For small arguments x <n, both I,,(2) and K,,(x) become, asymptotically,
simple powers of their argument

I,(a) © — (5) n>0

nl \2 =

—In(2) (6.6.2)
— (n=l)l2y-
a (5) n>0

These expressions are virtually identical to those for J,,(x) and Y,, (a) in this region,
except for the factor of —2/m difference between Y,,(x) and K(x). In the region
x >> n, however, the modified functions have quite different behavior than the
Bessel functions,

1
In(ce) * exp (e)

one (6.6.3)
Kl) © Ea exp(—x)

2nx

,

The modified functions evidently have exponential rather than sinusoidal be-
havior for large arguments (see Figure 6.6.1). The smoothness of the modified
Bessel functions, once the exponential factor is removed, makes a simple polynomial
approximation of a few terms quite suitable for the functions Jo, [;, Ko, and Ky.
The following routines, based on polynomial coefficients given by Abramowitz and
Stegun [1], evaluate these four functions, and will provide the basis for upward
recursion for n > 1 when x > n.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.6 Modified Bessel Functions of Integer Order

237

ee lal

modified Bessel functions

Figure 6.6.1. Modified Bessel functions Ip(a) through [3 (a), Ko(a) through K2(a).

#include <math.h>

float bessi0(float x)
Returns the modified Bessel function [o9(x) for any real x.

{
float ax,ans;
double y; Accumulate polynomials in double precision.
if ((ax=fabs(x)) < 3.75) { Polynomial fit.
y=x/3.75;
y*=y;
ans=1.0+y*(3.5156229+y*(3.0899424+y*(1.2067492
+y*(0.2659732+y* (0. 360768e-1+y*0 .45813e-2)))));
} else {
y=3.75/ax;
ans=(exp(ax)/sqrt (ax)) *(0.39894228+y*(0.1328592e-1
t+y*(0.225319e-2+y* (-0.157565e-2+y*(0.916281e-2
t+y*(-0.2057706e-1+y*(0.2635537e-1+y* (-0.1647633e-1
+y*0.392377e-2))))))))3
+
return ans;
3

#include <math.h>

float besskO(float x)
Returns the modified Bessel function K(x) for positive real x.
{

float bessi0(float x);

double y,ans; Accumulate polynomials in double precision.

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

238 Chapter 6. Special Functions

if (x <= 2.0) { Polynomial fit.
y=x*x/4.0;
ans=(-log(x/2.0)*bessi0(x) )+(-0.57721566+y*(0.42278420
t+y*(0.23069756+y* (0 .3488590e-1+y* (0. 262698e-2
+y*(0.10750e-3+y*0.74e-5))))));
} else {
y=2.0/x;
ans=(exp(-x)/sqrt (x) )*(1.25331414+y*(-0.7832358e-1
t+y*(0.2189568e-1+y*(-0.1062446e-1+y* (0.587872e-2
t+y*(-0.251540e-2+y*0.53208e-3))))));
+

return ans;

#include <math.h>

float bessil(float x)
Returns the modified Bessel function I(x) for any real x.

{
float ax,ans;
double y; Accumulate polynomials in double precision.
if ((ax=fabs(x)) < 3.75) { Polynomial fit.
y=x/3.75;
y*=y;
ans=ax*(0.5+y*(0.87890594+y* (0.51498869+y* (0. 15084934
+y*(0.2658733e-1+y* (0.301532e-2+y*0.32411e-3))))));
} else {
y=3.75/ax;
ans=0 .2282967e-1+y*(-0.2895312e-1+y*(0.1787654e-1
-y*0.420059e-2)) ;
ans=0 .39894228+y*(-0.3988024e-1+y*(-0.362018e-2
+y*(0.163801e-2+y*(-0.1031555e-1+y*ans))));
ans *= (exp(ax)/sqrt(ax));
+
return x < 0.0 ? -ans : ans;
3

#include <math.h>

float besski(float x)
Returns the modified Bessel function K1(x) for positive real x.

{
float bessii(float x);
double y,ans; Accumulate polynomials in double precision.
if (x <= 2.0) { Polynomial fit.
y=x*x/4.0;

ans=(log(x/2.0)*bessil(x))+(1.0/x)*(1.0+y*(0.15443144
t+y*(-0.67278579+y* (-0.18156897+y*(-0.1919402e-1
+y*(-0.110404e-2+y* (-0.4686e-4)))))))5
} else {
y=2.0/x;
ans=(exp(-x) /sqrt (x) )*(1.25331414+y*(0. 23498619
t+y*(-0.3655620e-1+y*(0.1504268e-1+y* (-0.780353e-2
ty* (0.325614e-2+y* (-0.68245e-3)))))));
+

return ans;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.6 Modified Bessel Functions of Integer Order 239

The recurrence relation for I,,(z) and K,,(z) is the same as that for J,(x)
and Y,,(«) provided that ix is substituted for z. This has the effect of changing
a sign in the relation,

Insa(t) =- (=) In(0) + Ina(a)
(6.64)
Knai(xz) =+ (=) K,,(«) + Kn-1(2)

These relations are always unstable for upward recurrence. For K ,,, itself growing,
this presents no problem. For I,,, however, the strategy of downward recursion is
therefore required once again, and the starting point for the recursion may be chosen
in the same manner as for the routine bess}. The only fundamental difference is
that the normalization formula for J, (a) has an alternating minus sign in successive
terms, which again arises from the substitution of iz for x in the formula used
previously for Jn

1 = Ip(x) — 2lo(x) + 2H:

2QIg(a) +++ (6.6.5)

In fact, we prefer simply to normalize with a call to bessi0.
With this simple modification, the recursion routines bessj and bessy become
the new routines bessi and bessk:

float bessk(int n, float x)
Returns the modified Bessel function n(x) for positive x and n > 2.

{
float besskO(float x);
float besski(float x);
void nrerror(char error_text[]);
int j;
float bk,bkm,bkp, tox;
if (m < 2) nrerror("Index n less than 2 in bessk");
tox=2.0/x;
Upward recurrence for all x...
..and here it is.
return bk;
3

#include <math.h>

#define ACC 40.0 Make larger to increase accuracy.
#define BIGNO 1.0e10

#define BIGNI 1.0e-10

float bessi(int n, float x)
Returns the modified Bessel function In(x) for any real x and n > 2.
{

float bessi0(float x);

void nrerror(char error_text[]);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

240 Chapter 6. Special Functions

int j;
float bi,bim,bip,tox,ans;

if (n < 2) nrerror("Index n less than 2 in bessi");
if (x == 0.0)
return 0.0;
else {
tox=2.0/fabs (x) ;
bip=ans=0.0;
bi=1.0;
for (j=2*(nt+(int) sqrt (ACC+n));j>0;j--) { Downward recurrence from even
n.

if (fabs(bi) > BIGNO) { Renormalize to prevent overflows.
ans *= BIGNI;
bi *= BIGNI;
bip *= BIGNI;

if (j == n) ans=bip;
}
ans *= bessi0(x)/bi; Normalize with bessi0.
return x < 0.0 && (n & 1) ? -ans : ans;

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §9.8. 1]

Carrier, G.F., Krook, M. and Pearson, C.E. 1966, Functions of a Complex Variable (New York:
McGraw-Hill), pp. 220ff.

6.7 Bessel Functions of Fractional Order, Airy
Functions, Spherical Bessel Functions

Many algorithms have been proposed for computing Bessel functions of fractional order
numerically. Most of them are, in fact, not very good in practice. The routines given here are
rather complicated, but they can be recommended wholeheartedly.

Ordinary Bessel Functions

The basic idea is Steed’s method, which was originally developed [1] for Coulomb wave
functions. The method calculates J,, J/,, Y,, and Y, simultaneously, and so involves four
relations among these functions. Three of the relations come from two continued fractions,
one of which is complex. The fourth is provided by the Wronskian relation

W=HY,-YWIL= 2 (6.7.1)
Tx
The first continued fraction, CF1, is defined by
f = J, _YU Iva
or a
ve 1 1 ' (6.7.2)
aw Av+1)/e— 2%v+2)/x—-

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.7 Bessel Functions of Fractional Order 241

You can easily derive it from the three-term recurrence relation for Bessel functions: Start with
equation (6.5.6) and use equation (5.5.18). Forward evaluation of the continued fraction by
one of the methods of §5.2 is essentially equivalent to backward recurrence of the recurrence
relation. The rate of convergence of CF1 is determined by the position of the turning point
rtp = \/v(v + 1) & v, beyond which the Bessel functions become oscillatory. If « S xp,
convergence is very rapid. If x = tp, then each iteration of the continued fraction effectively
increases v by one until x Ss Xp; thereafter rapid convergence sets in. Thus the number
of iterations of CF1 is of order x for large x. In the routine bessjy we set the maximum
allowed number of iterations to 10,000. For larger x, you can use the usual asymptotic
expressions for Bessel functions.

One can show that the sign of J, is the same as the sign of the denominator of CF1
once it has converged.

The complex continued fraction CF2 is defined by

Jy +iYy 1. é (/2)-v* (3/2)? -v

- i _
PYM +i, Ie | & Wetn+ WrwM+

(We sketch the derivation of CF2 in the analogous case of modified Bessel functions in the
next subsection.) This continued fraction converges rapidly for x z= tp, while convergence
fails as x — 0. We have to adopt a special method for small x, which we describe below. For
x not too small, we can ensure that z X tp by a stable recurrence of J, and Jj, downwards
toa value v = ps S 2, thus yielding the ratio f,, at this lower value of v. This is the stable
direction for the recurrence relation. The initial values for the recurrence are

J, = arbitrary, JIL = fudv, (6.7.4)

ve (6.7.3)

with the sign of the arbitrary initial value of J, chosen to be the sign of the denominator of
CF1. Choosing the initial value of J, very small minimizes the possibility of overflow during
the recurrence. The recurrence relations are

Jv-1 = “dy +5L
an (6.75)
a ee
x

Once CF2 has been evaluated at v = ju, then with the Wronskian (6.7.1) we have enough
relations to solve for all four quantities. The formulas are simplified by introducing the quantity

7 Pa dtu
y= Poe (6.76)
Then
Ww 1/2
w=*( oo) (677)
Iu = fudu (6.7.8)
Yu =u (6.79)
vi =Y, (v+ a) (6.7.10)
xy

The sign of J,, in (6.7.7) is chosen to be the same as the sign of the initial J, in (6.7.4).
Once all four functions have been determined at the value v = jz, we can find them at the
original value of v. For J, and Jj,, simply scale the values in (6.7.4) by the ratio of (6.7.7) to
the value found after applying the recurrence (6.7.5). The quantities Y, and Y/ can be found
by starting with the values in (6.7.9) and (6.7.10) and using the stable upwards recurrence

You = ty, Ya (6.7.11)
xr

together with the relation

Y= KY, —Yi41 (6.7.12)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

242 Chapter 6. Special Functions

Now turn to the case of small 2, when CF2 is not suitable. Temme [2] has given a
good method of evaluating Y; and Y,+41, and hence Y; from (6.7.12), by series expansions
that accurately handle the singularity as x — 0. The expansions work only for |v| < 1/2,
and so now the recurrence (6.7.5) is used to evaluate f, at a value v = y in this interval.
Then one calculates J, from

Ww

J, =————
“Yi —Yubn

(6.7.13)

and Jj, from (6.7.8). The values at the original value of v are determined by scaling as before,
and the Y’s are recurred up as before.
Temme’s series are

y, =- lag (6.7.14)
k=0

Here

ck = (6.7.15)

while the coefficients g, and h; are defined in terms of quantities px, qx, and f, that can
be found by recursion:

2. UT
9k = fx + —sin? (<) I
vy 2
he = —kge + De
Pk-1

Pk= Ty (6.7.16)
— Gk-1
ue +vu
fe = k fia + Pe-1 + Me-1
n= k? — py?
The initial values for the recurrences are
lay\-”
po=— (5) Paty)
lyayy
qo = = (5) rd-v) (6.7.17)
2 sinh 2
fo== aa [cosnors(v) 4g ee in (2) rac]
mn sin vt o x
with
()
o=vin{—
x
1 1 1
T - 6.7.18
1) Qv [Fa -v) T+ 5 ( )

r= 3 [pany tren]

The whole point of writing the formulas in this way is that the potential problems as v — 0

can be controlled by evaluating v/ sin v7, sinh o/c, and T carefully. In particular, Temme

gives Chebyshev expansions for [| (v) and ['2(v). We have rearranged his expansion for T';

to be explicitly an even series in 1 so that we can use our routine chebev as explained in 85.8.
The routine assumes v > 0. For negative v you can use the reflection formulas

Jv =cosvr J, — sinvr Y,

. (6.7.19)
Y_p =sinva Jy + cosuvnm Y,

The routine also assumes a > 0. For x < 0 the functions are in general complex, but
expressible in terms of functions with x > 0. For z = 0, Y, is singular.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

6.7 Bessel Functions of Fractional Order 243

Internal arithmetic in the routine is carried out in double precision. The complex
arithmetic is carried out explicitly with real variables.

#include <math.h>

#include "nrutil.h"

#define EPS 1.0e-10

#define FPMIN 1.0e-30
#define MAXIT 10000

#define XMIN 2.0

#define PI 3.141592653589793

void bessjy(float x, float xnu, float *rj, float *ry, float *rjp, float *ryp)
Returns the Bessel functions rj = J,, ry = Yi and their derivatives rjp = J), ryp = Yj, for
positive x and for xnu = v > 0. The relative accuracy is within one or two significant digits
of EPS, except near a zero of one of the functions, where EPS controls its absolute accuracy.
FPMIN is a number close to the machine's smallest floating-point number. All internal arithmetic
is in double precision. To convert the entire routine to double precision, change the float
declarations above to double and decrease EPS to 10~1®. Also convert the function beschb.
{
void beschb(double x, double *gam1, double *gam2, double *gampl,
double *gammi) ;
int i,isign,1l,nl;
double a,b,br,bi,c,cr,ci,d,del,deli,den,di,dlr,dli,dr,e,f,fact,fact2,
fact3,ff,gam,gam1,gam2,gammi,gampl,h,p,pimu,pimu2,q,r,rjl,
rjli,rjmu,rjp1,rjpl,rjtemp,ry1,rymu,rymup,rytemp,sum,sum1,
temp,w,x2,xi,xi2,xmu,xmu2;

if (x <= 0.0 || xnu < 0.0) nrerror("bad arguments in bessjy");
nl=(x < XMIN ? (int) (xnu+0.5) : IMAX(O, (int) (xnu-x+1.5)));
nl is the number of downward recurrences of the J's and upward recurrences of Y's. xmu
lies between —1/2 and 1/2 for x < XMIN, while it is chosen so that x is greater than the
turning point for x > XMIN.
xmu=xnu-nl;
xmu2=xmu*xmu ;
xi=1.0/x;
x1i2=2.0*xi;
w=xi2/PI; The Wronskian.
isign=1; Evaluate CF1 by modified Lentz’s method (§5.2).
h=xnu*xi; isign keeps track of sign changes in the de-
if (h < FPMIN) h=FPMIN; nominator.
b=xi2*xnu;
d=0.0;
c=h;
for (i=1;i<=MAXIT;i++) {
b += xi2;
d=b-d;
if (fabs(d) < FPMIN) d=FPMIN;
c=b-1.0/c;
if (fabs(c) < FPMIN) c=FPMIN;
d=1.0/d;
del=c*d;
h=del*h;
if (d < 0.0) isign = -isign;
if (fabs(del-1.0) < EPS) break;

+

if (i > MAXIT) nrerror("x too large in bessjy; try asymptotic expansion");
rjl=isign*FPMIN; Initialize J, and J{, for downward recurrence.
rjpl=h*rjl;

rjlt=rjl; Store values for later rescaling.

rjpi=rjpl;

fact=xnu*xi;
for (l=nl;1>=1;1--) {
act*rjl+rjpl;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

244 Chapter 6. Special Functions

rjpl=fact*rjtemp-rjl;
rjl=rjtemp;
+
if (rjl == 0.0) rj1=EPS;
f=rjpl/rjl; Now have unnormalized J, and The:
if (x < XMIN) { Use series.
x2=0.5*x;
pimu=PI*xmu;
fact = (fabs(pimu) < EPS ? 1.0 : pimu/sin(pimu));
d = -log(x2);
e=xmu*d;
fact2 = (fabs(e) < EPS ? 1.0 : sinh(e)/e);
beschb(xmu, &gam1,&gam2 ,&gamp1, &gammi) ; Chebyshev evaluation of I and Ia.
££=2.0/PI*fact*(gam1*cosh(e) +gam2*fact2*d) ; fo.
e=exp(e);
p=e/(gamp1*PI) ; Po-
q=1.0/(e*PI*gammi) ; qo-
pimu2=0.5*pimu;
fact3 = (fabs(pimu2) < EPS ? 1.0 : sin(pimu2)/pimu2) ;
r=PI*pimu2*fact3*fact3;
c=1.0;
d = -x2*x2;
sum=ff+r*q;
sumi=p;
for (i=1;i<=MAXIT;it+) {
ff=(ixff+ptq) /(ixi-xmu2) ;
c *= (d/i);
p /= (i-xmu);
q /= (itxmu) ;
del=c* (ff+r#q) ;
sum += del;
del1=c*p-i*del;
sumi += deli;
if (fabs(del) < (1.0+fabs(sum))*EPS) break;

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

}
if (i > MAXIT) nrerror("bessy series failed to converge");
rymu = -sum;
ry1l = -sum1*xi2;
rymup=xmu*xi*rymu-ry 1;
rjmu=w/(rymup-f*rymu) ; Equation (6.7.13).
} else { Evaluate CF2 by modified Lentz’s method (§5.2).
a=0.25-xmu2;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

fact=a*xi/(p*ptq*q) ;
cr=br+q*fact ;
ci=bitp*fact ;
den=br*br+bi*bi;
dr=br/den;
di = -bi/den;
dlr=cr*dr-ci*di;
dli=cr*dit+ci*dr;
temp=p*dlr-q*d1li;
q=p*dlitq*dlr;
p=temp;
for (i=2;i<=MAXIT;it+) {

a += 2*(i-1);

bi += 2.0;

dr=a*dr+br ;

di=a*ditbi;

if (fabs(dr)+fabs(di) < FPMIN) dr=FPMIN;

fact=a/(cr*crtci*ci) ;

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


6.7 Bessel Functions of Fractional Order 245

cr=brt+cr*fact;

ci=bi-ci*fact;

if (fabs(cr)+fabs(ci) < FPMIN) cr=FPMIN;
den=dr*dr+di*di;

dr /= den;
di /= -den;
dl: r*edr-cix*di;

dli=cr+*dit+ci*dr;

temp=p*dl1r-q*dli;

g=p*dli+q*dlr;

p=temp;

if (fabs(dlr-1.0)+fabs(dli) < EPS) break;

}
if (i > MAXIT) nrerror("cf2 failed in bessjy");
gam=(p-f)/q; Equations (6.7.6) — (6.7.10).

rjmu=sqrt (w/((p-f) *gam+q)) 5
rjmu=SIGN(rjmu,rjl);

rymu=r jmu*gam ;

rymup=rymu* (p+q/gam) ;
ryl=xmu*xi*rymu-rymup ;

}
fact=rjmu/rjl;
xrj=rjli*fact; Scale original J, and Ji.
*rjp=rjpi*fact;
for (i=1;i<=nl;i++) { Upward recurrence of Y,.
rytemp=(xmu+i) *xi2*ry1-rymu;
rymu=ry1;
ryl=rytemp;
+
xry=rymu;
*ryp=xnu*xi*rymu-ry1;

#tdefine NUSE1 5
#tdefine NUSE2 5

void beschb(double x, double *gam1, double *gam2, double *gampl, double *gammi)
Evaluates [) and [2 by Chebyshev expansion for |x| < 1/2. Also returns 1/I'(1 + x) and
1/I'(1 — x). If converting to double precision, set NUSE1 = 7, NUSE2 = 8.

{

float chebev(float a, float b, float c[], int m, float x);

float xx;

static float ci[] = {
-1.142022680371168e0 ,6.5165112670737e-3,
3.087090173086e-4 , -3.4706269649e-6 ,6.9437664e-9,
3.67795e-11,-1.356e-13};

static float c2[] = {
1.843740587300905e0, -7 .68528408447867e-2,
1.2719271366546e-3 , -4.9717367042e-6 ,-3.31261198e-8,
2.423096e-10,-1.702e-13,-1.49e-15};

xx=8.O*x*x-1.0; Multiply x by 2 to make range be —1 to 1,
*gam1=chebev(-1.0,1.0,c1,NUSE1,xx) ; and then apply transformation for eval-
*gam2=chebev(-1.0,1.0,c2,NUSE2,xx) ; uating even Chebyshev series.

*gampl= *gam2-x*(*gam1) ;
xgammi= *gam2+x*(*gam1) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

246 Chapter 6. Special Functions

Modified Bessel Functions

Steed’s method does not work for modified Bessel functions because in this case CF2 is
purely imaginary and we have only three relations among the four functions. Temme [3] has
given a normalization condition that provides the fourth relation.

The Wronskian relation is

W=I1,KL-KIL = at (6.7.20)
x
The continued fraction CF1 becomes
T, v 1 1
vect=-4 te 6.7.21
a a Ww+)/a+ AWw+2)/e+ ( )

To get CF2 and the normalization condition in a convenient form, consider the sequence
of confluent hypergeometric functions

2n(x) = U(v + 1/2 +n, 2v + 1, 2x) (6.7.22)
for fixed v. Then
Ki (a) = 1? (22)"e* z0(a) (6.7.23)
Ky4yi(z) 1 Jt, 2 l\a
= at: -= 6.7.24
K(x) a a nen Zo ( )

Equation (6.7.23) is the standard expression for A, in terms of a confluent hypergeometric
function, while equation (6.7.24) follows from relations between contiguous confluent hy-
pergeometric functions (equations 13.4.16 and 13.4.18 in Abramowitz and Stegun). Now
the functions z, satisfy the three-term recurrence relation (equation 13.4.15 in Abramowitz
and Stegun)

2n—1(@) = bn2n(@) + an412n+1 (6.7.25)
with
bn = 2(n+ 2)
(6.7.26)
anti = —[(n + 1/2)? — v7]
Following the steps leading to equation (5.5.18), we get the continued fraction CF2
ZA 1 a2
a see 6.7.27
zo bi + bot ( )

from which (6.7.24) gives K,+1/K, and thus K/,/K,.
Temme’s normalization condition is that

oo 1 v+1/2
So Cn2n = (=) (6.7.28)
ni

n=0
where ( |
(-1)" T(v+1/2+n)
n= By 6.7.2
c n! T(v+1/2—n) ( 9)
Note that the C;,’s can be determined by recursion:
C=l Cai =—-“"*c, (6.7.30)
n+1
We use the condition (6.7.28) by finding
— Zn
S= n— 6.7.31
s= °C x, ( )

Then

y\rre 4
=(= 73
*0 (3) 1+8 (6.7.32)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.7 Bessel Functions of Fractional Order 247

and (6.7.23) gives Ky.

Thompson and Barnett [4] have given a clever method of doing the sum (6.7.31)
simultaneously with the forward evaluation of the continued fraction CF2. Suppose the
continued fraction is being evaluated as

Z1 =

== Ahn 6.7.33
= » (6.7.33)
where the increments Ah,, are being found by, e.g., Steed’s algorithm or the modified Lentz’s

algorithm of §5.2. Then the approximation to S keeping the first N terms can be found as

N
Sy = > QnAhn (6.7.34)
n=1
Here
Qn => Cran (6.7.35)
k=1

and q, is found by recursion from
gett = (qr—1 — be ge)/an+1 (6.7.36)
starting with go = 0, qi = 1. For the case at hand, approximately three times as many terms

are needed to get S to converge as are needed simply for CF2 to converge.
To find A, and K,41 for small x we use series analogous to (6.7.14):

oo 2
Ky = Ch Ruy = = ceh 6.7.37
Docs Lyf zo k ( )

Here
(a? /4)*
k!
—k fie + Pk

Ck =

hy

Pk-1

Pk = (6.7.38)

dk =

fe= k fia + Pe-1 + Me-1
n= k? — py?

The initial values for the recurrences are
1 4 —v

p= 5(5) a+)

l(a

= 5 (5) r(1—v) (6.7.39)

fo- [cosh ors(v) 4 Snhey (2) rac]

sinvi o

Both the series for small x, and CF2 and the normalization relation (6.7.28) require
|v| < 1/2. In both cases, therefore, we recurse I, down to a value v = pu in this interval, find
K,, there, and recurse K, back up to the original value of v.

The routine assumes v > 0. For negative v use the reflection formulas

I_p=Ib+ 2 sin(v7) Ky,

T (6.7.40)
K,=Ky

Note that for large 2, I, ~ e”, K, ~ e~*, and so these functions will overflow or

underflow. It is often desirable to be able to compute the scaled quantities e~* J, and e* K,.

Simply omitting the factor e~” in equation (6.7.23) will ensure that all four quantities will

have the appropriate scaling. If you also want to scale the four quantities for small « when

the series in equation (6.7.37) are used, you must multiply each series by e”.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

248 Chapter 6. Special Functions

#include <math.h>

#define EPS 1.0e-10

#define FPMIN 1.0e-30
#define MAXIT 10000

#define XMIN 2.0

#define PI 3.141592653589793

void bessik(float x, float xnu, float *ri, float *rk, float *rip, float *rkp)
Returns the modified Bessel functions ri = I,, rk = K, and their derivatives rip = I,
rkp = Kj, for positive x and for xnu = v > 0. The relative accuracy is within one or two
significant digits of EPS. FPMIN is a number close to the machine's smallest floating-point
number. All internal arithmetic is in double precision. To convert the entire routine to double
precision, change the float declarations above to double and decrease EPS to 10-16. Also
convert the function beschb.

{

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

void beschb(double x, double *gam1, double *gam2, double *gampl,
double *gammi) ;

void nrerror(char error_text[]);

int i,l,nl;

double a,ai,b,c,d,del,del1,delh,dels,e,f,fact,fact2,ff,gam1,gam2,
gammi,gampl,h,p,pimu,q,qi,q2,qnew,ril,rili,rimu,rip1,ripl,
ritemp,rk1,rkmu,rkmup,rktemp,s,sum,sum1,x2,xi,xi2,xmu,xmu2;

if (x <= 0.0 || xnu < 0.0) nrerror("bad arguments in bessik");

nl=(int) (xnut0.5); nl is the number of downward re-
xmu=xnu-n1; currences of the J's and upward
xmu2=xmu*xmu ; recurrences of K's. xmu lies be-
xi=1.0/x; tween —1/2 and 1/2.
x1i2=2.0*xi;

h=xnu*xi; Evaluate CF1 by modified Lentz’s
if (h < FPMIN) h=FPMIN; method (§5.2).

b=xi2*xnu;

jA<=MAXIT; i++) {

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

b += xi2;
d=1.0/(b+d) ; Denominators cannot be zero here,
c=b+1.0/c; so no need for special precau-
del=c*d; tions.
h=del*h;
if (fabs(del-1.0) < EPS) break;
+
if (i > MAXIT) nrerror("x too large in bessik; try asymptotic expansion") ;
ril=FPMIN; Initialize J, and Ii, for downward re-
ripl=h*ril; currence.
rili=ril; Store values for later rescaling.
ripl=ripl;

fact=xnu*xi;

for (l=nl;1>=1;1--) {
ritemp=fact*ril+ripl;
fact -= xi;
ripl=fact*ritemp+ril;
ril=ritemp;

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

}
f=ripl/ril; Now have unnormalized I, and I,
if (x < XMIN) { Use series.

pimu=PI*xmu;
fact = (fabs(pimu) < EPS ? 1.0 : pimu/sin(pimu));

d = -log(x2);

e=xmu*d;

fact2 = (fabs(e) < EPS ? 1.0 : sinh(e)/e);

beschb(xmu, &gam1,&gam2,&gamp1, &gammi) ; Chebyshev evaluation of Ty and 2.

ff=fact*(gami*cosh(e)+gam2*fact2*d) ; fo.

6.7 Bessel Functions of Fractional Order

249

sum=ff;
e=exp(e);
p=0.5*e/gamp1;
-5/(exgammi) ;
.0;

for (i=1;i<=MAXIT;i++) {
ixff+p+q) /(i*i-xmu2) ;
(d/i) ;

p /= (i-xmu);
q /= (itxmu) ;
del=c*ff;

sum += del;
deli=c*(p-i*ff) ;
sumi += deli;

if (fabs(del) < fabs(sum)*EPS) break;

+

Ppo-
qo-

if (i > MAXIT) nrerror("bessk series failed to converge");

rkmu=sum;
rki=sumi*xi2;

} else {

+

b=2.0*(1.0+x) ;

d=1.0/b;
h=de1h:
qi=0.0;
q2=1.0;
ai=0.25-xmu2;
q=c=al;

a -al;

s=1.0+q*delh;
for (i=2;i<=MAXIT;it+) {

a 2* (4-1);

c = -atc/i;
qnew=(q1-b*q2) /a;
qi=q2;

q2=qnew;

q += cqnew;

b += 2.0;

d=1.0/(btaxd) ;
delh=(b*d-1.0)*delh;

h += delh;
dels=q*delh;
s t= dels;

if (fabs(dels/s) < EPS) break;

Evaluate CF2 by Steed’s algorithm
(§5.2), which is OK because there
can be no zero denominators.

Initializations for recurrence (6.7.35).

First term in equation (6.7.34).

Need only test convergence of sum since CF2 itself converges more quickly.

+

if (i > MAXIT) nrerror("bessik: failure to converge in cf2");

h=ai*h;
rkmu=sqrt (PI/(2.0*x))*exp(-x)/s;
rki=rkmu* (xmu+x+0.5-h) *xi;

rkmup=xmu*xi*rkmu-rk1 ;
rimu=xi/(f*rkmu-rkmup) ;

rimu*ril1)/ril;

*rip=(rimu+rip1)/ril;
for (i=1;i<=nl;i++) {

+

rktemp=(xmuti) *xi2*rk1+rkmu;
rkmu=rk1;
rki=rktemp;

*rk=rkmu;
*rkp=xnu*xi*rkmu-rk1 ;

Omit the factor exp(—a) to scale
all the returned functions by exp(x)
for x > XMIN.

Get I,, from Wronskian.
Scale original I, and I},

Upward recurrence of Ky.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

250 Chapter 6. Special Functions

Airy Functions

For positive x, the Airy functions are defined by

Ai(e) = = ¢Kisl2)

Bi(x) = [xivsl2) + 11;3(2)|

where

2
ya 23/2

(6.741)

(6.7.42)

(6.7.43)

By using the reflection formula (6.7.40), we can convert (6.7.42) into the computationally

more useful form

2

Bie) = ve |

so that Ai and Bi can be evaluated with a single call to bessik.

Tyj3(z) + 2Kya(2)|

(6.7.44)

The derivatives should not be evaluated by simply differentiating the above expressions
because of possible subtraction errors near x = 0. Instead, use the equivalent expressions

x

aV3

Bi (x) =a [Feat + 2kea(a]

The corresponding formulas for negative arguments are

Ai'(x) = — K/3(2)

VE

Ai(-—a

Wat

#include <math.h>

#define PI 3.1415927
#define THIRD (1.0/3.0)
#define TWOTHR (2.0*THIRD)
#define ONOVRT 0.57735027

7 [-ate) - avial®)

—_v! [ quale) + Ya]

(6.745)

|

(6.7.46)

void airy(float x, float *ai, float *bi, float *aip, float *bip)
Returns Airy functions Ai(x), Bi(a), and their derivatives Ai/(x), Bi/(«).

{
void bessik(float x, float xnu, float *ri, float *rk,
float *rkp);
void bessjy(float x, float xnu, float *rj, float *ry,
float *ryp);
float absx,ri,rip,rj,rjp,rk,rkp,rootx,ry,ryp,z;

absx=fabs (x);

rootx=sqrt (absx) ;

z=TWOTHR*absx*rootx;

if (x > 0.0) f
bessik(z, THIRD, &ri,&rk,&rip,&rkp) ;
*ai=rootx*O0NOVRT*rk/PI ;

float *rip,

float *rjp,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.7 Bessel Functions of Fractional Order 251

*bi=rootx* (rk/PI+2.0*0NOVRT*ri) ;
bessik(z,TWOTHR,&ri,&rk,&rip,&rkp) ;
*aip = -x*ONOVRT*rk/PI;
*bip=x* (rk/PI+2.0*ONOVRT*ri) ;

} else if (x < 0.0) {
bessjy(z, THIRD, &rj,&ry,&rjp,&ryp) ;
¥ai=0.5*rootx*(rj-ONOVRT*ry) ;
*bi = -0.5*rootx*(ry+ONOVRT*rj) ;
bessjy(z,TWOTHR,&rj,&ry,&rjp,&ryp) ;
*aip=0.5*absx* (ONOVRT*ry+rj) ;
*bip=0 . 5*absx* (ONOVRT*rj-ry) ;

} else { Case x = 0.
*ai=0.35502805;
*bi=(*ai) /ONOVRT;
*aip = -0.25881940;
*bip = -(*aip) /ONOVRT;

Spherical Bessel Functions

For integer n, spherical Bessel functions are defined by

. [a

n(x) = Rg nt (/2) (x)
[a

Yn(z) = Bp mt (1/2) (x)

They can be evaluated by a call to bessjy, and the derivatives can safely be found from
the derivatives of equation (6.7.47).

Note that in the continued fraction CF2 in (6.7.3) just the first term survives for v = 1/2.
Thus one can make a very simple algorithm for spherical Bessel functions along the lines of
bess jy by always recursing j, down to n = 0, setting p and q from the first term in CF2, and
then recursing yn up. No special series is required near x = 0. However, bessjy is already
so efficient that we have not bothered to provide an independent routine for spherical Bessels.

(6.747)

#include <math.h>
#define RTPIO2 1.2533141

void sphbes(int n, float x, float *sj, float *sy, float *sjp, float *syp)
Returns spherical Bessel functions jn(a), yn(a), and their derivatives j/, (x), y/,(x) for integer n.
{
void bessjy(float x, float xnu, float *rj, float *ry, float *rjp,
float *ryp);
void nrerror(char error_text[]);
float factor,order,rj,rjp,ry,ryp;

if @< oO || x <= 0.0) nrerror("bad arguments in sphbes");
order=n+0.5;

bessjy(x,order ,&rj,&ry,&rjp,&ryp) ;

factor=RTPI02/sqrt (x) ;

¥sj=factor*rj;

*sy=factor*ry ;

*sjp=factor*rjp-(+*sj)/(2.0*x) ;
*syp=factor*ryp-(*sy)/(2.0*x) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

252 Chapter 6. Special Functions

CITED REFERENCES AND FURTHER READING:

Barnett, A.R., Feng, D.H., Steed, J.W., and Goldfarb, L.J.B. 1974, Computer Physics Commu-
nications, vol. 8, pp. 377-395. [1]

Temme, N.M. 1976, Journal of Computational Physics, vol. 21, pp. 343-350 [2]; 1975, op. cit.,
vol. 19, pp. 324-337. [3]

Thompson, I.J., and Barnett, A.R. 1987, Computer Physics Communications, vol. 47, pp. 245—
257. [4]

Barnett, A.R. 1981, Computer Physics Communications, vol. 21, pp. 297-314.

Thompson, I.J., and Barnett, A.R. 1986, Journal of Computational Physics, vol. 64, pp. 490-509.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 10.

6.8 Spherical Harmonics

Spherical harmonics occur in a large variety of physical problems, for ex-
ample, whenever a wave equation, or Laplace’s equation, is solved by separa-
tion of variables in spherical coordinates. The spherical harmonic Yim (6, ¢),
—l <m <1, is a function of the two coordinates 0, ¢ on the surface of a sphere.

The spherical harmonics are orthogonal for different / and m, and they are
normalized so that their integrated square over the sphere is unity:

Qn 1
[ de | (cos 0)¥irmi*(0, )Yim (8,0) = SviSm'm (6.8.1)
0 -1

Here asterisk denotes complex conjugation.
Mathematically, the spherical harmonics are related to associated Legendre
polynomials by the equation

Yim(9, 6) =

Pi" (cos #)e"? (6.8.2)

By using the relation
Yi-m(0, 0) = (-1)"Yim*(8, 0) (6.8.3)

we can always relate a spherical harmonic to an associated Legendre polynomial
with m > 0. With 2 = cos@, these are defined in terms of the ordinary Legendre
polynomials (cf. §4.5 and §5.5) by

a" P(x) (684)

PI" (x) ( yr wn

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.8 Spherical Harmonics 253

The first few associated Legendre polynomials, and their corresponding nor-
malized spherical harmonics, are

Pyz)= 1 Yoo = Ve

Pi(a) = —(1—2?)!/? Yu =— \/s sin de®
Pi(x)= «2 Yio = \/& cose
P3(z)= 3(1-2?) Yoo = 1/35 sin? 0c?!
P}(x) = -3 (1-27)? Yo, = — y/ # sin 0 cos be'?
P§(x)= § (3a? — 1) Yoo = (3 cos? 0 — 4)

(6.8.5)

There are many bad ways to evaluate associated Legendre polynomials numer-
ically. For example, there are explicit expressions, such as

m (=) +m)! m/? (=m)(m+l+1) (1-2
PI) = Semi —m)! (1 a?yn? I(m +1) ( 2 )
(l= m)(l—_m = 1)(m+1+1)(m +142) (: 2)".

2\(m + 1)(m + 2) 2

(6.8.6)

where the polynomial continues up through the term in (1 — x)'~™. (See [1] for
this and related formulas.) This is not a satisfactory method because evaluation
of the polynomial involves delicate cancellations between successive terms, which
alternate in sign. For large 1, the individual terms in the polynomial become very
much larger than their sum, and all accuracy is lost.

In practice, (6.8.6) can be used only in single precision (32-bit) for / up
to 6 or 8, and in double precision (64-bit) for | up to 15 or 18, depending on
the precision required for the answer. A more robust computational procedure is
therefore desirable, as follows:

The associated Legendre functions satisfy numerous recurrence relations, tab-
ulated in [1-2]. These are recurrences on / alone, on m alone, and on both /
and m simultaneously. Most of the recurrences involving m are unstable, and so
dangerous for numerical work. The following recurrence on | is, however, stable
(compare 5.5.1):

(= m) PJ" = (21-1) PP, — (+ m—1) PR, (6.8.7)
It is useful because there is a closed-form expression for the starting value,
Pm = (-1)™(2m — 1)!(1 — a?)"/? (6.8.8)
(The notation n!! denotes the product of all odd integers less than or equal to 7.)
Using (6.8.7) with | = m + 1, and setting P”_; = 0, we find
Pm, = «(2m +1)P™ (68.9)

Equations (6.8.8) and (6.8.9) provide the two starting values required for (6.8.7)
for general 1.
The function that implements this is

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

254 Chapter 6. Special Functions

#include <math.h>

float plgndr(int 1, int m, float x)
Computes the associated Legendre polynomial P/"(x). Here m and I are integers satisfying
0 <m <1, while = lies in the range —1 < @ <1.

t
void nrerror(char error_text[]);
float fact,pll,pmm,pmmp1,somx2;
int i,11;
if (m <0 || m>1 || fabs(x) > 1.0)
nrerror("Bad arguments in routine plgndr");
pmm=1.0; Compute Py’.
if (m>0) {
somx2=sqrt ((1.0-x)*(1.0+x)) ;
fact=1.0;
for (i=1;i<=m;it+) {
pmm *= -fact*somx2;
fact += 2.0;
}
+
if (1 == m)
return pmm;
else { Compute Pr’. ).
pmmp1=x*(2*mt+1) *pmm;
if (1 == (m+1))
return pmmp1;
else { Compute P/”, 1 >m+1.
for (ll=m+2;11<=1;11++) {
pll=(x*(2*11-1) *pmmp1-(11+m-1)*pmm) /(11-m) ;
pmm=pmmp 1 ;
pmmp1=p11;
return pll;
}
+
}

CITED REFERENCES AND FURTHER READING:

Magnus, W., and Oberhettinger, F. 1949, Formulas and Theorems for the Functions of Mathe-
matical Physics (New York: Chelsea), pp. 54ff. [1]

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 8. [2]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.9 Fresnel Integrals, Cosine and Sine Integrals 255

6.9 Fresnel Integrals, Cosine and Sine Integrals

Fresnel Integrals

The two Fresnel integrals are defined by

C(x) = [ cos (Fe) dt, S(a) = [ sin (Fe) dt (6.9.1)

The most convenient way of evaluating these functions to arbitrary precision is
to use power series for small x and a continued fraction for large x. The series are

5 9

O(a) = 2 — (5) oa (5) a —
3 7 11

There is a complex continued fraction that yields both S(«) and C() simul-
taneously:

(6.9.2)

O(a) +i8(x) =F" ent z z=

s

F (Lie (6.9.3)

where

2 e
e* erfez

1 (2 1/2 1 3/2 2 )

Va \zt z+ 24+ et 24+ (694)

ar 1 1-2 3-4
Ja \2z24+1-— 22245- 22249

In the last line we have converted the “standard” form of the continued fraction to

its “even” form (see §5.2), which converges twice as fast. We must be careful not

to evaluate the alternating series (6.9.2) at too large a value of x; inspection of the

terms shows that 2 = 1.5 is a good point to switch over to the continued fraction.
Note that for large x

2 1x Tx

Thus the precision of the routine frenel may be limited by the precision of the
library routines for sine and cosine for large x.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

256 Chapter 6. Special Functions

#include <math.h>

#include "complex.h"

#define EPS 6.0e-8

#define MAXIT 100

#define FPMIN 1.0e-30

#define XMIN 1.5

#define PI 3.1415927

#tdefine PIBY2 (PI/2.0)

Here EPS is the relative error; MAXIT is the maximum number of iterations allowed; FPMIN
is a number near the smallest representable floating-point number; XMIN is the dividing line
between using the series and continued fraction.

#define TRUE 1

#define ONE Complex(1.0,0.0)

void frenel(float x, float *s, float *c)
Computes the Fresnel integrals S(a) and C(«) for all real x.
t
void nrerror(char error_text[]);
int k,n,odd;
float a,ax,fact,pix2,sign,sum,sumc,sums,term, test;
fcomplex b,cc,d,h,del,cs;

ax=fabs (x);

if (ax < sqrt(FPMIN)) { Special case: avoid failure of convergence
05 test because of underflow.
X5

} else if (ax <= XMIN) { Evaluate both series simultaneously.

sum=sums=0.0;

fact=PIBY2*ax*ax;

odd=TRUE;
term=ax;
n=3;

for (k=1;k<=MAXIT;kt+) {
term *= fact/k;
sum += sign*term/n;
test=fabs (sum) *EPS;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

if (odd) {
sign = -sign;
sums=sum ;
sum=sumc ;
} else {
sumc=sum ;
sum=sums ;
3
if (term < test) break;
odd=! odd;
n += 2;
}
if (k > MAXIT) nrerror("series failed in frenel");
*S=SUMS;
*c=sumc;
} else { Evaluate continued fraction by modified
pix2=PI*ax*ax; Lentz’s method (§5.2).

b=Complex(1.0,-pix2) ;
omplex(1.0/FPMIN,0.0) ;

n= -1;

for (k=2;k<=MAXIT;kt+) {
n += 2;
a = -n*(nt1);

b=Cadd(b,Complex(4.0,0.0));
d=Cdiv(ONE,Cadd(RCmul(a,d),b)); Denominators cannot be zero.

6.9 Fresnel Integrals, Cosine and Sine Integrals 257

cc=Cadd(b,Cdiv(Complex(a,0.0),cc));
del=Cmul (cc,d);
h=Cmul (h,del) ;
if (fabs(del.r-1.0)+fabs(del.i) < EPS) break;
}
if (k > MAXIT) nrerror("cf failed in frenel");
h=Cmul (Complex (ax,-ax) ,h) ;
cs=Cmul (Complex(0.5,0.5),
Csub (ONE, Cmul (Complex (cos (0.5*pix2) ,sin(0.5*pix2)),h)));

*C=CS.1;
*s=cs.i;

+

if (x < 0.0) { Use antisymmetry.
*c = -(*c);
*s = -(*s);

+

Cosine and Sine Integrals

The cosine and sine integrals are defined by

* cost —1
ytine | ost = at
t
>t
si(e) = [ a dt
0 t

Here y © 0.5772... is Euler’s constant. We only need a way to calculate the
functions for x > 0, because

Si(—x) = —Si(a), Ci(—a) = Ci(a) — ix (6.9.7)

(6.9.6)

Once again we can evaluate these functions by a judicious combination of power
series and complex continued fraction. The series are

(6.9.8)
The continued fraction for the exponential integral Ej (i:
zf 1 1 1 2 2
je+ I+ iat T+ tet (6.9.9)

in 1 1? 2?
l+ix— 3+ix-— 5+ix

The “even” form of the continued fraction is given in the last line and converges
twice as fast for about the same amount of computation. A good crossover point
from the alternating series to the continued fraction is x = 2 in this case. As for
the Fresnel integrals, for large x the precision may be limited by the precision of
the sine and cosine routines.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

258

Chapter 6. Special Functions

#include <math.h>
#include "complex.h"

#define
#define
#define
#define
#define
#define
#define
#define

EPS 6.0e-8 Relative error, or absolute error near a zero of Ci().
EULER 0.57721566 Euler’s constant +.

MAXIT 100 Maximum number of iterations allowed.

PIBY2 1.5707963 m/2.

FPMIN 1.0e-30 Close to smallest representable floating-point number.
TMIN 2.0 Dividing line between using the series and continued frac-
TRUE 1 tion.

ONE Complex(1.0,0.0)

void cisi(float x, float *ci, float ¥*si)

Computes the cosine and sine integrals Ci(a) and Si(x). Ci(0) is returned as a large negative
number and no error message is generated. For a < 0 the routine returns Ci(—a) and you must
supply the —iz yourself.

void nrerror(char error_text[]);

int

i,k,odd;

float a,err,fact,sign,sum,sumc,sums,t,term;
fcomplex h,b,c,d,del;

t=fabs (x);
if (t == 0.0) { Special case.
*si=0.0;
*ci = -1.0/FPMIN;
return;
+
if (t > TMIN) { Evaluate continued fraction by modified
b=Complex(1.0,t); Lentz's method (§5.2).

c=Complex(1.0/FPMIN,0.0) ;
d=h=Cdiv(ONE,b) ;
for (i=2;i<=MAXIT;it+) {
a = -(i-1)*(i-1);
b=Cadd(b, Complex(2.0,0.0));
d=Cdiv(ONE,Cadd(RCmul(a,d) ,b)); Denominators cannot be zero.
c=Cadd(b,Cdiv(Complex(a,0.0),c));
del=Cmul (c,d) ;
h=Cmul (h,del) ;
if (fabs(del.r-1.0)+fabs(del.i) < EPS) break;
}
if (i > MAXIT) nrerror("cf failed in cisi");
h=Cmul (Complex (cos(t) ,-sin(t)) ,h);
*ci = -h.r;
*si=PIBY2+th.i;

} else { Evaluate both series simultaneously.
if (t < sqrt(FPMIN)) { Special case: avoid failure of convergence
sumc=0.0; test because of underflow.
sums=t ;
} else {

sum=sums=sumc=0.0;
sign=fact=1.0;

odd=TRUE;
for (k=1;k<=MAXIT;kt++) {
fact t/k;

term=fact/k;
sum += sign*term;
err=term/fabs (sum) ;
if (odd) {
sign = -sign;
sums=sum;
sum=sumc;
} else {
sumc=sum;
sum=sums ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.10 Dawson’s Integral 259

3
if (err < EPS) break;
odd=! odd;
3
if (k > MAXIT) nrerror("maxits exceeded in cisi");
}
*si=sums;

*ci=sumc+log(t)+EULER;
}
if (x < 0.0) *si = -(*si);

CITED REFERENCES AND FURTHER READING:

Stegun, |.A., and Zucker, R. 1976, Journal of Research of the National Bureau of Standards,
vol. 80B, pp. 291-311; 1981, op. cit., vol. 86, pp. 661-686.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapters 5 and 7.

6.10 Dawson’s Integral

Dawson’s Integral F(x) is defined by

2
Fay =e" [ et dt (6.10.1)
0

The function can also be related to the complex error function by

F(z) = Wie [1 — erfe(—iz)]. (6.10.2)

A remarkable approximation for F(z), due to Rybicki [1], is

F(z) =lim — Yo * : (6.10.3)

What makes equation (6.10.3) unusual is that its accuracy increases exponentially
as h gets small, so that quite moderate values of h (and correspondingly quite rapid
convergence of the series) give very accurate approximations.

We will discuss the theory that leads to equation (6.10.3) later, in §13.11, as
an interesting application of Fourier methods. Here we simply implement a routine
for real values of x based on the formula.

It is first convenient to shift the summation index to center it approximately on
the maximum of the exponential term. Define no to be the even integer nearest to
x/h, and x9 = noh, 2’ = x — x, and n’ = n — no, so that

—(a’—n'h)?

, (6.10.4)

n! odd

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

260 Chapter 6. Special Functions

where the approximate equality is accurate when h is sufficiently small and N is
sufficiently large. The computation of this formula can be greatly speeded up if
we note that

en (alan hy? ene ew hy? ( zon) (6.10.5)

The first factor is computed once, the second is an array of constants to be stored,
and the third can be computed recursively, so that only two exponentials need be
evaluated. Advantage is also taken of the symmetry of the coefficients e =(n'h)? by
breaking the summation up into positive and negative values of n’ separately.

In the following routine, the choices h = 0.4 and N = 11 are made. Because
of the symmetry of the summations and the restriction to odd values of n, the limits
on the for loops are 1 to 6. The accuracy of the result in this float version is about
2 x 1077. In order to maintain relative accuracy near x = 0, where F(x) vanishes,
the program branches to the evaluation of the power series [2] for F(x), for || < 0.2.

#include <math.h>
#include "nrutil.h"
#define NMAX 6
#define H 0.4
#define A1 (2.0/3.0)
#define A2 0.4
#define A3 (2.0/7.0)

float dawson(float x)
Returns Dawson's integral F(x) = exp(—2?) Io exp(t?)dt for any real x.
t
int i,n0;
float d1i,d2,e1,e2,sum,x2,xp,xx,ans;
static float c[NMAX+1];
static int init = 0; Flag is 0 if we need to initialize, else 1.

if (init == 0) {
init=
for (i=1;i<=NMAX;i++) c[i]=exp(-SQR((2.0*i-1.0)*H));

+
if (fabs(x) < 0.2) { Use series expansion.
X2=xX*x;
ans=x*(1.0-A1*x2*(1.0-A2*x2* (1.0-A3*x2))) ;
} else { Use sampling theorem representation.
xx=fabs (x);
n0=2* (int) (0.5*xx/H+0.5) ;
xp=xx-n0*H;
e1=exp(2.0*xp*H) ;
e2=eli*el;
di=n0+1;
d2=d1-2.0;
sum=0.0;
for (i=1;i<=NMAX;i++,d1+=2.0,d2-=2.0,e1*=e2)
sum += c[i]*(e1/d1+1.0/(d2*e1)) ;
ans=0 .5641895835*SIGN(exp(-xp*xp) ,x)*sum; Constant is 1/\/7.
+

return ans;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.11 Elliptic Integrals and Jacobian Elliptic Functions 261

Other methods for computing Dawson’s integral are also known [2,3].

CITED REFERENCES AND FURTHER READING:
Rybicki, G.B. 1989, Computers in Physics, vol. 3, no. 2, pp. 85-87. [1]

Cody, W.J., Pociorek, K.A., and Thatcher, H.C. 1970, Mathematics of Computation, vol. 24,
pp. 171-178. [2]

McCabe, J.H. 1974, Mathematics of Computation, vol. 28, pp. 811-816. [3]

6.11 Elliptic Integrals and Jacobian Elliptic
Functions

Elliptic integrals occur in many applications, because any integral of the form

ic s) dt (6.11.1)

where R is a rational function of t and s, and s is the square root of a cubic or
quartic polynomial in t, can be evaluated in terms of elliptic integrals. Standard
references [1] describe how to carry out the reduction, which was originally done
by Legendre. Legendre showed that only three basic elliptic integrals are required.
The simplest of these is

* dt
h | Vai + bit)(a2 + bat) (ag + bgt)(a4 + dat) (6.112)

where we have written the quartic s? in factored form. In standard integral tables [2],
one of the limits of integration is always a zero of the quartic, while the other limit
lies closer than the next zero, so that there is no singularity within the interval. To
evaluate I; , we simply break the interval [y, x] into subintervals, each of which either
begins or ends on a singularity. The tables, therefore, need only distinguish the eight
cases in which each of the four zeros (ordered according to size) appears as the upper
or lower limit of integration. In addition, when one of the b’s in (6.11.2) tends to
zero, the quartic reduces to a cubic, with the largest or smallest singularity moving
to too; this leads to eight more cases (actually just special cases of the first eight).
The sixteen cases in total are then usually tabulated in terms of Legendre’s standard
elliptic integral of the 1st kind, which we will define below. By a change of the
variable of integration t, the zeros of the quartic are mapped to standard locations
on the real axis. Then only two dimensionless parameters are needed to tabulate
Legendre’s integral. However, the symmetry of the original integral (6.11.2) under
permutation of the roots is concealed in Legendre’s notation. We will get back to
Legendre’s notation below. But first, here is a better way:

Carlson [3] has given a new definition of a standard elliptic integral of the first kind,

i ie dt
Br(e,4,2) 2, (t+ a)(E+y)(E+ 2) (6.113)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

262 Chapter 6. Special Functions

where x, y, and z are nonnegative and at most one is zero. By standardizing the range of
integration, he retains permutation symmetry for the zeros. (Weierstrass’ canonical form
also has this property.) Carlson first shows that when x or y is a zero of the quartic in
(6.11.2), the integral J, can be written in terms of Rr in a form that is symmetric under
permutation of the remaining three zeros. In the general case when neither x nor y is a
zero, two such Rp functions can be combined into a single one by an addition theorem,
leading to the fundamental formula

Ty = 2Re(Ui2, U2, Ut) (6.11.4)

where
Uij = (Xi XGVR¥m + ViVi Xe Xm)/(@ — y) (6.11.5)
Xi=(aitbia)?, Yi = (ai + diy)? (6.11.6)

and i, 7, k,m is any permutation of 1,

U?, = Uz — (arbs — aab1)(a2b3 — agb2)

, 3,4. A short-cut in evaluating these expressions is

be (6.11.7)
Ui4 = Uj2 — (a1b3 — a3b1)(a2ba — aab2)

The U’s correspond to the three ways of pairing the four zeros, and f; is thus manifestly
symmetric under permutation of the zeros. Equation (6.11.4) therefore reproduces all sixteen
cases when one limit is a zero, and also includes the cases when neither limit is a zero.

Thus Carlson’s function allows arbitrary ranges of integration and arbitrary positions of
the branch points of the integrand relative to the interval of integration. To handle elliptic
integrals of the second and third kind, Carlson defines the standard integral of the third kind as

ce dt
Ra (x,y, 2,P) > (t+ p) (EF ayt+ E+)

which is symmetric in x, y, and z. The degenerate case when two arguments are equal
is denoted

(6.11.8)

Ro(2,y, 2) = Ra(2,y, 2,2) (6.119)

and is symmetric in x and y. The function Rp replaces Legendre’s integral of the second
kind. The degenerate form of Rr is denoted

Re(a,y) = Re(x,y,y) (6.11.10)

It embraces logarithmic, inverse circular, and inverse hyperbolic functions.

Carlson [4-7] gives integral tables in terms of the exponents of the linear factors of
the quartic in (6.11.1). For example, the integral where the exponents are ($,4,-4,—3)
can be expressed as a single integral in terms of Rp; it accounts for 144 separate cases in
Gradshteyn and Ryzhik [2]!

Refer to Carlson’s papers [3-7] for some of the practical details in reducing elliptic
integrals to his standard forms, such as handling complex conjugate zeros.

Turn now to the numerical evaluation of elliptic integrals. The traditional methods [8]
are Gauss or Landen transformations. Descending transformations decrease the modulus
k of the Legendre integrals towards zero, increasing transformations increase it towards
unity. In these limits the functions have simple analytic expressions. While these methods
converge quadratically and are quite satisfactory for integrals of the first and second kinds,
they generally lead to loss of significant figures in certain regimes for integrals of the third
kind. Carlson’s algorithms [9,10], by contrast, provide a unified method for all three kinds
with no significant cancellations.

The key ingredient in these algorithms is the duplication theorem:

Re(x,y,z) = 2Re(e@t+A,y+A,z2+A)

atrXA y+trA z4+2r (6.11.11)
Rr ; ;
4 4 4

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.11 Elliptic Integrals and Jacobian Elliptic Functions 263

where
d= (wy)? + (wz)? + (yz)? (6.11.12)

This theorem can be proved by a simple change of variable of integration[11]. Equation
(6.11.11) is iterated until the arguments of Ry are nearly equal. For equal arguments we have

Re(2,2,2) =a '/? (6.11.13)

When the arguments are close enough, the function is evaluated from a fixed Taylor expansion
about (6.11.13) through fifth-order terms. While the iterative part of the algorithm is only
linearly convergent, the error ultimately decreases by a factor of 4° = 4096 for each iteration.
Typically only two or three iterations are required, perhaps six or seven if the initial values
of the arguments have huge ratios. We list the algorithm for Rr here, and refer you to
Carlson’s paper [9] for the other cases.

Stage 1: For n = 0,1,2,... compute
Lin = (an + yn + 2n)/3

Xn =1-(an/tm), Yn=1-(yn/pn), Zn =1 — (Zn/pn)
én = max(|Xn|, [Yn], |Zn|)

If €n < tol go to Stage 2; else compute

An = (tn¥n)? + (@nzn)? + (Ynen)?
Inti = (an +An)/4,  Ynti = (Yn +An)/4,  2n41 = (2n + An)/4

and repeat this stage.
Stage 2: Compute

By =XnYn— Zn, Es = XnY¥nZn

Rr =(1- in B2 t Es t ne ~ 3, B2E3)/(pn)'/?

In some applications the argument p in Ry or the argument y in Ro is negative, and the
Cauchy principal value of the integral is required. This is easily handled by using the formulas

Ry(x,y,2,P) =
ly — y)Ra(x,y, 2,7) — 3Re(x,y, 2) + 8Ro(xz/y, py/y)] /(y — P)
(6.11.14)
where
= (z=y(y— 2)

ySy¢ ye 6.11.15
y=y vp ( )

is positive if p is negative, and

x 1/2

Ro(,y) = (; * ;) Ro(w—y,-) (6.11.16)

The Cauchy principal value of Ry has a zero at some value of p < 0, so (6.11.14) will give
some loss of significant figures near the zero.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
22
ao
so
es
az
oe
$3
22
5
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
go
§5
an
gd
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

9
o
3
Ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

264 Chapter 6. Special Functions

#include <math.h>
#include "nrutil.h"
#define ERRTOL 0.08
#define TINY 1.5e-38
#define BIG 3.0e37
#define THIRD (1.0/3.0)
#define C1 (1.0/24.0)
#define C2 0.1

#define C3 (3.0/44.0)
#define C4 (1.0/14.0)

float rf(float x, float y, float z)
Computes Carlson's elliptic integral of the first kind, Rp(a,y,z). x, y, and z must be nonneg-
ative, and at most one can be zero. TINY must be at least 5 times the machine underflow limit,
BIG at most one fifth the machine overflow limit.
t

float alamb,ave,delx,dely,delz,e2,e3,sqrtx,sqrty,sqrtz,xt,yt,zt;

if (FMIN(FMIN(x,y),z) < 0.0 || FMIN(FMIN(xty,x+z),y+z) < TINY ||
FMAX(FMAX(x,y),z) > BIG)
nrerror("invalid arguments in rf");

sqrtx=sqrt (xt) ;
sqrty=sqrt (yt) ;
sqrtz=sqrt (zt) ;
alamb=sqrtx*(sqrty+sqrtz)+sqrty*sqrtz;
xt=0.25*(xt+alamb) ;
yt=0.25*(yt+alamb) ;
zt=0.25*(zt+alamb) ;
ave=THIRD*(xt+yt+zt) ;
delx=(ave-xt) /ave;
dely=(ave-yt) /ave;
delz=(ave-zt)/ave;
} while (FMAX(FMAX(fabs(delx) ,fabs(dely)) ,fabs(delz)) > ERRTOL);
e2=delx*dely-delz*delz;
e3=delx*dely*delz;
return (1.0+(C1*e2-C2-C3*e3) *e2+C4*e3) /sqrt (ave) ;

A value of 0.08 for the error tolerance parameter is adequate for single precision (7
significant digits). Since the error scales as €8 , we see that 0.0025 will yield double prec
(16 significant digits) and require at most two or three more iterations. Since the coefficients
of the sixth-order truncation error are different for the other elliptic functions, these values for
the error tolerance should be changed to 0.04 and 0.0012 in the algorithm for Rc, and 0.05 and
0.0015 for Ry and Rp. As well as being an algorithm in its own right for certain combinations
of elementary functions, the algorithm for Rc is used repeatedly in the computation of Ry.

The C implementations test the input arguments against two machine-dependent con-
stants, TINY and BIG, to ensure that there will be no underflow or overflow during the
computation. We have chosen conservative values, corresponding to a machine minimum of
3 x 107%° and a machine maximum of 1.7 x 10°8. You can always extend the range of
admissible argument values by using the homogeneity relations (6.11.22), below.

#include <math.h>
#include "nrutil.h"
#define ERRTOL 0.05

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.11 Elliptic Integrals and Jacobian Elliptic Functions 265

#define TINY 1.0e-25
#define BIG 4.5e21

#define C1 (3.0/14.0)
#define C2 (1.0/6.0)
#define C3 (9.0/22.0)
#define C4 (3.0/26.0)
#define C5 (0.25*C3)
#define C6 (1.5*C4)

float rd(float x, float y, float z)

Computes Carlson's elliptic integral of the second kind, Rp(a,y,z). « and y must be non-
negative, and at most one can be zero. z must be positive. TINY must be at least twice the
negative 2/3 power of the machine overflow limit. BIG must be at most 0.1 x ERRTOL times
the negative 2/3 power of the machine underflow limit.

float alamb,ave,delx,dely,delz,ea,eb,ec,ed,ee,fac,sqrtx,sqrty,
sqrtz,sum,xt,yt,zt;

if (FMIN(x,y) < 0.0 || FMIN(x+y,z) < TINY || FMAX(FMAX(x,y),z) > BIG)
nrerror("invalid arguments in rd");

do {
sqrtx=sqrt (xt) ;
sqrty=sqrt (yt) ;
sqrtz=sqrt (zt) ;
alamb=sqrtx* (sqrty+sqrtz)+sqrty*sqrtz;
sum += fac/(sqrtz*(zttalamb)) ;
fac=0.25*fac;
xt=0.25*(xt+alamb) ;
yt=0.25*(yt+alamb) ;
zt=0.25*(zt+alamb) ;
ave=0.2*(xttyt+3.0*zt) ;
delx=(ave-xt) /ave;
dely=(ave-yt) /ave;
delz=(ave-zt)/ave;
} while (FMAX(FMAX(fabs(delx) ,fabs(dely)) ,fabs(delz)) > ERRTOL);
ea=delx*dely;
eb=delz*delz;

ee=ed+ectec;
return 3.0*sumtfac*(1.0+ed*(-C1+C5*ed-C6*delztee)
+delz* (C2*eetdelz* (-C3*ectdelz*C4*ea) )) /(ave*sqrt (ave)) ;

#include <math.h>
#include "nrutil.h"
#define ERRTOL 0.05
#define TINY 2.5e-13
#define BIG 9.0e11
#define C1 (3.0/14.0)
#define C2 (1.0/3.0)
#define C3 (3.0/22.0)
#define C4 (3.0/26.0)
#define C5 (0.75*C3)
#define C6 (1.5*C4)
#define C7 (0.5*C2)
#define C8 (C3+C3)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

266 Chapter 6. Special Functions

float rj(float x, float y, float z, float p)
Computes Carlson’s elliptic integral of the third kind, Ry(2,y,z,p). x, y, and z must be
nonnegative, and at most one can be zero. p must be nonzero. If p < 0, the Cauchy principal
value is returned. TINY must be at least twice the cube root of the machine underflow limit,
BIG at most one fifth the cube root of the machine overflow limit.
t

float rc(float x, float y);

float rf(float x, float y, float z);

float a,alamb,alpha,ans,ave,b,beta,delp,delx,dely,delz,ea,eb,ec,

ed,ee,fac,pt,rcx,rho,sqrtx,sqrty,sqrtz,sum,tau,xt,yt,zt;

doa,

if (FMIN(FMIN(x,y),z) < 0.0 || FMIN(FMIN(FMIN(xty,x+z),y+z),fabs(p)) < TINY
|| FMAX(FMAX(FMAX(x,y),z),fabs(p)) > BIG)
nrerror("invalid arguments in rj");

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

sum=0.0;

fac=1.0;

if (p> 0.0) {
Xt=x;
ytey;
zt=Z;
pt=p;

} else {

xt=FMIN(FMIN(x,y),z);
zt=FMAX (FMAX (x,y) ,z);
yt=xtyt+z-xt-zt;
a=1.0/(yt-p);

b=ax (zt-yt) *(yt-xt) ;
pt=ytt+b;
rho=xt*zt/yt;
tau=p*pt/yt;
rex=rc(rho,tau) ;

do {
sqrtx=sqrt (xt) ;
sqrty=sqrt (yt) ;
sqrtz=sqrt (zt) ;
alamb=sqrtx*(sqrty+sqrtz)+sqrty*sqrtz;
alpha=SQR (pt* (sqrtx+sqrty+sqrtz)+sqrtx*sqrty*sqrtz) ;
beta=pt+SQR(ptt+alamb) ;
sum += fac*rc(alpha, beta) ;
fac=0.25*fac;
xt=0.25*(xt+alamb) ;
yt=0.25*(yt+alamb) ;
zt=0.25*(zt+alamb) ;
pt=0.25* (pt+alamb) ;
ave=0.2*(xtt+yt+zt+pt+pt) ;
delx=(ave-xt) /ave;
dely=(ave-yt) /ave;
delz=(ave-zt)/ave;
delp=(ave-pt) /ave;
} while (FMAX(FMAX(FMAX(fabs(delx) ,fabs(dely)),
fabs(delz)) ,fabs(delp)) > ERRTOL);
ea=delx* (dely+delz) +dely+delz;
eb=delx*dely*delz;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

ee=eb+2.0*delp*(ea-ec) ;

ans=3 .0*sumtfac*(1.0+ed* (-C1+C5*ed-C6*ee) +eb* (C7+delp* (-C8+delp*C4) )
+delp*eax* (C2-delp*C3)-C2*delp*ec) /(ave*sqrt (ave)) ;

if (p <= 0.0) ans=a*(b*ans+3.0*(rex-rf (xt,yt,zt)));

return ans;

6.11 Elliptic Integrals and Jacobian Elliptic Functions 267

#include <math.h>

#include "nrutil.h"

#define ERRTOL 0.04

#define TINY 1.69e-38
#define SQRTNY 1.3e-19
#define BIG 3.e37

#define TNBG (TINY*BIG)
#define COMP1 (2.236/SQRTNY)
#define COMP2 (TNBG*TNBG/25.0)
#define THIRD (1.0/3.0)
#define C1 0.3

#define C2 (1.0/7.0)

#define C3 0.375

#define C4 (9.0/22.0)

float rc(float x, float y)

Computes Carlson's degenerate elliptic integral, Ro(x,y). x must be nonnegative and y must
be nonzero. If y < 0, the Cauchy principal value is returned. TINY must be at least 5 times
the machine underflow limit, BIG at most one fifth the machine maximum overflow limit.

{

float alamb,ave,s,w,xt,yt;
if (x < 0.0 I| y == 0.0 || (x+fabs(y)) < TINY || (x+fabs(y)) > BIG ||
(y<-COMP1 && x > 0.0 && x < COMP2))
nrerror("invalid arguments in rc");
if (y > 0.0) {
Xt=x;

yt = -ys
wesqrt (x) /sqrt (xt) ;
+
do {

alamb=2.0*sqrt (xt) *sqrt (yt)+yt;
xt=0.25*(xt+alamb) ;
yt=0.25*(yt+alamb) ;
ave=THIRD*(xtt+yttyt) ;
s=(yt-ave) /ave;
} while (fabs(s) > ERRTOL);
return w*(1.0+s*s*(Ci+s* (C2+s*(C3+s*C4))))/sqrt (ave) ;

At times you may want to express your answer in Legendre’s notation. Alter-

natively, you may be given results in that notation and need to compute their values
with the programs given above. It is a simple matter to transform back and forth.
The Legendre elliptic integral of the 1st kind is defined as

° dé
F(¢,k) = | —— (6.11.17)
<2.
0 1—k?sin

The complete elliptic integral of the Ist kind is given by

K(k) = F(n/2,k) (6.11.18)
In terms of Rr,

F(¢,k) = sin oRr (cos? ¢, 1 — k* sin? ¢, 1)
3 (6.11.19)
K(k) = Rr(0,1—k*,1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

268 Chapter 6. Special Functions

The Legendre elliptic integral of the 2nd kind and the complete elliptic integral of
the 2nd kind are given by

$
E(¢,k) = [ V1—k? sin? 6 do
0

= sin ¢Re(cos” ¢, 1 — k? sin? ¢, 1)

(6.11.20)
- tk? sin? dRp(cos? ¢,1 — k* sin? 4, 1)
E(k) = E(1/2,k) = Rp(O,1—k?, 1) — 2k? Rp (0,1 — k?, 1)
Finally, the Legendre elliptic integral of the 3rd kind is
° do
Il(¢,n,k) = i
0 (1+nsin? 6) V1 — k? sin? 6
= sin dRe(cos” ¢, 1 — k? sin? ¢, 1) (6.11.21)

- xn sin? Ry (cos? $,1—k? sin? ¢,1,1+4 nsin? ¢)

(Note that this sign convention for n is opposite that of Abramowitz and Stegun [12],
and that their sina is our k.)

#include <math.h>
#include "nrutil.h"

float ellf(float phi, float ak)
Legendre elliptic integral of the 1st kind F(¢,k), evaluated using Carlson's function Rp. The
argument ranges are 0 < ¢ < 7/2, 0 < ksing < 1.

{

float rf(float x, float y, float z);

float s;

s=sin(phi) ;

return s#rf(SQR(cos(phi)), (1.0-s*ak)*(1.0+s*ak) ,1.0);
3

#include <math.h>
#include "nrutil.h"

float elle(float phi, float ak)
Legendre elliptic integral of the 2nd kind E(¢,k), evaluated using Carlson's functions Rp and
Rr. The argument ranges are 0 < ¢ < 7/2, 0 < ksind < 1.
t
float rd(float x, float y, float z);
float rf(float x, float y, float z);
float cc,q,s;

s=sin(phi);

cec=SQR(cos(phi)) ;

q=(1.0-s*ak)*(1.0+s*ak) ;

return s*(rf(cc,q,1.0)-(SQR(s*ak) )*rd(cc,q,1.0)/3.0);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

6.11 Elliptic Integrals and Jacobian Elliptic Functions 269

#include <math.h>
#include "nrutil.h"

float ellpi(float phi, float en, float ak)

Legendre elliptic integral of the 3rd kind II(¢, n, k), evaluated using Carlson's functions Ry and
Rr. (Note that the sign convention on n is opposite that of Abramowitz and Stegun.) The
ranges of @ and k are0 < $6 < 7/2,0< ksing < 1.

{

float rf(float x, float y, float z);

float rj(float x, float y, float z, float p);

float cc,enss,q,s;

s=sin(phi);

enss=en*s*s;

cec=SQR(cos(phi)) ;

q=(1.0-s*ak)*(1.0+s*ak) ;

return s*(rf(cc,q,1.0)-enss*rj(cc,q,1.0,1.0+enss)/3.0);
3

Carlson’s functions are homogeneous of degree — 3 and -3, so

Rr(Aa, Ay, Az) = AW? Re(a, y, 2)
(6.11.22)
Ry (Aa, Ay, Az, Ap) = AB? Ry (a, y, 2P)

Thus to express a Carlson function in Legendre’s notation, permute the first three
arguments into ascending order, use homogeneity to scale the third argument to be
1, and then use equations (6.11.19)-(6.11.21).

Jacobian Elliptic Functions

The Jacobian elliptic function sn is defined as follows: instead of considering
the elliptic integral

u(y, k) =u = F(¢,k) (6.11.23)
consider the inverse function
y =sing = sn(u, k) (6.11.24)

Equivalently,

sn ad
u= oy (6.11.25)
0 V(lL—y?)( — ky?)

When k; = 0, sn is just sin. The functions cn and dn are defined by the relations
sn? + cn? = 1, ksn? + dn? = 1 (6.11.26)
The routine given below actually takes m. = k? = 1 — k? as an input parameter.

It also computes all three functions sn, cn, and dn since computing all three is no
harder than computing any one of them. For a description of the method, see [8].

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

270 Chapter 6. Special Functions

#include <math.h>
#define CA 0.0003 The accuracy is the square of CA.

void sncndn(float uu, float emmc, float *sn, float *cn, float ¥*dn)
Returns the Jacobian elliptic functions sn(u, ke), cn(u, ke), and dn(u, k-). Here uu = u, while
emmc = k2.
{
float a,b,c,d,emc,u;
float em[14],en[14];
int i,ii,1l,bo;

emc=emmc ;
u=uu;
if (emc) {
bo=(emce < 0.0);
if (bo) {
d=1.0-emc;
emc /= -1.0/d;
u *= (d=sqrt(d));

}

a=1.0;

*dn=1.0;

for (i=1;i<=13;i++) {
1=i;
em[i]=a;
en[i]=(emc=sqrt (emc)) ;
c=0.5*(atemc) ;
if (fabs(a-emc) <= CA*a) break;
emc ¥= a;
a=c;

}

u *= 3

*sn=sin(u) ;

*cn=cos(u) ;

if (*sn) {
a=(*cn) /(*sn) ;

ii>=1;ii--) f{
b=em[ii];

a *= c;

c ¥*= (*dn);
¥dn=(en[ii]+a)/(bta) ;
a=c/b;

}

a=1.0/sqrt (c*c+1.0);
¥sn=(*sn >= 0.0 ? a: -a);
*cn=c*(#sn) ;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

if (bo) {
a=(#dn) ;
#dn=(*cn) ;
*cn=a;
sn /= d;

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

+

} else {
*cn=1.0/cosh(u) ;
adn=(#cn) ;
*sn=tanh(u) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

6.12 Hypergeometric Functions 271

CITED REFERENCES AND FURTHER READING:

Erdélyi, A., Magnus, W., Oberhettinger, F., and Tricomi, FG. 1953, Higher Transcendental
Functions, Vol. Il, (New York: McGraw-Hill). [1]

Gradshteyn, I.S., and Ryzhik, |.W. 1980, Table of Integrals, Series, and Products (New York:
Academic Press). [2]

Carlson, B.C. 1977, SIAM Journal on Mathematical Analysis, vol. 8, pp. 231-242. [3]

Carlson, B.C. 1987, Mathematics of Computation, vol. 49, pp. 595-606 [4]; 1988, op. cit., vol. 51,
pp. 267-280 [5]; 1989, op. cit., vol. 53, pp. 327-333 [6]; 1991, op. cit., vol. 56, pp. 267-280.
7]

Bulirsch, R. 1965, Numerische Mathematik, vol. 7, pp. 78-90; 1965, op. cit., vol. 7, pp. 353-354;
1969, op. cit., vol. 13, pp. 305-315. [8]

Carlson, B.C. 1979, Numerische Mathematik, vol. 33, pp. 1-16. [9]

Carlson, B.C., and Notis, E.M. 1981, ACM Transactions on Mathematical Software, vol. 7,
pp. 398-403. [10]

Carlson, B.C. 1978, SIAM Journal on Mathematical Analysis, vol. 9, p. 524-528. [11]

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), Chapter 17. [12]

Mathews, J., and Walker, R.L. 1970, Mathematical Methods of Physics, 2nd ed. (Reading, MA:
W.A. Benjamin/Addison-Wesley), pp. 78-79.

6.12 Hypergeometric Functions

As was discussed in 85.14, a fast, general routine for the the complex hyperge-
ometric function 2F} (a, b, c; z), is difficult or impossible. The function is defined as
the analytic continuation of the hypergeometric series,

ab z . a(a+1)jb(b+1) 2

2Fi(a,b,¢;2z) =1

cl! ce(c +1) 2!
a(a $1)... @+j-Wbb+1)-.- (b+ 5-H
e(e+1)...(e+j-1) je

(6.12.1)
This series converges only within the unit circle |z| < 1 (see [1]), but one’s interest
in the function is not confined to this region.

Section 5.14 discussed the method of evaluating this function by direct path
integration in the complex plane. We here merely list the routines that result.

Implementation of the function hypgeo is straightforward, and is described by
comments in the program. The machinery associated with Chapter 16’s routine for
integrating differential equations, odeint, is only minimally intrusive, and need not
even be completely understood: use of odeint requires one zeroed global variable,
one function call, and a prescribed format for the derivative routine hypdrv.

The function hypgeo will fail, of course, for values of z too close to the
singularity at 1. (If you need to approach this singularity, or the one at oo, use the
“linear transformation formulas” in §15.3 of [1].) Away from z = 1, and for moderate
values of a,b,c, it is often remarkable how few steps are required to integrate the
equations. A half-dozen is typical.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

272 Chapter 6. Special Functions

#include <math.h>
#include "complex.h"
#include "nrutil.h"

#define EPS 1.0e-6 Accuracy parameter.
fcomplex aa,bb,cc,z0,dz; Communicates with hypdrv.
int kmax,kount; Used by odeint.

float *xp,**yp,dxsav;

doa,

fcomplex hypgeo(fcomplex a, fcomplex b, fcomplex c, fcomplex z)

Complex hypergeometric function 2F for complex a,b,c, and z, by direct integration of the
hypergeometric equation in the complex plane. The branch cut is taken to lie along the real
axis, Rez > 1.

t

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

void bsstep(float y[], float dydx[], int nv, float *xx, float htry,
float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []));
void hypdrv(float s, float yy[], float dyyds[]);
void hypser(fcomplex a, fcomplex b, fcomplex c, fcomplex z,
fcomplex *series, fcomplex *deriv);
void odeint(float ystart[], int nvar, float x1, float x2,
float eps, float hi, float hmin, int *nok, int *nbad,
void (*derivs) (float, float [], float []),
void (*rkqs)(float [], float [], int, float *, float, float,
float [], float *, float *, void (*)(float, float [], float [])));
int nbad,nok;
fcomplex ans,y[3];
float *yy;

kmax=0;

if (z.r*z.rtz.i*z.i <= 0.25) { Use series...
hypser(a,b,c,z,zans,&y[2]);
return ans;

+
else if (z.r < 0.0) z0=Complex(-0.5,0.0); ...or pick a starting point for the path
else if (z.r <= 1.0) z0=Complex(0.5,0.0); integration.
else z0=Complex(0.0,z.i >= 0.0 7 0.5 : -0.5);
aa=a; Load the global variables to pass pa-
3 rameters “over the head” of odeint
to hypdrv.

dz=Csub(z,z0) ;

hypser(aa,bb,cc,z0,&y[1] ,zy[2]); Get starting function and derivative.
yy=vector (1,4);

yy (1]=y(1] .r;

i;

yy (3]=y(2].r;

yy (4]=y(2].i;

odeint (yy,4,0.0,1.0,EPS,0.1,0.0001,&nok, &nbad ,hypdrv,bsstep) ;

The arguments to odeint are the vector of independent variables, its length, the starting
and ending values of the dependent variable, the accuracy parameter, an initial guess for
stepsize, a minimum stepsize, the (returned) number of good and bad steps taken, and the
names of the derivative routine and the (here Bulirsch-Stoer) stepping routine.
y(1]=Complex(yy [1] ,yy[2]);

free_vector(yy,1,4);

return y[1];

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


6.12 Hypergeometric Functions 273

#include "complex.h"
#define ONE Complex(1.0,0.0)

void hypser(fcomplex a, fcomplex b, fcomplex c, fcomplex z, fcomplex *series,
fcomplex *deriv)

Returns the hypergeometric series 2F) and its derivative, iterating to machine accuracy. For

|z| < 1/2 convergence is quite rapid.

void nrerror(char error_text[]);
int n;
fcomplex aa,bb,cc,fac,temp;

deriv->r=0.0;
deriv->i=0.0;
fac=Complex(1.0,0.0);
temp=fac;
aa=a;
bb:
cc=c;
for (n=1;n<=1000;n++) {

fac=Cmul (fac ,Cdiv(Cmul (aa, bb) ,cc));

deriv->r+=fac.r;

deriv->i+=fac.i;

fac=Cmul (fac ,RCmul(1.0/n,z)) ;

*series=Cadd(temp,fac) ;

if (series->r == temp.r && series->i == temp.i) return;

temp= *series;

aa=Cadd(aa, ONE) ;

bb=Cadd (bb , ONE) ;

cc=Cadd(cc, ONE) ;

+

nrerror("convergence failure in hypser");

#include "complex.h"
#define ONE Complex(1.0,0.0)

extern fcomplex aa,bb,cc,z0,dz; Defined in hypgeo.

void hypdrv(float s, float yy[], float dyyds[])
Computes derivatives for the hypergeometric equation, see text equation (5.14.4).
{

fcomplex z,y[3] ,dyds[3] ;

y(1]=Complex(yy[1] ,yy[2]);

y[2]=Complex(yy[3] ,yy[4]);

z=Cadd(z0,RCmul(s ,dz));

dyds [1] =Cmul (y [2] ,dz) ;

dyds [2] =Cmul (Csub(Cmul (Cmul (aa, bb) ,y[1]) ,Cmul (Csub(cc,
(mul (Cadd(Cadd(aa,bb) ,ONE) ,z)),y([2])),
Cdiv(dz,Cmul(z,Csub(ONE,z))));

dyyds [1]=dyds [1] .r;

dyyds [2]=dyds [1] .i;

dyyds [3]=dyds [2] .r;

dyyds [4] =dyds [2] .i;

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York). [1]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 7. Random Numbers

7.0 Introduction

It may seem perverse to use a computer, that most precise and deterministic of
all machines conceived by the human mind, to produce “random” numbers. More
than perverse, it may seem to be a conceptual impossibility. Any program, after all,
will produce output that is entirely predictable, hence not truly “random.”

Nevertheless, practical computer “random number generators” are in common
use. We will leave it to philosophers of the computer age to resolve the paradox in
a deep way (see, e.g., Knuth [1] §3.5 for discussion and references). One sometimes
hears computer-generated sequences termed pseudo-random, while the word random
is reserved for the output of an intrinsically random physical process, like the elapsed
time between clicks of a Geiger counter placed next to a sample of some radioactive
element. We will not try to make such fine distinctions.

A working, though imprecise, definition of randomness in the context of
computer-generated sequences, is to say that the deterministic program that produces
a random sequence should be different from, and — in all measurable respects —
statistically uncorrelated with, the computer program that uses its output. In other
words, any two different random number generators ought to produce statistically
the same results when coupled to your particular applications program. If they don’t,
then at least one of them is not (from your point of view) a good generator.

The above definition may seem circular, comparing, as it does, one generator to
another. However, there exists a body of random number generators which mutually
do satisfy the definition over a very, very broad class of applications programs.
And it is also found empirically that statistically identical results are obtained from
random numbers produced by physical processes. So, because such generators are
known to exist, we can leave to the philosophers the problem of defining them.

A pragmatic point of view, then, is that randomness is in the eye of the beholder
(or programmer). What is random enough for one application may not be random
enough for another. Still, one is not entirely adrift in a sea of incommensurable
applications programs: There is a certain list of statistical tests, some sensible and
some merely enshrined by history, which on the whole will do a very good job
of ferreting out any correlations that are likely to be detected by an applications
program (in this case, yours). Good random number generators ought to pass all of
these tests; or at least the user had better be aware of any that they fail, so that he or
she will be able to judge whether they are relevant to the case at hand.

274

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

7.1 Uniform Deviates 275

As for references on this subject, the one to turn to first is Knuth [1]. Then
try [2]. Only a few of the standard books on numerical methods [8-4] treat topics
relating to random numbers.

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), Chapter 3, especially §3.5. [1]

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New York: Springer-
Verlag). [2]

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
Chapter 11. [3]

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), Chapter 10. [4]

7.1 Uniform Deviates

Uniform deviates are just random numbers that lie within a specified range
(typically 0 to 1), with any one number in the range just as likely as any other. They
are, in other words, what you probably think “random numbers” are. However,
we want to distinguish uniform deviates from other sorts of random numbers, for
example numbers drawn from a normal (Gaussian) distribution of specified mean
and standard deviation. These other sorts of deviates are almost always generated by
performing appropriate operations on one or more uniform deviates, as we will see
in subsequent sections. So, a reliable source of random uniform deviates, the subject
of this section, is an essential building block for any sort of stochastic modeling or
Monte Carlo computer work.

System-Supplied Random Number Generators

Most C implementations have, lurking within, a pair of library routines for
initializing, and then generating, “random numbers.” In ANSI C, the synopsis is:

#include <stdlib.h>
#define RAND_MAX ...

void srand(unsigned seed) ;
int rand(void) ;

You initialize the random number generator by invoking srand(seed) with
some arbitrary seed. Each initializing value will typically result in a different
random sequence, or a least a different starting point in some one enormously long
sequence. The same initializing value of seed will always return the same random
sequence, however.

You obtain successive random numbers in the sequence by successive calls to
rand(). That function returns an integer that is typically in the range 0 to the
largest representable positive value of type int (inclusive). Usually, as in ANSI C,
this largest value is available as RAND_MAX, but sometimes you have to figure it out
for yourself. If you want a random float value between 0.0 (inclusive) and 1.0
(exclusive), you get it by an expression like

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

276 Chapter 7. Random Numbers

x = rand()/(RAND_MAX+1.0) ;

Now our first, and perhaps most important, lesson in this chapter is: be very,
very suspicious of a system-supplied rand () that resembles the one just described.
If all scientific papers whose results are in doubt because of bad rand()s were
to disappear from library shelves, there would be a gap on each shelf about as
big as your fist. System-supplied rand ()s are almost always linear congruential
generators, which generate a sequence of integers I, Iz, Iz3,..., each between 0 and
m — 1 (e.g., RAND_MAX) by the recurrence relation

Tj41 = al; +e (mod m) (7.1.1)

Here m is called the modulus, and a and c are positive integers called the multiplier
and the increment respectively. The recurrence (7.1.1) will eventually repeat itself,
with a period that is obviously no greater than m. If m, a, and c are properly chosen,
then the period will be of maximal length, i.e., of length m. In that case, all possible
integers between 0 and m — 1 occur at some point, so any initial “seed” choice of Io
is as good as any other: the sequence just takes off from that point.

Although this general framework is powerful enough to provide quite decent
random numbers, its implementation in many, if not most, ANSI C libraries is quite
flawed; quite a number of implementations are in the category “totally botched.”
Blame should be apportioned about equally between the ANSI C committee and
the implementors. The typical problems are these: First, since the ANSI standard
specifies that rand () return a value of type int — which is only a two-byte quantity
on many machines — RAND_MAX is often not very large. The ANSI C standard
requires only that it be at least 32767. This can be disastrous in many circumstances:
for a Monte Carlo integration (§7.6 and §7.8), you might well want to evaluate 10°
different points, but actually be evaluating the same 32767 points 30 times each, not
at all the same thing! You should categorically reject any library random number
routine with a two-byte returned value.

Second, the ANSI committee’s published rationale includes the following
mischievous passage: “The committee decided that an implementation should be
allowed to provide a rand function which generates the best random sequence
possible in that implementation, and therefore mandated no standard algorithm. It
recognized the value, however, of being able to generate the same pseudo-random
sequence in different implementations, and so it has published an example... .
[emphasis added]” The “example” is

unsigned long next=1;

int rand(void) /* NOT RECOMMENDED (see text) */
{

next = next*1103515245 + 12345;

return (unsigned int) (next/65536) % 32768;
+

void srand(unsigned int seed)
{
next=seed;

+

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.1 Uniform Deviates 277

This corresponds to equation (7.1.1) with a = 1103515245, c = 12345, and
m = 2°? (since arithmetic done on unsigned long quantities is guaranteed to
return the correct low-order bits). These are not particularly good choices for a and
c (the period is only 2°), though they are not gross embarrassments by themselves.
The real botches occur when implementors, taking the committee’s statement above
as license, try to “improve” on the published example. For example, one popular
32-bit PC-compatible compiler provides a long generator that uses the above
congruence, but swaps the high-order and low-order 16 bits of the returned value.
Somebody probably thought that this extra flourish added randomness; in fact it ruins
the generator. While these kinds of blunders can, of course, be fixed, there remains
a fundamental flaw in simple linear congruential generators, which we now discuss.

The linear congruential method has the advantage of being very fast, requiring
only a few operations per call, hence its almost universal use. It has the disadvantage
that it is not free of sequential correlation on successive calls. If k random numbers at
a time are used to plot points in k dimensional space (with each coordinate between
0 and 1), then the points will not tend to “fill up” the k-dimensional space, but rather
will lie on (k — 1)-dimensional “planes.” There will be at most about m?/* such
planes. If the constants m, a, and c are not very carefully chosen, there will be many
fewer than that. If m is as bad as 32768, then the number of planes on which triples
of points lie in three-dimensional space will be no greater than about the cube root
of 32768, or 32. Even if m is close to the machine’s largest representable integer,
e.g., ~~ 2°, the number of planes on which triples of points lie in three-dimensional
space is usually no greater than about the cube root of 2°, about 1600. You might
well be focusing attention on a physical process that occurs in a small fraction of the
total volume, so that the discreteness of the planes can be very pronounced.

Even worse, you might be using a generator whose choices of m, a, and c have
been botched. One infamous such routine, RANDU, with a = 65539 and m = 2°,
was widespread on IBM mainframe computers for many years, and widely copied
onto other systems [1]. One of us recalls producing a “random” plot with only 11
planes, and being told by his computer center’s programming consultant that he
had misused the random number generator: “We guarantee that each number is
random individually, but we don’t guarantee that more than one of them is random.”
Figure that out.

Correlation in k-space is not the only weakness of linear congruential generators.
Such generators often have their low-order (least significant) bits much less random
than their high-order bits. If you want to generate a random integer between | and

10, you should always do it using high-order bits, as in
j=1+(int) (10.0*rand()/(RAND_MAX+1.0)) ;
and never by anything resembling
jeit+(rand() % 10);
(which uses lower-order bits). Similarly you should never try to take apart a

“rand()” number into several supposedly random pieces. Instead use separate
calls for every piece.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

278 Chapter 7. Random Numbers

Portable Random Number Generators

Park and Miller [1] have surveyed a large number of random number generators
that have been used over the last 30 years or more. Along with a good theoretical
review, they present an anecdotal sampling of a number of inadequate generators that
have come into widespread use. The historical record is nothing if not appalling.

There is good evidence, both theoretical and empirical, that the simple multi-
plicative congruential algorithm

Tj41 = al; (mod m) (7.1.2)

can be as good as any of the more general linear congruential generators that have
c # 0 (equation 7.1.1) — if the multiplier a and modulus m are chosen exquisitely
carefully. Park and Miller propose a “Minimal Standard” generator based on the
choices

a=7 =16807 m=23' —1 = 2147483647 (7.1.3)

First proposed by Lewis, Goodman, and Miller in 1969, this generator has in
subsequent years passed all new theoretical tests, and (perhaps more importantly)
has accumulated a large amount of successful use. Park and Miller do not claim that
the generator is “‘perfect” (we will see below that it is not), but only that it is a good
minimal standard against which other generators should be judged.

It is not possible to implement equations (7.1.2) and (7.1.3) directly in a
high-level language, since the product of a and m — 1 exceeds the maximum value
for a 32-bit integer. Assembly language implementation using a 64-bit product
register is straightforward, but not portable from machine to machine. A trick
due to Schrage [2,3] for multiplying two 32-bit integers modulo a 32-bit constant,
without using any intermediates larger than 32 bits (including a sign bit) is therefore
extremely interesting: It allows the Minimal Standard generator to be implemented
in essentially any programming language on essentially any machine.

Schrage’s algorithm is based on an approximate factorization of m,

m=ag+r, ie, g=[m/al, r=mmoda (7.1.4)

with square brackets denoting integer part. If r is small, specifically r < q, and
0 < z < m-—1, it can be shown that both a(z mod q) and r[z/q] lie in the range
0,...,m — 1, and that

_ itis >
az mod m= a(z mod q) — r[z/q] if it is 2 0, (7.15)
a(z mod q) — r[z/q|+m__ otherwise
The application of Schrage’s algorithm to the constants (7.1.3) uses the values
q = 127773 and r = 2836.
Here is an implementation of the Minimal Standard generator:

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

7.1 Uniform Deviates 279

#tdefine IA 16807
#define IM 2147483647
#define AM (1.0/IM)
#define IQ 127773
#define IR 2836
#define MASK 123459876

float ran0(long *idum)

“Minimal” random number generator of Park and Miller. Returns a uniform random deviate
between 0.0 and 1.0. Set or reset idum to any integer value (except the unlikely value MASK)
to initialize the sequence; idum must not be altered between calls for successive deviates in

a sequence.
{
long k;
float ans;
*idum “= MASK; XORing with MASK allows use of zero and other
k=(*idum)/IQ; simple bit patterns for idum.
*idum=IA* (*idum-k*IQ)-IR*k; Compute idum=(IA*idum) % IM without over-
if (*idum < 0) *idum += IM; flows by Schrage’s method.
ans=AM* (*idum) ; Convert idum to a floating result.
*idum “= MASK; Unmask before return.

return ans;

The period of ran0 is 2°! — 2 = 2.1 x 10°. A peculiarity of generators of
the form (7.1.2) is that the value 0 must never be allowed as the initial seed — it
perpetuates itself — and it never occurs for any nonzero initial seed. Experience
has shown that users always manage to call random number generators with the see
idum=0. That is why ranO performs its exclusive-or with an arbitrary constant both
on entry and exit. If you are the first user in history to be proof against human error,
you can remove the two lines with the / operation.

Park and Miller discuss two other multipliers a that can be used with the same
m = 2°! — 1. These are a = 48271 (with q = 44488 and r = 3399) and a = 6962
(with g = 30845 and r = 23902). These can be substituted in the routine ranO
if desired; they may be slightly superior to Lewis et al.’s longer-tested values. No
values other than these should be used.

The routine ranO is a Minimal Standard, satisfactory for the majority o
applications, but we do not recommend it as the final word on random number
generators. Our reason is precisely the simplicity of the Minimal Standard. It is
not hard to think of situations where successive random numbers might be used
in a way that accidentally conflicts with the generation algorithm. For example,
since successive numbers differ by a multiple of only 1.6 x 104 out of a modulus of
more than 2 x 109, very small random numbers will tend to be followed by smaller
than average values. One time in 10°, for example, there will be a value < 10~°
returned (as there should be), but this will always be followed by a value less than
about 0.0168. One can easily think of applications involving rare events where this
property would lead to wrong results.

There are other, more subtle, serial correlations present in ranO. For example,
if successive points (J;, J;,1) are binned into a two-dimensional plane for i =
1,2,...,.N, then the resulting distribution fails the y? test when N is greater than a
few x107, much less than the period m — 2. Since low-order serial correlations have
historically been such a bugaboo, and since there is a very simple way to remove

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

280 Chapter 7. Random Numbers

them, we think that it is prudent to do so.

The following routine, rani, uses the Minimal Standard for its random value,
but it shuffles the output to remove low-order serial correlations. A random deviate
derived from the jth value in the sequence, Jj, is output not on the jth call, but rather
on a randomized later call, j + 32 on average. The shuffling algorithm is due to Bays
and Durham as described in Knuth [4], and is illustrated in Figure 7.1.1.

#define IA 16807

#define IM 2147483647
#define AM (1.0/IM)

#define IQ 127773

#define IR 2836

#define NTAB 32

#define NDIV (1+(IM-1)/NTAB)
#define EPS 1.2e-7

#define RNMX (1.0-EPS)

float rani(long *idum)

“Minimal” random number generator of Park and Miller with Bays-Durham shuffle and added
safeguards. Returns a uniform random deviate between 0.0 and 1.0 (exclusive of the endpoint
values). Call with idum a negative integer to initialize; thereafter, do not alter idum between
successive deviates in a sequence. RNMX should approximate the largest floating value that is
less than 1.

t
int j;
long k;
static long iy=0;
static long iv([NTAB];
float temp;
if (*idum <= 0 || !iy) { Initialize.
if (-(*idum) < 1) *idum=1; Be sure to prevent idum = 0.
else *idum = -(*idum);
for (j=NTAB+7;j>=0;j--) { Load the shuffle table (after 8 warm-ups).
k=(*idum) /1Q;
*idum=IA* (*idum-k*1Q) -IR*k;
if (*#idum < 0) *idum += IM;
if (j < NTAB) iv[j] = *idum;
}
iy=iv(0];
+
k=(*idum) /1Q; Start here when not initializing.
*idum=IA* (*idum-k*IQ)-IR*k; Compute idum=(IA*idum) % IM without over-
if (*idum < 0) *idum += IM; flows by Schrage’s method.
j=iy/NDIV; Will be in the range 0. .NTAB-1.
iy=iv(jl; Output previously stored value and refill the
iv(j] = *idum; shuffle table.
if ((temp=AM*iy) > RNMX) return RNMX; Because users don’t expect endpoint values.
else return temp;
3

The routine rani passes those statistical tests that ranO is known to fail. In
fact, we do not know of any statistical test that ran1 fails to pass, except when the
number of calls starts to become on the order of the period m, say > 10° © m/20.

For situations when even longer random sequences are needed, L’Ecuyer [6] has
given a good way of combining two different sequences with different periods so
as to obtain a new sequence whose period is the least common multiple of the two
periods. The basic idea is simply to add the two sequences, modulo the modulus of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.1 Uniform Deviates 281

OUTPUT

ivsi

Figure 7.1.1. | Shuffling procedure used in rani to break up sequential correlations in the Minimal
Standard generator. Circled numbers indicate the sequence of events: On each call, the random number
in iy is used to choose a random element in the array iv. That element becomes the output random
number, and also is the next iy. Its spot in iv is refilled from the Minimal Standard routine.

either of them (call it m). A trick to avoid an intermediate value that overflows the
integer wordsize is to subtract rather than add, and then add back the constant m — 1
if the result is < 0, so as to wrap around into the desired interval 0,...,m— 1.

Notice that it is not necessary that this wrapped subtraction be able to reach
all values 0,...,7™ — 1 from every value of the first sequence. Consider the absurd
extreme case where the value subtracted was only between | and 10: The resulting
sequence would still be no less random than the first sequence by itself. As a
practical matter it is only necessary that the second sequence have a range covering
substantially all of the range of the first. L’Ecuyer recommends the use of the two
generators m, = 2147483563 (with a, 40014, qi 53668, 71 12211) and
mz = 2147483399 (with az = 40692, qg2 = 52774, re = 3791). Both moduli
are slightly less than 2°!. The periods m; — 1 = 2 x 3 x 7 x 631 x 81031 and
mz —1= 2x 19 x 31 x 1019 x 1789 share only the factor 2, so the period of
the combined generator is + 2.3 x 1018. For present computers, period exhaustion
is a practical impossibility.

Combining the two generators breaks up serial correlations to a considerable
extent. We nevertheless recommend the additional shuffle that is implemented in
the following routine, ran2. We think that, within the limits of its floating-point
precision, ran2 provides perfect random numbers; a practical definition of “perfect”
is that we will pay $1000 to the first reader who convinces us otherwise (by finding a
statistical test that ran2 fails in a nontrivial way, excluding the ordinary limitations
of a machine’s floating-point representation).

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

282

Chapter 7.

Random Numbers

#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define

IM1 2147483563
IM2 2147483399
AM (1.0/IM1)
IMM1 (IM1-1)
IA1 40014

IA2 40692

IQ1 53668

IQ2 52774

IR1 12211

IR2 3791

NTAB 32

NDIV (1+IMM1/NTAB)
EPS 1.2e-7
RNMX (1.0-EPS)

float ran2(long *idum)
Long period (> 2 x 10!8) random number generator of L'Ecuyer with Bays-Durham shuffle

and added safeguards. Returns a uniform random deviate between 0.0 and 1.0 (exclusive of

the endpoint values). Call with idum a negative integer to initialize; thereafter, do not alter
idum between successive deviates in a sequence. RNMX should approximate the largest floating
value that is less than 1.

{

int j;

long k;

static long idum2=123456789;
static long iy=0;

static long iv([NTAB];

float temp;

if (*idum <= 0) {
if (-(+idum) < 1) *idum=1;
else *idum = -(*idum);
idum2=(*idum) ;
for (j=NTAB+7;j>=0;j--) {
k=(*idum)/1Q1;
*idum=IA1* (*idum-k*IQ1)-k*IR1;
if (*idum < 0) *idum += IM1;
if (j < NTAB) iv[j] = *idum;
}
iy=iv(0];
}
k=(*idum)/1Q1;
*idum=IA1* (*idum-k*IQ1)-k*IR1;
if (*idum < 0) *idum += IM1;
k=idum2/1Q2;
idum2=IA2* (idum2-k*IQ2)-k*IR2;
if (idum2 < 0) idum2 += IM2;
jeiy/NDIV;
i v[j]-idum2;
iv(j] = *idum;
aif (iy < 1) iy += IMM1;
if ((temp=AM*iy) > RNMX) return RNMX;
else return temp;

Initialize.
Be sure to prevent idum = 0.

Load the shuffle table (after 8 warm-ups).

Start here when not initializing.
Compute idum=(IA1*idum) % IM1 without
overflows by Schrage’s method.

Compute idum2=(IA2*idum) % IM2 likewise.
Will be in the range 0. .NTAB-1.
Here idum is shuffled, idum and idum2 are

combined to generate output.

Because users don’t expect endpoint values.

L’Ecuyer [6] lists additional short generators that can be combined into longer
ones, including generators that can be implemented in 16-bit integer arithmetic.

Finally, we give you Knuth’s suggestion [4] for a portable routine, which we
have translated to the present conventions as ran3. This is not based on the linear
congruential method at all, but rather on a subtractive method (see also [5]). One
might hope that its weaknesses, if any, are therefore of a highly different character

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.1 Uniform Deviates 283

from the weaknesses, if any, of rani above. If you ever suspect trouble with one
routine, it is a good idea to try the other in the same application. ran3 has one
nice feature: if your machine is poor on integer arithmetic (i.e., is limited to 16-bit
integers), you can declare mj, mk, and ma[] as float, define mbig and mseed
as 4000000 and 1618033, respectively, and the routine will be rendered entirely
floating-point.

#include <stdlib.h> Change to math.h in K&R C.

#define MBIG 1000000000

#define MSEED 161803398

#define MZ 0

#define FAC (1.0/MBIG)

According to Knuth, any large MBIG, and any smaller (but still large) MSEED can be substituted
for the above values.

float ran3(long *idum)
Returns a uniform random deviate between 0.0 and 1.0. Set idum to any negative value to
initialize or reinitialize the sequence.

{
static int inext,inextp;
static long ma[56]; The value 56 (range ma[1..55]) is special and
static int iff=0; should not be modified; see Knuth.
long mj,mk;
int i,ii,k;
if (*#idum < 0 || iff == 0) { Initialization.
iff=1;
mj=labs (MSEED-labs (*idum)) ; Initialize ma[55] using the seed idum and the
mj Z%= MBIG; large number MSEED.
ma[55]=mj;
mk=1;
for (i=1;i<=54;it+) { Now initialize the rest of the table,
ii=(21*i) % 55; in a slightly random order,
ma[ii] with numbers that are not especially random.
mk=mj-mk ;
if (mk < MZ) mk += MBIG;
mj=ma[iil;
}
for (k=1;k<=4;k++) We randomize them by “warming up the gener-
for (i= 55;itt+) { ator.”
ma[i] ma[1+(i+30) % 55];
if (ma[i] < MZ) ma[i] += MBIG;
3
inext=0; Prepare indices for our first generated number.
inextp=31; The constant 31 is special; see Knuth.
*idum=1;
+
Here is where we start, except on initialization.
if (++inext == 56) inext=1; Increment inext and inextp, wrapping around
if (++inextp == 56) inextp=1; 56 to 1.
mj=ma[inext]-ma[inextp] ; Generate a new random number subtractively.
if (mj < MZ) mj += MBIG; Be sure that it is in range.
ma[inext]=mj; Store it,
return mj*FAC; and output the derived uniform deviate.
3

Quick and Dirty Generators

One sometimes would like a “quick and dirty” generator to embed in a program, perhaps
taking only one or two lines of code, just to somewhat randomize things. One might wish to

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

284 Chapter 7. Random Numbers

process data from an experiment not always in exactly the same order, for example, so that
the first output is more “typical” than might otherwise be the case.

For this kind of application, all we really need is a list of “good” choices for m, a, and
c in equation (7.1.1). If we don’t need a period longer than 10* to 10°, say, we can keep the
value of (m — 1)a +c small enough to avoid overflows that would otherwise mandate the
extra complexity of Schrage’s method (above). We can thus easily embed in our programs

unsigned long jran,ia,ic,im;
float ran;

jran=(jran*iatic) % im;
ran=(float) jran / (float) im;

whenever we want a quick and dirty uniform deviate, or

jran=(jran*iatic) % im;
j=jlo+((jhi-jlo+1)*jran) /im;

whenever we want an integer between jlo and jhi, inclusive. (In both cases jran was once
initialized to any seed value between 0 and im-1.)

Be sure to remember, however, that when im is small, the kth root of it, which is the
number of planes in k-space, is even smaller! So a quick and dirty generator should never
be used to select points in k-space with k > 1.

With these caveats, some “good” choices for the constants are given in the accompanying
table. These constants (i) give a period of maximal length im, and, more important, (ii) pass
Knuth’s “spectral test” for dimensions 2, 3, 4,5, and 6. The increment ic is a prime, close to
the value (3 - 4V3) im; actually almost any value of ic that is relatively prime to im will do
just as well, but there is some “lore” favoring this choice (see [4], p. 84).

An Even Quicker Generator

In C, if you multiply two unsigned long int integers on a machine with a 32-bit long
integer representation, the value returned is the low-order 32 bits of the true 64-bit product. If
we now choose m = 2”, the “mod” in equation (7.1.1) is free, and we have simply

Ty = aly +e (7.16)

Knuth suggests a = 1664525 as a suitable multiplier for this value of m. H.W. Lewis
has conducted extensive tests of this value of a with c = 1013904223, which is a prime close
to (5 — 2)m. The resulting in-line generator (we will call it rangd1) is simply

unsigned long idum;

idum = 1664525L*idum + 1013904223L;

This is about as good as any 32-bit linear congruential generator, entirely adequate for many
uses. And, with only a single multiply and add, it is very fast.

To check whether your machine has the desired integer properties, see if you can
generate the following sequence of 32-bit values (given here in hex): 00000000, 3C6EF35F,
47502932, DICCF6E9, AAF95334, 6252E503, 9F2EC686, 57FE6C2D, A3D95FA8, 81FD-
BEE7, 94FOAFIA, CBF633B1.

If you need floating-point values instead of 32-bit integers, and want to avoid a divide by
floating-point 2° , a dirty trick is to mask in an exponent that makes the value lie between 1 and
2, then subtract 1.0. The resulting in-line generator (call it ranqd2) will look something like

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.1 Uniform Deviates 285

Constants for Quick and Dirty Random Number Generators

overflow at im ia ic overflow at im ia ic
6075 106 1283 86436 1093 18257
270 121500 1021 25673
7875 211 1663 259200 421 54773

921 927
7875 421 1663 117128 1277 24749
2”? 121500 2041 25673
6075 1366 1283 312500 741 66037

6655 936 1399 28
11979 430 = 253 145800 3661 30809
2?3 175000 2661 36979
14406 967 304 233280 1861 49297
29282 419 6173 244944 1597 51749

53125 171 11213 29
24 139968 3877 29573
12960 1741 273 214326 3613 45289
14000 1541 2957 714025 1366 150889

21870 1291 462 20
31104 625 657 134456 8121 28411
139968 205 29573 259200 7141 54773

925 93
29282 1255 6173 233280 9301 49297
81000 421 17117 714025 4096 150889

134456 281 2841 232

926

unsigned long idum,itemp;

float rand;
#ifdef vax
static unsigned long jflone = 0x00004080;
static unsigned long jflmsk = Oxffff007f;
#else
static unsigned long jflone = 0x3f800000;
static unsigned long jflmsk = Ox007fffff;

#endif

idum = 1664525L*idum + 1013904223L;
itemp = jflone | (jflmsk & idum);
rand = (*(float *)&itemp)-1.0;

The hex constants 3F800000 and OO7FFFFF are the appropriate ones for computers using
the IEEE representation for 32-bit floating-point numbers (e.g., IBM PCs and most UNIX
workstations). For DEC VAXes, the correct hex constants are, respectively, 00004080 and
FFFFOO07F. Notice that the IEEE mask results in the floating-point number being constructed
out of the 23 low-order bits of the integer, which is not ideal. (Your authors have tried
very hard to make almost all of the material in this book machine and compiler independent
— indeed, even programming language independent. This subsection is a rare aberration.
Forgive us. Once in a great while the temptation to be really dirty is just irresistible.)

Relative Timings and Recommendations

Timings are inevitably machine dependent. Nevertheless the following table

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

286 Chapter 7. Random Numbers

is indicative of the relative timings, for typical machines, of the various uniform
generators discussed in this section, plus ran4 from §7.5. Smaller values in the table
indicate faster generators. The generators ranqd1 and ranqd2 refer to the “quick
and dirty” generators immediately above.

Generator Relative Execution Time
ran0O =1.0
rani 1.3
ran2 = 2.0
ran3 = 0.6
ranqd1 = 0.10
ranqd2 = 0.25
ran4 x 4.0

On balance, we recommend rani for general use. It is portable, based on
Park and Miller’s Minimal Standard generator with an additional shuffle, and has no
known (to us) flaws other than period exhaustion.

If you are generating more than 100,000,000 random numbers in a single
calculation (that is, more than about 5% of rani’s period), we recommend the use
of ran2, with its much longer period.

Knuth’s subtractive routine ran3 seems to be the timing winner among portable
routines. Unfortunately the subtractive method is not so well studied, and not a
standard. We like to keep ran3 in reserve for a “second opinion,” substituting it when
we suspect another generator of introducing unwanted correlations into a calculation.

The routine ran4 generates extremely good random deviates, and has some
other nice properties, but it is slow. See §7.5 for discussion.

Finally, the quick and dirty in-line generators ranqd1 and ranqd2 are very fast,
but they are somewhat machine dependent, and at best only as good as a 32-bit linear
congruential generator ever is — in our view not good enough in many situations.
We would use these only in very special cases, where speed is critical.

CITED REFERENCES AND FURTHER READING:
Park, S.K., and Miller, K.W. 1988, Communications of the ACM, vol. 31, pp. 1192-1201. [1]
Schrage, L. 1979, ACM Transactions on Mathematical Software, vol. 5, pp. 132-138. [2]

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New York: Springer-
Verlag). [3]

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), §§3.2-3.3. [4]

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software (Englewood Cliffs,
NJ: Prentice Hall), Chapter 10. [5]

LEcuyer, P. 1988, Communications of the ACM, vol. 31, pp. 742-774. [6]

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), Chapter 10.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.2 Transformation Method: Exponential and Normal Deviates 287

7.2 Transformation Method: Exponential and
Normal Deviates

In the previous section, we learned how to generate random deviates with
a uniform probability distribution, so that the probability of generating a number
between x and x + dx, denoted p(x)dz, is given by

dz O0<a<l
Pdr = { 72.1
p(x )der 0 otherwise ( )
The probability distribution p(x) is of course normalized, so that
oo
—co

Now suppose that we generate a uniform deviate x and then take some prescribed
function of it, y(a). The probability distribution of y, denoted p(y)dy, is determined
by the fundamental transformation law of probabilities, which is simply

Ip(y)dy| = |p(a)dar| (7.2.3)
or
dx
= p(x) |— 724
ply) = (a) | (724)
Exponential Deviates
As an example, suppose that y(z) = — n(x), and that p(x) is as given by
equation (7.2.1) for a uniform deviate. Then
(y)dy = || dy = eva (7.2.5)
P(y)dy = | 7) dy =e Ndy 2.

which is distributed exponentially. This exponential distribution occurs frequently

in real problems, usually as the distribution of waiting times between independent

Poisson-random events, for example the radioactive decay of nuclei. You can also

easily see (from 7.2.4) that the quantity y/A has the probability distribution Ae ~*”.
So we have

#include <math.h>

float expdev(long *idum)
Returns an exponentially distributed, positive, random deviate of unit mean, using
rani(idum) as the source of uniform deviates.
t
float rani(long *idum) ;
float dum;

do

dum=ran1 (idum) ;
while (dum == 0.0);
return -log(dum) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

288 Chapter 7. Random Numbers

uniform
deviate in F(y) = {y p(dy

<— p(y)

transformed
deviate out

Figure 7.2.1. Transformation method for generating a random deviate y from a known probability
distribution p(y). The indefinite integral of p(y) must be known and invertible. A uniform deviate x is
chosen between 0 and 1. Its corresponding y on the definite-integral curve is the desired deviate.

Let’s see what is involved in using the above transformation method to generate
some arbitrary desired distribution of y’s, say one with p(y) = f(y) for some positive
function f whose integral is 1. (See Figure 7.2.1.) According to (7.2.4), we need
to solve the differential equation

& — Fly) (7.2.6)

dy
But the solution of this is just x = Fy), where F(y) is the indefinite integral of
f(y). The desired transformation which takes a uniform deviate into one distributed
as f(y) is therefore

where F~! is the inverse function to F. Whether (7.2.7) is feasible to implement
depends on whether the inverse function of the integral of f(y) is itself feasible to
compute, either analytically or numerically. Sometimes it is, and sometimes it isn’t.

Incidentally, (7.2.7) has an immediate geometric interpretation: Since F(y) is
the area under the probability curve to the left of y, (7.2.7) is just the prescription:
choose a uniform random z, then find the value y that has that fraction x of
probability area to its left, and return the value y.

Normal (Gaussian) Deviates

Transformation methods generalize to more than one dimension. If x1, x2,
are random deviates with a joint probability distribution p(x1,22,...)

dx ,dx2..., and if y1,y2,... are each functions of all the x’s (same number of
y’s as x’s), then the joint probability distribution of the y’s is
O(@1,€2,...)
P(yis ¥2,-+-)dyrdye P(@1, 22, |e yn dye (7.2.8)

where |O(_)/O(_)| is the Jacobian determinant of the «’s with respect to the y’s
(or reciprocal of the Jacobian determinant of the y’s with respect to the x’s).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.2 Transformation Method: Exponential and Normal Deviates 289

An important example of the use of (7.2.8) is the Box-Muller method for
generating random deviates with a normal (Gaussian) distribution,

1 yp,
Ply)dy = =e dy (7.29)

TT

Consider the transformation between two uniform deviates on (0,1), 71,272, and
two quantities yi, yo,

yt = V—2|n 2, cos 2722

(7.2.10)
yo = V—2Ina, sin2rx
Equivalently we can write
_ Lis 2
a1 = exp |—5(yt + y2)
(7.2.11)

1 Y
r= scaretan=

mv Y

Now the Jacobian determinant can readily be calculated (try it!):

dey
O(x1,%2) _ | dyr __ ae] l= ar) (72.12)
(yr, y2) a V2 V2

Since this is the product of a function of y2 alone and a function of y; alone, we see
that each y is independently distributed according to the normal distribution (7.2.9).

One further trick is useful in applying (7.2.10). Suppose that, instead of picking
uniform deviates x; and x2 in the unit square, we instead pick v; and v2 as the
ordinate and abscissa of a random point inside the unit circle around the origin. Then
the sum of their squares, R? = v? +v3 is a uniform deviate, which can be used for 71,
while the angle that (v, v2) defines with respect to the v1 axis can serve as the random
angle 2722. What’s the advantage? It’s that the cosine and sine in (7.2.10) can now
be written as v;/ VR? and v2/ VR? , obviating the trigonometric function calls!

We thus have

#include <math.h>

float gasdev(long *idum)
Returns a normally distributed deviate with zero mean and unit variance, using rani (idum)
as the source of uniform deviates.
{
float rani(long *idum) ;
static int iset=0;
static float gset;
float fac,rsq,v1,v2;

if (*idum < 0) iset=0; Reinitialize.
if (iset == 0) { We don't have an extra deviate handy, so

-O*ran1(idum)-1.0; pick two uniform numbers in the square ex-
.O*rani(idum)-1.0; tending from -1 to +1 in each direction,
rsq=vil+*vit+v2+v2; see if they are in the unit circle,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

290 Chapter 7. Random Numbers

} while (rsq >= 1.0 || rsq == 0.0); and if they are not, try again.
fac=sqrt (-2.0*log(rsq) /rsq) ;

Now make the Box-Muller transformation to get two normal deviates. Return one and
save the other for next time.

gset=vi*fac;

iset=1; Set flag.
return v2*fac;
} else { We have an extra deviate handy,
iset=0; so unset the flag,
return gset; and return it.

See Devroye [1] and Bratley [2] for many additional algorithms.

CITED REFERENCES AND FURTHER READING:

Devroye, L. 1986, Non-Uniform Random Variate Generation (New York: Springer-Verlag), $9.1.
(1)

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New York: Springer-
Verlag). [2]

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), pp. 116ff.

7.3 Rejection Method: Gamma, Poisson,
Binomial Deviates

The rejection method is a powerful, general technique for generating random
deviates whose distribution function p(x)dz (probability of a value occurring between
x and a + dx) is known and computable. The rejection method does not require
that the cumulative distribution function [indefinite integral of p(x)] be readily
computable, much less the inverse of that function — which was required for the
transformation method in the previous section.

The rejection method is based on a simple geometrical argument:

Draw a graph of the probability distribution p(«) that you wish to generate, so
that the area under the curve in any range of x corresponds to the desired probability
of generating an x in that range. If we had some way of choosing a random point in
two dimensions, with uniform probability in the area under your curve, then the x
value of that random point would have the desired distribution.

Now, on the same graph, draw any other curve f(x) which has finite (not
infinite) area and lies everywhere above your original probability distribution. (This
is always possible, because your original curve encloses only unit area, by definition
of probability.) We will call this f(x) the comparison function. Imagine now
that you have some way of choosing a random point in two dimensions that is
uniform in the area under the comparison function. Whenever that point lies outside
the area under the original probability distribution, we will reject it and choose
another random point. Whenever it lies inside the area under the original probability
distribution, we will accept it. It should be obvious that the accepted points are
uniform in the accepted area, so that their x values have the desired distribution. It

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.3 Rejection Method: Gamma, Poisson, Binomial Deviates 291

A

first random
deviate in. A Prt rrr ttrrr crn

f(x)

second random
deviate in

Xo

Figure 7.3.1. Rejection method for generating a random deviate x from a known probability distribution
p(x) that is everywhere less than some other function f(a). The transformation method is first used to
generate a random deviate «x of the distribution f (compare Figure 7.2.1). A second uniform deviate is

used to decide whether to accept or reject that a. If it is rejected, a new deviate of f is found; and so on.

The ratio of accepted to rejected points is the ratio of the area under p to the area between p and f.

should also be obvious that the fraction of points rejected just depends on the ratio
of the area of the comparison function to the area of the probability distribution
function, not on the details of shape of either function. For example, a comparison
function whose area is less than 2 will reject fewer than half the points, even if it
approximates the probability function very badly at some values of x, e.g., remains
finite in some region where p(x) is zero.

It remains only to suggest how to choose a uniform random point in two
dimensions under the comparison function f(x). A variant of the transformation
method (§7.2) does nicely: Be sure to have chosen a comparison function whose
indefinite integral is known analytically, and is also analytically invertible to give x
as a function of “area under the comparison function to the left of 2’ Now pick a
uniform deviate between 0 and A, where A is the total area under f(x), and use it
to get a corresponding x. Then pick a uniform deviate between 0 and f(z) as the y
value for the two-dimensional point. You should be able to convince yourself that the
point (x, y) is uniformly distributed in the area under the comparison function f(z).

An equivalent procedure is to pick the second uniform deviate between zero
and one, and accept or reject according to whether it is respectively less than or
greater than the ratio p(x)/ f(x).

So, to summarize, the rejection method for some given p(x) requires that one
find, once and for all, some reasonably good comparison function f(a). Thereafter,
each deviate generated requires two uniform random deviates, one evaluation of f (to
get the coordinate y), and one evaluation of p (to decide whether to accept or reject
the point x, y). Figure 7.3.1 illustrates the procedure. Then, of course, this procedure
must be repeated, on the average, A times before the final deviate is obtained.

Gamma Distribution

The gamma distribution of integer order a > 0 is the waiting time to the ath
event in a Poisson random process of unit mean. For example, when a = 1, it is just
the exponential distribution of $7.2, the waiting time to the first event.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

292 Chapter 7. Random Numbers

A gamma deviate has probability p,(2)dx of occurring with a value between
x and x + dx, where

r>0 (73.1)

To generate deviates of (7.3.1) for small values of a, it is best to add up a
exponentially distributed waiting times, i.e., logarithms of uniform deviates. Since
the sum of logarithms is the logarithm of the product, one really has only to generate
the product of a uniform deviates, then take the log.

For larger values of a, the distribution (7.3.1) has a typically “bell-shaped”
form, with a peak at x = a and a half-width of about \/a.

We will be interested in several probability distributions with this same qual-
itative form. A useful comparison function in such cases is derived from the
Lorentzian distribution

1/1
P(y)dy = — (<3) dy (7.3.2)

whose inverse indefinite integral is just the tangent function. It follows that the
x-coordinate of an area-uniform random point under the comparison function

co

= J 733
for any constants ag, co, and xo, can be generated by the prescription
x = ao tan(7U) + x9 (73.4)

where U is a uniform deviate between 0 and 1. Thus, for some specific “bell-shaped”
p(x) probability distribution, we need only find constants a9, co, Zo, with the product
aoco (which determines the area) as small as possible, such that (7.3.3) is everywhere
greater than p(x).

Ahrens has done this for the gamma distribution, yielding the following
algorithm (as described in Knuth [1]):

#include <math.h>

float gamdev(int ia, long *idum)
Returns a deviate distributed as a gamma distribution of integer order ia, i.e., a waiting time
to the iath event in a Poisson process of unit mean, using rani(idum) as the source of
uniform deviates.
{

float rani(long *idum) ;

void nrerror(char error_text[]);

int j;

float am,e,s,v1,v2,x,y;

if (ia < 1) nrerror("Error in routine gamdev") ;

if (ia <6) { Use direct method, adding waiting
x=1.0; times.
for (j=1;j<sia;jt++) x *= rani(idum);
x = -log(x);

} else { Use rejection method.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.3 Rejection Method: Gamma, Poisson, Binomial Deviates 293

do {
do {
do { These four lines generate the tan-
vi=rani(idum) ; gent of a random angle, i.e., they
v2=2.0*ran1(idum)-1.0; are equivalent to
} while (vitvitv2*v2 > 1.0); y = tan(m * rani(idum)).
y=v2/v1;
am=ia-1;
s=sqrt(2.0*am+1.0);
x=s*ytam; We decide whether to reject x:
} while (x <= 0.0); Reject in region of zero probability.
e=(1.0+y*y) *exp (am*log(x/am)-s*y) ; Ratio of prob. fn. to comparison fn.
} while (rani(idum) > e); Reject on basis of a second uniform
+ deviate.
return x;

Poisson Deviates

The Poisson distribution is conceptually related to the gamma distribution. It
gives the probability of a certain integer number m of unit rate Poisson random
events occurring in a given interval of time x, while the gamma distribution was the
probability of waiting time between x and x + dz to the mth event. Note that m takes
on only integer values > 0, so that the Poisson distribution, viewed as a continuous
distribution function p,(m)dm, is zero everywhere except where m is an integer
> 0. At such places, it is infinite, such that the integrated probability over a region
containing the integer is some finite number. The total probability at an integer 7 is

j+e gie-®
Prob(j) = | Px(m)dm = 5 (7.3.5)
j-e J:

At first sight this might seem an unlikely candidate distribution for the rejection
method, since no continuous comparison function can be larger than the infinitely
tall, but infinitely narrow, Dirac delta functions in p,,(m). However, there is a trick
that we can do: Spread the finite area in the spike at 7 uniformly into the interval
between j and j + 1. This defines a continuous distribution q,,(m)dm given by

[m] ,-«
qe(m)dm = ~—<_ dm (73.6)

[m]!

where [m] represents the largest integer less than m. If we now use the rejection
method to generate a (noninteger) deviate from (7.3.6), and then take the integer
part of that deviate, it will be as if drawn from the desired distribution (7.3.5). (See
Figure 7.3.2.) This trick is general for any integer-valued probability distribution.
For x large enough, the distribution (7.3.6) is qualitatively bell-shaped (albeit
with a bell made out of small, square steps), and we can use the same kind of
Lorentzian comparison function as was already used above. For small x, we can
generate independent exponential deviates (waiting times between events); when the
sum of these first exceeds x, then the number of events that would have occurred in
waiting time x becomes known and is one less than the number of terms in the sum.
These ideas produce the following routine:

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

294 Chapter 7. Random Numbers

accept

1

1
3
A
1

1
“S

Figure 7.3.2. Rejection method as applied to an integer-valued distribution. The method is performed
on the step function shown as a dashed line, yielding a real-valued deviate. This deviate is rounded down
to the next lower integer, which is output.

#include <math.h>
#define PI 3.141592654

float poidev(float xm, long *idum)
Returns as a floating-point number an integer value that is a random deviate drawn from a
Poisson distribution of mean xm, using ran1(idum) as a source of uniform random deviates.
{
float gammin(float xx);
float rani(long *idum) ;
static float sq,alxm,g,oldm=(-1.0); oldm is a flag for whether xm has changed
float em,t,y; since last call.

if (xm < 12.0) {
if (xm != oldm) {

Use direct method.

oldm=xm;
g=exp(-xm) ; If xm is new, compute the exponential.

em = -1;

t=1.0;

do { Instead of adding exponential deviates it is equiv-
++em; alent to multiply uniform deviates. We never

t *= rani(idum);
} while (t > g);

actually have to take the log, merely com-
pare to the pre-computed exponential.

} else { Use rejection method.
if (xm != oldm) { If xm has changed since the last call, then pre-
oldm=xm; compute some functions that occur below.

sq=sqrt (2.0*xm) ;

alxm=log (xm) ;

g=xm*alxm-gamm1n(xm+1.0) ;

The function gamm1n is the natural log of the gamma function, as given in §6.1.
}
do {

do { y is a deviate from a Lorentzian comparison func-

y=tan(PI*rani(idum)); tion.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.3 Rejection Method: Gamma, Poisson, Binomial Deviates 295

em=sq*y+xm; em is y, shifted and scaled.
} while (em < 0.0); Reject if in regime of zero probability.
em=floor (em) ; The trick for integer-valued distributions.

t=0.9*(1.0+y*y) exp (em*alxm-gamm1n(em+1.0)-g) ;
The ratio of the desired distribution to the comparison function; we accept or
reject by comparing it to another uniform deviate. The factor 0.9 is chosen so
that t never exceeds 1.
} while (rani(idum) > t);
+

return em;

Binomial Deviates

If an event occurs with probability g, and we make n trials, then the number of
times m that it occurs has the binomial distribution,

[palo _ (") gg" (73.7)

j-e

The binomial distribution is integer valued, with m taking on possible values
from 0 to n. It depends on two parameters, n and gq, so is correspondingly a
bit harder to implement than our previous examples. Nevertheless, the techniques
already illustrated are sufficiently powerful to do the job:

#include <math.h>
#define PI 3.141592654

float bnidev(float pp, int n, long *idum)
Returns as a floating-point number an integer value that is a random deviate drawn from
a binomial distribution of n trials each of probability pp, using ran1(idum) as a source of
uniform random deviates.
{

float gammin(float xx);

float rani(long *idum) ;

int j;

static int nold=(-1);

float am,em,g,angle,p,bnl,sq,t,y;

static float pold=(-1.0),pc,plog,pclog,en,oldg;

p=(pp <= 0.5 ? pp: 1.0-pp);
The binomial distribution is invariant under changing pp to 1-pp, if we also change the
answer to n minus itself; we'll remember to do this below.

am=n*p; This is the mean of the deviate to be produced.
if (m < 25) { Use the direct method while n is not too large.
bn1=0.0; This can require up to 25 calls to rani.

for (j=1;j<=n; j++)
if (rani(idum) < p) ++bnl;

} else if (am << 1.0) f If fewer than one event is expected out of 25
g=exp(-am) ; or more trials, then the distribution is quite
t=1.0; accurately Poisson. Use direct Poisson method.

for (j=0;j<=n;jt+) {
t *= rani(idum);
if (t < g) break;
}
bnl=(j <= n? j: n);
} else { Use the rejection method.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

296 Chapter 7. Random Numbers

if (m != nold) { If n has changed, then compute useful quanti-

en=n; ties.
oldg=gammln(ent+1.0) ;
nold=n;
} if (p != pold) { If p has changed, then compute useful quanti-

pce=1.0-p; ties.
plog=log(p) ;
pclog=log(pc) ;

pold=p;
sq=sqrt (2.0*am*pc) ; The following code should by now seem familiar:
do { rejection method with a Lorentzian compar-
do { ison function.

angle=PI*ran1(idum) ;

y=tan (angle) ;

em=sq*y+am;
} while (em < 0.0 || em >= (en+1.0)); Reject.
em=floor (em) ; Trick for integer-valued distribution.
t=1.2*sq*(1.0+y*y) *exp(oldg-gamm1n(em+1.0)

-gamm1n(en-em+1.0)+em*plog+(en-em) *pclog) ;

} while (rani(idum) > t); Reject. This happens about 1.5 times per devi-
bnl=em; ate, on average.
+
if (p != pp) bni=n-bn1; Remember to undo the symmetry transforma-
return bnl; tion.

See Devroye [2] and Bratley [3] for many additional algorithms.

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), pp. 120ff. [1]

Devroye, L. 1986, Non-Uniform Random Variate Generation (New York: Springer-Verlag), §X.4.
[2]

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New York: Springer-
Verlag). [3].

7.4 Generation of Random Bits

The C language gives you useful access to some machine-level bitwise operations
such as << (left shift). This section will show you how to put such abilities to good use.

The problem is how to generate single random bits, with 0 and 1 equally
probable. Of course you can just generate uniform random deviates between zero
and one and use their high-order bit (i.e., test if they are greater than or less than
0.5). However this takes a lot of arithmetic; there are special-purpose applications,
such as real-time signal processing, where you want to generate bits very much
faster than that.

One method for generating random bits, with two variant implementations, is
based on “primitive polynomials modulo 2.” The theory of these polynomials is
beyond our scope (although §7.7 and §20.3 will give you small tastes of it). Here,

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.4 Generation of Random Bits 297

suffice it to say that there are special polynomials among those whose coefficients
are zero or one. An example is

e®4e>4+a°%+a'42° (7.4.1)
which we can abbreviate by just writing the nonzero powers of x, e.g.,
(18,5, 2, 1,0)

Every primitive polynomial modulo 2 of order n (=18 above) defines a recurrence
relation for obtaining a new random bit from the n preceding ones. The recurrence
relation is guaranteed to produce a sequence of maximal length, i.e., cycle through
all possible sequences of n bits (except all zeros) before it repeats. Therefore one
can seed the sequence with any initial bit pattern (except all zeros), and get 2” — 1
random bits before the sequence repeats.

Let the bits be numbered from | (most recently generated) through n (generated
n steps ago), and denoted a1, a2,...,@,. We want to give a formula for a new bit
ao. After generating ag we will shift all the bits by one, so that the old a,, is finally
lost, and the new ap becomes a. We then apply the formula again, and so on.

“Method I” is the easiest to implement in hardware, requiring only a single
shift register n bits long and a few XOR (“exclusive or” or bit addition mod 2)
gates, the operation denoted in C by “”’. For the primitive polynomial given above,
the recurrence formula is

do = 41g \ as Nag A a4 (7.4.2)

The terms that are A\’d together can be thought of as “taps” on the shift register,
/A’d into the register’s input. More generally, there is precisely one term for each
nonzero coefficient in the primitive polynomial except the constant (zero bit) term.
So the first term will always be a,, for a primitive polynomial of degree n, while the
last term might or might not be a1, depending on whether the primitive polynomial
has a term in a},

While it is simple in hardware, Method I is somewhat cumbersome in C, because
the individual bits must be collected by a sequence of full-word masks:

int irbiti(umsigned long *iseed)
Returns as an integer a random bit, based on the 18 low-significance bits in iseed (which is
modified for the next call).

{
unsigned long newbit; The accumulated XOR's.
newbit = (*iseed >> 17) & 1 Get bit 18.
- (eiseed >> 4) & 1 XOR with bit 5.
- (eiseed >> 1) & 1 XOR with bit 2.
- (*iseed & 1); XOR with bit 1.

*iseed=(*iseed << 1) | newbit;
return (int) newbit;

Leftshift the seed and put the result of the
XOR's in its bit 1.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

298 Chapter 7. Random Numbers

(b)

Figure 7.4.1. Two related methods for obtaining random bits from a shift register and a primitive
polynomial modulo 2. (a) The contents of selected taps are combined by exclusive-or (addition modulo
2), and the result is shifted in from the right. This method is easiest to implement in hardware. (b)
Selected bits are modified by exclusive-or with the leftmost bit, which is then shifted in from the right.
This method is easiest to implement in software.

“Method II” is less suited to direct hardware implementation (though still
possible), but is beautifully suited to C. It modifies more than one bit among the
saved n bits as each new bit is generated (Figure 7.4.1). It generates the maximal
length sequence, but not in the same order as Method I. The prescription for the
primitive polynomial (7.4.1) is:

ao = G18

a5 = a5 \ ao

(7.43)

ag = a2 A ao

ay =a, A ao

In general there will be an exclusive-or for each nonzero term in the primitive
polynomial except 0 and n. The nice feature about Method II is that all the
exclusive-or’s can usually be done as a single full-word exclusive-or operation:

#define IB1 1 Powers of 2.
#define IB2 2

#define IB5 16

#define IB18 131072

#tdefine MASK (IB1+IB2+IB5)

int irbit2(umsigned long *iseed)
Returns as an integer a random bit, based on the 18 low-significance bits in iseed (which is
modified for the next call).

{
if (*iseed & IB18) { Change all masked bits, shift, and put 1 into bit 1.
*iseed=((*iseed ~ MASK) << 1) | IB1;
return 1;

} else { Shift and put 0 into bit 1.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

299

7.4 Generation of Random Bits

*iseed <<= 1;
return 0;

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permissi is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

Some Primitive Polynomials Modulo 2 (after Watson)

S S S S a S S SS

a = a = — 4 a av saa A

S SHSSSSK SS SSEMSSSSSENSSSSSSSN YSEWSSSKWSSEWS Saw NS
SF S¥sedN oF Se AAW AANA AWN NNN CNN SH
SOAS AHD SAK S MGA SON oH HMI oOK A Nan COW OU COT
CACSSKN UMS SIM SHY HY OWN SWS YK SUS HN td der SSK SNS SK SKK
SoS S SSF Ss SoS ¢

S SSS S SE S SF S WNEa ANS SF NSSSN WS

ot nn. os“ 4 S caw $d BE Maw
SSSSSS MOSS T AIS SASNASSSSEM SAN SSF SUF CSW FST SF FMM SW
Sota GE TINE FG SAGA AAAS ACU AAC AMC NACKTE CF OUNS

A word of caution is: Don’t use sequential bits from these routines as the bits

of a large, supposedly random, integer, or as the bits in the mantissa of a supposedly

300 Chapter 7. Random Numbers

random floating-point number. They are not very random for that purpose; see
Knuth [1]. Examples of acceptable uses of these random bits are: (i) multiplying a
signal randomly by +1 at a rapid “chip rate,” so as to spread its spectrum uniformly
(but recoverably) across some desired bandpass, or (ii) Monte Carlo exploration
of a binary tree, where decisions as to whether to branch left or right are to be
made randomly.

Now we do not want you to go through life thinking that there is something
special about the primitive polynomial of degree 18 used in the above examples.
(We chose 18 because 2! is small enough for you to verify our claims directly by
numerical experiment.) The accompanying table [2] lists one primitive polynomial
for each degree up to 100. (In fact there exist many such for each degree. For
example, see §7.7 for a complete table up to degree 10.)

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), pp. 29ff. [1]

Horowitz, P., and Hill, W. 1989, The Art of Electronics, 2nd ed. (Cambridge: Cambridge University
Press), §§9.32-9.37.

Tausworthe, R.C. 1965, Mathematics of Computation, vol. 19, pp. 201-209.
Watson, E.J. 1962, Mathematics of Computation, vol. 16, pp. 368-369. [2]

7.5 Random Sequences Based on Data
Encryption

In Numerical Recipes’ first edition, we described how to use the Data Encryption Standard
(DES) [1-3] for the generation of random numbers. Unfortunately, when implemented in
software in a high-level language like C, DES is very slow, so excruciatingly slow, in fact, that
our previous implementation can be viewed as more mischievous than useful. Here we give
a much faster and simpler algorithm which, though it may not be secure in the cryptographic
sense, generates about equally good random numbers.

DES, like its progenitor cryptographic system LUCIFER, is a so-called “block product
cipher” [4]. It acts on 64 bits of input by iteratively applying (16 times, in fact) a kind of highly
nonlinear bit-mixing function. Figure 7.5.1 shows the flow of information in DES during
this mixing. The function g, which takes 32-bits into 32-bits, is called the “cipher function.”
Meyer and Matyas [4] discuss the importance of the cipher function being nonlinear, as well
as other design criteria.

DES constructs its cipher function g from an intricate set of bit permutations and table
lookups acting on short sequences of consecutive bits. Apparently, this function was chosen
to be particularly strong cryptographically (or conceivably as some critics contend, to have
an exquisitely subtle cryptographic flaw!). For our purposes, a different function g that can
be rapidly computed in a high-level computer language is preferable. Such a function may
weaken the algorithm cryptographically. Our purposes are not, however, cryptographic: We
want to find the fastest g, and smallest number of iterations of the mixing procedure in Figure
7.5.1, such that our output random sequence passes the standard tests that are customarily
applied to random number generators. The resulting algorithm will not be DES, but rather a
kind of “pseudo-DES,” better suited to the purpose at hand.

Following the criterion, mentioned above, that g should be nonlinear, we must give the
integer multiply operation a prominent place in g. Because 64-bit registers are not generally
accessible in high-level languages, we must confine ourselves to multiplying 16-bit operands

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.5 Random Sequences Based on Data Encryption 301

left 32-bit word right 32-bit word

32-bit XOR

left 32-bit word

right 32-bit word

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

32-bit XOR

left 32-bit word right 32-bit word

Figure 7.5.1. The Data Encryption Standard (DES) iterates a nonlinear function g on two 32-bit words,
in the manner shown here (after Meyer and Matyas[4]).

into a 32-bit result. So, the general idea of g, almost forced, is to calculate the three
distinct 32-bit products of the high and low 16-bit input half-words, and then to combine
these, and perhaps additional fixed constants, by fast operations (e.g., add or exclusive-or)
into a single 32-bit result.

There are only a limited number of ways of effecting this general scheme, allowing
systematic exploration of the alternatives. Experimentation, and tests of the randomness of
the output, lead to the sequence of operations shown in Figure 7.5.2. The few new elements
in the figure need explanation: The values C, and C2 are fixed constants, chosen randomly
with the constraint that they have exactly 16 1-bits and 16 0-bits; combining these constants
via exclusive-or ensures that the overall g has no bias towards 0 or | bits.

The “reverse half-words” operation in Figure 7.5.2 turns out to be essential; otherwise,
the very lowest and very highest bits are not properly mixed by the three multiplications.
The nonobvious choices in g are therefore: where along the vertical “pipeline” to do the
reverse; in what order to combine the three products and C2; and with which operation (add
or exclusive-or) should each combining be done? We tested these choices exhaustively before
settling on the algorithm shown in the figure.

It remains to determine the smallest number of iterations Nj, that we can get away with.
The minimum meaningful Nit is evidently two, since a single iteration simply moves one
32-bit word without altering it. One can use the constants C, and C2 to help determine an
appropriate Nz: When Nj, = 2 and C; = C2 = 0 (an intentionally very poor choice), the
generator fails several tests of randomness by easily measurable, though not overwhelming,
amounts. When N;; = 4, on the other hand, or with Nj; = 2 but with the constants
C,, C2 nonsparse, we have been unable to find any statistical deviation from randomness in
sequences of up to 10° floating numbers 7; derived from this scheme. The combined strength
of Nig = 4 and nonsparse C1, C'2 should therefore give sequences that are random to tests
even far beyond those that we have actually tried. These are our recommended conservative

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

302 Chapter 7. Random Numbers

reverse
half-words

Figure 7.5.2. The nonlinear function g used by the routine psdes.

parameter values, notwithstanding the fact that Niz = 2 (which is, of course, twice as fast)
has no nonrandomness discernible (by us).

Implementation of these ideas is straightforward. The following routine is not quite
strictly portable, since it assumes that unsigned long integers are 32-bits, as is the case
on most machines. However, there is no reason to believe that longer integers would be in
any way inferior (with suitable extensions of the constants Ci,C2). C does not provide a
convenient, portable way to divide a long integer into half words, so we must use a combination
of masking (& Oxffff) with left- and right-shifts by 16 bits (<<16 and >>16). On some
machines the half-word extraction could be made faster by the use of C’s union construction,
but this would generally not be portable between “big-endian” and “little-endian” machines.
(Big- and little-endian refer to the order in which the bytes are stored in a word.)

#define NITER 4

void psdes(unsigned long *lword, unsigned long *irword)
“Pseudo-DES” hashing of the 64-bit word (lword,irword). Both 32-bit arguments are re-
turned hashed on all bits.
{
unsigned long i,ia,ib,iswap,itmph=0,itmp1=0;
static unsigned long ci[NITER]={
Oxbaa96887L, Ox1e17d32cL, Ox03bcdc3cL, Ox0f33d1b2L};
static unsigned long c2[NITER]={
Ox4b0f3b58L, Oxe874f0c3L, 0x6955c5a6L, Ox55a7ca46L};

for (i=0;i<NITER;i++) {
Perform niter iterations of DES logic, using a simpler (non-cryptographic) nonlinear func-
tion instead of DES'’s.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.5 Random Sequences Based on Data Encryption 303

ia=(iswap=(*irword)) 7 ci[il];
itmpl = ia & Oxffff;
itmph = ia >> 16;
ib=itmpl*itmpl+ ~(itmph*itmph) ;
*irword=(*lword) ~ (((ia = (ib >> 16) |

((ib & Oxffff£) << 16)) ~ c2[i])+itmp1*itmph) ;
*lword=iswap;

The bit-rich constants c1 and (below)
c2 guarantee lots of nonlinear mix-
ing.

The routine ran4, listed below, uses psdes to generate uniform random deviates. We
adopt the convention that a negative value of the argument idum sets the left 32-bit word, while
a positive value i sets the right 32-bit word, returns the ith random deviate, and increments
idum to i + 1. This is no more than a convenient way of defining many different sequences
(negative values of idum), but still with random access to each sequence (positive values of
idum). For getting a floating-point number from the 32-bit integer, we like to do it by the
masking trick described at the end of §7.1, above. The hex constants 3F800000 and 0O7FFFFF
are the appropriate ones for computers using the IEEE representation for 32-bit floating-point
numbers (e.g., IBM PCs and most UNIX workstations). For DEC VAXes, the correct hex
constants are, respectively, 00004080 and FFFFO07F. For greater portability, you can instead
construct a floating number by making the (signed) 32-bit integer nonnegative (typically, you
add exactly 2°! if it is negative) and then multiplying it by a floating constant (typically 2.-*").

An interesting, and sometimes useful, feature of the routine ran4, below, is that it allows
random access to the nth random value in a sequence, without the necessity of first generating
values 1 ---n—1. This property is shared by any random number generator based on hashing

(the technique of mapping data keys, which may be highly clustered in value, approximately

uniformly into a storage address space) [5,6
some certain rare situation becomes recogni:

. One might have a simulation problem in which
zable by its consequences only considerably after

it has occurred. One may wish to restart the simulation back at that occurrence, using identical

random values but, say, varying some other

control parameters. The relevant question might

then be something like “what random numl

ers were used in cycle number 337098901?” It

might already be cycle number 395100273 before the question comes up. Random generators
based on recursion, rather than hashing, cannot easily answer such a question.

float ran4(long *idum)

Returns a uniform random deviate in the range 0.0 to 1.0, generated by pseudo-DES (DES-
like) hashing of the 64-bit word (idums, idum), where idums was set by a previous call with
negative idum. Also increments idum. Routine can be used to generate a random sequence
by successive calls, leaving idum unaltered between calls; or it can randomly access the nth
deviate in a sequence by calling with idum = n. Different sequences are initialized by calls with
differing negative values of idum.

void psdes(unsigned long *lword, unsigned long *irword);

unsigned long irword,itemp,lword;

static long idums = 0;

The hexadecimal constants jflone and jf1msk below are used to produce a floating number
between 1. and 2. by bitwise masking. They are machine-dependent. See text.

#if defined(vax) || defined(_vax_) || defined(__vax__) || defined(VAX)
static unsigned long jflone = 0x00004080;
static unsigned long jflmsk = Oxffff007f;
#else
static unsigned long jflone = 0x3f800000;
static unsigned long jflmsk = Ox007fffff;

#endif

if (*idum < 0) {
idums = -(*idum);
*idum=1;

Reset idums and prepare to return the first
deviate in its sequence.

+

irword=(*idum) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

304 Chapter 7. Random Numbers

psdes (&lword, &irword) ; “Pseudo-DES” encode the words.
itemp=jflone | (jflmsk & irword); Mask to a floating number between 1 and
++(*idum) ; 2.

return (*(float *)&itemp)-1.0; Subtraction moves range to 0. to 1.

The accompanying table gives data for verifying that ran4 and psdes work correctly
on your machine. We do not advise the use of ran4 unless you are able to reproduce the
hex values shown. Typically, ran4 is about 4 times slower than ranO (87.1), or about 3
times slower than ran1.

Values for Verifying the Implementation of psdes
idum before psdes call after psdes call (hex) ran4(idum)
lword irword lword irword VAX PC
-1 1 1 604D1DCE 509C0C23 0.275898 0.219120
99 1 99 D97F8571 A66CB41A 0.208204 0.849246
-99 99 1 7822309D 64300984 0.034307 0.375290
99 99 99 D7F376F0O 59BA89EB 0.838676 0.457334
Successive calls to ran4 with arguments —1, 99, —99, and 99 should produce exactly the
lword and irword values shown. Masking conversion to a returned floating random value
is allowed to be machine dependent; values for VAX and PC are shown.

CITED REFERENCES AND FURTHER READING:

Data Encryption Standard, 1977 January 15, Federal Information Processing Standards Publi-
cation, number 46 (Washington: U.S. Department of Commerce, National Bureau of Stan-
dards). [1]

Guidelines for Implementing and Using the NBS Data Encryption Standard, 1981 April 1, Federal
Information Processing Standards Publication, number 74 (Washington: U.S. Department
of Commerce, National Bureau of Standards). [2]

Validating the Correctness of Hardware Implementations of the NBS Data Encryption Stan-
dard, 1980, NBS Special Publication 500-20 (Washington: U.S. Department of Commerce,
National Bureau of Standards). [3]

Meyer, C.H. and Matyas, S.M. 1982, Cryptography: A New Dimension in Computer Data Security
(New York: Wiley). [4]

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), Chapter 6. [5]

Vitter, J.S., and Chen, W-C. 1987, Design and Analysis of Coalesced Hashing (New York: Oxford
University Press). [6]

7.6 Simple Monte Carlo Integration

Inspirations for numerical methods can spring from unlikely sources. “Splines”
first were flexible strips of wood used by draftsmen. “Simulated annealing” (we
shall see in §10.9) is rooted in a thermodynamic analogy. And who does not feel at
least a faint echo of glamor in the name “Monte Carlo method”?

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.6 Simple Monte Carlo Integration 305

Suppose that we pick N random points, uniformly distributed in a multidimen-
sional volume V. Call them x71,...,2,. Then the basic theorem of Monte Carlo
integration estimates the integral of a function f over the multidimensional volume,

[ravevan +V (P)— tA" (7.6.1)

Here the angle brackets denote taking the arithmetic mean over the NV sample points,

(f)

1 N 1 N
wofe) (MRL Pe (762)

The “plus-or-minus” term in (7.6.1) is a one standard deviation error estimate for
the integral, not a rigorous bound; further, there is no guarantee that the error is
distributed as a Gaussian, so the error term should be taken only as a rough indication
of probable error.

Suppose that you want to integrate a function g over a region W that is not
easy to sample randomly. For example, W might have a very complicated shape.
No problem. Just find a region V that includes W and that can easily be sampled
(Figure 7.6.1), and then define f to be equal to g for points in W and equal to zero
for points outside of W (but still inside the sampled V). You want to try to make
V enclose W as closely as possible, because the zero values of f will increase the
error estimate term of (7.6.1). And well they should: points chosen outside of W
have no information content, so the effective value of N, the number of points, is
reduced. The error estimate in (7.6.1) takes this into account.

General purpose routines for Monte Carlo integration are quite complicated
(see §7.8), but a worked example will show the underlying simplicity of the method.
Suppose that we want to find the weight and the position of the center of mass of an
object of complicated shape, namely the intersection of a torus with the edge of a
large box. In particular let the object be defined by the three simultaneous conditions

24 (verry - 3). <1 (763)

(torus centered on the origin with major radius = 4, minor radius = 2)
c>1 y>-3 (7.6.4)

(two faces of the box, see Figure 7.6.2). Suppose for the moment that the object
has a constant density p.

We want to estimate the following integrals over the interior of the complicated
object:

Je dx dy dz |v dx dy dz [ve aedya: [+ dx dy dz

(7.6.5)
The coordinates of the center of mass will be the ratio of the latter three integrals
(linear moments) to the first one (the weight).
In the following fragment, the region V,, enclosing the piece-of-torus W, is the
rectangular box extending from | to 4 in x, —3 to 4 in y, and —1 to 1 in z.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

306 Chapter 7. Random Numbers

alqepeor

Figure 7.6.1. Monte Carlo integration. Random points are chosen within the area A. The integral of the
function f is estimated as the area of A multiplied by the fraction of random points that fall below the
curve f. Refinements on this procedure can improve the accuracy of the method; see text.

LUE PUAS JO ‘(KJUO BOVAWY YON) EZP2-ZZ8-008-|. 129 10 WOOJU-MMMy/: diay

‘SIIOUD 10 Syoog sadioay jeoeLUNN Jepso Of “payiqiyoud Ajjouys si ‘ueyndwod JeAses Aue 0} (auO si) Bulpnjoul) si

-eulyoew jo BulAdoo Aue Jo ‘uojonpoidas sayyin4 “asn seuossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}U! 10} payues6

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

s
2
3
g
S
e
Gq
G
o
<
®
3
Ey
3
g
2
(es)
2
g
a
eS
=
@.
a
o
Z
é
a
>
>
3
g
=
&

aysqem is

Figure 7.6.2. Example of Monte Carlo integration (see text). The region of interest is a piece of a torus,
bounded by the intersection of two planes. The limits of integration of the region cannot easily be written
in analytically closed form, so Monte Carlo is a useful technique.

7.6 Simple Monte Carlo Integration 307

#include "nrutil.h"

n=... Set to the number of sample points desired.
den=... Set to the constant value of the density.
SW=swx=swy=swz=0.0; Zero the various sums to be accumulated.
varw=varx=vary=varz=0.0;
vol=3.0*7.0*2.0; Volume of the sampled region.
for(j=1;j<=n;jt+) {
x=1.0+3.0*ran2(&idum) ; Pick a point randomly in the sampled re-
=(-3.0)+7.0*ran2(&idum) ; gion.
z=(-1.0)+2.0*ran2(kidum) ;
if (z*z+SQR(sqrt (x*xty*y)-3.0) < 1.0) { Is it in the torus?
sw += den; If so, add to the various cumulants.
swx += x*den;
swy += y*den;
swz += z*den;
varw += SQR(den);
varx += SQR(x*den) ;
vary += SQR(y*den) ;
varz += SQR(z*den) ;
}
+
w=vol*sw/n; The values of the integrals (7.6.5),

x=vol*swx/n;

y=vol*swy/n;

z=vol*swz/n;

dw=vol*sqrt ((varw/n-SQR(sw/n))/n) ; and their corresponding error estimates.
dx=vol*sqrt ((varx/n-SQR(swx/n))/n) ;

dy=vol*sqrt ((vary/n-SQR(swy/n))/n) ;

dz=vol*sqrt ((varz/n-SQR(swz/n) )/n) ;

A change of variable can often be extremely worthwhile in Monte Carlo
integration. Suppose, for example, that we want to evaluate the same integrals,
but for a piece-of-torus whose density is a strong function of z, in fact varying
according to

(7.6.6)
One way to do this is to put the statement
den=exp(5.0*z) ;

inside the if (...) block, just before den is first used. This will work, but it is
a poor way to proceed. Since (7.6.6) falls so rapidly to zero as z decreases (down
to its lower limit —1), most sampled points contribute almost nothing to the sum
of the weight or moments. These points are effectively wasted, almost as badly as
those that fall outside of the region W. A change of variable, exactly as in the
transformation methods of §7.2, solves this problem. Let

ds = e**dz so that s= 18 » 2==lIn(5s) (7.6.7)

Then pdz = ds, and the limits —1 < z < 1 become .00135 < s < 29.682. The
program fragment now looks like this

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

308 Chapter 7. Random Numbers

#include "nrutil.h"
n=... Set to the number of sample points desired.
sw=swx=swy=swz=0 .0;

varw=varx=vary=varz=0.0;

ss=0.2*(exp(5.0)-exp(-5.0)) Interval of s to be random sampled.
vol=3.0*7.0*ss Volume in x,y,s-space.
for(j=1;j<=n;jt+) {
.0+3.0*ran2(&idum) ;
-3.0)+7.0*ran2(&idum) ;
s=0.00135+ss*ran2(&idum) ;

Pick a point in s.

z=0.2*1log(5.0*s) ; Equation (7.6.7).
if (z*z+SQR(sqrt (x*xty*y)-3.0) < 1.0) {
sw += 1.0; Density is 1, since absorbed into definition
swx += of s.
swy
swz +=
varw
varx
vary
varz
}
+
w=vol*sw/n; The values of the integrals (7.6.5),
x=vol*swx/n;
ol*swy/n;
z=vol*swz/n;
dw=vol*sqrt ((varw/n-SQR(sw/n))/n) ; and their corresponding error estimates.

dx=vol*sqrt ((varx/n-SQR(swx/n))/n) ;
dy=vol*sqrt ((vary/n-SQR(swy/n))/n) ;
dz=vol*sqrt ((varz/n-SQR(swz/n))/n) ;

If you think for a minute, you will realize that equation (7.6.7) was useful only
because the part of the integrand that we wanted to eliminate (e °*) was both integrable
analytically, and had an integral that could be analytically inverted. (Compare §7.2.)
In general these properties will not hold. Question: What then? Answer: Pull out
of the integrand the “best” factor that can be integrated and inverted. The criterion
for “best” is to try to reduce the remaining integrand to a function that is as close
as possible to constant.

The limiting case is instructive: If you manage to make the integrand f exactly
constant, and if the region V, of known volume, exactly encloses the desired region
W,, then the average of f that you compute will be exactly its constant value, and the
error estimate in equation (7.6.1) will exactly vanish. You will, in fact, have done
the integral exactly, and the Monte Carlo numerical evaluations are superfluous. So,
backing off from the extreme limiting case, to the extent that you are able to make f
approximately constant by change of variable, and to the extent that you can sample a
region only slightly larger than W, you will increase the accuracy of the Monte Carlo
integral. This technique is generically called reduction of variance in the literature.

The fundamental disadvantage of simple Monte Carlo integration is that its
accuracy increases only as the square root of N, the number of sampled points. If
your accuracy requirements are modest, or if your computer budget is large, then
the technique is highly recommended as one of great generality. In the next two
sections we will see that there are techniques available for “breaking the square root
of N barrier” and achieving, at least in some cases, higher accuracy with fewer
function evaluations.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.7 Quasi- (that is, Sub-) Random Sequences 309

CITED REFERENCES AND FURTHER READING:

Hammersley, J.M., and Handscomb, D.C. 1964, Monte Carlo Methods (London: Methuen).
Shreider, Yu. A. (ed.) 1966, The Monte Carlo Method (Oxford: Pergamon).

Sobol’, I.M. 1974, The Monte Carlo Method (Chicago: University of Chicago Press).
Kalos, M.H., and Whitlock, P.A. 1986, Monte Carlo Methods (New York: Wiley).

7.7 Quasi- (that is, Sub-) Random Sequences

We have just seen that choosing N points uniformly randomly in an n-
dimensional space leads to an error term in Monte Carlo integration that decreases
as 1/V/N. In essence, each new point sampled adds linearly to an accumulated sum
that will become the function average, and also linearly to an accumulated sum of
squares that will become the variance (equation 7.6.2). The estimated error comes
from the square root of this variance, hence the power N ~!/?,

Just because this square root convergence is familiar does not, however, mean
that it is inevitable. A simple counterexample is to choose sample points that lie
on a Cartesian grid, and to sample each grid point exactly once (in whatever order).
The Monte Carlo method thus becomes a deterministic quadrature scheme — albeit
a simple one — whose fractional error decreases at least as fast as N ~! (even faster
if the function goes to zero smoothly at the boundaries of the sampled region, or
is periodic in the region).

The trouble with a grid is that one has to decide in advance how fine it should
be. One is then committed to completing all of its sample points. With a grid, it is
not convenient to “sample until” some convergence or termination criterion is met.
One might ask if there is not some intermediate scheme, some way to pick sample
points “at random,” yet spread out in some self-avoiding way, avoiding the chance
clustering that occurs with uniformly random points.

A similar question arises for tasks other than Monte Carlo integration. We might
want to search an n-dimensional space for a point where some (locally computable)
condition holds. Of course, for the task to be computationally meaningful, there
had better be continuity, so that the desired condition will hold in some finite n-
dimensional neighborhood. We may not know a priori how large that neighborhood
is, however. We want to “sample until” the desired point is found, moving smoothly
to finer scales with increasing samples. Is there any way to do this that is better
than uncorrelated, random samples?

The answer to the above question is “yes.” Sequences of n-tuples that fill
n-space more uniformly than uncorrelated random points are called quasi-random
sequences. That term is somewhat of a misnomer, since there is nothing “random”
about quasi-random sequences: They are cleverly crafted to be, in fact, sub-random.
The sample points in a quasi-random sequence are, in a precise sense, “maximally
avoiding” of each other.

A conceptually simple example is Halton’s sequence [1]. In one dimension, the
jth number H; in the sequence is obtained by the following steps: (i) Write j as a
number in base b, where b is some prime. (For example 7 = 17 in base b = 3 is 122.)
(ii) Reverse the digits and put a radix point (i.e., a decimal point base b) in front of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

310 Chapter 7. Random Numbers

a
ate oe ae! 4
OL, a ' fie | : it . J Peete et eat
0 2 A 6 8 0 2 A 6 8
points 1 to 128 points 129 to 512
if
0 2 A 6 8 1

points 513 to 1024 points 1 to 1024

Figure 7.7.1. First 1024 points of a two-dimensional Sobol’ sequence. The sequence is generated
number-theoretically, rather than randomly, so successive points at any stage “know” how to fill in the
gaps in the previously generated distribution.

the sequence. (In the example, we get 0.221 base 3.) The result is H;. To get a
sequence of n-tuples in n-space, you make each component a Halton sequence with
a different prime base b. Typically, the first n primes are used.

It is not hard to see how Halton’s sequence works: Every time the number of
digits in j increases by one place, 7’s digit-reversed fraction becomes a factor of
b finer-meshed. Thus the process is one of filling in all the points on a sequence
of finer and finer Cartesian grids — and in a kind of maximally spread-out order
on each grid (since, e.g., the most rapidly changing digit in 7 controls the most
significant digit of the fraction).

Other ways of generating quasi-random sequences have been suggested by
Faure, Sobol’, Niederreiter, and others. Bratley and Fox [2] provide a good review
and references, and discuss a particularly efficient variant of the Sobol’ [3] sequence
suggested by Antonov and Saleev [4]. It is this Antonov-Saleev variant whose
implementation we now discuss.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

7.7 Quasi- (that is, Sub-) Random Sequences 311

Degree Primitive Polynomials Modulo 2*

1 0 (ie., 2 + 1)

2 |1l Ge, a? +e+4+1

3 | 1,2 (ie, 28 + a+ 1 and a? + 2? +1)

4 |1,4(0¢,a4+a+41 and ct +2341)

5 2,4, 7, 11, 13, 14

6 1, 13, 16, 19, 22, 25

7 1,4, 7, 8, 14, 19, 21, 28, 31, 32, 37, 41, 42, 50, 55, 56, 59, 62

8 14, 21, 22, 38, 47, 49, 50, 52, 56, 67, 70, 84, 97, 103, 115, 122

9 8, 13, 16, 22, 25, 44, 47, 52, 55, 59, 62, 67, 74, 81, 82, 87, 91, 94, 103, 104, 109, 122,
124, 137, 138, 143, 145, 152, 157, 167, 173, 176, 181, 182, 185, 191, 194, 199, 218, 220)
227, 229, 230, 234, 236, 241, 244, 253

10 4, 13, 19, 22, 50, 55, 64, 69, 98, 107, 115, 121, 127, 134, 140, 145, 152, 158, 161, 171]
181, 194, 199, 203, 208, 227, 242, 251, 253, 265, 266, 274, 283, 289, 295, 301, 316,
319, 324, 346, 352, 361, 367, 382, 395, 398, 400, 412, 419, 422, 426, 428, 433, 446,
454, 457, 472, 493, 505, 508

*Expressed as a decimal integer representing the interior bits (that is, omitting the
high-order bit and the unit bit).

The Sobol’ sequence generates numbers between zero and one directly as binary fractions
of length w bits, from a set of w special binary fractions, Vi, i = 1, 2,..., w, called direction
numbers. In Sobol’s original method, the jth number X; is generated by XORing (bitwise
exclusive or) together the set of V;’s satisfying the criterion on i, “the ith bit of j is nonzero.”
As j increments, in other words, different ones of the V;’s flash in and out of X; on different
time scales. V alternates between being present and absent most quickly, while Vi. goes from
present to absent (or vice versa) only every 2*~? steps.

Antonov and Saleev’s contribution was to show that instead of using the bits of the
integer j to select direction numbers, one could just as well use the bits of the Gray code of
j, G(j). (For a quick review of Gray codes, look at §20.2.)

Now G‘(j) and G(j + 1) differ in exactly one bit position, namely in the position of the
rightmost zero bit in the binary representation of j (adding a leading zero to 7 if necessary). A
consequence is that the 7 + 1st Sobol’-Antonov-Saleev number can be obtained from the jth
by XORing it with a single V;, namely with i the position of the rightmost zero bit in 7. This
makes the calculation of the sequence very efficient, as we shall see.

Figure 7.7.1 plots the first 1024 points generated by a two-dimensional Sobol’ sequence.
One sees that successive points do “know” about the gaps left previously, and keep filling
them in, hierarchically.

We have deferred to this point a discussion of how the direction numbers V; are generated.
Some nontrivial mathematics is involved in that, so we will content ourself with a cookbook
summary only: Each different Sobol’ sequence (or component of an n-dimensional sequence)
is based on a different primitive polynomial over the integers modulo 2, that is, a polynomial
whose coefficients are either 0 or 1, and which generates a maximal length shift register
sequence. (Primitive polynomials modulo 2 were used in §7.4, and are further discussed in
§20.3.) Suppose P is such a polynomial, of degree q,

P=2x%+a,a7' + ae? ? +--+ ag-1a41 (7.7.1)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

312 Chapter 7. Random Numbers

Initializing Values Used in sobseq

Degree Polynomial Starting Values

1 0 1 (3) (5) (15)

2 1 1 1 (7) dl

3 1 1 3 7 (5)

3 2 1 3 3 (15)

4 1 1 1 3 13

4 4 1 1 5 9
Parenthesized values are not freely specifiable, but are forced by the required recurrence
for this degree.

Define a sequence of integers Mj by the q-term recurrence relation,

M; = 2a1Mi-1 © 2?a2Mi-2 @ +» @ 29 Mi-g41dq—1 @ (24Mi-g ® Mig) (7.7.2)

Here bitwise XOR is denoted by ®. The starting values for this recurrence are that M),..., Mg

can be arbitrary odd integers less than 2,...,2%, respectively. Then, the direction numbers
Vi are given by

Vi = Mi /2' i=1,...,w (7.7.3)

The accompanying table lists all primitive polynomials modulo 2 with degree q < 10.
Since the coefficients are either 0 or 1, and since the coefficients of x? and of 1 are predictably
1, it is convenient to denote a polynomial by its middle coefficients taken as the bits of a binary
number (higher powers of x being more significant bits). The table uses this convention.

Turn now to the implementation of the Sobol’ sequence. Successive calls to the function
sobseq (after a preliminary initializing call) return successive points in an n-dimensional
Sobol’ sequence based on the first n primitive polynomials in the table. As given, the routine
is initialized for maximum n of 6 dimensions, and for a word length w of 30 bits. These
parameters can be altered by changing MAXBIT (= w) and MAXDIM, and by adding more
initializing data to the arrays ip (the primitive polynomials from the table), mdeg (their
degrees), and iv (the starting values for the recurrence, equation 7.7.2). A second table,
above, elucidates the initializing data in the routine.

#include "nrutil.h"
#define MAXBIT 30
#define MAXDIM 6

void sobseq(int *n, float x[])
When n is negative, internally initializes a set of MAXBIT direction numbers for each of MAXDIM
different Sobol’ sequences. When n is positive (but <MAXDIM), returns as the vector x[1. .n]
the next values from n of these sequences. (n must not be changed between initializations.)
{

int j,k,1;

unsigned long i,im,ipp;

static float fac;

static unsigned long in,ix[MAXDIM+1] ,*iu[MAXBIT+1] ;

static unsigned long mdeg[MAXDIM+1]={0,1,2,3,3,4,4};

static unsigned long ip[MAXDIM+1]={0,0,1,1,2,1,4};

static unsigned long iv[MAXDIM*MAXBIT+1]={

0,1,1,1,1,1,1,3,1,3,3,1,1,5,7,7,3,3,5,15,11,5,15,13,9};

if (an <0) { Initialize, don’t return a vector.
for (k=1;k<=MAXDIM;k++) ix[k]=0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.7 Quasi- (that is, Sub-) Random Sequences 313

in=0;
if (iv[1] != 1) return;
fac=1.0/(1L << MAXBIT) ;
for (j=1,k=0;j<=MAXBIT; j++,k+=MAXDIM) iulj] = @iv(k];
To allow both 1D and 2D addressing.
for (k=1;k<=MAXDIM;k++) {
for (j=1;j<=mdeg[k]; j++) iulj] [k] <<= (MAXBIT-j);
Stored values only require normalization.
for (j=mdeg[k]+1;j<=MAXBIT;j++) { Use the recurrence to get other val-
ipp=ip([k] ; ues.
i=iu[j-mdeg[k]] [k];
i “= (i >> mdeg[k]);
for (l=mdeg[k]-1;1>=1;1--) {
if (ipp & 1) i *= iulj-1] [kl];

ipp >>= 4;
}
iu(j] [k]=i;
}
+
} else { Calculate the next vector in the se-
im=int+; quence.

for (j=1;j<=MAXBIT; j++) {
if (!(im & 1)) break;
im >>= 1;

Find the rightmost zero bit.

+
if (j > MAXBIT) nrerror("MAXBIT too small in sobseq") ;
im=(j-1)*MAXDIM;

for (k=1;k<=IMIN(*n,MAXDIM) ;k++) { XOR the appropriate direction num-
ix[k] *= iv[im+k]; ber into each component of the
x[k]=ix[k] *fac; vector and convert to a floating
} number.

How good is a Sobol’ sequence, anyway? For Monte Carlo integration of a smooth
function in n dimensions, the answer is that the fractional error will decrease with N, the
number of samples, as (In V)"/N, i.e., almost as fast as 1/N. As an example, let us integrate
a function that is nonzero inside a torus (doughnut) in three-dimensional space. If the major
radius of the torus is Ro, the minor radial coordinate r is defined by

1/2
r= (((c? + y?)/? — Ro)? + ”) (7.74)
Let us try the function
mr?
f(e.y.2) = 4 1408 | oe r<ro (775)
0 r>T0

which can be integrated analytically in cylindrical coordinates, giving

[| [exauas te.u2) = 20? Ro (7.75)

With parameters Ro = 0.6, ro = 0.3, we did 100 successive Monte Carlo integrations of
equation (7.7.4), sampling uniformly in the region —1 < x,y,z < 1, for the two cases of
uncorrelated random points and the Sobol’ sequence generated by the routine sobseq. Figure
7.7.2 shows the results, plotting the r.m.s. average error of the 100 integrations as a function
of the number of points sampled. (For any single integration, the error of course wanders
from positive to negative, or vice versa, so a logarithmic plot of fractional error is not very
informative.) The thin, dashed curve corresponds to uncorrelated random points and shows
the familiar N~'/? asymptotics. The thin, solid gray curve shows the result for the Sobol’

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

314 Chapter 7. Random Numbers

01

fractional accuracy of integral

pseudo-random, hard boundary «=
pseudo-random, soft boundary
quasi-random, hard boundary ==
quasi-random, soft boundary § ——

001

100 1000 10000 105

number of points N

Figure 7.7.2. Fractional accuracy of Monte Carlo integrations as a function of number of points sampled,
for two different integrands and two different methods of choosing random points. The quasi-random
Sobol’ sequence converges much more rapidly than a conventional pseudo-random sequence. Quasi-
random sampling does better when the integrand is smooth (“soft boundary”) than when it has step
discontinuities (“hard boundary”). The curves shown are the r.m.s. average of 100 trials.

sequence. The logarithmic term in the expected (In N)?/N is readily apparent as curvature
in the curve, but the asymptotic N~! is unmistakable.

To understand the importance of Figure 7.7.2, suppose that a Monte Carlo integration of
f with 1% accuracy is desired. The Sobol’ sequence achieves this accuracy in a few thousand
samples, while pseudorandom sampling requires nearly 100,000 samples. The ratio would
be even greater for higher desired accuracies.

A different, not quite so favorable, case occurs when the function being integrated has
hard (discontinuous) boundaries inside the sampling region, for example the function that is
one inside the torus, zero outside,

1 r<ro

fley.2)= {5 rer (7.7)

where r is defined in equation (7.7.4). Not by coincidence, this function has the same analytic
integral as the function of equation (7.7.5), namely 277a?Ro.

The carefully hierarchical Sobol’ sequence is based on a set of Cartesian grids, but the
boundary of the torus has no particular relation to those grids. The result is that it is essentially
random whether sampled points in a thin layer at the surface of the torus, containing on the
order of N2/3 points, come out to be inside, or outside, the torus. The square root law, applied
to this thin layer, gives N/ fluctuations in the sum, or N~?/3 fractional error in the Monte
Carlo integral. One sees this behavior verified in Figure 7.7.2 by the thicker gray curve. The
thicker dashed curve in Figure 7.7.2 is the result of integrating the function of equation (7.7.7)
using independent random points. While the advantage of the Sobol’ sequence is not quite so
dramatic as in the case of a smooth function, it can nonetheless be a significant factor (~5)
even at modest accuracies like 1%, and greater at higher accuracies.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

7.7 Quasi- (that is, Sub-) Random Sequences 315

Note that we have not provided the routine sobseq with a means of starting the
sequence at a point other than the beginning, but this feature would be easy to add. Once
the initialization of the direction numbers iv has been done, the jth point can be obtained
directly by XORing together those direction numbers corresponding to nonzero bits in the
Gray code of j, as described above.

The Latin Hypercube

We might here give passing mention the unrelated technique of Latin square or
Latin hypercube sampling, which is useful when you must sample an N-dimensional
space exceedingly sparsely, at / points. For example, you may want to test the
crashworthiness of cars as a simultaneous function of 4 different design parameters,
but with a budget of only three expendable cars. (The issue is not whether this is a
good plan — it isn’t — but rather how to make the best of the situation!)

The idea is to partition each design parameter (dimension) into IZ segments, so
that the whole space is partitioned into MV cells. (You can choose the segments in
each dimension to be equal or unequal, according to taste.) With 4 parameters and 3
cars, for example, you end up with 3 x 3 x 3 x 3 = 81 cells.

Next, choose M cells to contain the sample points by the following algorithm:
Randomly choose one of the MM cells for the first point. Now eliminate all cells
that agree with this point on any of its parameters (that is, cross out all cells in the
same row, column, etc.), leaving (M — 1% candidates. Randomly choose one of
these, eliminate new rows and columns, and continue the process until there is only
one cell left, which then contains the final sample point.

The result of this construction is that each design parameter will have been
tested in every one of its subranges. If the response of the system under test is
dominated by one of the design parameters, that parameter will be found with this
sampling technique. On the other hand, if there is an important interaction among
different design parameters, then the Latin hypercube gives no particular advantage.
Use with care.

CITED REFERENCES AND FURTHER READING:

Halton, J.H. 1960, Numerische Mathematik, vol. 2, pp. 84-90. [1]

Bratley P., and Fox, B.L. 1988, ACM Transactions on Mathematical Software, vol. 14, pp. 88—
100. [2]

Lambert, J.P. 1988, in Numerical Mathematics — Singapore 1988, ISNM vol. 86, R.P. Agarwal,
Y.M. Chow, and S.J. Wilson, eds. (Basel: Birkhatiser), pp. 273-284.

Niederreiter, H. 1988, in Numerical Integration III, ISNM vol. 85, H. Brass and G. Hammerlin,
eds. (Basel: Birkhaiiser), pp. 157-171.

Sobol’, I.M. 1967, USSR Computational Mathematics and Mathematical Physics, vol. 7, no. 4,
pp. 86-112. [3]

Antonov, I.A., and Saleev, V.M 1979, USSR Computational Mathematics and Mathematical
Physics, vol. 19, no. 1, pp. 252-256. [4]

Dunn, O.J., and Clark, V.A. 1974, Applied Statistics: Analysis of Variance and Regression (New
York, Wiley) [discusses Latin Square].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

316 Chapter 7. Random Numbers

7.8 Adaptive and Recursive Monte Carlo
Methods

This section discusses more advanced techniques of Monte Carlo integration. As
examples of the use of these techniques, we include two rather different, fairly sophisticated,
multidimensional Monte Carlo codes: vegas [1,2], and miser [3]. The techniques that we
discuss all fall under the general rubric of reduction of variance (§7.6), but are otherwise
quite distinct.

Importance Sampling

The use of importance sampling was already implicit in equations (7.6.6) and (7.6.7).
We now return to it in a slightly more formal way. Suppose that an integrand f can be written
as the product of a function h that is almost constant times another, positive, function g. Then
its integral over a multidimensional volume V is

[tava [t/aaav = [roa (78.1)

In equation (7.6.7) we interpreted equation (7.8.1) as suggesting a change of variable to
G, the indefinite integral of g. That made gdV a perfect differential. We then proceeded
to use the basic theorem of Monte Carlo integration, equation (7.6.1). A more general
interpretation of equation (7.8.1) is that we can integrate f by instead sampling h — not,
however, with uniform probability density dV, but rather with nonuniform density gdV. In
this second interpretation, the first interpretation follows as the special case, where the means
of generating the nonuniform sampling of gdV is via the transformation method, using the
indefinite integral G (see §7.2).

More directly, one can go back and generalize the basic theorem (7.6.1) to the case
of nonuniform sampling: Suppose that points 2; are chosen within the volume V with a
probability density p satisfying

[va =1 (7.8.2)

The generalized fundamental theorem is that the integral of any function f is estimated, using
N sample points 21,...,2N, by

a [sav= [Lpav~ (2) 2 [Pie Ule* gay

where angle brackets denote arithmetic means over the N points, exactly as in equation
(7.6.2). As in equation (7.6.1), the “plus-or-minus” term is a one standard deviation error
estimate. Notice that equation (7.6.1) is in fact the special case of equation (7.8.3), with
p = constant = 1/V.

What is the best choice for the sampling density p? Intuitively, we have already seen
that the idea is to make h = f/p as close to constant as possible. We can be more rigorous
by focusing on the numerator inside the square root in equation (7.8.3), which is the variance
per sample point. Both angle brackets are themselves Monte Carlo estimators of integrals,
so we can write

We now find the optimal p subject to the constraint equation (7.8.2) by the functional variation

0 £ ([Lav-[[ sav] + [rav) (7.8.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.8 Adaptive and Recursive Monte Carlo Methods 317

with \ a Lagrange multiplier. Note that the middle term does not depend on p. The variation
(which comes inside the integrals) gives 0 = —f?/p? + 2 or

=i __is (78.6)

Pe vx Sislav
where 4 has been chosen to enforce the constraint (7.8.2).

If f has one sign in the region of integration, then we get the obvious result that the
optimal choice of p — if one can figure out a practical way of effecting the sampling — is
that it be proportional to | f|. Then the variance is reduced to zero. Not so obvious, but seen
to be true, is the fact that p o |f| is optimal even if f takes on both signs. In that case the
variance per sample point (from equations 7.8.4 and 7.8.6) is

S = Soptimal = (fine) - (fav) (78.7)

One curiosity is that one can add a constant to the integrand to make it all of one sign,
since this changes the integral by a known amount, constant x V. Then, the optimal choice
of p always gives zero variance, that is, a perfectly accurate integral! The resolution of
this seeming paradox (already mentioned at the end of §7.6) is that perfect knowledge of p
in equation (7.8.6) requires perfect knowledge of f | f|dV, which is tantamount to already
knowing the integral you are trying to compute!

If your function f takes on a known constant value in most of the volume V, it is
certainly a good idea to add a constant so as to make that value zero. Having done that, the
accuracy attainable by importance sampling depends in practice not on how small equation
(7.8.7) is, but rather on how small is equation (7.8.4) for an implementable p, likely only a
crude approximation to the ideal.

Stratified Sampling

The idea of stratified sampling is quite different from importance sampling. Let us
expand our notation slightly and let ((f)) denote the true average of the function f over
the volume V (namely the integral divided by V), while (f) denotes as before the simplest
(uniformly sampled) Monte Carlo estimator of that average:

(hepfta =x Lseo (788)

The variance of the estimator, Var((f)), which measures the square of the error of the

Monte Carlo integration, is asymptotically related to the variance of the function, Var (f) =
(f?)) — (f))?, by the relation

Var ((f)) = “"D. (789)

(compare equation 7.6.1).

Suppose we divide the volume V into two equal, disjoint subvolumes, denoted a and b,
and sample N/2 points in each subvolume. Then another estimator for ((f)), different from
equation (7.8.8), which we denote (f)’, is

(Y= 5 (Nat We) (78.10)

in other words, the mean of the sample averages in the two half-regions. The variance of
estimator (7.8.10) is given by

Var ((f)’)

i [Var ((f),,) + Var ((f),)]

_ 1 [Vara (f) , Varo (f)
~ 4| N72 N/2

= ay (Vata (f) + Vas (P)

(7.8.11)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

318 Chapter 7. Random Numbers

Here Var, (f) denotes the variance of f in subregion a, that is, (f?))2 — (f))2, and
correspondingly for b.
From the definitions already given, it is not difficult to prove the relation

Var (f) = 5 [Vara (f) + Varo (A) + 5 Ua — (Ae)? (7.8.12)

(In physics, this formula for combining second moments is the “parallel axis theorem.”)
Comparing equations (7.8.9), (7.8.11), and (7.8.12), one sees that the stratified (into two
subvolumes) sampling gives a variance that is never larger than the simple Monte Carlo case
— and smaller whenever the means of the stratified samples, ((f)), and ((f)),, are different.

We have not yet exploited the possibility of sampling the two subvolumes with different
numbers of points, say Na in subregion a and Ny, = N — Na in subregion b. Let us do so
now. Then the variance of the estimator is

_ 1 Vara (f) 4 Vary (f)

,
Var ((f)’) Z Ne NoN, (7.8.13)
which is minimized (one can easily verify) when
Na Oa
a2 = 4 7.8.14
N- oat+o ( )

Here we have adopted the shorthand notation oa = [Vara (f})]'/?, and correspondingly for b.

If Na satisfies equation (7.8.14), then equation (7.8.13) reduces to

Var ((f)") = (Ga te)" (78.15)

Equation (7.8.15) reduces to equation (7.8.9) if Var (f) = Vara (f) = Var, (f), in which case
stratifying the sample makes no difference.

A standard way to generalize the above result is to consider the volume V divided into
more than two equal subregions. One can readily obtain the result that the optimal allocation of
sample points among the regions is to have the number of points in each region j proportional
to o; (that is, the square root of the variance of the function f in that subregion). In spaces
of high dimensionality (say d & A) this is not in practice very useful, however. Dividing a
volume into kK segments along each dimension implies K* subvolumes, typically much too
large a number when one contemplates estimating all the corresponding o;’s.

Mixed Strategies

Importance sampling and stratified sampling seem, at first sight, inconsistent with each
other. The former concentrates sample points where the magnitude of the integrand | f| is
largest, that latter where the variance of f is largest. How can both be right?

The answer is that (like so much else in life) it all depends on what you know and how
well you know it. Importance sampling depends on already knowing some approximation to
your integral, so that you are able to generate random points x; with the desired probability
density p. To the extent that your p is not ideal, you are left with an error that decreases
only as NoV?, Things are particularly bad if your p is far from ideal in a region where the
integrand f is changing rapidly, since then the sampled function h = f/p will have a large
variance. Importance sampling works by smoothing the values of the sampled function h, and
is effective only to the extent that you succeed in this.

Stratified sampling, by contrast, does not necessarily require that you know anything
about f. Stratified sampling works by smoothing out the fluctuations of the number of points
in subregions, not by smoothing the values of the points. The simplest stratified strategy,
dividing V into N equal subregions and choosing one point randomly in each subregion,
already gives a method whose error decreases asymptotically as N~', much faster than
N72, (Note that quasi-random numbers, §7.7, are another way of smoothing fluctuations in
the density of points, giving nearly as good a result as the “blind” stratification strategy.)

However, “asymptotically” is an important caveat: For example, if the integrand is
negligible in all but a single subregion, then the resulting one-sample integration is all but

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.8 Adaptive and Recursive Monte Carlo Methods 319

useless. Information, even very crude, allowing importance sampling to put many points in
the active subregion would be much better than blind stratified sampling.

Stratified sampling really comes into its own if you have some way of estimating the
variances, so that you can put unequal numbers of points in different subregions, according to
(7.8.14) or its generalizations, and if you can find a way of dividing a region into a practical
number of subregions (notably not K@ with large dimension d), while yet significantly
reducing the variance of the function in each subregion compared to its variance in the full
volume. Doing this requires a lot of knowledge about f, though different knowledge from
what is required for importance sampling.

In practice, importance sampling and stratified sampling are not incompatible. In many,
if not most, cases of interest, the integrand f is small everywhere in V except for a small
fractional volume of “active regions.” In these regions the magnitude of | f| and the standard
deviation o = [Var (f)]'/? are comparable in size, so both techniques will give about the
same concentration of points. In more sophisticated implementations, it is also possible to
“nest” the two techniques, so that (e.g.) importance sampling on a crude grid is followed
by stratification within each grid cell.

Adaptive Monte Carlo: VEGAS

The VEGAS algorithm, invented by Peter Lepage [1,2], is widely used for multidimen-
sional integrals that occur in elementary particle physics. VEGAS is primarily based on
importance sampling, but it also does some stratified sampling if the dimension d is small
enough to avoid K@ explosion (specifically, if (¢/2)* < N/2, with N the number of sample
points). The basic technique for importance sampling in VEGAS is to construct, adaptively,
a multidimensional weight function g that is separable,

PX g(x,y, 2,--.) = gx(x)gy(y)gz(2)--- (7.8.16)

Such a function avoids the K* explosion in two ways: (i) It can be stored in the computer
as d separate one- dimensional functions, each defined by K tabulated values, say — so that
K x dreplaces K%. (ii) It can be sampled as a probability density by consecutively sampling
the d one-dimensional functions to obtain coordinate vector components (x, y, z,...).

The optimal separable weight function can be shown to be [1]

goto) x | fay fae. Lea) ° (78.17)

(and correspondingly for y,z,...). Notice that this reduces to g x |f| (7.8.6) in one
dimension. Equation (7.8.17) immediately suggests VEGAS’ adaptive strategy: Given a
set of g-functions (initially all constant, say), one samples the function f, accumulating not
only the overall estimator of the integral, but also the Ad estimators ( subdivisions of the
independent variable in each of d dimensions) of the right-hand side of equation (7.8.17).
These then determine improved g functions for the next iteration.

When the integrand f is concentrated in one, or at most a few, regions in d-space, then
the weight function g’s quickly become large at coordinate values that are the projections of
these regions onto the coordinate axes. The accuracy of the Monte Carlo integration is then
enormously enhanced over what simple Monte Carlo would give.

The weakness of VEGAS is the obvious one: To the extent that the projection of the
function f onto individual coordinate directions is uniform, VEGAS gives no concentration
of sample points in those dimensions. The worst case for VEGAS, e.g., is an integrand that
is concentrated close to a body diagonal line, e.g., one from (0,0,0,...) to (1,1, 1,...).
Since this geometry is completely nonseparable, VEGAS can give no advantage at all. More
generally, VEGAS may not do well when the integrand is concentrated in one-dimensional
(or higher) curved trajectories (or hypersurfaces), unless these happen to be oriented close
to the coordinate directions.

The routine vegas that follows is essentially Lepage’s standard version, minimally
modified to conform to our conventions. (We thank Lepage for permission to reproduce the
program here.) For consistency with other versions of the VEGAS algorithm in circulation,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

320 Chapter 7. Random Numbers

we have preserved original variable names. The parameter NDMX is what we have called kK,
the maximum number of increments along each axis; MXDIM is the maximum value of d; some
other parameters are explained in the comments

The vegas routine performs m = itmx statistically independent evaluations of the
desired integral, each with N = ncal1 function evaluations. While statistically independent,
these iterations do assist each other, since each one is used to refine the sampling grid for
the next one. The results of all iterations are combined into a single best answer, and its
estimated error, by the relations

m m m —1/2
fi 1 1
Tbes es 7.8.18
best > a > 3 best ( )
Also returned is the quantity
2 LA Ui = Toes)?
=— —_ 78.1
x /ms—— > 3 (7.8.19)

1

If this is significantly larger than 1, then the results of the iterations are statistically
inconsistent, and the answers are suspect.

The input flag init can be used to advantage. One might have a call with init=0,
ncal1=1000, itmx=5 immediately followed by a call with init=1,ncal1=100000, itmx=1.
The effect would be to develop a sampling grid over 5 iterations of a small number of samples,
then to do a single high accuracy integration on the optimized grid.

Note that the user-supplied integrand function, fxn, has an argument wgt in addition
to the expected evaluation point x. In most applications you ignore wgt inside the function.
Occasionally, however, you may want to integrate some additional function or functions along
with the principal function f. The integral of any such function g can be estimated by

Ty = D> wig(x) (7.8.20)

where the w;,’s and x’s are the arguments wgt and x, respectively. It is straightforward to
accumulate this sum inside your function fxn, and to pass the answer back to your main
program via global variables. Of course, g(x) had better resemble the principal function f to
some degree, since the sampling will be optimized for f.

#include <stdio.h>
#include <math.h>
#include "nrutil.h"
#define ALPH 1.5
#define NDMX 50
#define MXDIM 10
#define TINY 1.0e-30

extern long idum; For random number initialization in main.

void vegas(float regn[], int ndim, float (*fxn)(float [], float), int init,
unsigned long ncall, int itmx, int nprn, float *tgral, float *sd,
float *chi2a)
Performs Monte Carlo integration of a user-supplied ndim-dimensional function fxn over a
rectangular volume specified by regn[1..2*ndim], a vector consisting of ndim “lower left”
coordinates of the region followed by ndim “upper right” coordinates. The integration consists
of itmx iterations, each with approximately ncall calls to the function. After each iteration
the grid is refined; more than 5 or 10 iterations are rarely useful. The input flag init signals
whether this call is a new start, or a subsequent call for additional iterations (see comments
below). The input flag nprn (normally 0) controls the amount of diagnostic output. Returned
answers are tgral (the best estimate of the integral), sd (its standard deviation), and chi2a
(x? per degree of freedom, an indicator of whether consistent results are being obtained). See
text for further details.
{

float ran2(long *idum) ;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

7.8 Adaptive and Recursive Monte Carlo Methods 321

void rebin(float rc, int nd, float r{], float xin[], float xif[]);
static int i,it,j,k,mds,nd,ndo,ng,npg, ia[MXDIM+1] ,kg[MXDIM+1] ;
static float calls,dv2g,dxg,f,f2,f2b,fb,rc,ti,tsi,wgt,xjac,xn,xnd,xo;
static float d[NDMX+1] [MXDIM+1] ,di [NDMX+1] [MXDIM+1] ,dt [MXDIM+1] ,

dx (MXDIM+1], r{NDMX+1] ,x[MXDIM+1] ,xi [MXDIM+1] [NDMX+1] , xin [NDMX+1] ;
static double schi,si,swgt;
Best make everything static, allowing restarts.

if (init <= 0) { Normal entry. Enter here on a cold start.
mds=ndo=1; Change to mds=0 to disable stratified sampling,
for (j=1;j<=ndim; j++) xilj][1]=1.0; i.e., use importance sampling only.

+
if (init <= 1) si=swgt=schi=0.0;
Enter here to inherit the grid from a previous call, but not its answers.

if (init <= 2) { Enter here to inherit the previous grid and its
nd=NDMX; answers.
ng=1;
if (mds) { Set up for stratification.
ng=(int) pow(ncal1/2.0+0.25,1.0/ndim) ;
mds=1;
if ((2*ng-NDMX) >= 0) {
mds = -1;
npg=ng/NDMX+1;
nd=ng/npg;
ng=npg*nd;
3
}

for (k=1,i=1;i<=ndim;it+) k *= ng;
npg=IMAX(ncall/k,2);
calls=(float)npg * (float)k;
dxg=1.0/ng;
for (dv2g=1,i=1;i<=ndim;it+) dv2g *= dxg;
dv2g=SQR(calls*dv2g) /npg/npg/(npg-1.0) ;
xnd=nd;
dxg *= xnd;
xjac=1.0/calls;
for (j=1;j<=ndim;j++) {

dx [j]=regn[j+ndim] -regn[j];

xjac *= dx[j];

}

if (nd != ndo) { Do binning if necessary.
for ( ji<=IMAX(nd,ndo);it++) r[i]=1.0;
for ( ‘=ndim; j++) rebin(ndo/xnd,nd,r,xin,xilj]);
ndo=nd;

}

if (nprn >= 0) {
printf("%s: ndim= %3d ncall= 7%8.0f\n",

" Input parameters for vegas",ndim,calls) ;
printf("%28s it=%5d itmx=%5d\n"," ",it,itmx);
printf("%28s nprn=/3d ALPH=%5.2f\n"," ",nprn,ALPH) ;
printf("%28s mds=/3d nd=%4d\n"," ",mds,nd) ;
for (j=1;j<=ndim;j++) {

printf("%30s x1[%2d]= %11.4g xul%2d]=

"",\5,regn(j],j,regn[j+ndim]) ;

1.4g\n",

+
+
for (it=1;it<=itmx;itt++) {
Main iteration loop. Can enter here (init > 3) to do an additional itmx iterations with
all other parameters unchanged.

ti=tsi=0.0;
for (j=1;j<=ndim;j++) {
kg[j]=1;

for ( ji<=nd;i++) d[i] [j]=dilil [j]=0.0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

322 Chapter 7. Random Numbers

+
for Gi) {
fb=f2b=0.0;
for ( jk<=npg;k++) {
wgt=xjac;
for (j=1;j<=ndim;j++) {
xn=(kg[j]-ran2(&idum) ) *dxg+1.0;
ia(j]=IMAX(IMIN( (int) (xn) ,NDMX) , 1); zz 208
if (ialj] > 1) { oesgs
xo=xi(j] Cialj]]-xilj] Lialj]-11; S83 3
rc=xilj] [ialj]-1]+(xn-ialj])*xo; Zo2ss
} else { = 2358
xo=xilj] [ialj]]; 85055
re=(xn-ia[j])*xo; gs aa 8$
+ 28s es
x({jl=regn[j]+rc*dx[j] ; B28
wgt *= xo*xnd; = 28m
teSED
3 E855
fewgt*(*fxn) (x, wet) ; es3ase
£2=fHf; NocoD
fb += f; Na ge8
3 Sao
£2b += £2; Seeen
for (j=1;j<=ndim;j++) { 38 S$ Pa
dilialj]][j] += f; gsegge
if (mds >= 0) dfialjJ][j] += £2; 38 ean
+ 333 0r
eeoam
3 ZEBo>
f2b=sqrt (f2b*npg) ; Sogn
£2b=(f£2b-fb) *(f2b+£b) ; 2a 850
if (£2b <= 0.0) £2b=TINY; Ssegn
. e2582
ti += fb; as<320
tsi += £2b; een
if (mds < 0) { Use stratified sampling. ases Ee
for (j=1;j<=ndim; j++) d[ialjJ]][j] += £2b; 33285
3 SPs 2Q
=ndim:k>=1:k-- S3BSGO
for (k=ndim;k>=1;k--) { af S92
kglk] %= ng; ao8se
if (++kg[k] != 1) break; 2ERRz
3 ZR SS
ZEs
if (k < 1) break; $3585
} je528
tsi *= dv2g; Compute final results for this iteration. giaZz Z
wgt=1.0/tsi; 283 3 a
si += wgt*ti; 2S3ec
schi += wget*ti*ti; 8 oB2e
swgt += wet; 83228
*tgral=si/swgt ; eae £5
*chi2a=(schi-si*(*tgral) )/(it-0.9999) ; A 208
if (*chi2a < 0.0) *chi2a = 0 a9eg
*sd=sqrt (1.0/swgt) ; 268 5
tsi=sqrt (tsi); g z3 8
if (mprn >= 0) { See.
printf("%s %3d : integral = %14.7g +/- %9.2g\n", 229
"iteration no.",it,ti,tsi); ae3
printf("%s integral =/14.7g+/-79.2g chix*2/IT n = %9.2g\n", £38

"all iterations: ",*tgral,+*sd,*chi2a) ;
if (mprn) {
for (j=1;j<=ndim;j++) {

printf(" DATA FOR axis %2d\n",j);
printf ("%6s/13s/11s/13s/11s/%13s\n",
"X" "delta i","X","delta i","X", "delta i");
for (i=1+nprn/2;i<=nd;i += nprn+2) {
printf ("%8.5£%12.4g%12.5£%12.4g%12.5£%12.4g\n",
xi fj] [i] ,di fil (j],xi(j] [i+1],

7.8 Adaptive and Recursive Monte Carlo Methods 323

+

for

for

di (i+1] [5] ,xi lj] (4+2] ,ai [i+2] (51);

}

(j=1;j<=ndim;j++) { Refine the grid. Consult references to understand

xo=d[1] [j]; the subtlety of this procedure. The refine-
xn=d[2] [j]; ment is damped, to avoid rapid, destabiliz-
[1] [j]=(xotxn) /2.0; ing changes, and also compressed in range
at [jJ=a[1) [3]; by the exponent ALPH.
for (i=2;i<nd;it+) {

re=xo+xn ;

XO=XNn;

xn=d[it+1] [j];
ali] [j] = (rcetxn)/3.0;
dt{j] += alil (jl;
}
d[nd] [j]=(xot+xn) /2.0;
dt[j] += dnd] [jl];

(j=15 j<endim;j++) f¢

rc=0.0;

for (i=1;i<=nd;i++) {
if (a{i] (j] < TINY) ali] (j]=TINY;
rli]=pow((1.0-da[i] [j]/at[j])/

(Log (dt [j] )-log(a[i] [j])) , ALPH) ;

re += r[i];

}

rebin(rc/xnd,nd,r,xin,xilj]);

void rebin(float rc, int nd, float r[], float xin[], float xif])
Utility routine used by vegas, to rebin a vector of densities xi into new bins defined by a

vector ©.

{

int i,k=0;
float dr=0.0,xn=0.0,x0=0.0;

for (i=

jisnd;it+) {

while (rc > dr)

dr += r[+tk];

if (k > 1) xo=xilk-1];

xn=xi[k] ;

dr -= rc;

xin [i] =xn- (xn-xo) *dr/r [k] ;
}
for (i=1;i<nd;it+) xili]=xin[i];
xilnd]=1.0;

Recursive Stratified Sampling

The problem with stratified sampling, we have seen, is that it may not avoid the K@
explosion inherent in the obvious, Cartesian, tessellation of a d-dimensional volume. A
technique called recursive stratified sampling [3] attempts to do this by successive bisections
of a volume, not along all d dimensions, but rather along only one dimension at a time.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

324 Chapter 7. Random Numbers

The starting points are equations (7.8.10) and (7.8.13), applied to bisections of successively
smaller subregions.

Suppose that we have a quota of N evaluations of the function f, and want to evaluate
(f)’ in the rectangular parallelepiped region R = (Xa, x»). (We denote such a region by the
two coordinate vectors of its diagonally opposite corners.) First, we allocate a fraction p of
N towards exploring the variance of f in R: We sample pN function values uniformly in
R and accumulate the sums that will give the d different pairs of variances corresponding to
the d different coordinate directions along which R can be bisected. In other words, in pN
samples, we estimate Var (f) in each of the regions resulting from a possible bisection of R,

1
Rai =(Xa,Xb — =e + (Xp — Xa)ei)
1 2 (78.21)
Roi =(Ka + am (Xp — Xa)ei, Xb)

Here e; is the unit vector in the ith coordinate direction, i = 1,2,...,d.

Second, we inspect the variances to find the most favorable dimension 7 to bisect. By
equation (7.8.15), we could, for example, choose that 7 for which the sum of the square roots
of the variance estimators in regions Rai and Roi is minimized. (Actually, as we will explain,
we do something slightly different.)

Third, we allocate the remaining (1 — p)N function evaluations between the regions
Rai and Rp. If we used equation (7.8.15) to choose i, we should do this allocation according
to equation (7.8.14).

We now have two parallelepipeds each with its own allocation of function evaluations for
estimating the mean of f. Our “RSS” algorithm now shows itself to be recursive: To evaluate
the mean in each region, we go back to the sentence beginning “First,...’ in the paragraph
above equation (7.8.21). (Of course, when the allocation of points to a region falls below
some number, we resort to simple Monte Carlo rather than continue with the recursion.)

Finally, we combine the means, and also estimated variances of the two subvolumes,
using equation (7.8.10) and the first line of equation (7.8.11).

This completes the RSS algorithm in its simplest form. Before we describe some
additional tricks under the general rubric of “implementation details,” we need to return briefly
to equations (7.8.13)-(7.8.15) and derive the equations that we actually use instead of these.
The right-hand side of equation (7.8.13) applies the familiar scaling law of equation (7.8.9)
twice, once to a and again to b. This would be correct if the estimates (f), and (f), were
each made by simple Monte Carlo, with uniformly random sample points. However, the two
estimates of the mean are in fact made recursively. Thus, there is no reason to expect equation
(7.8.9) to hold. Rather, we might substitute for equation (7.8.13) the relation,

Var ((f)’) i we) aon

where a is an unknown constant > 1 (the case of equality corresponding to simple Monte
Carlo). In that case, a short calculation shows that Var ((f)’) is minimized when

(7.8.22)

2 1/(+e)
No Nita f) _ (78.23)
N © Vara (f)'/ OF" + Vary (f) OF?
and that its minimum value is
lta
Var ((f)’) oc [Vara (f)VOF) + Vary (Hor) (7.8.24)

Equations (7.8.22)-(7.8.24) reduce to equations (7.8.13)-(7.8.15) when a@ = 1. Numerical
experiments to find a self-consistent value for a find that a + 2. That is, when equation
(7.8.23) with a = 2 is used recursively to allocate sample opportunities, the observed variance
of the RSS algorithm goes approximately as N~?, while any other value of a in equation
(7.8.23) gives a poorer fall-off. (The sensitivity to a is, however, not very great; it is not
known whether a = 2 is an analytically justifiable result, or only a useful heuristic.)

The principal difference between miser’s implementation and the algorithm as described
thus far lies in how the variances on the right-hand side of equation (7.8.23) are estimated. We

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.8 Adaptive and Recursive Monte Carlo Methods 325

find empirically that it is somewhat more robust to use the square of the difference of maximum
and minimum sampled function values, instead of the genuine second moment of the samples.
This estimator is of course increasingly biased with increasing sample size; however, equation
(7.8.23) uses it only to compare two subvolumes (a and b) having approximately equal numbers
of samples. The “max minus min” estimator proves its worth when the preliminary sampling
yields only a single point, or small number of points, in active regions of the integrand. In
many realistic cases, these are indicators of nearby regions of even greater importance, and it
is useful to let them attract the greater sampling weight that “max minus min” provides.

Asecond modification embodied in the code is the introduction of a “dithering parameter,”
dith, whose nonzero value causes subvolumes to be divided not exactly down the middle, but
rather into fractions 0.5+dith, with the sign of the + randomly chosen by a built-in random
number routine. Normally dith can be set to zero. However, there is a large advantage in
taking dith to be nonzero if some special symmetry of the integrand puts the active region
exactly at the midpoint of the region, or at the center of some power-of-two submultiple of
the region. One wants to avoid the extreme case of the active region being evenly divided
into 2% abutting corners of a d-dimensional space. A typical nonzero value of dith, on
those occasions when it is useful, might be 0.1. Of course, when the dithering parameter
is nonzero, we must take the differing sizes of the subvolumes into account; the code does
this through the variable fracl.

One final feature in the code deserves mention. The RSS algorithm uses a single set
of sample points to evaluate equation (7.8.23) in all d directions. At bottom levels of the
recursion, the number of sample points can be quite small. Although rare, it can happen that
in one direction all the samples are in one half of the volume; in that case, that direction
is ignored as a candidate for bifurcation. Even more rare is the possibility that all of the
samples are in one half of the volume in all directions. In this case, a random direction is
chosen. If this happens too often in your application, then you should increase MNPT (see
line if (!jb)... in the code).

Note that miser, as given, returns as ave an estimate of the average function value
(f)), not the integral of f over the region. The routine vegas, adopting the other convention,
returns as tgral the integral. The two conventions are of course trivially related, by equation
(7.8.8), since the volume V of the rectangular region is known.

#include <stdlib.h>

#include <math.h>

#include "nrutil.h"

#define PFAC 0.1

#define MNPT 15

#define MNBS 60

#define TINY 1.0e-30

#define BIG 1.0e30

Here PFAC is the fraction of remaining function evaluations used at each stage to explore the
variance of func. At least MNPT function evaluations are performed in any terminal subregion;
a subregion is further bisected only if at least MNBS function evaluations are available. We take
MNBS = 4*MNPT.

static long iran=0;

void miser(float (*func)(float []), float regn[], int ndim, unsigned long npts,

float dith, float *ave, float *var)
Monte Carlo samples a user-supplied ndim-dimensional function func in a rectangular volume
specified by regn[1..2*ndim], a vector consisting of ndim “lower-left” coordinates of the
region followed by ndim “upper-right” coordinates. The function is sampled a total of npts
times, at locations determined by the method of recursive stratified sampling. The mean value
of the function in the region is returned as ave; an estimate of the statistical uncertainty of ave
(square of standard deviation) is returned as var. The input parameter dith should normally
be set to zero, but can be set to (e.g.) 0.1 if func’s active region falls on the boundary of a
power-of-two subdivision of region.
{

void ranpt(float pt[], float regn[], int n);

float *regn_temp;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

326

Chapter 7. Random Numbers

unsigned long n,npre,nptl,nptr;

int j, 5b;

float avel,varl;

float fracl,fval;

float rgl,rgm,rgr,s,sigl,siglb,sigr,sigrb;
float sum,sumb,summ,summ2;

float *fmaxl,*fmaxr,*fminl,*fminr;

float *pt,*rmid;

pt=vector(1,ndim) ;

if (npts < MNBS) {

summ=summ2=0.0; Monte Carlo.
for (n=1;n<=npts;nt+) {

ranpt (pt ,regn,ndim) ;

fval=(*func) (pt);

summ += fval;

summ2 += fval * fval;

+

*ave=summ/npts ;
*var=FMAX (TINY, (summ2-summ*summ/npts) /(npts*npts) ) ;

+

else {

Do the preliminary (uniform) sampling.

rmid=vector(1,ndim) ;

npre=LMAX((unsigned long) (npts*PFAC) ,MNPT) ;
fmaxl=vector(1,ndim) ;

fmaxr=vector(1,ndim) ;

fminl=vector(1,ndim) ;

fminr=vector(1,ndim) ;

for (j=1;j<=ndim;j++) {

iran=(iran*2661+36979) % 175000; each dimension.
s=SIGN(dith, (float) (iran-87500)) ;
rmid[j]=(0.5+s)*regn[j]+(0.5-s) *regn [ndim+j] ;

fminl[j

‘minr [j]=BIG;

fmax1[j]=fmaxr[j] = -BIG;

for (n=1;n<=npre;ntt+) {

Loop over the points in the sample.

ranpt (pt ,regn,ndim) ;
fval=(*func) (pt);

for (j=1;j<=ndim;j++) {

if (pt[j]<=rmia(j]) { dimension.
fmin1[j]=FMIN(fmin1[j] ,fval) ;

fmax1[j]=FMAX(fmax1[j] ,fval) ;
}
else {
fminr[j]=FMIN(fminr[j] ,fval) ;
fmaxr [j]=FMAX(fmaxr[j] ,fval) ;
}
=BIG; Choose which dimension jb to bisect.

siglb=sigrb=1.0;
for (j=1;j<=ndim;j++) {
if (fmaxl(j] > fminl[j] &&# fmaxr[j] > fminr[j]) {

sigl=FMAX (TINY, pow(fmax1[j]-fmin1[j] ,2.0/3.0));

sigr=FMAX (TINY, pow(fmaxr [j]-fminr[j] ,2.0/3.0));
sum=sigltsigr; Equation (7.8.24), see text.
if (sum<=sumb) {

Too few points to bisect; do straight

Initialize the left and right bounds for

Find the left and right bounds for each

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7.8 Adaptive and Recursive Monte Carlo Methods 327

}

free_vector(fminr,1,ndim) ;
free_vector(fminl,1,ndim) ;
free_vector(fmaxr,1,ndim) ;
free_vector(fmaxl,1,ndim) ;

aif (!jb) jb=1+(ndim*iran) /175000; MNPT may be too small.
rgl=regn[jb] ; Apportion the remaining points between
rgm=rmid[jb] ; left and right.

rgr=regn[ndim+jb] ;
fracl=fabs((rgm-rgl)/(rgr-rgl)) ;
nptl=(unsigned long) (MNPT+(npts-npre-2*MNPT) *fracl*siglb
/(fracl*siglb+(1.0-fracl)*sigrb)) ; Equation (7.8.23).
nptr=npts-npre-npt1;
regn_temp=vector(1,2*ndim) ; Now allocate and integrate the two sub-
for (j=1;j<=ndim;j++) { regions.
regn_temp[j]=regn[j];
regn_temp([ndim+j]=regn[ndim+j] ;
}
regn_temp([ndim+jb]=rmid[jb] ;
miser(func,regn_temp,ndim,nptl,dith,kavel,&varl) ;
regn_temp[jb]=rmid[jb] ; Dispatch recursive call; will return back
regn_temp [ndim+jb]=regn[ndim+jb] ; here eventually.
miser(func,regn_temp,ndim,nptr,dith,ave,var) ;
free_vector(regn_temp,1,2*ndim) ;
*ave=fracl*avel+(1-fracl)*(*ave) ;
a#var=fracl*fracl*varl+(1-fracl) *(1-fracl) *(*var) ;
Combine left and right regions by equation (7.8.11) (1st line).
free_vector(rmid,1,ndim) ;
}

free_vector(pt,1,ndim) ;

The miser routine calls a short function ranpt to get a random point within a specified
d-dimensional region. The following version of ranpt makes consecutive calls to a uniform
random number generator and does the obvious scaling. One can easily modify ranpt to
generate its points via the quasi-random routine sobseq (§7.7). We find that miser with
sobseq can be considerably more accurate than miser with uniform random deviates. Since
the use of RSS and the use of quasi-random numbers are completely separable, however, we
have not made the code given here dependent on sobseq. A similar remark might be made
regarding importance sampling, which could in principle be combined with RSS. (One could
in principle combine vegas and miser, although the programming would be intricate.)

extern long idum;

void ranpt(float pt[], float regn[], int n)

Returns a uniformly random point pt in an n-dimensional rectangular region. Used by miser;
calls rani for uniform deviates. Your main program should initialize the global variable idum
to a negative seed integer.

{
float rani(long *idum) ;
int j;
for (j=1;j<=n;j++)

pt (j]=regn[j]+(regn[n+j]-regn[j]) *ran1(&idum) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

328 Chapter 7. Random Numbers

CITED REFERENCES AND FURTHER READING:

Hammersley, J.M. and Handscomb, D.C. 1964, Monte Carlo Methods (London: Methuen).
Kalos, M.H. and Whitlock, P.A. 1986, Monte Carlo Methods (New York: Wiley).

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New York: Springer-Verlag).
Lepage, G.P. 1978, Journal of Computational Physics, vol. 27, pp. 192-203. [1]

Lepage, G.P. 1980, “VEGAS: An Adaptive Multidimensional Integration Program,” Publication
CLNS-80/447, Cornell University. [2]

Press, W.H., and Farrar, G.R. 1990, Computers in Physics, vol. 4, pp. 190-195. [3]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 8. Sorting

8.0 Introduction

This chapter almost doesn’t belong in a book on numerical methods. However,
some practical knowledge of techniques for sorting is an indispensable part of any
good programmer’s expertise. We would not want you to consider yourself expert in
numerical techniques while remaining ignorant of so basic a subject.

In conjunction with numerical work, sorting is frequently necessary when data
(either experimental or numerically generated) are being handled. One has tables
or lists of numbers, representing one or more independent (or “control”) variables,
and one or more dependent (or ““measured”) variables. One may wish to arrange
these data, in various circumstances, in order by one or another of these variables.
Alternatively, one may simply wish to identify the “median” value, or the “upper
quartile” value of one of the lists of values. This task, closely related to sorting,
is called selection.

Here, more specifically, are the tasks that this chapter will deal with:

e Sort, i.e., rearrange, an array of numbers into numerical order.

e Rearrange an array into numerical order while performing the corre-
sponding rearrangement of one or more additional arrays, so that the
correspondence between elements in all arrays is maintained.

e Given an array, prepare an index table for it, i.e., a table of pointers telling
which number array element comes first in numerical order, which second,
and so on.

e Given an array, prepare a rank table for it, i.e., a table telling what is
the numerical rank of the first array element, the second array element,
and so on.

e Select the Mth largest element from an array.

For the basic task of sorting N elements, the best algorithms require on the
order of several times NV logy N operations. The algorithm inventor tries to reduce
the constant in front of this estimate to as small a value as possible. Two of the
best algorithms are Quicksort (§8.2), invented by the inimitable C.A.R. Hoare, and
Heapsort (§8.3), invented by J.W.J. Williams.

For large NV (say > 1000), Quicksort is faster, on most machines, by a factor of
1.5 or 2; it requires a bit of extra memory, however, and is a moderately complicated
program. Heapsort is a true “sort in place,’ and is somewhat more compact to
program and therefore a bit easier to modify for special purposes. On balance, we
recommend Quicksort because of its speed, but we implement both routines.

329

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

330 Chapter 8. Sorting

For small V one does better to use an algorithm whose operation count goes
as a higher, i.e., poorer, power of N, if the constant in front is small enough. For
N < 20, roughly, the method of straight insertion (§8.1) is concise and fast enough.
We include it with some trepidation: It is an N? algorithm, whose potential for
misuse (by using it for too large an JV) is great. The resultant waste of computer
time is so awesome, that we were tempted not to include any N’? routine at all. We
will draw the line, however, at the inefficient NV? algorithm, beloved of elementary
computer science texts, called bubble sort. If you know what bubble sort is, wipe it
from your mind; if you don’t know, make a point of never finding out!

For N < 50, roughly, Shell’s method (§8.1), only slightly more complicated to
program than straight insertion, is competitive with the more complicated Quicksort
on many machines. This method goes as N’3/? in the worst case, but is usually faster.

See references [1,2] for further information on the subject of sorting, and for
detailed references to the literature.

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley). [1]
Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), Chapters 8-13. [2]

8.1 Straight Insertion and Shell’s Method

Straight insertion is an N? routine, and should be used only for small NV,
say < 20.

The technique is exactly the one used by experienced card players to sort their
cards: Pick out the second card and put it in order with respect to the first; then pick
out the third card and insert it into the sequence among the first two; and so on until
the last card has been picked out and inserted.

void piksrt(int n, float arr[])
Sorts an array arr[1..n] into ascending numerical order, by straight insertion. n is input; arr
is replaced on output by its sorted rearrangement.

{
int i,j;
float a;
for (j=2;j<=n;j++) { Pick out each element in turn.
a=arr [jl];
i=j-1;
while (i > 0 && arr[i] > a) { Look for the place to insert it.
arr [i+i]=arr [i];
in-3
}
arr [iti]=a; Insert it.
+
3

What if you also want to rearrange an array brr at the same time as you sort
arr? Simply move an element of brr whenever you move an element of arr:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.1 Straight Insertion and Shell’s Method 331

void piksr2(int n, float arr[{], float brr[])
Sorts an array arr[1..n] into ascending numerical order, by straight insertion, while making
the corresponding rearrangement of the array brr[1..n].

{
int i,j;
float a,b;
for (j=2;j<=n;j++) { Pick out each element in turn.
while (i > 0 && arr[i] > a) { Look for the place to insert it.
arr [i+1]=arr [i] ;
brr[i+1]=brr [i] ;
in-3
}
arr [iti]=a; Insert it.
brr[i+1]=b;
+
3

For the case of rearranging a larger number of arrays by sorting on one of
them, see $8.4.

Shell’s Method

This is actually a variant on straight insertion, but a very powerful variant indeed.
The rough idea, e.g., for the case of sorting 16 numbers n; ... 716, is this: First sort,
by straight insertion, each of the 8 groups of 2 (n1,1n9), (n2,n10), ---, (ns, N16).
Next, sort each of the 4 groups of 4 (ni, 75,79, 713),---, (M4, Ns, N12, N16). Next
sort the 2 groups of 8 records, beginning with (ni,
Finally, sort the whole list of 16 numbers.

Of course, only the last sort is necessary for putting the numbers into order. So
what is the purpose of the previous partial sorts? The answer is that the previous
sorts allow numbers efficiently to filter up or down to positions close to their final
resting places. Therefore, the straight insertion passes on the final sort rarely have to
go past more than a “few” elements before finding the right place. (Think of sorting
a hand of cards that are already almost in order.)

The spacings between the numbers sorted on each pass through the data (8,4,2,1
in the above example) are called the increments, and a Shell sort is sometimes
called a diminishing increment sort. There has been a lot of research into how to
choose a good set of increments, but the optimum choice is not known. The set
...,8,4,2, 1 is in fact not a good choice, especially for N a power of 2. A much
better choice is the sequence

15,7, Ng, M11, 13, N15).

(3* — 1)/2,...,40,13,4,1 (8.1.1)
which can be generated by the recurrence
ip=1, tpg =3ipn +1, K=1,2,... (8.1.2)

It can be shown (see [1]) that for this sequence of increments the number of operations
required in all is of order N°/? for the worst possible ordering of the original data.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

332 Chapter 8. Sorting

For “randomly” ordered data, the operations count goes approximately as N !:?5, at
least for N < 60000. For N > 50, however, Quicksort is generally faster. The
program follows:

void shell(unsigned long n, float a[])

Sorts an array a[] into ascending numerical order by Shell's method (diminishing increment
sort). a is replaced on output by its sorted rearrangement. Normally, the argument n should
be set to the size of array a, but if n is smaller than this, then only the first n elements of a
are sorted. This feature is used in selip.

{
unsigned long i,j,inc;
float v;
inc=1; Determine the starting increment.
do {
inc *= 3;
inct+;
} while (inc <= n);
do { Loop over the partial sorts.
inc /= 3;
for (i=inct1;i<=n;i++) { Outer loop of straight insertion.
vealil;
j=is
while (a[j-inc] > v) { Inner loop of straight insertion.
a(jl=alj-inc] ;
j -= inc;
if (j <= inc) break;
3
aljl=v;
}
} while (inc > 1);
3

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §5.2.1. [1]

Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), Chapter 8.

8.2 Quicksort

Quicksort is, on most machines, on average, for large N, the fastest known
sorting algorithm. It is a “partition-exchange” sorting method: A “partitioning
element” a is selected from the array. Then by pairwise exchanges of elements, the
original array is partitioned into two subarrays. At the end of a round of partitioning,
the element a is in its final place in the array. All elements in the left subarray are
<a, while all elements in the right subarray are > a. The process is then repeated
on the left and right subarrays independently, and so on.

The partitioning process is carried out by selecting some element, say the
leftmost, as the partitioning element a. Scan a pointer up the array until you find
an element > a, and then scan another pointer down from the end of the array
until you find an element < a. These two elements are clearly out of place for the
final partitioned array, so exchange them. Continue this process until the pointers
cross. This is the right place to insert a, and that round of partitioning is done. The

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.2 Quicksort 333

question of the best strategy when an element is equal to the partitioning element
is subtle; we refer you to Sedgewick [1] for a discussion. (Answer: You should
stop and do an exchange.)

For speed of execution, we do not implement Quicksort using recursion. Thus
the algorithm requires an auxiliary array of storage, of length 2 log. N, which it uses
as a push-down stack for keeping track of the pending subarrays. When a subarray
has gotten down to some size /, it becomes faster to sort it by straight insertion
(88.1), so we will do this. The optimal setting of MW is machine dependent, but
M = 7 is not too far wrong. Some people advocate leaving the short subarrays
unsorted until the end, and then doing one giant insertion sort at the end. Since
each element moves at most 7 places, this is just as efficient as doing the sorts
immediately, and saves on the overhead. However, on modern machines with paged
memory, there is increased overhead when dealing with a large array all at once. We
have not found any advantage in saving the insertion sorts till the end.

As already mentioned, Quicksort’s average running time is fast, but its worst
case running time can be very slow: For the worst case it is, in fact, an N’? method!
And for the most straightforward implementation of Quicksort it turns out that the
worst case is achieved for an input array that is already in order! This ordering
of the input array might easily occur in practice. One way to avoid this is to use
a little random number generator to choose a random element as the partitioning
element. Another is to use instead the median of the first, middle, and last elements
of the current subarray.

The great speed of Quicksort comes from the simplicity and efficiency of its
inner loop. Simply adding one unnecessary test (for example, a test that your pointer
has not moved off the end of the array) can almost double the running time! One
avoids such unnecessary tests by placing “sentinels” at either end of the subarray
being partitioned. The leftmost sentinel is < a, the rightmost > a. With the
“median-of-three” selection of a partitioning element, we can use the two elements
that were not the median to be the sentinels for that subarray.

Our implementation closely follows [1]:

#include "nrutil.h"

#define SWAP(a,b) temp=(a) ;(a)=(b) ; (b)=temp;

#define M 7

#define NSTACK 50

Here M is the size of subarrays sorted by straight insertion and NSTACK is the required auxiliary
storage.

void sort(unsigned long n, float arr[])
Sorts an array arr[1..n] into ascending numerical order using the Quicksort algorithm. n is
input; arr is replaced on output by its sorted rearrangement.
{
unsigned long i,ir=n,j,k,1=1,*istack;
int jstack=0;
float a,temp;

istack=lvector(1,NSTACK) ;
for (;;) { Insertion sort when subarray small enough.
if (ir-l <M) {
for (j=1+1;j<=ir;jt+) {
a=arr[j];
for (i=j-1;i>=1;i--) {
if (arr[i] <= a) break;
arr [i+1]=arr[i] ;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

334 Chapter 8. Sorting

3
arr[iti]=a;
3
if (jstack == 0) break;
ir=istack[jstack--]; Pop stack and begin a new round of parti-
1sistack[jstack--] ; tioning.
} else {
k=(1+ir) >> 1; Choose median of left, center, and right el-
SWAP (arr [k] ,arr[1+1]) ements as partitioning element a. Also

if (arr(1] > arr([ir]) {
SWAP (arr [1] ,arr[ir])
}
if (arr[1+1] > arr[ir]) {
SWAP (arr [1+1] ,arr[ir])
}
if (arr(1] > arr[1+1]) {
SWAP(arr[1] ,arr[1+1])

}

i=l+1; Initialize pointers for partitioning.

jeir;

a=arr[1+1]; Partitioning element.

for (53) { Beginning of innermost loop.
do i++; while (arr[i] < a); Scan up to find element > a.
do j--; while (arr[j] > a); Scan down to find element < a.
if (j < i) break; Pointers crossed. Partitioning complete.
SWAP (arr [i] ,arr[j]); Exchange elements.

} End of innermost loop.

arr (1+1]=arr([j]; Insert partitioning element.

arr [jl=a;

jstack += 2;

Push pointers to larger subarray on stack, process smaller subarray immediately.
if (jstack > NSTACK) nrerror("NSTACK too small in sort.");
if (ir-it1 >= j-1) {
istack[jstack]=ir;
istack[jstack-1]=i;
ir=j-1;
} else {
istack[jstack]=j-1;
istack[jstack-1]=1;
1=i;

}
}
free_lvector(istack,1,NSTACK) ;

As usual you can move any other arrays around at the same time as you sort
arr. At the risk of being repetitious:

#include "nrutil.h"

#define SWAP(a,b) temp=(a) ;(a)=(b) ; (b)=temp;
#define M 7

#define NSTACK 50

void sort2(unsigned long n, float arr[], float brr[])
Sorts an array arr [1..n] into ascending order using Quicksort, while making the corresponding
rearrangement of the array brr[1..n].
{
unsigned long i,ir=n,j,k,1=1,*istack;
int jstack=0;
float a,b,temp;

rearrange so that a[1] < a[1+1] <alir].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.2 Quicksort 335

istack=lvector(1,NSTACK) ;
for (;;) { Insertion sort when subarray small enough.
if (ir-l <M) {
for (j=1+1;j<=ir;jt+) {
a=arr[j];
b=brr[j];
for (i=j-1;i>=1;i--) {
if (arr[i] <= a) break;
arr [i+1]=arr [i];
brr [i+1]=brr [i] ;

doa,

3
a
a
3 o
arr [it+i]=a; 2
brr[i+1]=b; =
t a
if (!jstack) { a
free_lvector(istack,1,NSTACK) ; a
return;
3
ir=istack[jstack] ; Pop stack and begin a new round of parti-
1=istack[jstack-1] ; tioning.
jstack -= 2;
} else {
k=(1+ir) >> 1; Choose median of left, center and right el-
SWAP (arr [k] ,arr[1+1]) ements as partitioning element a. Also
SWAP (brr [k] ,brr[1+1]) rearrange so that a[1] < a[1+1] <alir].

if (arr[1] > arr[ir]) {
SWAP (arr [1] ,arr[ir])
SWAP (brr [1] ,brr [ir] )

3

if (arr({1+1] > arr[ir]) {
SWAP (arr [1+1] ,arr[ir])
SWAP (brr [1+1] ,brr[ir])

3

if (arr(1] > arr[1+1]) {
SWAP (arr [1] ,arr[1+1])
SWAP (brr [1] ,brr[1+1])

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

3

i=l+1; Initialize pointers for partitioning.

jrir;

a=arr[1+1]; Partitioning element.

b=brr[1+1] ;

for (53) { Beginning of innermost loop.
do i++; while (arr[i] < a); Scan up to find element > a.
do j--; while (arr[j] > a); Scan down to find element < a.
if (j < i) break; Pointers crossed. Partitioning complete.
SWAP (arr [i] ,arr[j]) Exchange elements of both arrays.
SWAP (brr [i] ,brr[j])

} End of innermost loop.

arr (1+1]=arr([j]; Insert partitioning element in both arrays.

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

jstack += 2;

Push pointers to larger subarray on stack, process smaller subarray immediately.

if (jstack > NSTACK) nrerror("NSTACK too small in sort2.");

if (ir-it1 >= j-1) {
istack[jstack]=ir;
istack[jstack-1]=i;
ir=j-1;

} else {
istack[jstack]=j
istack[jstack-1]
1=i;

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


336 Chapter 8. Sorting

You could, in principle, rearrange any number of additional arrays along with
brr, but this becomes wasteful as the number of such arrays becomes large. The
preferred technique is to make use of an index table, as described in §8.4.

CITED REFERENCES AND FURTHER READING:
Sedgewick, R. 1978, Communications of the ACM, vol. 21, pp. 847-857. [1]

8.3 Heapsort

While usually not quite as fast as Quicksort, Heapsort is one of our favorite
sorting routines. It is a true “in-place” sort, requiring no auxiliary storage. It is an
N log, N process, not only on average, but also for the worst-case order of input data.
In fact, its worst case is only 20 percent or so worse than its average running time.

It is beyond our scope to give a complete exposition on the theory of Heapsort.
We will mention the general principles, then let you refer to the references [1,2], or
analyze the program yourself, if you want to understand the details.

A set of N numbers a;, i = 1,..., N, is said to form a “heap” if it satisfies
the relation

aj 2a; for 1<j/2<j<N (8.3.1)

Here the division in 7/2 means “integer divide,” i.e., is an exact integer or else is
rounded down to the closest integer. Definition (8.3.1) will make sense if you think
of the numbers a; as being arranged in a binary tree, with the top, “boss,” node being
a1, the two “underling” nodes being a and a3, their four underling nodes being a4
through a7, etc. (See Figure 8.3.1.) In this form, a heap has every “supervisor” greater
than or equal to its two “supervisees,” down through the levels of the hierarchy.

If you have managed to rearrange your array into an order that forms a heap,
then sorting it is very easy: You pull off the “top of the heap,’ which will be the
largest element yet unsorted. Then you “promote” to the top of the heap its largest
underling. Then you promote its largest underling, and so on. The process is like
what happens (or is supposed to happen) in a large corporation when the chairman
of the board retires. You then repeat the whole process by retiring the new chairman
of the board. Evidently the whole thing is an N log, N process, since each retiring
chairman leads to log, N promotions of underlings.

Well, how do you arrange the array into a heap in the first place? The answer
is again a “sift-up” process like corporate promotion. Imagine that the corporation
starts out with N/2 employees on the production line, but with no supervisors. Now
a supervisor is hired to supervise two workers. If he is less capable than one of
his workers, that one is promoted in his place, and he joins the production line.
After supervisors are hired, then supervisors of supervisors are hired, and so on up

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.3 Heapsort 337

ay

/

ag

WA
4
,
:
4
N
.
N
x
:
,
7
7

Lo

Figure 8.3.1. Ordering implied by a “heap,” here of 12 elements. Elements connected by an upward
path are sorted with respect to one another, but there is not necessarily any ordering among elements
related only “laterally.”

in an

the corporate ladder. Each employee is brought in at the top of the tree, but then
immediately sifted down, with more capable workers promoted until their proper
corporate level has been reached.

In the Heapsort implementation, the same “sift-up” code can be used for the
initial creation of the heap and for the subsequent retirement-and-promotion phase.
One execution of the Heapsort function represents the entire life-cycle of a giant
corporation: N/2 workers are hired; N/2 potential supervisors are hired; there is a
sifting up in the ranks, a sort of super Peter Principle: in due course, each of the
original employees gets promoted to chairman of the board.

void hpsort(unsigned long n, float ra[])
Sorts an array ra[1..n] into ascending numerical order using the Heapsort algorithm. n is
input; ra is replaced on output by its sorted rearrangement.
{
unsigned long i,ir,j,1;
float rra;

if (m < 2) return;

l=(mn >> 1)41;

ir=n;

The index 1 will be decremented from its initial value down to 1 during the “hiring” (heap
creation) phase. Once it reaches 1, the index ir will be decremented from its initial value
down to 1 during the “retirement-and-promotion” (heap selection) phase.

for Gi) f

if (’>1){ Still in hiring phase.
rra=ra[--1];
} else { In retirement-and-promotion phase.
rra=ra[ir]; Clear a space at end of array.
rafir]=ra[1]; Retire the top of the heap into it.
if (--ir == 1) { Done with the last promotion.
ra[1]=rra; The least competent worker of all!
break;
3
}
i=1; Whether in the hiring phase or promotion phase, we
j=ltl; here set up to sift down element rra to its proper
while (j <= ir) { level.
if (j < ir && ralj] < ralj+1]) j++; Compare to the better underling.
if (rra < ra{j]) { Demote rra.

ralil=ralj];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

338 Chapter 8. Sorting

irji
j <<= 1;
} else break; Found rra's level. Terminate the sift-down.
}
ra(il=rra; Put rra into its slot.

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §5.2.3. [1]

Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), Chapter 11. [2]

8.4 Indexing and Ranking

The concept of keys plays a prominent role in the management of data files. A
data record in such a file may contain several items, or fields. For example, a record
in a file of weather observations may have fields recording time, temperature, and
wind velocity. When we sort the records, we must decide which of these fields we
want to be brought into sorted order. The other fields in a record just come along
for the ride, and will not, in general, end up in any particular order. The field on
which the sort is performed is called the key field.

For a data file with many records and many fields, the actual movement of Nv

records into the sorted order of their keys K;, i = 1,..., N, can be a daunting task.
Instead, one can construct an index table Jj, 7 = 1,...,.N, such that the smallest
K;, has i = I, the second smallest has i = Iz, and so on up to the largest Kk; with

i = Iy. In other words, the array
Ki, j=1,2,...,N (8.4.1)

is in sorted order when indexed by 7. When an index table is available, one need not
move records from their original order. Further, different index tables can be made
from the same set of records, indexing them to different keys.

The algorithm for constructing an index table is straightforward: Initialize the
index array with the integers from | to N, then perform the Quicksort algorithm,
moving the elements around as if one were sorting the keys. The integer that initially
numbered the smallest key thus ends up in the number one position, and so on.

#include "nrutil.h"

#define SWAP(a,b) itemp=(a);(a)=(b); (b)=itemp;
#define M 7

#define NSTACK 50

void indexx(unsigned long n, float arr[], unsigned long indx[])
Indexes an array arr[1..n], i.e., outputs the array indx[1..n] such that arr [indx[j]] is
in ascending order for j = 1,2,...,N. The input quantities n and arr are not changed.
{
unsigned long i,indxt,ir=n,itemp,j,k,1=1;
int jstack=0,*istack;
float a;

istack=ivector(1,NSTACK) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.4 Indexing and Ranking

original index rank sorted
array table table array
G) 4 3
© ©
4) 3 7
@ @
(2) 6 8
io) i) i)
@ 2 14
2) i) @
© 1 15
© iC) i)
@ 5 32
© © ©
(b) (c) (d)
Figure 8.4.1. (a) An unsorted array of six numbers. (b) Index table, whose entries are pointers to the

elements of (a) in ascending order. (c) Rank table, whose entries are the ranks of the corresponding

elements of (a). (d) Sorted array of the elements in (a).

for (j=1;j<=n;j++) indx[j]=j;
for (53) f
if (ir-l <™) {

for (j=1+1;j<=ir;jt+) {

indxt=indx[j];
a=arr [indxt] ;

for (i=j-1;i>=1;i--) {
if (arr[indx[i]]
indx[i+1]=indx [i] ;

3
indx [i+1]=indxt;
3
if (jstack == 0) break;
ir=istack[jstack--];
1=istack[jstack--] ;
} else {
k=(l+ir) >> 1;

SWAP (indx[k] , indx[1+1]) ;

<= a) break;

if (arr(indx(1]] > arr[indx[ir]]) {

SWAP (indx [1] , indx[ir])

}

if (arr[indx[1+1]] > arr[indx[ir]]) {

SWAP (indx[1+1] , indx[ir])

}

if (arr([indx(1]] > arr[indx[1+1]]) {

SWAP (indx [1] , indx[1+1])

}

i=1+1;

j=ir;

indxt=indx [1+1] ;
a=arr [indxt] ;
for (;;) ¢

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

340 Chapter 8. Sorting

do i++; while (arr[indx[i]] < a);
do j--; while (arr[indx[j]] > a);
if (j < i) break;

SWAP (indx [i] , indx[j])

3

indx [1+1]=indx[j];

indx [j]=indxt;

jstack += 2;

if (jstack > NSTACK) nrerror("NSTACK too small in indexx.");

if (ir-it1 >= j-1) {
istack[jstack]=ir;
istack[jstack-1]=i;
ir=j-1;

} else {
istack[jstack]=j-
istack[jstack-1]=1;
1=i;

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

}
+
free_ivector(istack,1,NSTACK) ;

If you want to sort an array while making the corresponding rearrangement of
several or many other arrays, you should first make an index table, then use it to
rearrange each array in turn. This requires two arrays of working space: one to
hold the index, and another into which an array is temporarily moved, and from
which it is redeposited back on itself in the rearranged order. For 3 arrays, the
procedure looks like this:

#include "nrutil.h"

void sort3(unsigned long n, float ra[], float rb[], float rc[])

Sorts an array ra[1..n] into ascending numerical order while making the corresponding re-
arrangements of the arrays rb[1..n] and rc[1..n]. An index table is constructed via the
routine indexx.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

{
void indexx(unsigned long n, float arr[], unsigned long indx[]);
unsigned long j,*iwksp;
float *wksp;
iwksp=lvector(1,n);
wksp=vector(1,n);
indexx(n,ra,iwksp) ; Make the index table.
for (j j<=n;jt++) wksp[j]=ralj]; Save the array ra.
for ra(j]=wksp[iwksp[j]]; Copy it back in rearranged order.
for wksp[j]=rb(j]; Ditto rb.
for rb[j]=wksp [iwksp[j]];
for wksp[j]=rc(jl; Ditto rc.
for re[j]=wksp [iwksp[j]];
free_vector(wksp,1,n);
free_lvector(iwksp,1,n);
}

The generalization to any other number of arrays is obviously straightforward.

A rank table is different from an index table. A rank table’s jth entry gives the
rank of the jth element of the original array of keys, ranging from | (if that element

8.5 Selecting the Mth Largest 341

was the smallest) to N (if that element was the largest). One can easily construct
a rank table from an index table, however:

void rank(unsigned long n, unsigned long indx[], unsigned long irank[])

Given indx[1..n] as output from the routine indexx, returns an array irank[1..n], the
corresponding table of ranks.

{

unsigned long j;

for (j=1;j<=n;j++) irank[indx[j]]=j;

Figure 8.4.1 summarizes the concepts discussed in this section.

8.5 Selecting the Mth Largest

Selection is sorting’s austere sister. (Say that five times quickly!) Where sorting
demands the rearrangement of an entire data array, selection politely asks for a single
returned value: What is the kth smallest (or, equivalently, the m = N+1—kth largest)
element out of NV elements? The fastest methods for selection do, unfortunately,
rearrange the array for their own computational purposes, typically putting all smaller
elements to the left of the kth, all larger elements to the right, and scrambling the
order within each subset. This side effect is at best innocuous, at worst downright
inconvenient. When the array is very long, so that making a scratch copy of it is taxing
on memory, or when the computational burden of the selection is a negligible part
a larger calculation, one turns to selection algorithms without side effects, which
leave the original array undisturbed. Such in place selection is slower than the faster
selection methods by a factor of about 10. We give routines of both types, below.
The most common use of selection is in the statistical characterization of a set
of data. One often wants to know the median element in an array, or the top and
bottom quartile elements. When N is odd, the median is the kth element, with
k = (N+1)/2. When N is even, statistics books define the median as the arithmetic
mean of the elements k = N/2 and k = N/2+ 1 (that is, N/2 from the bottom
and N/2 from the top). If you accept such pedantry, you must perform two separate
selections to find these elements. For N > 100 we usually define k = N/2 to be
the median element, pedants be damned.

The fastest general method for selection, allowing rearrangement, is partition-
ing, exactly as was done in the Quicksort algorithm (88.2). Selecting a “random”
partition element, one marches through the array, forcing smaller elements to the
left, larger elements to the right. As in Quicksort, it is important to optimize the
inner loop, using “sentinels” (§8.2) to minimize the number of comparisons. For
sorting, one would then proceed to further partition both subsets. For selection,
we can ignore one subset and attend only to the one that contains our desired kth
element. Selection by partitioning thus does not need a stack of pending operations,
and its operations count scales as N rather than as N log N (see [1]). Comparison
with sort in §8.2 should make the following routine obvious:

°

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

342 Chapter 8. Sorting

#define SWAP(a,b) temp=(a) ; (a)=(b) ; (b)=temp;

float select(unsigned long k, unsigned long n, float arr[])
Returns the kth smallest value in the array arr[1..n]. The input array will be rearranged
to have this value in location arr[k], with all smaller elements moved to arr[1..k-1] (in
arbitrary order) and all larger elements in arr[k+1..n] (also in arbitrary order).
{

unsigned long i,ir,j,l,mid;

float a,temp;

1=1;
ir=n;
for Gi) f
aif (ir <= 1+1) { Active partition contains 1 or 2 elements.
if (ir == 1+1 && arr[ir] < arr[1]) { Case of 2 elements.
SWAP (arr [1] ,arr[ir])
3
return arr[k];
} else {
mid=(l+ir) >> 1; Choose median of left, center, and right el-
SWAP (arr [mid] ,arr[1+1]) ements as partitioning element a. Also
if (arr[{1] > arr[ir]) { rearrange so that arr[1] < arr[1+1],
SWAP (arr [1] ,arr[ir]) arr[ir] > arr[1+1].
}
if (arr(1+1] > arr[ir]) {
SWAP (arr [1+1] ,arr[ir])
}
if (arr(1] > arr[1+1]) {
SWAP (arr [1] ,arr[1+1])
}
i=l+1; Initialize pointers for partitioning.
jeir;
a=arr[1+1]; Partitioning element.
for (53) { Beginning of innermost loop.
do i++; while (arr[i] < a); Scan up to find element > a.
do j--; while (arr[j] > a); Scan down to find element < a.
if (j < i) break; Pointers crossed. Partitioning complete.
SWAP (arr [i] ,arr[j])
} End of innermost loop.
arr (1+1]=arr([j]; Insert partitioning element.
Keep active the partition that contains the
if (j <= k) 1si; kth element.
}
+

In-place, nondestructive, selection is conceptually simple, but it requires a lot
of bookkeeping, and it is correspondingly slower. The general idea is to pick some
number M of elements at random, to sort them, and then to make a pass through
the array counting how many elements fall in each of the MM + 1 intervals defined
by these elements. The kth largest will fall in one such interval — call it the “live”
interval. One then does a second round, first picking |Z random elements in the live
interval, and then determining which of the new, finer, MZ + 1 intervals all presently
live elements fall into. And so on, until the kth element is finally localized within a
single array of size M, at which point direct selection is possible.

How shall we pick M@? The number of rounds, log yy N = logy N/ log, M,
will be smaller if 1/7 is larger; but the work to locate each element among M + 1
subintervals will be larger, scaling as log, M for bisection, say. Each round

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.5 Selecting the Mth Largest 343

requires looking at all N elements, if only to find those that are still alive, while
the bisections are dominated by the N that occur in the first round. Minimizing
O(N logy, N) + O(N logy M) thus yields the result

Mw 2V i082 N (8.5.1)

The square root of the logarithm is so slowly varying that secondary considerations of
machine timing become important. We use MW = 64 as a convenient constant value.

Two minor additional tricks in the following routine, selip, are (i) augmenting
the set of MW random values by an M + Ist, the arithmetic mean, and (ii) choosing
the / random values “on the fly” in a pass through the data, by a method that makes
later values no less likely to be chosen than earlier ones. (The underlying idea is to
give element m > M an M/m chance of being brought into the set. You can prove
by induction that this yields the desired result.)

#include "nrutil.h"

#define M 64

#define BIG 1.0e30

#define FREEALL free_vector(sel,1,M+2);free_lvector(isel,1,M+2);

float selip(unsigned long k, unsigned long n, float arr[])
Returns the kth smallest value in the array arr[1..n]. The input array is not altered.
t

void shell(unsigned long n, float a[]);

unsigned long i,j,jl,jm,ju,kk,mm,nlo,nxtmm,*isel;

float ahi,alo,sum,*sel;

if (k <1! k>n || n <= 0) nrerror("bad input to selip");
isel=lvector(1,M+2) ;
sel=vector(1,M+2) ;

kk=k:
ahi=BIG;
alo = -BIG;
for (;;) { Main iteration loop, until desired ele-
mm=nlo=0; ment is isolated.
sum=0.0;
nxtmm=M+1 ;
for (i=1;i<=n;it+) { Make a pass through the whole array.

if (arr[i] >= alo && arr[i] <= ahi) {
Consider only elements in the current brackets.
mm++;
if (arr[i] == alo) nlo++; In case of ties for low bracket.
Now use statistical procedure for selecting m in-range elements with equal
probability, even without knowing in advance how many there are!
if (mm <= M) sel[mm]=arr [i];
else if (mm nxtmm) {
nxtmm=mm+mm/M;
sel[1 + ((itmm+kk) % M)]=arr[i]; The % operation provides a some-
} what random number.
sum += arr[i];

3

}

if (kk <= nlo) { Desired element is tied for lower bound;
FREEALL return it.
return alo;

}

else if (mm <= M) { All in-range elements were kept. So re-

shell(mm, sel) ; turn answer by direct method.
ahi = sel [kk];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

344 Chapter 8.

Sorting

FREEALL
return ahi;
}
sel [M+1]=sum/mm;
shell (M+1,sel) ;
sel [M+2]=ahi;
for (j=1;j<=M+2;j++) isel[j]=0;
for (i=1;i<=n;it+) {

Augment selected set by mean value (fixes
degeneracies), and sort it.

Zero the count array.
Make another pass through the array.

if (arr[i] >= alo && arr[i] <= ahi) { For each in-range element..
jl=0;
ju=M+2;
while (ju-jl > 1) {
jm=(jutjl)/2;
if (arr[i] >=
else ju=jm;

...find its position among the select by
bisection...
sel({jm]) jl=jm;

}

isel[ju]++; ..and increment the counter.

}
j=; Now we can narrow the bounds to just
while (kk > isel[j]) { one bin, that is, by a factor of order
alo=sel[j]; m.
kk -= isel[j++];
}
ahi=sel[j];

Approximate timings: selip is about 10 times slower than select. Indeed,
for N in the range of ~ 10°, selip is about 1.5 times slower than a full sort with
sort, while select is about 6 times faster than sort. You should weigh time
against memory and convenience carefully.

Of course neither of the above routines should be used for the trivial cases of
finding the largest, or smallest, element in an array. Those cases, you code by hand
as simple for loops. There are also good ways to code the case where k is modest in
comparison to NV, so that extra memory of order k is not burdensome. An example
is to use the method of Heapsort (88.3) to make a single pass through an array of
length N while saving the m largest elements. The advantage of the heap structure
is that only log m, rather than m, comparisons are required every time a new element
is added to the candidate list. This becomes a real savings when m > O( VN), but
it never hurts otherwise and is easy to code. The following program gives the idea.

void hpsel(unsigned long m, unsigned long n, float arr[{], float heap[])
Returns in heap[1. .m] the largest m elements of the array arr[1..n], with heap[1] guaran-
teed to be the the mth largest element. The array arr is not altered. For efficiency, this routine
should be used only when m < n.
t

void sort(unsigned long n, float arr[]);

void nrerror(char error_text[]);

unsigned long i,j,k;

float swap;

if (m > n/2 || m < 1) nrerror("probable misuse of hpsel");
for (i=1;i<=m;i++) heap[i]=arr [i];
sort (m,heap) ; Create initial heap by overkill! We assume m <n.
for (i=m+1;i<=n;it++) { For each remaining element...
if (arr[i] > heap[1]) { Put it on the heap?
heap[1]=arr [i];
for (j=15;) {

Sift down.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

8.6 Determination of Equivalence Classes 345

k=j << 1;

if (k > m) break;

if (k != m && heap[k] > heap[k+1]) k++;
if (heap[j] <= heap[k]) break;
swap=heap[k] ;

heap [k] =heap[j] ;

heap [j]=swap;

jek;

CITED REFERENCES AND FURTHER READING:
Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), pp. 126ff. [1]

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Programming (Reading,
MA: Addison-Wesley).

8.6 Determination of Equivalence Classes

A number of techniques for sorting and searching relate to data structures whose details
are beyond the scope of this book, for example, trees, linked lists, etc. These structures and
their manipulations are the bread and butter of computer science, as distinct from numerical
analysis, and there is no shortage of books on the subject.

In working with experimental data, we have found that one particular such manipulation,
namely the determination of equivalence classes, arises sufficiently often to justify inclusion
here.

The problem is this: There are N “elements” (or “data points” or whatever), numbered
1,...,N. You are given pairwise information about whether elements are in the same
equivalence class of “sameness,” by whatever criterion happens to be of interest. For example,
you may have a list of facts like: “Element 3 and element 7 are in the same class; element
19 and element 4 are in the same class; element 7 and element 12 are in the same class, ... 2”
Alternatively, you may have a procedure, given the numbers of two elements j and k, for
deciding whether they are in the same class or different classes. (Recall that an equivalence
relation can be anything satisfying the RST properties: reflexive, symmetric, transitive. This
is compatible with any intuitive definition of “sameness.”)

The desired output is an assignment to each of the N elements of an equivalence class
number, such that two elements are in the same class if and only if they are assigned the
same class number.

Efficient algorithms work like this: Let F'(j) be the class or “family” number of element
j. Start off with each element in its own family, so that F(j) = j. The array F'(j) can be
interpreted as a tree structure, where F'(j) denotes the parent of 7. If we arrange for each family
to be its own tree, disjoint from all the other “family trees” then we can label each family
(equivalence class) by its most senior great-great-...grandparent. The detailed topology of
the tree doesn’t matter at all, as long as we graft each related element onto it somewhere.

Therefore, we process each elemental datum “‘j is equivalent to k” by (i) tracking j
up to its highest ancestor, (ii) tracking k up to its highest ancestor, (iii) giving j to k as a
new parent, or vice versa (it makes no difference). After processing all the relations, we go
through all the elements 7 and reset their F'(j)’s to their highest possible ancestors, which
then label the equivalence class

The following routine, based on Knuth [1], assumes that there are m elemental pieces
of information, stored in two arrays of length m, lista,listb, the interpretation being
that Lista[j] and listb[j], j=1...m, are the numbers of two elements which (we are
thus told) are related.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

346 Chapter 8. Sorting

void eclass(int nf[], int n, int lista[], int listb[], int m)
Given m equivalences between pairs of n individual elements in the form of the input arrays
lista[1..m] and listb[1..m], this routine returns in nf [1..n] the number of the equiv-
alence class of each of the n elements, integers between 1 and n (not all such integers used).
{

int 1,k,j;

for (k=1;k<=n;k++) nf [k]=k; Initialize each element its own class.

for (1=1;1<=m;1++) { For each piece of input information...
=lista[1];
while (nf[j] != j) j=nf(jl; Track first element up to its ancestor.
k=listb[1];
while (nf[k] != k) k=nf[k]; Track second element up to its ancestor.
if (j !=k) nf[jl=k; If they are not already related, make them
} so.
for (j=1;j<=n; j++) Final sweep up to highest ancestors.
while (nf[j] != nf(nf[j]]) nf[j]=nf(nf[jl];

Alternatively, we may be able to construct a function equiv (j,k) that returns a nonzero
(true) value if elements j and k are related, or a zero (false) value if they are not. Then we
want to loop over all pairs of elements to get the complete picture. D. Eardley has devised
a clever way of doing this while simultaneously sweeping the tree up to high ancestors in a
manner that keeps it current and obviates most of the final sweep phase:

void eclazz(int nf[], int n, int (*equiv)(int, int))
Given a user-supplied boolean function equiv which tells whether a pair of elements, each in
the range 1...n, are related, return in nf [1..n] equivalence class numbers for each element.
{

int kk,jj;

nf [1]
for (
nf [jj]=3j;
for (kk=1;kk<=(jj-1);kk++) { Loop over second element of all pairs.
nf [kk] =nf [nf [kk]]; Sweep it up this much.
if ((*equiv) (jj,kk)) nf (nf [nf [kk]]]=jj;
Good exercise for the reader to figure out why this much ancestry is necessary!

ji<en;jj++) { Loop over first element of all pairs.

+
+
for (jj=1;jj<=n;jjt++) nf(jjl=nf nf [jjll; Only this much sweeping is needed
} finally.

CITED REFERENCES AND FURTHER READING:

Knuth, D.E. 1968, Fundamental Algorithms, vol. 1 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §2.3.3. [1]

Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), Chapter 30.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 9. Root Finding and
Nonlinear Sets of Equations

9.0 Introduction

We now consider that most basic of tasks, solving equations numerically. While
most equations are born with both a right-hand side and a left-hand side, one
traditionally moves all terms to the left, leaving

f(x) =0 (9.0.1)

whose solution or solutions are desired. When there is only one independent variable,
the problem is one-dimensional, namely to find the root or roots of a function.

With more than one independent variable, more than one equation can be
satisfied simultaneously. You likely once learned the implicit function theorem
which (in this context) gives us the hope of satisfying N equations in N unknowns
simultaneously. Note that we have only hope, not certainty. A nonlinear set of
equations may have no (real) solutions at all. Contrariwise, it may have more than
one solution. The implicit function theorem tells us that “generically” the solutions
will be distinct, pointlike, and separated from each other. If, however, life is so
unkind as to present you with a nongeneric, i.e., degenerate, case, then you can get
a continuous family of solutions. In vector notation, we want to find one or more
N-dimensional solution vectors x such that

f(x) =0 (9.0.2)

where f is the N-dimensional vector-valued function whose components are the
individual equations to be satisfied simultaneously.

Don’t be fooled by the apparent notational similarity of equations (9.0.2) and
(9.0.1). Simultaneous solution of equations in N dimensions is much more difficult
than finding roots in the one-dimensional case. The principal difference between one
and many dimensions is that, in one dimension, it is possible to bracket or “trap” a root
between bracketing values, and then hunt it down like a rabbit. In multidimensions,
you can never be sure that the root is there at all until you have found it.

Except in linear problems, root finding invariably proceeds by iteration, and
this is equally true in one or in many dimensions. Starting from some approximate
trial solution, a useful algorithm will improve the solution until some predetermined
convergence criterion is satisfied. For smoothly varying functions, good algorithms

347

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

348 Chapter 9. Root Finding and Nonlinear Sets of Equations

will always converge, provided that the initial guess is good enough. Indeed one can
even determine in advance the rate of convergence of most algorithms.

It cannot be overemphasized, however, how crucially success depends on having
a good first guess for the solution, especially for multidimensional problems. This
crucial beginning usually depends on analysis rather than numerics. Carefully crafted
initial estimates reward you not only with reduced computational effort, but also
with understanding and increased self-esteem. Hamming’s motto, “the purpose of
computing is insight, not numbers,” is particularly apt in the area of finding roots.
You should repeat this motto aloud whenever your program converges, with ten-digit
accuracy, to the wrong root of a problem, or whenever it fails to converge because
there is actually no root, or because there is a root but your initial estimate was
not sufficiently close to it.

“This talk of insight is all very well, but what do I actually do?” For one-
dimensional root finding, it is possible to give some straightforward answers: You
should try to get some idea of what your function looks like before trying to find
its roots. If you need to mass-produce roots for many different functions, then you
should at least know what some typical members of the ensemble look like. Next,
you should always bracket a root, that is, know that the function changes sign in an
identified interval, before trying to converge to the root’s value.

Finally (this is advice with which some daring souls might disagree, but we
give it nonetheless) never let your iteration method get outside of the best bracketing
bounds obtained at any stage. We will see below that some pedagogically important
algorithms, such as secant method or Newton-Raphson, can violate this last constraint,
and are thus not recommended unless certain fixups are implemented.

Multiple roots, or very close roots, are a real problem, especially if the
multiplicity is an even number. In that case, there may be no readily apparent
sign change in the function, so the notion of bracketing a root — and maintaining
the bracket — becomes difficult. We are hard-liners: we nevertheless insist on
bracketing a root, even if it takes the minimum-searching techniques of Chapter 10
to determine whether a tantalizing dip in the function really does cross zero or not.
(You can easily modify the simple golden section routine of $10.1 to return early
if it detects a sign change in the function. And, if the minimum of the function is
exactly zero, then you have found a double root.)

As usual, we want to discourage you from using routines as black boxes without
understanding them. However, as a guide to beginners, here are some reasonable
starting points:

e Brent’s algorithm in 89.3 is the method of choice to find a bracketed root
of a general one-dimensional function, when you cannot easily compute
the function’s derivative. Ridders’ method (89.2) is concise, and a close
competitor.

e When you can compute the function’s derivative, the routine rtsafe in
§9.4, which combines the Newton-Raphson method with some bookkeep-
ing on bounds, is recommended. Again, you must first bracket your root.

e Roots of polynomials are a special case. Laguerre’s method, in §9.5,
is recommended as a starting point. Beware: Some polynomials are
ill-conditioned!

e Finally, for multidimensional problems, the only elementary method is
Newton-Raphson (§9.6), which works very well if you can supply a

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.0 Introduction 349

good first guess of the solution. Try it. Then read the more advanced
material in §9.7 for some more complicated, but globally more convergent,
alternatives.

Avoiding implementations for specific computers, this book must generally
steer clear of interactive or graphics-related routines. We make an exception right
now. The following routine, which produces a crude function plot with interactively
scaled axes, can save you a lot of grief as you enter the world of root finding.

#include <stdio.h>
#define ISCR 60
#define JSCR 21
#define BLANK ’ ”
#define ZERO ’-’
#define YY ’1’
#define XX ’-’
#define FF ’x’

Number of horizontal and vertical positions in display.

void scrsho(float (*fx) (float))
For interactive CRT terminal use. Produce a crude graph of the function £x over the prompted-
for interval x1,x2. Query for another plot until the user signals satisfaction.
{
int jz,j,i;
float ysml,ybig,x2,x1,x,dyj,dx,y[ISCR+1] ;
char scr[ISCR+1] [JSCR+1] ;

for G3) f

printf("\nEnter x1 x2 (x1l=x2 to stop):\n");

scanf("%f %f",&x1,&x2);

if (xt x2) break;

for (j=1;j<=JSCR; j++)
scr [1] [j]=scr[ISCR] [j]=YY;

for (i=2;i<=(ISCR-1);i++) {
scr [i] (1]=scr [i] [JSCR]=xx;
for (j=2;j<=(JSCR-1) ; j++)

scr [i] [j]=BLANK;

Query for another plot, quit
if x1=x2.

Fill vertical sides with character '1’.

Fill top, bottom with character '-’.
Fill interior with blanks.

}

dx=(x2-x1)/(ISCR-1) ;

x=x1;

ysml=ybig=0.0;

for (i=1;i<=ISCR;it+) {
ylid=(+£x) (x);
if (y[i] < ysml) ysml=y[i];
if (y[i] > ybig) ybig=y[il;
x += dx;

Limits will include 0.

Evaluate the function at equal intervals.
Find the largest and smallest val-
ues.

+

if (ybig == ysml) ybig=ysml+1.0;
dyj=(JSCR-1) /(ybig-ysm1) ;
jz=1-(int) (ysml*dyj) ;
for (i=1;i<=ISCR;it+) {
scr [i] [jz]=ZERO;
jai+(int) ((y[i]-ysml)*dyj) ;
scr [i] [j]=FF;
}
printf(" %10.3f ",ybig);

Be sure to separate top and bottom.

Note which row corresponds to 0.
Place an indicator at function height and

0.

for (i=1;i<=ISCR;it++) printf("%c",scr [i] [JSCR]);

printf("\n") ;
for (j=(JSCR-1);j>=2;j--) {
print£("%12s"," ");

Display.

for (i=1;i<=ISCR;it++) printf("%c",scr [i] [j]);

printf("\n");
}
printf(" %10.3f ",ysm1);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

350 Chapter 9. Root Finding and Nonlinear Sets of Equations

for (i=1;i<=ISCR;it++) printf("%c",scr [i] [1]);
printf ("\n");
printf("%8s %10.3f %44s 7%10.3f\n"," ",x1," ",x2);

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 5.

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapters 2, 7, and 14.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), Chapter 8.

Householder, A.S. 1970, The Numerical Treatment of a Single Nonlinear Equation (New York:
McGraw-Hill).

9.1 Bracketing and Bisection

We will say that a root is bracketed in the interval (a,b) if f(a) and f(b) have
opposite signs. If the function is continuous, then at least one root must lie in
that interval (the intermediate value theorem). If the function is discontinuous, but
bounded, then instead of a root there might be a step discontinuity which crosses
zero (see Figure 9.1.1). For numerical purposes, that might as well be a root, since
the behavior is indistinguishable from the case of a continuous function whose zero
crossing occurs in between two “adjacent” floating-point numbers in a machine’s
finite-precision representation. Only for functions with singularities is there the
possibility that a bracketed root is not really there, as for example

f(e) =— (9.1.1)

r—-C

Some root-finding algorithms (e.g., bisection in this section) will readily converge
to c in (9.1.1). Luckily there is not much possibility of your mistaking c, or any
number x close to it, for a root, since mere evaluation of | f(z)| will give a very
large, rather than a very small, result.

If you are given a function in a black box, there is no sure way of bracketing
its roots, or of even determining that it has roots. If you like pathological examples,
think about the problem of locating the two real roots of equation (3.0.1), which dips
below zero only in the ridiculously small interval of about « = 7 + 107°.

In the next chapter we will deal with the related problem of bracketing a
function’s minimum. There it is possible to give a procedure that always succeeds;
in essence, “Go downhill, taking steps of increasing size, until your function starts
back uphill.” There is no analogous procedure for roots. The procedure “go downhill
until your function changes sign,” can be foiled by a function that has a simple
extremum. Nevertheless, if you are prepared to deal with a “failure” outcome, this
procedure is often a good first start; success is usual if your function has opposite
signs in the limit « — +oo.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

351

9.1 Bracketing and Bisection

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer:

Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Awww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

X

~

XQ X3

a

(a)

>

(b)

(c)

at

(d)

Some situations encountered while root finding: (a) shows an isolated root a, bracketed

Figure 9.1.1.

(b) illustrates that there is not necessarily

by two points a and b at which the function has opposite signs

a sign change in the function near a double root (in fact, there is not necessarily a root!); (c) is a

pathological function with many roots; in (d) the function has opposite signs at points a and b, but the

points bracket a singularity, not a root.

352 Chapter 9. Root Finding and Nonlinear Sets of Equations

#include <math.h>
#define FACTOR 1.6
#tdefine NTRY 50

int zbrac(float (*func) (float), float *x1, float *x2)
Given a function func and an initial guessed range x1 to x2, the routine expands the range
geometrically until a root is bracketed by the returned values x1 and x2 (in which case zbrac
returns 1) or until the range becomes unacceptably large (in which case zbrac returns 0).
{

void nrerror(char error_text[]);

int j;

float f1,f2;

aif (*x1 == *x2) nrerror("Bad initial range in zbrac");
£1=(*func) (*x1);
£2=(+func) (*x2) ;
for (j=1;j<=NTRY;j++) {
if (£1*f2 < 0.0) return 1;
if (fabs(f1) < fabs(f2))
£1=(*func) (*x1 += FACTOR*(*x1-*x2)) ;
else
£2=(*func) (*x2 += FACTOR*(*x2-*x1));
+

return 0;

Alternatively, you might want to “look inward” on an initial interval, rather
than “look outward” from it, asking if there are any roots of the function f(a) in
the interval from x; to x2 when a search is carried out by subdivision into n equal
intervals. The following function calculates brackets for up to nb distinct intervals
which each contain one or more roots.

void zbrak(float (*fx)(float), float x1, float x2, int n, float xbi[],

float xb2[], int *nb)
Given a function £x defined on the interval from x1-x2 subdivide the interval into n equally
spaced segments, and search for zero crossings of the function. nb is input as the maximum num-
ber of roots sought, and is reset to the number of bracketing pairs xb1[1..nb], xb2[1..nb]
that are found.
{

int nbb,i;

float x,fp,fc,dx;

nbb=0;
dx=(x2-x1)/n; Determine the spacing appropriate to the mesh.
fp=(*fx) (x=x1);
for (i=1;i<=n;i++) { Loop over all intervals
fc=(*fx) (x += dx);
if (fc+fp <= 0.0) { If a sign change occurs then record values for the
xb1 [+4+nbb] =x-dx; bounds.
xb2 [nbb] =x;
if(*nb == nbb) return;
}
fp=fc;
+
nb = nbb;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.1 Bracketing and Bisection 353

Bisection Method

Once we know that an interval contains a root, several classical procedures are
available to refine it. These proceed with varying degrees of speed and sureness
towards the answer. Unfortunately, the methods that are guaranteed to converge plod
along most slowly, while those that rush to the solution in the best cases can also dash
rapidly to infinity without warning if measures are not taken to avoid such behavior.

The bisection method is one that cannot fail. It is thus not to be sneered at as
a method for otherwise badly behaved problems. The idea is simple. Over some
interval the function is known to pass through zero because it changes sign. Evaluate
the function at the interval’s midpoint and examine its sign. Use the midpoint to
replace whichever limit has the same sign. After each iteration the bounds containing
the root decrease by a factor of two. If after n iterations the root is known to
be within an interval of size €,,, then after the next iteration it will be bracketed
within an interval of size

Ent = €n/2 (9.1.2)

neither more nor less. Thus, we know in advance the number of iterations required
to achieve a given tolerance in the solution,

n=logy < (9.13)
where €o is the size of the initially bracketing interval, € is the desired ending
tolerance.

Bisection must succeed. If the interval happens to contain two or more roots,
bisection will find one of them. If the interval contains no roots and merely straddles
a singularity, it will converge on the singularity.

When a method converges as a factor (less than 1) times the previous uncertainty
to the first power (as is the case for bisection), it is said to converge linearly. Methods
that converge as a higher power,

€n4+1 = constant x (€,,)’ m>1 (9.1.4)

are said to converge superlinearly. In other contexts “linear” convergence would be
termed “exponential,” or “geometrical.” That is not too bad at all: Linear convergence
means that successive significant figures are won linearly with computational effort.
It remains to discuss practical criteria for convergence. It is crucial to keep in
mind that computers use a fixed number of binary digits to represent floating-point
numbers. While your function might analytically pass through zero, it is possible that
its computed value is never zero, for any floating-point argument. One must decide
what accuracy on the root is attainable: Convergence to within 10~° in absolute
value is reasonable when the root lies near 1, but certainly unachievable if the root
lies near 107°. One might thus think to specify convergence by a relative (fractional)
criterion, but this becomes unworkable for roots near zero. To be most general, the
routines below will require you to specify an absolute tolerance, such that iterations
continue until the interval becomes smaller than this tolerance in absolute units.
Usually you may wish to take the tolerance to be e(|x1| + |x2|)/2 where e is the
machine precision and x and x2 are the initial brackets. When the root lies near zero
you ought to consider carefully what reasonable tolerance means for your function.
The following routine quits after 40 bisections in any event, with 2~4° ~ 10-}”.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

354 Chapter 9. Root Finding and Nonlinear Sets of Equations

#include <math.h>
#define JMAX 40 Maximum allowed number of bisections.

float rtbis(float (*func) (float), float x1, float x2, float xacc)
Using bisection, find the root of a function func known to lie between x1 and x2. The root,
returned as rtbis, will be refined until its accuracy is txacc.

{
void nrerror(char error_text[]);
int j;
float dx,f,fmid,xmid,rtb;
f£=(*func) (x1);
fmid=(*func) (x2) ;
if (£*fmid >= 0.0) nrerror("Root must be bracketed for bisection in rtbis");
rtb = f < 0.0 ? (dx=x2-x1,x1) : (dx=x1-x2,x2); Orient the search so that £>0
for (j=1;j<=JMAX;j++) { lies at x+dx.
fmid=(*func) (xmid=rtb+(dx *= 0.5)); Bisection loop.
if (fmid <= 0.0) rtb=xmid;
if (fabs(dx) < xacc || fmid == 0.0) return rtb;
+
nrerror("Too many bisections in rtbis");
return 0.0; Never get here.
3

9.2 Secant Method, False Position Method,
and Ridders’ Method

For functions that are smooth near a root, the methods known respectively
as false position (or regula falsi) and secant method generally converge faster than
bisection. In both of these methods the function is assumed to be approximately
linear in the local region of interest, and the next improvement in the root is taken as
the point where the approximating line crosses the axis. After each iteration one of
the previous boundary points is discarded in favor of the latest estimate of the root.

The only difference between the methods is that secant retains the most recent
of the prior estimates (Figure 9.2.1; this requires an arbitrary choice on the first
iteration), while false position retains that prior estimate for which the function value
has opposite sign from the function value at the current best estimate of the root,
so that the two points continue to bracket the root (Figure 9.2.2). Mathematically,
the secant method converges more rapidly near a root of a sufficiently continuous
function. Its order of convergence can be shown to be the “golden ratio” 1.618...,
so that

lim |ex41| © const x |e,|'°"* (9.2.1)
k—-o0o

The secant method has, however, the disadvantage that the root does not necessarily
remain bracketed. For functions that are not sufficiently continuous, the algorithm
can therefore not be guaranteed to converge: Local behavior might send it off
towards infinity.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

355

9.2 Secant Method, False Position Method, and Ridders’ Method

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer: Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

Secant method. Extrapolation or interpolation lines (dashed) are drawn through the two

most recently evaluated points, whether or not they bracket the function. The points are numbered in

the order that they are used.

Figure 9.2.1.

False position method. Interpolation lines (dashed) are drawn through the most recent

Figure 9.2.2.

for many steps. False position

active”

points that bracket the root. In this example, point | thus remains

g
3g
£
3
8
2
ic}
g
FI
3
ie)
FI
3
&
=
I
§
g
2
3
S
EI
=
2
BS
a
£
g


356 Chapter 9. Root Finding and Nonlinear Sets of Equations

Figure 9.2.3. Example where both the secant and false position methods will take many iterations to
arrive at the true root. This function would be difficult for many other root-finding methods.

False position, since it sometimes keeps an older rather than newer function
evaluation, has a lower order of convergence. Since the newer function value will
sometimes be kept, the method is often superlinear, but estimation of its exact order
is not so easy.

Here are sample implementations of these two related methods. While these
methods are standard textbook fare, Ridders’ method, described below, or Brent’s
method, in the next section, are almost always better choices. Figure 9.2.3 shows the
behavior of secant and false-position methods in a difficult situation.

#include <math.h>
#define MAXIT 30 Set to the maximum allowed number of iterations.

float rtflsp(float (*func) (float), float x1, float x2, float xacc)
Using the false position method, find the root of a function func known to lie between x1 and
x2. The root, returned as rtflsp, is refined until its accuracy is +xacc.
{
void nrerror(char error_text[]);
int j;
float fl,fh,xl,xh,swap,dx,del,f,rtf;

£1=(+func) (x1);

fh=(*func) (x2); Be sure the interval brackets a root.
if (f1*fh > 0.0) nrerror("Root must be bracketed in rtflsp");
if (f1 < 0.0) { Identify the limits so that x1 corresponds to the low

x1: 3 side.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

9.2 Secant Method, False Position Method, and Ridders’ Method 357

fl=fh;
fh=swap;

3 j<=MAXIT; j++) { False position loop.
rtf=xl+dx*fl/(f1l-fh) ; Increment with respect to latest value.
f=(*func) (rtf) ;
if (f < 0.0) { Replace appropriate limit.
del=xl-rtf;
xl=rtf;
fl=f;
} else {
del=xh-rtf;
xh=rtf;
fh=f;

}
dx=xh-x1;
if (fabs(del) < xacc || f == 0.0) return rtf; Convergence.

+
nrerror("Maximum number of iterations exceeded in rtflsp");
return 0.0; Never get here.

#include <math.h>
#define MAXIT 30 Maximum allowed number of iterations.

float rtsec(float (*func) (float), float x1, float x2, float xacc)
Using the secant method, find the root of a function func thought to lie between x1 and x2.
The root, returned as rtsec, is refined until its accuracy is txacc.
t
void nrerror(char error_text[]);
int j;
float fl,f,dx,swap,xl,rts;

f1=(*func) (x1);
f=(*func) (x2) ;
if (fabs(f1) < fabs(f)) { Pick the bound with the smaller function value as
rts=x1; the most recent guess.
xl=x2;
swap=f1l;
fl=f;
f=swap;
} else {
xl=x1;
rts=x2;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

}
for (j=1;j<=MAXIT; j++) { Secant loop.
dx=(xl-rts)*f/(f-f1) ; Increment with respect to latest value.
xl=rts;
fl=f;
rts += dx;
f=(*func) (rts);
if (fabs(dx) < xacc || f == 0.0) return rts; Convergence.

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

+
nrerror("Maximum number of iterations exceeded in rtsec");
return 0.0; Never get here.

358 Chapter 9. Root Finding and Nonlinear Sets of Equations

Ridders’ Method

A powerful variant on false position is due to Ridders [1]. When a root is
bracketed between x, and x2, Ridders’ method first evaluates the function at the
midpoint 73 = (#1 + #2)/2. It then factors out that unique exponential function
which turns the residual function into a straight line. Specifically, it solves for a
factor e® that gives

f (v1) — 2f(as)e® + f(wa)e?? =0 (9.2.2)

This is a quadratic equation in e®, which can be solved to give

2 — fea) + sign[f(e2)] V F(es)? — Fle) Fl@2)
: J (x2)

Now the false position method is applied, not to the values f (x1), f(v3), f(z), but
to the values f(a1), f(x3)e®, f(x2)e?@, yielding a new guess for the root, 24. The
overall updating formula (incorporating the solution 9.2.3) is

(9.2.3)

sient (es) — f(x2)|f (ws) (9.24)
Fea)? = Fea) fra)

Equation (9.2.4) has some very nice properties. First, x4 is guaranteed to lie
in the interval (21,72), so the method never jumps out of its brackets. Second,
the convergence of successive applications of equation (9.2.4) is quadratic, that is,
m = 2 in equation (9.1.4). Since each application of (9.2.4) requires two function
evaluations, the actual order of the method is 2, not 2; but this is still quite
respectably superlinear: the number of significant digits in the answer approximately
doubles with each two function evaluations. Third, taking out the function’s “bend”
via exponential (that is, ratio) factors, rather than via a polynomial technique (e.g.,
fitting a parabola), turns out to give an extraordinarily robust algorithm. In both
reliability and speed, Ridders’ method is generally competitive with the more highly
developed and better established (but more complicated) method of Van Wijngaarden,
Dekker, and Brent, which we next discuss.

v4 = 434+ (43-21

#include <math.h>
#include "nrutil.h"
#define MAXIT 60

#define UNUSED (-1.11e30)

float zriddr(float (*func) (float), float x1, float x2, float xacc)
Using Ridders’ method, return the root of a function func known to lie between x1 and x2.
The root, returned as zriddr, will be refined to an approximate accuracy xacc.
{
int j;
float ans,fh,fl,fm,fnew,s,xh,xl,xm,xnew;

f1=(*func) (x1);
fh=(+func) (x2) ;
if ((f1 > 0.0 && fh < 0.0) || (f1 < 0.0 && fh > 0.0)) {
xl=x1;
xh=x2;
ans=UNUSED ; Any highly unlikely value, to simplify logic
below.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.3 Van Wijngaarden—Dekker—Brent Method 359

for (j=1;j<=MAXIT; j++) {
xm=0.5*(x1+xh) ;

fm=(*func) (xm) ; First of two function evaluations per it-
s=sqrt (fm*fm-f1*fh) ; eration.
if (s == 0.0) return ans;
xnew=xm+(xm-x1)*((f1 >= fh ? 1.0 : -1.0)*fm/s); Updating formula.
if (fabs(xnew-ans) <= xacc) return ans;
ans=xnew;
fnew=(*func) (ans) ; Second of two function evaluations per
if (fnew == 0.0) return ans; iteration.
if (SIGN(fm,fnew) != fm) { Bookkeeping to keep the root bracketed
xl=xm; on next iteration.
fl=fm;
xh=ans;
fh=fnew;
} else if (SIGN(fl,fnew) != f1) {
xh=ans;
fh=fnew;
} else if (SIGN(fh,fnew) != fh) {
xl=ans;
fl=fnew;

} else nrerror("never get here.");
if (fabs(xh-xl) <= xacc) return ans;

}
nrerror("zriddr exceed maximum iterations") ;
+
else {
if (f1 == 0.0) return x1;
if (fh == 0.0) return x2;
nrerror("root must be bracketed in zriddr.");
+
return 0.0; Never get here.

CITED REFERENCES AND FURTHER READING:

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §8.3.

Ostrowski, A.M. 1966, Solutions of Equations and Systems of Equations, 2nd ed. (New York:
Academic Press), Chapter 12.

Ridders, C.J.F. 1979, IEEE Transactions on Circuits and Systems, vol. CAS-26, pp. 979-980. [1]

9.3 Van Wijngaarden—Dekker—Brent Method

While secant and false position formally converge faster than bisection, one
finds in practice pathological functions for which bisection converges more rapidly.
These can be choppy, discontinuous functions, or even smooth functions if the
second derivative changes sharply near the root. Bisection always halves the interval,
while secant and false position can sometimes spend many cycles slowly pulling
distant bounds closer to a root. Ridders’ method does a much better job, but it
too can sometimes be fooled. Is there a way to combine superlinear convergence
with the sureness of bisection?

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

360 Chapter 9. Root Finding and Nonlinear Sets of Equations

Yes. We can keep track of whether a supposedly superlinear method is actually
converging the way it is supposed to, and, if it is not, we can intersperse bisection
steps so as to guarantee at least linear convergence. This kind of super-strategy
requires attention to bookkeeping detail, and also careful consideration of how
roundoff errors can affect the guiding strategy. Also, we must be able to determine
reliably when convergence has been achieved.

An excellent algorithm that pays close attention to these matters was developed
in the 1960s by van Wijngaarden, Dekker, and others at the Mathematical Center
in Amsterdam, and later improved by Brent [1]. For brevity, we refer to the final
form of the algorithm as Brent’s method. The method is guaranteed (by Brent)
to converge, so long as the function can be evaluated within the initial interval
known to contain a root.

Brent’s method combines root bracketing, bisection, and inverse quadratic
interpolation to converge from the neighborhood of a zero crossing. While the false
position and secant methods assume approximately linear behavior between two
prior root estimates, inverse quadratic interpolation uses three prior points to fit an
inverse quadratic function (a as a quadratic function of y) whose value at y = 0 is
taken as the next estimate of the root x. Of course one must have contingency plans
for what to do if the root falls outside of the brackets. Brent’s method takes care of
all that. If the three point pairs are [a, f(a)], [b, f(®)], [c, f(c)] then the interpolation
formula (cf. equation 3.1.1) is

y-f@ly-fOle y= fOlly- fla
[F(o) — F(@I[F() — FO) © [F(@) — FOMF@ — FO)

_ y= folly = f(@e

* [FG) = FONFO) — F@)

Setting y to zero gives a result for the next root estimate, which can be written as

(93.1)

r=b+P/Q (9.3.2)
where, in terms of
R=fwH/FO, S=fO/f@ T=f@/fO (9.3.3)
we have
P=S(I(R—T)(c—b) —(1— R)(b—a)] (9.3.4)
Q = (T-1)(R-1)(S —1) (9.3.5

In practice b is the current best estimate of the root and P/Q ought to be a “small”
correction. Quadratic methods work well only when the function behaves smoothly;
they run the serious risk of giving very bad estimates of the next root or causing
machine failure by an inappropriate division by a very small number (Q ~ 0).
Brent’s method guards against this problem by maintaining brackets on the root
and checking where the interpolation would land before carrying out the division.
When the correction P/Q would not land within the bounds, or when the bounds
are not collapsing rapidly enough, the algorithm takes a bisection step. Thus,

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

9.3 Van Wijngaarden—Dekker-—Brent Method 361

Brent’s method combines the sureness of bisection with the speed of a higher-order
method when appropriate. We recommend it as the method of choice for general
one-dimensional root finding where a function’s values only (and not its derivative
or functional form) are available.

#include <math.h>

#include "nrutil.h"

#define ITMAX 100 Maximum allowed number of iterations.
#define EPS 3.0e-8 Machine floating-point precision.

float zbrent(float (*func) (float), float x1, float x2, float tol)
Using Brent's method, find the root of a function func known to lie between x1 and x2. The
root, returned as zbrent, will be refined until its accuracy is tol.
{
int iter;
float a=x1,b=x2,c=x2,d,e,min1,min2;
float fa=(*func) (a) ,fb=(*func) (b) ,fc,p,q,r,s,tol1,xm;

if ((fa > 0.0 && fb > 0.0) || (fa < 0.0 && fb < 0.0))
nrerror("Root must be bracketed in zbrent");
fc=fb;
for (iter=1;iter<=ITMAX;iter++) {
if ((fb > 0.0 && fc > 0.0) || (fb < 0.0 && fc < 0.0)) {
Rename a, b, ¢ and adjust bounding interval

d.
}
if (fabs(fc) < fabs(fb)) {
a=b;
}
tol1=2.0*EPS*fabs (b)+0.5*tol; Convergence check.
xm=0.5*(c-b) ;

if (fabs(xm) <= tol1i || fb == 0.0) return b;
if (fabs(e) >= toll && fabs(fa) > fabs(fb)) {
s=fb/fa; Attempt inverse quadratic interpolation.
if (a==c) {
p=2.0*xm*s ;
q=1.0-s;
} else {
q=fa/fc;
r=fb/fc;
p=s*(2.0*xm*q* (q-r)-(b-a) *(r-1.0));
gq=(q-1.0)*(r-1.0)*(s-1.0) ;

if (p > 0.0) q = -q; Check whether in bounds.
p=fabs(p) ;

min1=3.0*xm*q-fabs (tol1*q) ;

min2=fabs(e*q) ;

if (2.0*p < (mini < min2 ? mini : min2)) {

e=d; Accept interpolation.
d=p/q;
} else {
d=xm; Interpolation failed, use bisection.
e=d;
3
} else { Bounds decreasing too slowly, use bisection.
d=xm;

e=d;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

362 Chapter 9. Root Finding and Nonlinear Sets of Equations

}

a=b; Move last best guess to a.

fa=fb;

if (fabs(d) > tol1) Evaluate new trial root.
bt=d;

else

b += SIGN(toli,xm) ;
fb=(*func) (b) ;

+
nrerror("Maximum number of iterations exceeded in zbrent");
return 0.0; Never get here.

CITED REFERENCES AND FURTHER READING:

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood Cliffs, NJ: Prentice-
Hall), Chapters 3, 4. [1]

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NU: Prentice-Hall), §7.2.

9.4 Newton-Raphson Method Using Derivative

Perhaps the most celebrated of all one-dimensional root-finding routines is New-
ton’s method, also called the Newton-Raphson method. This method is distinguished
from the methods of previous sections by the fact that it requires the evaluation
of both the function f(x), and the derivative f’(a), at arbitrary points x. The
Newton-Raphson formula consists geometrically of extending the tangent line at a
current point x; until it crosses zero, then setting the next guess x; +, to the abscissa
of that zero-crossing (see Figure 9.4.1). Algebraically, the method derives from the
familiar Taylor series expansion of a function in the neighborhood of a point,

fle+0)~ fle) + fayp4 Oey... (0.4.1)

For small enough values of 5, and for well-behaved functions, the terms beyond
linear are unimportant, hence f(a +6) = 0 implies

5--@. (9.4.2)
f(@)

Newton-Raphson is not restricted to one dimension. The method readily
generalizes to multiple dimensions, as we shall see in §9.6 and §9.7, below.

Far from a root, where the higher-order terms in the series are important, the
Newton-Raphson formula can give grossly inaccurate, meaningless corrections. For
instance, the initial guess for the root might be so far from the true root as to let
the search interval include a local maximum or minimum of the function. This can
be death to the method (see Figure 9.4.2). If an iteration places a trial guess near
such a local extremum, so that the first derivative nearly vanishes, then Newton-
Raphson sends its solution off to limbo, with vanishingly small hope of recovery.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

363

9.4 Newton-Raphson Method Using Derivative

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer: Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

f(x)

f(x)

Newton’s method extrapolates the local derivative to find the next estimate of the root. In

this example it works well and converges quadratically.

Figure 9.4.1.

Figure 9.4.2. Unfortunate case where Newton’s method encounters a local extremum and shoots off to

outer space. Here bracketing bounds, as in rtsafe, would save the day.

364 Chapter 9. Root Finding and Nonlinear Sets of Equations

f(x)

Figure 9.4.3. Unfortunate case where Newton’s method enters a nonconvergent cycle. This behavior
is often encountered when the function f is obtained, in whole or in part, by table interpolation. With
a better initial guess, the method would have succeeded.

Like most powerful tools, Newton-Raphson can be destructive used in inappropriate
circumstances. Figure 9.4.3 demonstrates another possible pathology.

Why do we call Newton-Raphson powerful? The answer lies in its rate of
convergence: Within a small distance « of x the function and its derivative are
approximately:

fla-+e) = f(x) +ef(a) tee ©) bese, oa)
f(@+e = fia) +ef"(a) +.
By the Newton-Raphson formula,
vig = Uj - fe (9.44)
so that
G41 = 6 S (vi) (9.4.5)

f(wi)”

When a trial solution x; differs from the true root by €;, we can use (9.4.3) to express
f (xi), f’(vi) in (9.4.4) in terms of €; and derivatives at the root itself. The result is
a recurrence relation for the deviations of the trial solutions

€in1 = —C2 aa ; . (9.46)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.4 Newton-Raphson Method Using Derivative 365

Equation (9.4.6) says that Newton-Raphson converges quadratically (cf. equa-
tion 9.2.3). Near a root, the number of significant digits approximately doubles
with each step. This very strong convergence property makes Newton-Raphson the
method of choice for any function whose derivative can be evaluated efficiently, and
whose derivative is continuous and nonzero in the neighborhood of a root.

Even where Newton-Raphson is rejected for the early stages of convergence
(because of its poor global convergence properties), it is very common to “polish
up” a root with one or two steps of Newton-Raphson, which can multiply by two
or four its number of significant figures!

For an efficient realization of Newton-Raphson the user provides a routine that
evaluates both f(z) and its first derivative f’(2) at the point x. The Newton-Raphson
formula can also be applied using a numerical difference to approximate the true
local derivative,

P(e) ~ Let) = fl) ) = F(@) (9.4.2)

This is not, however, a recommended procedure for the following reasons: (i) You
are doing two function evaluations per step, so at best the superlinear order of
convergence will be only /2. (ii) If you take dx too small you will be wiped out
by roundoff, while if you take it too large your order of convergence will be only
linear, no better than using the initial evaluation f’(xo) for all subsequent steps.
Therefore, Newton-Raphson with numerical derivatives is (in one dimension) always
dominated by the secant method of §9.2. (In multidimensions, where there is a
paucity of available methods, Newton-Raphson with numerical derivatives must be
taken more seriously. See §§9.6-9.7.)

The following function calls a user supplied function funcd(x,fn,df) which
supplies the function value as fn and the derivative as df. We have included input
bounds on the root simply to be consistent with previous root-finding routines:
Newton does not adjust bounds, and works only on local information at the point
x. The bounds are used only to pick the midpoint as the first guess, and to reject
the solution if it wanders outside of the bounds.

#include <math.h>
#define JMAX 20 Set to maximum number of iterations.
float rtnewt(void (*funcd) (float, float *, float *), float x1, float x2,

float xacc)
Using the Newton-Raphson method, find the root of a function known to lie in the interval
[x1,x2]. The root rtnewt will be refined until its accuracy is known within +xacc. funcd
is a user-supplied routine that returns both the function value and the first derivative of the
function at the point x.
{

void nrerror(char error_text[]);

int j;

float df,dx,f,rtn;

rtn=0.5*(x1+x2) ; Initial guess.
for (j=1;j<=JMAX;j++) {
(*funcd) (rtn,&f ,&df) ;
dx=f/df ;
rtn -= dx;
af ((x1-rtn)*(rtn-x2) < 0.0)
nrerror("Jumped out of brackets in rtnewt");

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

366 Chapter 9. Root Finding and Nonlinear Sets of Equations

if (fabs(dx) < xacc) return rtn; Convergence.
+
nrerror("Maximum number of iterations exceeded in rtnewt");
return 0.0; Never get here.

While Newton-Raphson’s global convergence properties are poor, it is fairly
easy to design a fail-safe routine that utilizes a combination of bisection and Newton-
Raphson. The hybrid algorithm takes a bisection step whenever Newton-Raphson
would take the solution out of bounds, or whenever Newton-Raphson is not reducing
the size of the brackets rapidly enough.

#include <math.h>
#define MAXIT 100 Maximum allowed number of iterations.

float rtsafe(void (*funcd) (float, float *, float *), float x1, float x2,

float xacc)
Using a combination of Newton-Raphson and bisection, find the root of a function bracketed
between x1 and x2. The root, returned as the function value rtsafe, will be refined until
its accuracy is known within txacc. funcd is a user-supplied routine that returns both the
function value and the first derivative of the function.
{

void nrerror(char error_text[]);

int j;

float df,dx,dxold,f,fh,fl;

float temp,xh,xl,rts;

(*funcd) (x1, &f1,&df) ;

(*funcd) (x2,&fh,&df) ;

if ((f1 > 0.0 && fh > 0.0) || (f1 < 0.0 && fh < 0.0))
nrerror("Root must be bracketed in rtsafe");

if (f1 == 0.0) return x1;

if (fh == 0.0) return x2;

if (f1 < 0.0) { Orient the search so that f(x1) < 0.
xl=x1;
xh=x2;
} else {
xh=x1;
xl=x2;
+
rts=0.5*(x1+x2); Initialize the guess for root,
dxold=fabs(x2-x1) ; the “stepsize before last,”
dx=dxold; and the last step.

(*funcd) (rts ,&f ,&df) ;

for (j=1;j<=MAXIT; j++) { Loop over allowed iterations.

aif ((((rts-xh) *df-f)*((rts-x1)*df-f) > 0.0) Bisect if Newton out of range,
|| (fabs(2.0*£) > fabs(dxold*df))) { or not decreasing fast enough.
dxold=dx;
dx=0.5*(xh-x1) ;
rts=xl+dx;
if (xl == rts) return rts; Change in root is negligible.
} else { Newton step acceptable. Take it.
dxold=dx;
dx=f /df ;
temp=rts;
rts -= dx;
if (temp == rts) return rts;
}
if (fabs(dx) < xacc) return rts; Convergence criterion.

(*funcd) (rts ,&f ,&df) ;
The one new function evaluation per iteration.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.4 Newton-Raphson Method Using Derivative 367

if (f < 0.0) Maintain the bracket on the root.
xl=rts;
else
xh=rts;
+
nrerror("Maximum number of iterations exceeded in rtsafe");
return 0.0; Never get here.

For many functions the derivative f’(x) often converges to machine accuracy
before the function f(x) itself does. When that is the case one need not subsequently
update f’(2). This shortcut is recommended only when you confidently understand
the generic behavior of your function, but it speeds computations when the derivative
calculation is laborious. (Formally this makes the convergence only linear, but if the
derivative isn’t changing anyway, you can do no better.)

Newton-Raphson and Fractals

An interesting sidelight to our repeated warnings about Newton-Raphson’s
unpredictable global convergence properties — its very rapid local convergence
notwithstanding — is to investigate, for some particular equation, the set of starting
values from which the method does, or doesn’t converge to a root.

Consider the simple equation

23-1=0 (9.4.8)

whose single real root is z = 1, but which also has complex roots at the other two
cube roots of unity, exp(+277/3). Newton’s method gives the iteration

241 = 23 - So (9.4.9)

Up to now, we have applied an iteration like equation (9.4.9) only for real
starting values zo, but in fact all of the equations in this section also apply in the
complex plane. We can therefore map out the complex plane into regions from which
a starting value zo, iterated in equation (9.4.9), will, or won’t, converge to z = 1.
Naively, we might expect to find a “basin of convergence” somehow surrounding
the root z = 1. We surely do not expect the basin of convergence to fill the whole
plane, because the plane must also contain regions that converge to each of the two
complex roots. In fact, by symmetry, the three regions must have identical shapes.
Perhaps they will be three symmetric 120° wedges, with one root centered in each?

Now take a look at Figure 9.4.4, which shows the result of a numerical
exploration. The basin of convergence does indeed cover 1/3 the area of the complex
plane, but its boundary is highly irregular — in fact, fractal. (A fractal, so called,
has self-similar structure that repeats on all scales of magnification.) How does this
fractal emerge from something as simple as Newton’s method, and an equation as
simple as (9.4.8)? The answer is already implicit in Figure 9.4.2, which showed how,
on the real line, a local extremum causes Newton’s method to shoot off to infinity.
Suppose one is slightly removed from such a point. Then one might be shot off
not to infinity, but — by luck — right into the basin of convergence of the desired

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

368 Chapter 9. Root Finding and Nonlinear Sets of Equations

Figure 9.4.4. The complex z plane with real and imaginary components in the range (—2,2). The
black region is the set of points from which Newton’s method converges to the root z = 1 of the equation
23 — 1 = 0. Its shape is fractal.

root. But that means that in the neighborhood of an extremum there must be a tiny,
perhaps distorted, copy of the basin of convergence — a kind of “one-bounce away”
copy. Similar logic shows that there can be “two-bounce” copies, “three-bounce”
copies, and so on. A fractal thus emerges.

Notice that, for equation (9.4.8), almost the whole real axis is in the domain of
convergence for the root z = 1. We say “almost” because of the peculiar discrete
points on the negative real axis whose convergence is indeterminate (see figure).
What happens if you start Newton’s method from one of these points? (Try it.)

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 2.

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), 88.4.

Ortega, J., and Rheinboldt, W. 1970, Iterative Solution of Nonlinear Equations in Several Vari-
ables (New York: Academic Press).

Mandelbrot, B.B. 1983, The Fractal Geometry of Nature (San Francisco: W.H. Freeman).

Peitgen, H.-O., and Saupe, D. (eds.) 1988, The Science of Fractal Images (New York: Springer-
Verlag).

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.5 Roots of Polynomials 369

9.5 Roots of Polynomials

Here we present a few methods for finding roots of polynomials. These will
serve for most practical problems involving polynomials of low-to-moderate degree
or for well-conditioned polynomials of higher degree. Not as well appreciated as it
ought to be is the fact that some polynomials are exceedingly ill-conditioned. The
tiniest changes in a polynomial’s coefficients can, in the worst case, send its roots
sprawling all over the complex plane. (An infamous example due to Wilkinson is
detailed by Acton [1].)

Recall that a polynomial of degree n will have n roots. The roots can be real
or complex, and they might not be distinct. If the coefficients of the polynomial are
real, then complex roots will occur in pairs that are conjugate, ie., if 7; = a+ bi
is a root then x2 = a — bi will also be a root. When the coefficients are complex,
the complex roots need not be related.

Multiple roots, or closely spaced roots, produce the most difficulty for numerical
algorithms (see Figure 9.5.1). For example, P(x) = ( — a)” has a double real root
at x = a. However, we cannot bracket the root by the usual technique of identifying
neighborhoods where the function changes sign, nor will slope-following methods
such as Newton-Raphson work well, because both the function and its derivative
vanish at a multiple root. Newton-Raphson may work, but slowly, since large
roundoff errors can occur. When a root is known in advance to be multiple, then
special methods of attack are readily devised. Problems arise when (as is generally
the case) we do not know in advance what pathology a root will display.

Deflation of Polynomials

When seeking several or all roots of a polynomial, the total effort can be
significantly reduced by the use of deflation. As each root r is found, the polynomial
is factored into a product involving the root and a reduced polynomial of degree
one less than the original, i.c., P(x) = ( — r)Q(x). Since the roots of Q are
exactly the remaining roots of P, the effort of finding additional roots decreases,
because we work with polynomials of lower and lower degree as we find successive
roots. Even more important, with deflation we can avoid the blunder of having our
iterative method converge twice to the same (nonmultiple) root instead of separately
to two different roots.

Deflation, which amounts to synthetic division, is a simple operation that acts
on the array of polynomial coefficients. The concise code for synthetic division by a
monomial factor was given in §5.3 above. You can deflate complex roots either by
converting that code to complex data type, or else — in the case of a polynomial with
real coefficients but possibly complex roots — by deflating by a quadratic factor,

[a — (a + ib)| [x — (a — ib)| = 2? — 2ax + (a? + B*) (9.5.1)

The routine poldiv in §5.3 can be used to divide the polynomial by this factor.
Deflation must, however, be utilized with care. Because each new root is known
with only finite accuracy, errors creep into the determination of the coefficients of
the successively deflated polynomial. Consequently, the roots can become more and
more inaccurate. It matters a lot whether the inaccuracy creeps in stably (plus or

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

370 Chapter 9. Root Finding and Nonlinear Sets of Equations

f(x) f(x)

(a) (b)

Figure 9.5.1. (a) Linear, quadratic, and cubic behavior at the roots of polynomials. Only under high
magnification (b) does it become apparent that the cubic has one, not three, roots, and that the quadratic
has two roots rather than none.

minus a few multiples of the machine precision at each stage) or unstably (erosion of
successive significant figures until the results become meaningless). Which behavior
occurs depends on just how the root is divided out. Forward deflation, where the
new polynomial coefficients are computed in the order from the highest power of x
down to the constant term, was illustrated in §5.3. This turns out to be stable if the
root of smallest absolute value is divided out at each stage. Alternatively, one can do
backward deflation, where new coefficients are computed in order from the constant
term up to the coefficient of the highest power of x. This is stable if the remaining
root of largest absolute value is divided out at each stage.

A polynomial whose coefficients are interchanged “end-to-end,” so that the
constant becomes the highest coefficient, etc., has its roots mapped into their
reciprocals. (Proof: Divide the whole polynomial by its highest power x” and
rewrite it as a polynomial in 1/.) The algorithm for backward deflation is therefore
virtually identical to that of forward deflation, except that the original coefficients are
taken in reverse order and the reciprocal of the deflating root is used. Since we will
use forward deflation below, we leave to you the exercise of writing a concise coding
for backward deflation (as in §5.3). For more on the stability of deflation, consult [2].

To minimize the impact of increasing errors (even stable ones) when using
deflation, it is advisable to treat roots of the successively deflated polynomials as
only tentative roots of the original polynomial. One then polishes these tentative roots
by taking them as initial guesses that are to be re-solved for, using the nondeflated
original polynomial P. Again you must beware lest two deflated roots are inaccurate
enough that, under polishing, they both converge to the same undeflated root; in that
case you gain a spurious root-multiplicity and lose a distinct root. This is detectable,
since you can compare each polished root for equality to previous ones from distinct
tentative roots. When it happens, you are advised to deflate the polynomial just
once (and for this root only), then again polish the tentative root, or to use Maehly’s
procedure (see equation 9.5.29 below).

Below we say more about techniques for polishing real and complex-conjugate

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.5 Roots of Polynomials 371

tentative roots. First, let’s get back to overall strategy.

There are two schools of thought about how to proceed when faced with a
polynomial of real coefficients. One school says to go after the easiest quarry, the
real, distinct roots, by the same kinds of methods that we have discussed in previous
sections for general functions, i.e., trial-and-error bracketing followed by a safe
Newton-Raphson as in rtsafe. Sometimes you are only interested in real roots, in
which case the strategy is complete. Otherwise, you then go after quadratic factors
of the form (9.5.1) by any of a variety of methods. One such is Bairstow’s method,
which we will discuss below in the context of root polishing. Another is Muller’s
method, which we here briefly discuss.

Muller’s Method

Muller’s method generalizes the secant method, but uses quadratic interpolation
among three points instead of linear interpolation between two. Solving for the
zeros of the quadratic allows the method to find complex pairs of roots. Given three
previous guesses for the root x;~2, 2:1, Zi, and the values of the polynomial P(x)
at those points, the next approximation x ;+1 is produced by the following formulas,

_ 8-1
1 to — 2
A=qP(x; 14+ q)P(ai-1) + PP(xi-:

gP (xi) — q(. + q)P(@i-1) + GF P(ai-2) (052)
B= (2g +1)P(ai) — (1+ 9)? Pair) +? P(ai-2)
C= (1+q)P(a)

followed by
2C
ig = Vj — (ji — Vie ———_—_—— 953
tint = i — (@ ~ 2-1) | Fea (253)

where the sign in the denominator is chosen to make its absolute value or modulus
as large as possible. You can start the iterations with any three values of x that you
like, e.g., three equally spaced values on the real axis. Note that you must allow
for the possibility of a complex denominator, and subsequent complex arithmetic,
in implementing the method.

Muller’s method is sometimes also used for finding complex zeros of analytic
functions (not just polynomials) in the complex plane, for example in the IMSL
routine ZANLY [3].

Laguerre’s Method

The second school regarding overall strategy happens to be the one to which we
belong. That school advises you to use one of a very small number of methods that
will converge (though with greater or lesser efficiency) to all types of roots: real,
complex, single, or multiple. Use such a method to get tentative values for all n
roots of your nth degree polynomial. Then go back and polish them as you desire.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

372 Chapter 9. Root Finding and Nonlinear Sets of Equations

Laguerre’s method is by far the most straightforward of these general, complex
methods. It does require complex arithmetic, even while converging to real roots;
however, for polynomials with all real roots, it is guaranteed to converge to a
root from any starting point. For polynomials with some complex roots, little is
theoretically proved about the method’s convergence. Much empirical experience,
however, suggests that nonconvergence is extremely unusual, and, further, can almost
always be fixed by a simple scheme to break a nonconverging limit cycle. (This is
implemented in our routine, below.) An example of a polynomial that requires this
cycle-breaking scheme is one of high degree (% 20), with all its roots just outside of
the complex unit circle, approximately equally spaced around it. When the method
converges on a simple complex zero, it is known that its convergence is third order.

In some instances the complex arithmetic in the Laguerre method is no
disadvantage, since the polynomial itself may have complex coefficients.

To motivate (although not rigorously derive) the Laguerre formulas we can note
the following relations between the polynomial and its roots and derivatives

In|Pa(z)| = In |x — ay| + Ina — ve] +...+ In |x — 2] (9.5.5)
din |P,(x 1
alFa(a)l _ p44 G (9.58)
dx 2-2, £2
@in|Pr(x)| 1 ; 1 ;
dx? “(a —ay)2 | (w@— 29)? |
Pp! 2 pl"
= |i@} _in®= 5.7
Ea Pr (95.7)

Starting from these relations, the Laguerre formulas make what Acton [1] nicely calls
“a rather drastic set of assumptions”: The root x; that we seek is assumed to be
located some distance a from our current guess x, while all other roots are assumed
to be located at a distance b

G-X1 =a 5; LC-aj=b i=2,3,. (9.5.8)
Then we can express (9.5.6), (9.5.7) as
1 on-l
=+=" = G (9.5.9)
att =H (9.5.10)
which yields as the solution for a
a de (9.5.11)

Gt /n—D(nl — G)

where the sign should be taken to yield the largest magnitude for the denominator.
Since the factor inside the square root can be negative, a can be complex. (A more
rigorous justification of equation 9.5.11 is in [4].)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.5 Roots of Polynomials 373

The method operates iteratively: For a trial value x, a is calculated by equation
(9.5.11). Then x — a becomes the next trial value. This continues until a is
sufficiently small.

The following routine implements the Laguerre method to find one root of a
given polynomial of degree m, whose coefficients can be complex. As usual, the first
coefficient a[0] is the constant term, while a[m] is the coefficient of the highest
power of x. The routine implements a simplified version of an elegant stopping
criterion due to Adams [5], which neatly balances the desire to achieve full machine
accuracy, on the one hand, with the danger of iterating forever in the presence of
roundoff error, on the other.

#include <math.h>

#include "complex.h"

#include "nrutil.h"

#define EPSS 1.0e-7

#define MR 8

#define MT 10

#tdefine MAXIT (MT*MR)

Here EPSS is the estimated fractional roundoff error. We try to break (rare) limit cycles with
MR different fractional values, once every MT steps, for MAXIT total allowed iterations.

void laguer(fcomplex a[], int m, fcomplex *x, int *its)
Given the degree m and the m+1 complex coefficients a[0. .m] of the polynomial Sy, ali]x’,
and given a complex value x, this routine improves x by Laguerre’s method until it converges,
within the achievable roundoff limit, to a root of the given polynomial. The number of iterations
taken is returned as its.
{
int iter,j;
float abx,abp,abm,err;
fcomplex dx,x1,b,d,f,g,h,sq,gp,gm,g2;
static float frac[MR+1] = {0.0,0.5,0.25,0.75,0.13,0.38,0.62,0.88,1.0};
Fractions used to break a limit cycle.

for (iter=1;iter<=MAXIT;iter++) { Loop over iterations up to allowed maximum.

err=Cabs(b) ;

d=f=Complex(0.0,0.0);

abx=Cabs (*x) ;

for (j=m-1;j>=0;j--) { Efficient computation of the polynomial and
£=Cadd(Cmul (*x,f) ,d); its first two derivatives. f stores P’” /2.
d=Cadd(Cmul (*x,d) ,b) ;
b=Cadd(Cmul(*x,b) ,a[j]);
err=Cabs(b)t+abx*err;

}

err *= EPSS;

Estimate of roundoff error in evaluating polynomial.

if (Cabs(b) <= err) return; We are on the root.

g=Cdiv(d,b); The generic case: use Laguerre’s formula.

g2=Cmul (g,g);
h=Csub(g2,RCmul(2.0,Cdiv(£,b)));
sq=Csqrt(RCmul((float) (m-1),Csub(RCmul((float) m,h),g2)));
gp=Cadd(g,sq);
gm=Csub(g,sq) ;
abp=Cabs (gp) ;
abm=Cabs (gm) ;
if (abp < abm) gp=gm;
dx=((FMAX(abp,abm) > 0.0 ? Cdiv(Complex((float) m,0.0),gp)
: RCmul (1+abx ,Complex(cos((float) iter) ,sin((float)iter)))));
x1=Csub (*x,dx) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

374 Chapter 9. Root Finding and Nonlinear Sets of Equations

if (x->r xi.r && x->i == x1.i) return; Converged.

aif (iter % MT) *x=x1;

else *x=Csub(*x,RCmul (frac [iter/MT] ,dx));

Every so often we take a fractional step, to break any limit cycle (itself a rare occur-

rence).
+
nrerror("too many iterations in laguer");
Very unusual — can occur only for complex roots. Try a different starting guess for the
root.
return;

Here is a driver routine that calls laguer in succession for each root, performs
the deflation, optionally polishes the roots by the same Laguerre method — if you
are not going to polish in some other way — and finally sorts the roots by their real
parts. (We will use this routine in Chapter 13.)

#include <math.h>

#include "complex.h"

#define EPS 2.0e-6

#define MAXM 100

A small number, and maximum anticipated value of m.

void zroots(fcomplex a[], int m, fcomplex roots[], int polish)

Given the degree m and the m+1 complex coefficients a[0. .m] of the polynomial )77"_p a(i)2%,
this routine successively calls laguer and finds all m complex roots in roots[1..m]. The
boolean variable polish should be input as true (1) if polishing (also by Laguerre’s method)
is desired, false (0) if the roots will be subsequently polished by other means.

void laguer(fcomplex a[], int m, fcomplex *x, int *its);
int i,its,j,jj;
fcomplex x,b,c,ad[MAXM] ;

for (j= =m;jt+) ad[jl=aljl; Copy of coefficients for successive deflation.

for (j=m;j>=1;j--) { Loop over each root to be found.
x=Complex(0.0,0.0); Start at zero to favor convergence to small-
laguer (ad, j,&x,&its) ; est remaining root, and find the root.

if (fabs(x.i) <= 2.0*EPS*fabs(x.r)) x.i=0.0;
roots [j]=x;
b=ad[j] 5 Forward deflation.

for (jj=j-1;55>=0;jj--) {
c=ad[jjl;
ad[jjl=b;
b=Cadd(Cmul(x,b) ,c) ;
}
+
if (polish)
for (j=1;j<=m; j++) Polish the roots using the undeflated coeffi-
laguer(a,m,&roots[j] ,&its); cients.
for (j=2;j<=m;jt++) { Sort roots by their real parts by straight in-
x=roots[j]; sertion.
for (i=j-1;i>=1;i--) {
if (roots[i].r <= x.r) break;
roots [it1]=roots [i];
}
roots [iti]=x;
+

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.5 Roots of Polynomials 375

Eigenvalue Methods

The eigenvalues of a matrix A are the roots of the “characteristic polynomial”
P(x) = det[A — xl]. However, as we will see in Chapter 11, root-finding is not
generally an efficient way to find eigenvalues. Turning matters around, we can
use the more efficient eigenvalue methods that are discussed in Chapter 11 to find
the roots of arbitrary polynomials. You can easily verify (see, e.g., [6]) that the
characteristic polynomial of the special m x m companion matrix

_Gm-1  _Gm-2 a tg
am am am am
1 0 =. 0 0
A= 0 Loo. 0 0 (9.5.12)
0 0 1 0

is equivalent to the general polynomial

m

P(x) = eax! (9.5.13)

i=0

If the coefficients a; are real, rather than complex, then the eigenvalues of A can be
found using the routines balanc and hqr in §$11.5—11.6 (see discussion there). This
method, implemented in the routine zrhqr following, is typically about a factor 2
slower than zroots (above). However, for some classes of polynomials, it is a more
robust technique, largely because of the fairly sophisticated convergence methods
embodied in hgqr. If your polynomial has real coefficients, and you are having
trouble with zroots, then zrhqr is a recommended alternative.

#include "nrutil.h"
#define MAXM 50

void zrhqr(float a[], int m, float rtr[], float rti[])

Find all the roots of a polynomial with real coefficients, an a(i)a*, given the degree m
and the coefficients a[0..m]. The method is to construct an upper Hessenberg matrix whose
eigenvalues are the desired roots, and then use the routines balanc and hqr. The real and
imaginary parts of the roots are returned in rtr[1..m] and rti[1..m], respectively.

void balanc(float **a, int n);

void hqr(float **a, int n, float wr[], float wil]);
int j,k;

float **hess,xr,xi;

hess=matrix(1,MAXM,1,MAXM) ;
if (m > MAXM || alm] == 0.0) nrerror("bad args in zrhqr");
for (k=1;k<=m;k++) { Construct the matrix.

hess[1] [k] = -alm-k]/al[m] ;

for (j=2;j<=m;j++) hess[j] [k]=0.0;

if (k != m) hess[k+1] [k]=1.0;

+

balanc (hess ,m) ; Find its eigenvalues.

hqr (hess ,m,rtr,rti);

for (j=2;j<=m;jt++) { Sort roots by their real parts by straight insertion.

xr=rtr[j];
xisrtilj];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

376 Chapter 9. Root Finding and Nonlinear Sets of Equations

for (k=j-1;k>=1;k--) {
if (rtr{k] <= xr) break;
rtr(k+1]=rtr([k] ;
rti(k+1]=rti[k] ;
}
rtr[k+1]=xr;
rti([k+1]=xi;
}
free_matrix(hess,1,MAXM,1,MAXM) ;

Other Sure-Fire Techniques

The Jenkins-Traub method has become practically a standard in black-box
polynomial root-finders, e.g., in the IMSL library [3]. The method is too complicated
to discuss here, but is detailed, with references to the primary literature, in [4].

The Lehmer-Schur algorithm is one of a class of methods that isolate roots in
the complex plane by generalizing the notion of one-dimensional bracketing. It is
possible to determine efficiently whether there are any polynomial roots within a
circle of given center and radius. From then on it is a matter of bookkeeping to
hunt down all the roots by a series of decisions regarding where to place new trial
circles. Consult [1] for an introduction.

Techniques for Root-Polishing

Newton-Raphson works very well for real roots once the neighborhood of
a root has been identified. The polynomial and its derivative can be efficiently
simultaneously evaluated as in §5.3. For a polynomial of degree n with coefficients
c[0]...c[n], the following segment of code embodies one cycle of Newton-
Raphson:

p=c[n] *x+c[n-1];
pi=c[n] ;
for(i=n-2;i>=0;i--) {
pi=ptp1*x;
p=c [i]+p*x;
+
if (pl == 0.0) nrerror("derivative should not vanish") ;
x -= p/p1;

Once all real roots of a polynomial have been polished, one must polish the
complex roots, either directly, or by looking for quadratic factors.

Direct polishing by Newton-Raphson is straightforward for complex roots if the
above code is converted to complex data types. With real polynomial coefficients,
note that your starting guess (tentative root) must be off the real axis, otherwise
you will never get off that axis — and may get shot off to infinity by a minimum
or maximum of the polynomial.

For real polynomials, the alternative means of polishing complex roots (or, for that
matter, double real roots) is Bairstow’s method, which seeks quadratic factors. The advantage

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.5 Roots of Polynomials 377

of going after quadratic factors is that it avoids all complex arithmetic. Bairstow’s method
seeks a quadratic factor that embodies the two roots « = a + ib, namely

a? —2axr + (a? +0’) = 2? + Br+C (9.5.14)
In general if we divide a polynomial by a quadratic factor, there will be a linear remainder
P(x) = (#7 + Br + C)Q(x) + Rr+ 8. (9.5.15)

Given B and C, R and S can be readily found, by polynomial division (§5.3). We can
consider R and S' to be adjustable functions of B and C, and they will be zero if the quadratic
factor is a divisor of P(x).

In the neighborhood of a root a first-order Taylor series expansion approximates the
variation of R,S with respect to small changes in B,C
OR OR
(B+ 6B, 6C) = R(B, 6B 6 9.5.16
R( C +80) = R(B,C) + 55 dB + 5580 (9.5.16)
+ os os
S(B+6B,C + 6C) = S(B,C) ap oP ace’ (9.5.17)

To evaluate the partial derivatives, consider the derivative of (9.5.15) with respect to C’. Since
P(x) is a fixed polynomial, it is independent of C, hence

aQ | aR, as

24 i i i
0=(a° + Ba 5G + Q(x) 4 a0 + aa (9.5.18)
which can be rewritten as
; 2 j 0Q AR, Os
-—Q(x) = (a + Br CVG + 5a? + 5G (9.5.19)

Similarly, P(x) is independent of B, so differentiating (9.5.15) with respect to B gives

dQ OR, as
5B t+ aB?+t OB (9.5.20)

Now note that equation (9.5.19) matches equation (9.5.15) in form. Thus if we perform a
second synthetic division of P(2),i.e.,a division of Q(x), yielding a remainder Rix+ $1, then
OR os

aoa TR 5G = 5 (9.5.21)

To get the remaining partial derivatives, evaluate equation (9.5.20) at the two roots of the
quadratic, x; and x—. Since

Q(r+) = Rive + Si (9.5.22)
we get
OR os
Bet aR = ee (Ries + $1) (9.5.23)
OR os +
ap’ + 9B —a_(Rix_ + $1) (9.5.24)
Solve these two equations for the partial derivatives, using
t,+2_=-B tyr =C (9.5.25)
and find
OR os
AB 7 BRi- Si AB 7 CR, (9.5.26)

Bairstow’s method now consists of using Newton-Raphson in two dimensions (which is
actually the subject of the next section) to find a simultaneous zero of R and S. Synthetic
division is used twice per cycle to evaluate R, S and their partial derivatives with respect to
B,C. Like one-dimensional Newton-Raphson, the method works well in the vicinity of a root
pair (real or complex), but it can fail miserably when started at a random point. We therefore
recommend it only in the context of polishing tentative complex roots.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

378 Chapter 9. Root Finding and Nonlinear Sets of Equations

#include <math.h>

#include "nrutil.h"

#define ITMAX 20 At most ITMAX iterations.
#define TINY 1.0e-6

void qroot(float p[], int n, float *b, float *c, float eps)

Given n+1 coefficients p[0..n] of a polynomial of degree n, and trial values for the coefficients
of a quadratic factor x*x+b*x+c, improve the solution until the coefficients b,c change by less
than eps. The routine poldiv §5.3 is used.

{
void poldiv(float u[], int n, float v[], int nv, float q[], float r[]);
int iter;
float sc,sb,s,rc,rb,r,dv,delc,delb;
float *q,*qq,*rem;
float a[3];
q=vector(0,n);
qq=vector(0,n);
rem=vector(0,n);
da[2]=1.0;
for (iter=1;iter<=ITMAX;iter++) {
d[1]=(*b) ;
d[0]=(*c) ;
poldiv(p,n,d,2,q,rem) ;
s=rem[0] ; First division r,s.
r=rem[1];
poldiv(q, (n-1) ,d,2,qq,rem) ;
sb = -(*c)*(re = -rem[1]); Second division partial r,s with respect to
rb = -(*b)*rc+(sc = -rem[0]); c.
dv=1.0/(sb*rc-sc*rb) ; Solve 2x2 equation.
delb=(r*sc-s*rc) *dv;
delc=(-r*sb+s*rb) *dv;
*b (delb=(r*sc-s*rc) *dv) ;
*c += (delc=(-r+#sb+s*rb) *dv) ;
if ((fabs(delb) <= eps*fabs(*b) || fabs(*b) < TINY)
&& (fabs(delc) <= eps*fabs(*c) || fabs(*c) < TINY)) {
free_vector(rem,0,n); Coefficients converged.
free_vector(qq,0,n);
free_vector(q,0,n);
return;
}
+
nrerror("Too many iterations in routine qroot");
3

We have already remarked on the annoyance of having two tentative roots
collapse to one value under polishing. You are left not knowing whether your
polishing procedure has lost a root, or whether there is actually a double root,
which was split only by roundoff errors in your previous deflation. One solution
is deflate-and-repolish; but deflation is what we are trying to avoid at the polishing
stage. An alternative is Maehly’s procedure. Maehly pointed out that the derivative
of the reduced polynomial

P,(x) = — ay = - (95.27)
can be written as
, i
Pi(c) P'(x) = PO) _ (ya)! (0.5.28)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.6 Newton-Raphson Method for Nonlinear Systems of Equations 379

Hence one step of Newton-Raphson, taking a guess x, into a new guess rx%41,
can be written as

ey, = Ie “ 9.5.29
en Pian) — Pn) Dt (@e — 21)

This equation, if used with i ranging over the roots already polished, will prevent a
tentative root from spuriously hopping to another one’s true root. It is an example
of so-called zero suppression as an alternative to true deflation.

Muller’s method, which was described above, can also be useful at the polishing
stage.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 7. [1]

Peters G., and Wilkinson, J.H. 1971, Journal of the Institute of Mathematics and its Applications,
vol. 8, pp. 16-35. [2]

IMSL Math/Library Users Manual (IMSL Inc., 2500 CityWest Boulevard, Houston TX 77042). [3]

Ralston, A., and Rabinowitz, P. 1978, A First Course in Numerical Analysis, 2nd ed. (New York:
McGraw-Hill), §8.9-8.13. [4]

Adams, D.A. 1967, Communications of the ACM, vol. 10, pp. 655-658. [5]

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA: Addison-
Wesley), §4.4.3. [6]

Henrici, P. 1974, Applied and Computational Complex Analysis, vol. 1 (New York: Wiley).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§§5.5-5.9.

9.6 Newton-Raphson Method for Nonlinear
Systems of Equations

We make an extreme, but wholly defensible, statement: There are no good, gen-
eral methods for solving systems of more than one nonlinear equation. Furthermore,
it is not hard to see why (very likely) there never will be any good, general methods:
Consider the case of two dimensions, where we want to solve simultaneously

f(x,y)

=0
(9.6.1)

g(x,y) = 0
The functions f and g are two arbitrary functions, each of which has zero
contour lines that divide the (2, y) plane into regions where their respective function
is positive or negative. These zero contour boundaries are of interest to us. The
solutions that we seek are those points (if any) that are common to the zero contours
of f and g (see Figure 9.6.1). Unfortunately, the functions f and g have, in general,
no relation to each other at all! There is nothing special about a common point from
either f’s point of view, or from g’s. In order to find all common points, which are

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

380 Chapter 9. Root Finding and Nonlinear Sets of Equations

7 T
no root here! 1 !
rs ~—— two roots here
1
\ g pos
\
of}
Nou
y aa
N
\
. ---
No Leer
eon
eor7 x
N
x g neg
N
N
s
s
~S
Le Se g pos
a
,
¢
73
x
Figure 9.6.1. | Solution of two nonlinear equations in two unknowns. Solid curves refer to f(x,y),

dashed curves to g(a, y). Each equation divides the (x,y) plane into positive and negative regions,
bounded by zero curves. The desired solutions are the intersections of these unrelated zero curves. The
number of solutions is a priori unknown.

the solutions of our nonlinear equations, we will (in general) have to do neither more
nor less than map out the full zero contours of both functions. Note further that
the zero contours will (in general) consist of an unknown number of disjoint closed
curves. How can we ever hope to know when we have found all such disjoint pieces?

For problems in more than two dimensions, we need to find points mutually
common to N unrelated zero-contour hypersurfaces, each of dimension N — 1. You
see that root finding becomes virtually impossible without insight! You will almost
always have to use additional information, specific to your particular problem, to
answer such basic questions as, “Do I expect a unique solution?” and “Approximately
where?” Acton [1] has a good discussion of some of the particular strategies that
can be tried.

In this section we will discuss the simplest multidimensional root finding
method, Newton-Raphson. This method gives you a very efficient means of
converging to a root, if you have a sufficiently good initial guess. It can also
spectacularly fail to converge, indicating (though not proving) that your putative
root does not exist nearby. In §9.7 we discuss more sophisticated implementations
of the Newton-Raphson method, which try to improve on Newton-Raphson’s poor
global convergence. A multidimensional generalization of the secant method, called
Broyden’s method, is also discussed in §9.7.

A typical problem gives NV functional relations to be zeroed, involving variables
avj,i = 1,2,...,N:

F,(a1,%2,..., =0 i=1,2,...,N. (9.6.2)

We let x denote the entire vector of values x; and F denote the entire vector of
functions F;. In the neighborhood of x, each of the functions F’'; can be expanded

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.6 Newton-Raphson Method for Nonlinear Systems of Equations 381

in Taylor series

N .
F(x + dx) = Fi(x) 4 ee * §2rj + O(5x?). (9.6.3)
j=l 7?

The matrix of partial derivatives appearing in equation (9.6.3) is the Jacobian matrix J:

EF,
J, a *. (9.6.4)
Ox;
In matrix notation equation (9.6.3) is
F(x + 6x) = F(x) + J - 6x + O(6x”). (9.6.5)

By neglecting terms of order 6x? and higher and by setting F(x + 6x) = 0, we
obtain a set of linear equations for the corrections 6x that move each function closer
to zero simultaneously, namely

J- 6x =—-F. (9.6.6)

Matrix equation (9.6.6) can be solved by LU decomposition as described in
§2.3. The corrections are then added to the solution vector,

Xnew = Xold + 6x (9.6.7)

and the process is iterated to convergence. In general it is a good idea to check the
degree to which both functions and variables have converged. Once either reaches
machine accuracy, the other won’t change.

The following routine mnewt performs ntrial iterations starting from an initial
guess at the solution vector x[1..n]. Iteration stops if either the sum of the
magnitudes of the functions F’; is less than some tolerance tolf, or the sum of the
absolute values of the corrections to 6x; is less than some tolerance tolx. mnewt
calls a user supplied function usrfun which must provide the function values F and
the Jacobian matrix J. If J is difficult to compute analytically, you can try having
usrfun call the routine fdjac of §9.7 to compute the partial derivatives by finite
differences. You should not make ntrial too big; rather inspect to see what is
happening before continuing for some further iterations.

#include <math.h>
#include "nrutil.h"

void usrfun(float *x,int n,float *fvec,float **fjac);
#define FREERETURN {free_matrix(fjac,1,n,1,n);free_vector(fvec,1,n);\
free_vector(p,1,n) ;free_ivector(indx,1,n) ;return;}

void mnewt(int ntrial, float x[], int n, float tolx, float tolf)
Given an initial guess x[1..n] for a root in n dimensions, take ntrial Newton-Raphson steps
to improve the root. Stop if the root converges in either summed absolute variable increments
tolx or summed absolute function values tolf.
{

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

382 Chapter 9. Root Finding and Nonlinear Sets of Equations

int k,i,*indx;
float errx,errf,d,*fvec,**fjac,*p;

indx=ivector(1,n);

p=vector(1,n);

fvec=vector(1,n);

fjac=matrix(1,n,1,n);

for (k=1;k<=ntrial;k++) {
usrfun(x,n,fvec,fjac); User function supplies function values at x in
errf=0.0; fvec and Jacobian matrix in fjac.
for (i=1;i<=n;it++) errf += fabs(fvec[i]); Check function convergence.
if (errf <= tolf) FREERETURN
for (i=1;i<=n;it+) pli] = -fvec[i]; Right-hand side of linear equations.
ludemp(fjac,n, indx, &d) ; Solve linear equations using LU decomposition.
lubksb(fjac,n,indx,p) ;

errx=0.0; Check root convergence.
for (i=1;i<=n;it+) { Update solution.
errx += fabs(p[i]);
x(i] += pli];
}
if (errx <= tolx) FREERETURN
3
FREERETURN

Newton’s Method versus Minimization

In the next chapter, we will find that there are efficient general techniques for
finding a minimum of a function of many variables. Why is that task (relatively)
easy, while multidimensional root finding is often quite hard? Isn’t minimization
equivalent to finding a zero of an V-dimensional gradient vector, not so different from
zeroing an N-dimensional function? No! The components of a gradient vector are not
independent, arbitrary functions. Rather, they obey so-called integrability conditions
that are highly restrictive. Put crudely, you can always find a minimum by sliding
downhill on a single surface. The test of “downhillness” is thus one-dimensional.
There is no analogous conceptual procedure for finding a multidimensional root,
where “downhill” must mean simultaneously downhill in N separate function spaces,
thus allowing a multitude of trade-offs, as to how much progress in one dimension
is worth compared with progress in another.

It might occur to you to carry out multidimensional root finding by collapsing
all these dimensions into one: Add up the sums of squares of the individual functions
F;, to get a master function F’ which (i) is positive definite, and (ii) has a global
minimum of zero exactly at all solutions of the original set of nonlinear equations.
Unfortunately, as you will see in the next chapter, the efficient algorithms for finding
minima come to rest on global and local minima indiscriminately. You will often
find, to your great dissatisfaction, that your function F’ has a great number of local
minima. In Figure 9.6.1, for example, there is likely to be a local minimum wherever
the zero contours of f and g make a close approach to each other. The point labeled
M is such a point, and one sees that there are no nearby roots.

However, we will now see that sophisticated strategies for multidimensional
root finding can in fact make use of the idea of minimizing a master function F’, by
combining it with Newton’s method applied to the full set of functions F’;. While
such methods can still occasionally fail by coming to rest on a local minimum of

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 383

F’, they often succeed where a direct attack via Newton’s method alone fails. The
next section deals with these methods.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 14. [1]

Ostrowski, A.M. 1966, Solutions of Equations and Systems of Equations, 2nd ed. (New York:
Academic Press).

Ortega, J., and Rheinboldt, W. 1970, Iterative Solution of Nonlinear Equations in Several Vari-
ables (New York: Academic Press).

9.7 Globally Convergent Methods for Nonlinear
Systems of Equations

We have seen that Newton’s method for solving nonlinear equations has an
unfortunate tendency to wander off into the wild blue yonder if the initial guess is
not sufficiently close to the root. A global method is one that converges to a solution
from almost any starting point. In this section we will develop an algorithm that
combines the rapid local convergence of Newton’s method with a globally convergent
strategy that will guarantee some progress towards the solution at each iteration.
The algorithm is closely related to the quasi-Newton method of minimization which
we will describe in §10.7.

Recall our discussion of $9.6: the Newton step for the set of equations

F(x) =0 (9.7.1)
is
Xnew = Xold + OX (9.7.2)
where
ox=-J-1-F (9.7.3)

Here J is the Jacobian matrix. How do we decide whether to accept the Newton step
5x? A reasonable strategy is to require that the step decrease |F|” = F - F. This is
the same requirement we would impose if we were trying to minimize

f= oF -F (9.7.4)

(The 4 is for later convenience.) Every solution to (9.7.1) minimizes (9.7.4), but
there may be local minima of (9.7.4) that are not solutions to (9.7.1). Thus, as
already mentioned, simply applying one of our minimum finding algorithms from
Chapter 10 to (9.7.4) is not a good idea.

To develop a better strategy, note that the Newton step (9.7.3) is a descent
direction for f:

Vf-dx=(F-J)-(-J7!-F) =-F-F <0 (9.7.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

384 Chapter 9. Root Finding and Nonlinear Sets of Equations

Thus our strategy is quite simple: We always first try the full Newton step,
because once we are close enough to the solution we will get quadratic convergence.
However, we check at each iteration that the proposed step reduces f. If not, we
backtrack along the Newton direction until we have an acceptable step. Because the
Newton step is a descent direction for f, we are guaranteed to find an acceptable step
by backtracking. We will discuss the backtracking algorithm in more detail below.
Note that this method essentially minimizes f by taking Newton steps designed
to bring F to zero. This is not equivalent to minimizing f directly by taking Newton
steps designed to bring Vf to zero. While the method can still occasionally fail by
landing on a local minimum of f, this is quite rare in practice. The routine newt
below will warn you if this happens. The remedy is to try a new starting point.

Line Searches and Backtracking

When we are not close enough to the minimum of f, taking the full Newton step p = 6x
need not decrease the function; we may move too far for the quadratic approximation to be
valid. All we are guaranteed is that initially f decreases as we move in the Newton direction.
So the goal is to move to a new point Xnew along the direction of the Newton step p, but
not necessarily all the way:

Xnew = Xolad + AP, O0<A<1 (9.7.6)

The aim is to find \ so that f(Xo1a + Ap) has decreased sufficiently. Until the early 1970s,
standard practice was to choose A so that Xnew exactly minimizes f in the direction p. However,
we now know that it is extremely wasteful of function evaluations to do so. A better strategy
is as follows: Since p is always the Newton direction in our algorithms, we first try A = 1, the
full Newton step. This will lead to quadratic convergence when x is sufficiently close to the
solution. However, if f(xXnew) does not meet our acceptance criteria, we backtrack along the
Newton direction, trying a smaller value of A, until we find a suitable point. Since the Newton
direction is a descent direction, we are guaranteed to decrease f for sufficiently small A.

What should the criterion for accepting a step be? It is not sufficient to require merely
that f(xnew) < f(Xeia). This criterion can fail to converge to a minimum of f in one of
two ways. First, it is possible to construct a sequence of steps satisfying this criterion with
f decreasing too slowly relative to the step lengths. Second, one can have a sequence where
the step lengths are too small relative to the initial rate of decrease of f. (For examples of
such sequences, see [1], p. 117.)

A simple way to fix the first problem is to require the average rate of decrease of f to
be at least some fraction a of the initial rate of decrease Vf - p:

f(Xnew) S f(Xo1a) + AVS + (Knew — Xoia) (9.7.7)

Here the parameter a satisfies 0 < a < 1. We can get away with quite small values of
a; a = 10-* isa good choice.

The second problem can be fixed by requiring the rate of decrease of f at Xnew to be
greater than some fraction (3 of the rate of decrease of f at Xoiq. In practice, we will not
need to impose this second constraint because our backtracking algorithm will have a built-in
cutoff to avoid taking steps that are too small.

Here is the strategy for a practical backtracking routine: Define

9(A) = fKo1a + Ap) (9.7.8)

so that

JQ) =VF-P (9.7.9)

If we need to backtrack, then we model g with the most current information we have and
choose to minimize the model. We start with g(0) and g’(0) available. The first step is

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 385

always the Newton step, \ = 1. If this step is not acceptable, we have available g(1) as well.
We can therefore model g(A) as a quadratic:

9A) © [9(1) — 9(0) — g'(0)]* + 9'(0) + 9(0) (9.7.10)
Taking the derivative of this quadratic, we find that it is a minimum when
,
g'(0)
h=- 97.11
BoA) — 9(0) — FO) O74)
Since the Newton step failed, we can show that \ < 5 for small a. We need to guard against
too small a value of A, however. We set Amin = 0.1.
On second and subsequent backtracks, we model g as a cubic in A, using the previous
value g(Ai) and the second most recent value g(Az):
g(d) = ad? + bd? + g/(0)A + g(0) (9.7.12)

Requiring this expression to give the correct values of g at A1 and Az gives two equations
that can be solved for the coefficients a and b:

[Ss aaL She aise] La) foe a

The minimum of the cubic (9.7.12) is at

_ —b + /b? — 3ag'(0)
A= oe (9.7.14)

(9.7.13)

We enforce that » lie between Amax = 0.5A1 and Amin = 0.1A1.

The routine has two additional features, a minimum step length alamin and a maximum
step length stpmax. Insrch will also be used in the quasi-Newton minimization routine
dfpmin in the next section.

#include <math.h>

#include "nrutil.h"

#define ALF 1.0e-4 Ensures sufficient decrease in function value.
#define TOLX 1.0e-7 Convergence criterion on Ax.

void lnsrch(int n, float xold[], float fold, float g[], float p[], float x[],

float *f, float stpmax, int *check, float (*func) (float []))
Given an n-dimensional point xold[1..n], the value of the function and gradient there, fold
and g[1..n], anda direction p[1. .n], finds a new point x[1..n] along the direction p from
xold where the function func has decreased “sufficiently.” The new function value is returned
in £. stpmax is an input quantity that limits the length of the steps so that you do not try to
evaluate the function in regions where it is undefined or subject to overflow. p is usually the
Newton direction. The output quantity check is false (0) on a normal exit. It is true (1) when
x is too close to xold. In a minimization algorithm, this usually signals convergence and can
be ignored. However, in a zero-finding algorithm the calling program should check whether the
convergence is spurious. Some “difficult” problems may require double precision in this routine.
{

int i;

float a,alam,alam2,alamin,b,disc,f2,rhsi,rhs2,slope,sum,temp,

test ,tmplam;

*check=0;
for (sum=0.0,i=1;i<=n;i++) sum += p[i]l*p[il;
sum=sqrt (sum) ;
if (sum > stpmax)
for (i=1;i<=n;i++) pli] *= stpmax/sum; Scale if attempted step is too big.
for (slope=0.0,i=1;i<=n;i++)
slope += gli]*plil];
if (slope >= 0.0) nrerror("Roundoff problem in lnsrch.");
test=0.0 Compute Amin-
for (i=1;i<=n;it+) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

386 Chapter 9. Root Finding and Nonlinear Sets of Equations

temp=fabs (p[i] ) /FMAX (fabs (xold[i]) ,1.0);
if (temp > test) test=temp;

+
alamin=TOLX/test;
alam=1.0; Always try full Newton step first.
for (;;) { Start of iteration loop.
for (i=1;i<=n;it+) x[i]=xold[i]+alam*p [i] ;
*£=(*func) (x) ;
if (alam < alamin) { Convergence on Az. For zero find-
for (i=1;i<=n;it+) x{i]=xold[il]; ing, the calling program should
*check=1; verify the convergence.
return;
} else if (*f <= fold+ALF*alam*slope) return; Sufficient function decrease.
else { Backtrack.
if (alam == 1.0)
tmplam = -slope/(2.0*(*f-fold-slope)); First time.
else { Subsequent backtracks.
rhsi = *f-fold-alam*slope;
rhs2=f2-fold-alam2*slope;
rhsi/(alam*alam)-rhs2/(alam2*alam2) )/(alam-alam2) ;
-alam2*rhs1/(alam*alam)+alam*rhs2/(alam2*alam2) ) /(alam-alam2) ;
if (a == 0.0) tmplam = -slope/(2.0*b) ;
else {
disc=b*b-3.0*a*slope;
if (disc < 0.0) tmplam=0.5*alam;
else if (b <= 0.0) tmplam=(-btsqrt (disc))/(3.0*a) ;
else tmplam=-slope/(btsqrt (disc)) ;
}
if (tmplam > 0.5*alam)
tmplam=0.5*alam; A < 0.5A1.
3
}
alam2=alam;
£2 = *f;
alam=FMAX(tmplam,0.1*alam) ; A> 0.1A1.
} Try again.

Here now is the globally convergent Newton routine newt that uses lnsrch. A feature
of newt is that you need not supply the Jacobian matrix analytically; the routine will attempt to
compute the necessary partial derivatives of F by finite differences in the routine fdjac. This
routine uses some of the techniques described in §5.7 for computing numerical derivatives. Of
course, you can always replace fdjac with a routine that calculates the Jacobian analytically
if this is easy for you to do.

#include <math.h>

#include "nrutil.h"

#define MAXITS 200

#define TOLF 1.0e-4

#define TOLMIN 1.0e-6

#define TOLX 1.0e-7

#define STPMX 100.0

Here MAXITS is the maximum number of iterations; TOLF sets the convergence criterion on
function values; TOLMIN sets the criterion for deciding whether spurious convergence to a
minimum of fmin has occurred; TOLX is the convergence criterion on 6x; STPMX is the scaled
maximum step length allowed in line searches.

int nn; Global variables to communicate with fmin.

float *fvec;

void (*nrfuncv) (int n, float v[], float #[]);

#define FREERETURN {free_vector(fvec,1,n) ;free_vector(xold,1i,n);\
free_vector(p,1,n) ;free_vector(g,1,n) ;free_matrix(fjac,1,n,1,n);\
free_ivector(indx,1,n);return;}

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 387

void newt(float x[], int n, int *check,

void (*vecfunc) (int, float [], float []))
Given an initial guess x[1..n] for a root in n dimensions, find the root by a globally convergent
Newton's method. The vector of functions to be zeroed, called fvec[1..n] in the routine
below, is returned by the user-supplied routine vecfunc(n,x,fvec). The output quantity
check is false (0) on a normal return and true (1) if the routine has converged to a local
minimum of the function fmin defined below. In this case try restarting from a different initial
guess.
t

void fdjac(int n, float x[], float fvec[], float **df,

void (*vecfunc)(int, float [], float []));
float fmin(float x[]);

void lnsrch(int n, float xold[], float fold, float g[], float p[], float xf],

float *f, float stpmax, int *check, float (*func) (float []));
void lubksb(float **a, int n, int *indx, float b[]);
void ludemp(float **a, int n, int *indx, float *d);
int i,its,j,*indx;
float d,den,f,fold,stpmax,sum,temp,test,**fjac,*g,*p,*xold;

indx=ivector(1,n) ;
fjac=matrix(1,n,1,n);
g=vector(1,n);
p=vector(1,n);
xold=vector(1,n);

fvec=vector(1,n); Define global variables.

nn=n;

nrfuncv=vecfunc;

f=fmin(x) ; fvec is also computed by this call.
test=0.0; Test for initial guess being a root. Use
for (i=1;i<=n;i++) more stringent test than simply TOLF.

if (fabs(fvec[i]) > test) test=fabs(fvec[i]);
if (test < 0.01*TOLF) {

*check=0;
FREERETURN
}
for (sum=0.0,i=1;i<=n;i++) sum += SQR(x[i]); Calculate stpmax for line searches.

stpmax=STPMX*FMAX (sqrt (sum) , (float)n) ;

for (its=1;its<=MAXITS;its++) { Start of iteration loop.
fdjac(n,x,fvec,fjac,vecfunc) ;
If analytic Jacobian is available, you can replace the routine fdjac below with your
own routine.

for (i=1;i<=n;it+) { Compute Vf for the line search.
for (sum=0.0,j=1;j<=n;jt++) sum += fjac[j] [i]*fvec[j];
gli]=sum;

}

for (i=1;i<=n;i++) xold[i]=x([il; Store x,

fold=f; and f.

for (i=1;i<=n;it+) pli] = -fvec[i]; Right-hand side for linear equations.

ludcmp(fjac,n,indx,&d) ; Solve linear equations by LU decompo-
lubksb(fjac,n,indx,p) ; sition.
1nsrch(n,xold,fold,g,p,x,&f,stpmax,check,fmin) ;
1nsrch returns new x and f. It also calculates fvec at the new x when it calls fmin.
test=0.0; Test for convergence on function val-
for (i=1;i<=n;it++) ues.

if (fabs(fvec[i]) > test) test=fabs(fvec[i]);
if (test < TOLF) {

*check=0;
FREERETURN

}

if (*check) { Check for gradient of f zero, i.e., spuri-
test=0.0; ous convergence.

den=FMAX(f ,0.5*n) ;
for (i=1;i<=nji++) {
temp=fabs(g[i] ) *FMAX(fabs(x[i]),1.0)/den;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

388 Chapter 9. Root Finding and Nonlinear Sets of Equations

if (temp > test) test=temp;

3
*check=(test < TOLMIN ? 1 : 0);
FREERETURN
}
test=0.0; Test for convergence on 6x.

for (i=1;i<=n;it+) {
temp= (fabs (x [i]-xold[i] )) /FMAX(fabs(x[i]) ,1.0);
if (temp > test) test=temp;
}
if (test < TOLX) FREERETURN
+

nrerror("MAXITS exceeded in newt");

#include <math.h>
#include "nrutil.h"
#define EPS 1.0e-4 Approximate square root of the machine precision.

void fdjac(int n, float x[], float fvec[], float **df,

void (*vecfunc) (int, float [], float []))
Computes forward-difference approximation to Jacobian. On input, x[1..n] is the point at
which the Jacobian is to be evaluated, fvec[1..n] is the vector of function values at the
point, and vecfunc(n,x,f) is a user-supplied routine that returns the vector of functions at
x. On output, df[1..n] [1..n] is the Jacobian array.

t
int i,j;
float h,temp,*f;
f=vector(1,n);
h=EPS*fabs (temp) ;
if (h == 0.0) h=EPS;
x[j]=tempth; Trick to reduce finite precision error.
h=x[j]-temp;
(*vecfunc) (n,x,f);
x[j]=temp;
for (i=1;i<=n;it+) af [i] [j]=(f[i]-fvec[i])/h; Forward difference for-
} mula.
free_vector(f,1,n);
}

#include "nrutil.h"

extern int nn;
extern float *fvec;
extern void (*nrfuncv) (int n, float v[], float £[]);

float fmin(float x[])
Returns f = $F -F at x. The global pointer *nrfuncv points to a routine that returns the
vector of functions at x. It is set to point to a user-supplied routine in the calling program.
Global variables also communicate the function values back to the calling program.
{

int i;

float sum;

(*nrfuncv) (nn,x,fvec) ;

for (sum=0.0,i=1;i<=nn;i++) sum += SQR(fvec[i]);
return 0.5*sum;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 389

The routine newt assumes that typical values of all components of x and of F are of order
unity, and it can fail if this assumption is badly violated. You should rescale the variables by
their typical values before invoking newt if this problem occurs.

Multidimensional Secant Methods: Broyden’s Method

Newton’s method as implemented above is quite powerful, but it still has several
disadvantages. One drawback is that the Jacobian matrix is needed. In many problems
analytic derivatives are unavailable. If function evaluation is expensive, then the cost of
finite-difference determination of the Jacobian can be prohibitive.

Just as the quasi-Newton methods to be discussed in §10.7 provide cheap approximations
for the Hessian matrix in minimization algorithms, there are quasi-Newton methods that
provide cheap approximations to the Jacobian for zero finding. These methods are often called
secant methods, since they reduce to the secant method (§9.2) in one dimension (see, e.g.,[1]).
The best of these methods still seems to be the first one introduced, Broyden’s method [2].

Let us denote the approximate Jacobian by B. Then the ith quasi-Newton step dx;
is the solution of

B, - 6x; = —F; (9.7.15)

where 6xi = xi+1 — xi (ef. equation 9.7.3). The quasi-Newton or secant condition is that
Bj+1 satisfy

Biyi Ox: = OF; (9.7.16)

where 6F; = F,41 — F;. This is the generalization of the one-dimensional secant approxima-
tion to the derivative, 5F'/dx. However, equation (9.7.16) does not determine B;+1 uniquely
in more than one dimension.

Many different auxiliary conditions to pin down B,;1 have been explored, but the
best-performing algorithm in practice results from Broyden’s formula. This formula is based
on the idea of getting Bi;1 by making the least change to Bj consistent with the secant
equation (9.7.16). Broyden showed that the resulting formula is

4 (OF; — By « Oxi) ® Oxi
OX; + OX;
You can easily check that Bj,1 satisfies (9.7.16).
Early implementations of Broyden’s method used the Sherman-Morrison formula,
equation (2.7.2), to invert equation (9.7.17) analytically,
(6x; — B; | - 6F;) @ 6x; -B;*
6x; BP 1. OF;

Bi =B: (9.7.17)

-1 -1
Bui =B, +4

(9.7.18)

Then instead of solving equation (9.7.3) by e.g., LU decomposition, one determined
6x; = —B;* - Fi (9.7.19)

by matrix multiplication in O(N?) operations. The disadvantage of this method is that
it cannot easily be embedded in a globally convergent strategy, for which the gradient of
equation (9.7.4) requires B, not Bl,

V(iF-F) ~B’-F (9.7.20)

Accordingly, we implement the update formula in the form (9.7.17).

However, we can still preserve the O(N”) solution of (9.7.3) by using Q R decomposition
(§2.10) instead of LU decomposition. The reason is that because of the special form of equation
(9.7.17), the QR decomposition of B; can be updated into the QR decomposition of B;+1 in
O(N?) operations (§2.10). All we need is an initial approximation Bo to start the ball rolling.
It is often acceptable to start simply with the identity matrix, and then allow O(N) updates to
produce a reasonable approximation to the Jacobian. We prefer to spend the first N function
evaluations on a finite-difference approximation to initialize B via a call to fdjac.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

390 Chapter 9. Root Finding and Nonlinear Sets of Equations

Since B is not the exact Jacobian, we are not guaranteed that 5x is a descent direction for
1

f = 3F-F (cf. equation 9.7.5). Thus the line search algorithm can fail to return a suitable step
if B wanders far from the true Jacobian. In this case, we reinitialize B by another call to fdjac.

Like the secant method in one dimension, Broyden’s method converges superlinearly
once you get close enough to the root. Embedded in a global strategy, it is almost as robust
as Newton’s method, and often needs far fewer function evaluations to determine a zero.
Note that the final value of B is not always close to the true Jacobian at the root, even
when the method converges.

The routine broydn given below is very similar to newt in organization. The principal
differences are the use of QR decomposition instead of LU, and the updating formula instead
of directly determining the Jacobian. The remarks at the end of newt about scaling the
variables apply equally to broydn.

#include <math.h>

#include "nrutil.h"

#define MAXITS 200

#define EPS 1.0e-7

#define TOLF 1.0e-4

#define TOLX EPS

#define STPMX 100.0

#define TOLMIN 1.0e-6

Here MAXITS is the maximum number of iterations; EPS is a number close to the machine

precision; TOLF is the convergence criterion on function values; TOLX is the convergence criterion

on 5x; STPMX is the scaled maximum step length allowed in line searches; TOLMIN is used to
decide whether spurious convergence to a minimum of fmin has occurred.

#define FREERETURN {free_vector(fvec,1,n) ;free_vector(xold,1i,n);\
free_vector(w,1,n);free_vector(t,1,n);free_vector(s,1,n);\
free_matrix(r,1,n,1,n) ;free_matrix(qt,1,n,1,n);free_vector(p,1,n);\
free_vector(g,1,n) ;free_vector(fvcold,1,n) ;free_vector(d,1,n);\
free_vector(c,1,n);return;}

int nn; Global variables to communicate with fmin.
float *fvec;
void (*nrfuncv) (int n, float v[], float #[]);

void broydn(float x[], int n, int *check,

void (*vecfunc) (int, float [], float []))
Given an initial guess x[1..n] for a root in n dimensions, find the root by Broyden’s method
embedded in a globally convergent strategy. The vector of functions to be zeroed, called
fvec[1..n] in the routine below, is returned by the user-supplied routine vecfunc(n,x,fvec).
The routine fdjac and the function fmin from newt are used. The output quantity check
is false (0) on a normal return and true (1) if the routine has converged to a local minimum
of the function fmin or if Broyden’s method can make no further progress. In this case try
restarting from a different initial guess.
t

void fdjac(int n, float x[], float fvec[], float **df,

void (*vecfunc) (int, float [], float []));
float fmin(float x[]);
void Insrch(int n, float xold[], float fold, float g[], float p[], float x[],
float *f, float stpmax, int *check, float (*func) (float []));

void qrdcmp(float **a, int n, float *c, float *d, int *sing);

void qrupdt(float **r, float **qt, int n, float u[], float v[]);

void rsolv(float **a, int n, float d[], float b[]);

int i,its,j,k,restrt,sing,skip;

float den,f,fold,stpmax,sum,temp,test,*c,*d,*fvcold;

float *g,*p,**qt,**r,*s,*t,*w,*xold;

c=vector(1,n);
d=vector(1,n);
fvcold=vector(1,n);
g=vector(1,n);
p=vector(1,n);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 391

qt=matrix(1,n,1,n);
r=matrix(1,n,1,n);
s=vector(1,n);
t=vector(1,n);
w=vector(1,n);
xold=vector(1,n);

fvec=vector(1,n); Define global variables.
nn=n;
nrfuncv=vecfunc;
f=fmin(x) ; The vector fvec is also computed by this
test=0.0; call.
for (i=1;i<=n;it++) Test for initial guess being a root. Use more
if (fabs(fvec[i]) > test)test=fabs(fvec[i]); stringent test than sim-
if (test < 0.01*TOLF) { ply TOLF.
*check=0;
FREERETURN
}
for (sum=0.0,i=1;i<=n;i++) sum += SQR(x[i]); Calculate stpmax for line searches.

stpmax=STPMX*FMAX (sqrt (sum) , (float)n) ;

Ensure initial Jacobian gets computed.

for (its=1;its<=MAXITS;its++) { Start of iteration loop.
if (restrt) {
fdjac(n,x,fvec,r,vecfunc) ; Initialize or reinitialize Jacobian in r.
qrdcmp(r,n,c,d,&sing) ; QR decomposition of Jacobian.
if (sing) nrerror("singular Jacobian in broydn") ;
for (i=1;i<=n;it+) { Form Q?’ explicitly.

for (j=1;j<=n;j++) qt[i] [j]=0.0;
qt [i] [i]=1.0;

3
for (k=1;k<n;k++) {
if (c{k]) f
j sj+4) f
jis=n;i++)
sum += r[i] (k] *qt [i] Cj];
sum /= c[k];
for (i=k;i<=n;i++)
qt Ci] [j] -= sum*r [i] (k];
+
3
3
for (i=1;i<=n;it+) { Form R explicitly.
r[i] Cil=alil ;
for (j=1;j<i;j++) ri] [j]=0.0;
3
} else { Carry out Broyden update.
for (i=1;i<=n;it+) s[i]=x[i]-xold[il; s = 6x.
for (i=1;i pitt) { t=R-s.
for (sum=0.0,j=i;j<=n;jt++) sum += r[i] [j]*s[jl;
tl[il=sum;
3
skip=1;
for (i=1;i<=n;it+) { w=0o0F-B
for (sum=0.0,j=1;j<=n;jt++) sum += qt[j] [i]+tlj];
wli]=fvec[i]-fvcold[i]-sum;
if (fabs(w[i]) >= EPS*(fabs(fvec[i])+fabs(fvcold[i]))) skip=0;
Don't update with noisy components of w.
else w[i]=0.0;
3
if (!skip) {

for (i=1;i<=n;it++) { t= Qr ow.
-0,j=1;j<=n;jt++) sum += qt(i] [j]+*wljl];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

392

Chapter 9. Root Finding and Nonlinear Sets of Equations

+

for (den=0.0,i=1;i<=n;i++) den += SQR(s[i]);
for (i=1;i<=n;it++) s[i] /= den; Store s/(s-s) ins.
qrupdt(r,qt,n,t,s); Update R and Q?.
for (i=1;i<=n;it++) {
if (r[i] [4] == 0.0) nrerror("r singular in broydn");
dalil=r [il [i]; Diagonal of R stored in d.

3

}

for (i=1;i<=n;it+) { Right-hand side for linear equations is —Q?r “F.
for (sum=0.0,j=1;j<=n;jt++) sum += qt[i] [j]*fvec[j];

pli] = -sum;

}

for (i=n;i>=1;i--) { Compute Vf ~ (Q-R)” -F for the line search.
for (sum=0.0,j=1;j<=i;jt++) sum -= r[j] [il*plj];
gli]=sum;

}

for (i=1;i<=n;it+) { Store x and F.
xold[il=x([il] ;
fvcold[il=fvec [il] ;

}

fold=f; Store f.

rsolv(r,n,d,p); Solve linear equations.
1nsrch(n,xold,fold,g,p,x,&f,stpmax,check,fmin) ;
lnsrch returns new x and f. It also calculates fvec at the new x when it calls fmin.
test=0.0; Test for convergence on function values.
for (i=1;i<=n;it++)

if (fabs(fvec[i]) > test) test=fabs(fvec[i]);
if (test < TOLF) {

*check=0;
FREERETURN
}
if (*check) { True if line search failed to find a new x.
if (restrt) FREERETURN Failure; already tried reinitializing the Jaco-
else { bian.
test=0.0; Check for gradient of f zero, i.e., spurious
den=FMAX(f ,0.5*n) ; convergence.

for (i=1;i<=n;it++) {
temp=fabs (g[i] )*FMAX(fabs(x[i]) ,1.0)/den;
if (temp > test) test=temp;

3
if (test < TOLMIN) FREERETURN
else restrt=1; Try reinitializing the Jacobian.

Successful step; will use Broyden update for
next step.
Test for convergence on dx.

temp= (fabs (x [i]-xold[i]))/FMAX (fabs (x[i]) ,1.0);
if (temp > test) test=temp;

3
if (test < TOLX) FREERETURN

+

nrerror("MAXITS exceeded in broydn");
FREERETURN

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

9.7 Globally Convergent Methods for Nonlinear Systems of Equations 393

More Advanced Implementations

One of the principal ways that the methods described so far can fail is if J (in Newton’s
method) or B in (Broyden’s method) becomes singular or nearly singular, so that 5x cannot
be determined. If you are lucky, this situation will not occur very often in practice. Methods
developed so far to deal with this problem involve monitoring the condition number of J and
perturbing J if singularity or near singularity is detected. This is most easily implemented
if the QR decomposition is used instead of LU in Newton’s method (see [1] for details).
Our personal experience is that, while such an algorithm can solve problems where J is
exactly singular and the standard Newton’s method fails, it is occasionally less robust on
other problems where LU decomposition succeeds. Clearly implementation details involving
roundoff, underflow, etc., are important here and the last word is yet to be written.

Our global strategies both for minimization and zero finding have been based on line
searches. Other global algorithms, such as the hook step and dogleg step methods, are based
instead on the model-trust region approach, which is related to the Levenberg-Marquardt
algorithm for nonlinear least-squares (§15.5). While somewhat more complicated than line
searches, these methods have a reputation for robustness even when starting far from the
desired zero or minimum [1].

CITED REFERENCES AND FURTHER READING:

Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Englewood Cliffs, NJ: Prentice-Hall). [1]

Broyden, C.G. 1965, Mathematics of Computation, vol. 19, pp. 577-593. [2]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 10. Minimization or
Maximization of Functions

10.0 Introduction

In a nutshell: You are given a single function f that depends on one or more
independent variables. You want to find the value of those variables where f takes
on a maximum or a minimum value. You can then calculate what value of f is
achieved at the maximum or minimum. The tasks of maximization and minimization
are trivially related to each other, since one person’s function f could just as well
be another’s —f. The computational desiderata are the usual ones: Do it quickly,
cheaply, and in small memory. Often the computational effort is dominated by
the cost of evaluating f (and also perhaps its partial derivatives with respect to all
variables, if the chosen algorithm requires them). In such cases the desiderata are
sometimes replaced by the simple surrogate: Evaluate f as few times as possible.

An extremum (maximum or minimum point) can be either global (truly
the highest or lowest function value) or local (the highest or lowest in a finite
neighborhood and not on the boundary of that neighborhood). (See Figure 10.0.1.)
Finding a global extremum is, in general, a very difficult problem. Two standard
heuristics are widely used: (i) find local extrema starting from widely varying
starting values of the independent variables (perhaps chosen quasi-randomly, as in
§7.7), and then pick the most extreme of these (if they are not all the same); or
(ii) perturb a local extremum by taking a finite amplitude step away from it, and
then see if your routine returns you to a better point, or “always” to the same
one. Relatively recently, so-called “simulated annealing methods” (§10.9) have
demonstrated important successes on a variety of global extremization problems.

Our chapter title could just as well be optimization, which is the usual name
for this very large field of numerical research. The importance ascribed to the
various tasks in this field depends strongly on the particular interests of whom
you talk to. Economists, and some engineers, are particularly concerned with
constrained optimization, where there are a priori limitations on the allowed values
of independent variables. For example, the production of wheat in the U.S. must
be a nonnegative number. One particularly well-developed area of constrained
optimization is linear programming, where both the function to be optimized and
the constraints happen to be linear functions of the independent variables. Section
10.8, which is otherwise somewhat disconnected from the rest of the material that we
have chosen to include in this chapter, implements the so-called “simplex algorithm”
for linear programming problems.

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

10.0 Introduction 395

Xi X

Figure 10.0.1. | Extrema of a function in an interval. Points A, C’, and E are local, but not global
maxima. Points B and F are local, but not global minima. The global maximum occurs at G, which
is on the boundary of the interval so that the derivative of the function need not vanish there. The
global minimum is at D. At point E, derivatives higher than the first vanish, a situation which can
cause difficulty for some algorithms. The points X, Y, and Z are said to “bracket” the minimum F’,
since Y is less than both X and Z.

One other section, §10.9, also lies outside of our main thrust, but for a different
reason: so-called “annealing methods” are relatively new, so we do not yet know
where they will ultimately fit into the scheme of things. However, these methods
have solved some problems previously thought to be practically insoluble; they
address directly the problem of finding global extrema in the presence of large
numbers of undesired local extrema.

The other sections in this chapter constitute a selection of the best established
algorithms in unconstrained minimization. (For definiteness, we will henceforth
regard the optimization problem as that of minimization.) These sections are
connected, with later ones depending on earlier ones. If you are just looking for
the one “perfect” algorithm to solve your particular application, you may feel that
we are telling you more than you want to know. Unfortunately, there is no perfect
optimization algorithm. This is a case where we strongly urge you to try more than
one method in comparative fashion. Your initial choice of method can be based
on the following considerations:

e You must choose between methods that need only evaluations of the

function to be minimized and methods that also require evaluations of the

lerivative of that function. In the multidimensional case, this derivative

is the gradient, a vector quantity. Algorithms using the derivative are

somewhat more powerful than those using only the function, but not

always enough so as to compensate for the additional calculations of

lerivatives. We can easily construct examples favoring one approach or
favoring the other. However, if you can compute derivatives, be prepared
to try using them.

e For one-dimensional minimization (minimize a function of one variable)
without calculation of the derivative, bracket the minimum as described in
§10.1, and then use Brent’s method as described in §10.2. If your function
has a discontinuous second (or lower) derivative, then the parabolic

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

396

Chapter 10. Minimization or Maximization of Functions

interpolations of Brent’s method are of no advantage, and you might wish
to use the simplest form of golden section search, as described in §10.1.

e For one-dimensional minimization with calculation of the derivative, §10.3
supplies a variant of Brent’s method which makes limited use of the
first derivative information. We shy away from the alternative of using
derivative information to construct high-order interpolating polynomials.
In our experience the improvement in convergence very near a smooth,
analytic minimum does not make up for the tendency of polynomials
sometimes to give wildly wrong interpolations at early stages, especially
for functions that may have sharp, “exponential” features.

We now turn to the multidimensional case, both with and without computation

of first derivatives.

with calculation of first derivatives.

e You must choose between methods that require storage of order N? and

those that require only of order NV’, where N is the number of dimensions.
For moderate values of N and reasonable memory sizes this is not a
serious constraint. There will be, however, the occasional application
where storage may be critical.

e We give in §10.4 a sometimes overlooked downhill simplex method due
to Nelder and Mead. (This use of the word “simplex” is not to be
confused with the simplex method of linear programming.) This method
just crawls downhill in a straightforward fashion that makes almost no
special assumptions about your function. This can be extremely slow, but
it can also, in some cases, be extremely robust. Not to be overlooked is
the fact that the code is concise and completely self-contained: a general
N-dimensional minimization program in under 100 program lines! This
method is most useful when the minimization calculation is only an
incidental part of your overall problem. The storage requirement is of
order N?, and derivative calculations are not required.

e Section 10.5 deals with direction-set methods, of which Powell’s method
is the prototype. These are the methods of choice when you cannot easily
calculate derivatives, and are not necessarily to be sneered at even if you
can. Although derivatives are not needed, the method does require a
one-dimensional minimization sub-algorithm such as Brent’s method (see
above). Storage is of order N?.

There are two major families of algorithms for multidimensional minimization
Both families require a one-dimensional
minimization sub-algorithm, which can itself either use, or not use, the derivative
information, as you see fit (depending on the relative effort of computing the function
and of its gradient vector). We do not think that either family dominates the other in

all applications; you should think of them as available alternatives:

e The first family goes under the name conjugate gradient methods, as typi-
fied by the Fletcher-Reeves algorithm and the closely related and probably
superior Polak-Ribiere algorithm. Conjugate gradient methods require
only of order a few times N storage, require derivative calculations and

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.1 Golden Section Search in One Dimension 397

one-dimensional sub-minimization. Turn to §10.6 for detailed discussion
and implementation.

e The second family goes under the names quasi-Newton or variable metric
methods, as typified by the Davidon-Fletcher-Powell (DFP) algorithm
(sometimes referred to just as Fletcher-Powell) or the closely related
Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. These methods
require of order N? storage, require derivative calculations and one-
dimensional sub-minimization. Details are in §10.7.

You are now ready to proceed with scaling the peaks (and/or plumbing the
depths) of practical optimization.

CITED REFERENCES AND FURTHER READING:

Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Englewood Cliffs, NJ: Prentice-Hall).

Polak, E. 1971, Computational Methods in Optimization (New York: Academic Press).
Gill, P.E., Murray, W., and Wright, M.H. 1981, Practical Optimization (New York: Academic Press).

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 17.

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic
Press), Chapter Ill.1.

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood Cliffs, NJ: Prentice-
Hall).

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood Cliffs, NJ: Prentice-Hall),
Chapter 10.

10.1 Golden Section Search in One Dimension

Recall how the bisection method finds roots of functions in one dimension
(89.1): The root is supposed to have been bracketed in an interval (a,b). One
then evaluates the function at an intermediate point x and obtains a new, smaller
bracketing interval, either (a, x) or (x, b). The process continues until the bracketing
interval is acceptably small. It is optimal to choose x to be the midpoint of (a, b)
so that the decrease in the interval length is maximized when the function is as
uncooperative as it can be, i.e., when the luck of the draw forces you to take the
bigger bisected segment.

There is a precise, though slightly subtle, translation of these considerations to
the minimization problem: What does it mean to bracket a minimum? A root of a
unction is known to be bracketed by a pair of points, a and b, when the function
has opposite sign at those two points. A minimum, by contrast, is known to be
bracketed only when there is a triplet of points, a < b < c (orc < b < a), such that
f(b) is less than both f(a) and f(c). In this case we know that the function (if it
is nonsingular) has a minimum in the interval (a,c).

The analog of bisection is to choose a new point x, either between a and b or
between b and c. Suppose, to be specific, that we make the latter choice. Then we
evaluate f(x). If f(b) < f(x), then the new bracketing triplet of points is (a, b, x);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

398 Chapter 10. Minimization or Maximization of Functions

Figure 10.1.1. Successive bracketing of a minimum. The minimum is originally bracketed by points
1,3,2. The function is evaluated at 4, which replaces 2; then at 5, which replaces 1; then at 6, which
replaces 4. The rule at each stage is to keep a center point that is lower than the two outside points. After
the steps shown, the minimum is bracketed by points 5,3,6.

contrariwise, if f(b) > f(a), then the new bracketing triplet is (b, x,c). In all cases
the middle point of the new triplet is the abscissa whose ordinate is the best minimum
achieved so far; see Figure 10.1.1. We continue the process of bracketing until the
distance between the two outer points of the triplet is tolerably small.

How small is “tolerably” small? For a minimum located at a value b, you
might naively think that you will be able to bracket it in as small a range as
(1 —)b < b < (1+ ©)b, where € is your computer’s floating-point precision, a
number like 3 x 10~® (for float) or 10~!5 (for double). Not so! In general, the
shape of your function f(a) near b will be given by Taylor’s theorem

The second term will be negligible compared to the first (that is, will be a factor €
smaller and will act just like zero when added to it) whenever

la — b| < velb| (eo (10.1.2)

The reason for writing the right-hand side in this way is that, for most functions,
the final square root is a number of order unity. Therefore, as a rule of thumb, it
is hopeless to ask for a bracketing interval of width less than ,/€ times its central
value, a fractional width of only about 10~4 (single precision) or 3 x 10~* (double
precision). Knowing this inescapable fact will save you a lot of useless bisections!
The minimum-finding routines of this chapter will often call for a user-supplied
argument tol, and return with an abscissa whose fractional precision is about +tol
(bracketing interval of fractional size about 2xtol). Unless you have a better

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.1 Golden Section Search in One Dimension 399

estimate for the right-hand side of equation (10.1.2), you should set tol equal to
(not much less than) the square root of your machine’s floating-point precision, since
smaller values will gain you nothing.

It remains to decide on a strategy for choosing the new point x, given (a, b,c).
Suppose that b is a fraction w of the way between a and c, i.e.

b-a c—b

c—-a c-a

=l-w (10.1.3)

Also suppose that our next trial point x is an additional fraction z beyond b,

«z—b

c—-a

=z (10.1.4)

Then the next bracketing segment will either be of length w + z relative to the current
one, or else of length 1 — w. If we want to minimize the worst case possibility, then
we will choose z to make these equal, namely

z=1-2w (10.1.5)

We see at once that the new point is the symmetric point to b in the original interval,
namely with |b — a| equal to |x — c|. This implies that the point x lies in the larger
of the two segments (z is positive only if w < 1/2).

But where in the larger segment? Where did the value of w itself come from?
Presumably from the previous stage of applying our same strategy. Therefore, if z
is chosen to be optimal, then so was w before it. This scale similarity implies that
x should be the same fraction of the way from b to c (if that is the bigger segment)
as was b from a to c, in other words,

z
10.1.6
l-w ( )
Equations (10.1.5) and (10.1.6) give the quadratic equation
: 3-5
w?-3w+1=0 ~ yielding w= ae = 0.38197 (10.1.7)

In other words, the optimal bracketing interval (a, b,c) has its middle point b a
fractional distance 0.38197 from one end (say, a), and 0.61803 from the other end
(say, b). These fractions are those of the so-called golden mean or golden section,
whose supposedly aesthetic properties hark back to the ancient Pythagoreans. This
optimal method of function minimization, the analog of the bisection method for
finding zeros, is thus called the golden section search, summarized as follows:

Given, at each stage, a bracketing triplet of points, the next point to be tried
is that which is a fraction 0.38197 into the larger of the two intervals (measuring
from the central point of the triplet). If you start out with a bracketing triplet whose
segments are not in the golden ratios, the procedure of choosing successive points
at the golden mean point of the larger segment will quickly converge you to the
proper, self-replicating ratios.

The golden section search guarantees that each new function evaluation will
(after self-replicating ratios have been achieved) bracket the minimum to an interval

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

400 Chapter 10. Minimization or Maximization of Functions

just 0.61803 times the size of the preceding interval. This is comparable to, but not
quite as good as, the 0.50000 that holds when finding roots by bisection. Note that
the convergence is linear (in the language of Chapter 9), meaning that successive
significant figures are won linearly with additional function evaluations. In the
next section we will give a superlinear method, where the rate at which successive
significant figures are liberated increases with each successive function evaluation.

Routine for Initially Bracketing a Minimum

The preceding discussion has assumed that you are able to bracket the minimum
in the first place. We consider this initial bracketing to be an essential part of any
one-dimensional minimization. There are some one-dimensional algorithms that
do not require a rigorous initial bracketing. However, we would never trade the
secure feeling of knowing that a minimum is “in there somewhere” for the dubious
reduction of function evaluations that these nonbracketing routines may promise.
Please bracket your minima (or, for that matter, your zeros) before isolating them!

There is not much theory as to how to do this bracketing. Obviously you want
to step downhill. But how far? We like to take larger and larger steps, starting with
some (wild?) initial guess and then increasing the stepsize at each step either by
a constant factor, or else by the result of a parabolic extrapolation of the preceding
points that is designed to take us to the extrapolated turning point. It doesn’t much
matter if the steps get big. After all, we are stepping downhill, so we already have
the left and middle points of the bracketing triplet. We just need to take a big enough
step to stop the downhill trend and get a high third point.

Our standard routine is this:

#include <math.h>

#include "nrutil.h"

#define GOLD 1.618034

#define GLIMIT 100.0

#define TINY 1.0e-20

#define SHFT(a,b,c,d) (a)=(b); (b)=(c) ; (c)=(d);

Here GOLD is the default ratio by which successive intervals are magnified; GLIMIT is the
maximum magnification allowed for a parabolic-fit step.

void mnbrak(float *ax, float *bx, float *cx, float *fa, float *fb, float *fc,
float (*func) (float))

Given a function func, and given distinct initial points ax and bx, this routine searches in

the downhill direction (defined by the function as evaluated at the initial points) and returns

new points ax, bx, cx that bracket a minimum of the function. Also returned are the function

values at the three points, fa, fb, and fc.

{

float ulim,u,r,q,fu,dum;

*fa=(+*func) (*ax) ;
*fb=(+*func) (*bx) ;

if (*fb > *fa) { Switch roles of a and b so that we can go
SHFT (dum, *ax, *bx,dum) downhill in the direction from a to b.
SHFT (dum, *fb,*fa,dum)

+

*cx=(*bx) +GOLD* (*bx-*ax) ; First guess for c.

*£c=(#func) (*cx) ;

while (*fb > *fc) { Keep returning here until we bracket.

r=(*bx-*ax) *(*£b-*fc) ; Compute wu by parabolic extrapolation from
*bx-*cx) * (*£b-*£a) 5 a,b,c. TINY is used to prevent any pos-
u=(*bx)- ((*bx-*cx) #q- (*bx-*ax) *r) / sible division by zero.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.1 Golden Section Search in One Dimension 401

(2.0*SIGN(FMAX (fabs (q-r) , TINY) ,q-r)) ;
ulim=(*bx)+GLIMIT* (*cx-*bx) ;
We won't go farther than this. Test various possibilities:

if ((*bx-u)*(u-*cx) > 0.0) { Parabolic u is between b and c: try it.
fu=(*func) (u) ;
if (fu < *fc) { Got a minimum between b and c.
*ax=(*bx) 5
*bx=u;
*fa=(*£b) ;
*fb=fu;
return;
} else if (fu > *fb) { Got a minimum between between a and u.
*CX=U;
*fc=fu;
return;
}
u=(*cx) +GOLD* (*cx-*bx) ; Parabolic fit was no use. Use default mag-
fu=(*func) (u) ; nification.
} else if ((*cx-u)*(u-ulim) > 0.0) { Parabolic fit is between c and its
fu=(*func) (u) ; allowed limit.

if (fu < *fc) {
SHFT (*bx , *cx,u, *cx+GOLD* (*cx-*bx) )
SHFT(*fb,*fc,fu, (*func) (u))

}

} else if ((u-ulim)*(ulim-*cx) >= 0.0) { Limit parabolic u to maximum
u=ulim; allowed value.
fu=(*func) (u) ;

} else { Reject parabolic u, use default magnifica-
u=(*cx) +GOLD* (*cx-*bx) ; tion.
fu=(*func) (u) ;

}

SHFT (*ax, *bx, *cx,u) Eliminate oldest point and continue.

SHFT(*fa,*fb,*fc,fu)

(Because of the housekeeping involved in moving around three or four points and
their function values, the above program ends up looking deceptively formidable.
That is true of several other programs in this chapter as well. The underlying ideas,
however, are quite simple.)

Routine for Golden Section Search

#include <math.h>

#define R 0.61803399 The golden ratios.
#define C (1.0-R)

#define SHFT2(a,b,c) (a)=(b);(b)=(c);

#define SHFT3(a,b,c,d) (a)=(b); (b)=(c) ; (c)=(d);

float golden(float ax, float bx, float cx, float (*f)(float), float tol,

float *xmin)
Given a function f, and given a bracketing triplet of abscissas ax, bx, cx (such that bx is
between ax and cx, and f (bx) is less than both £(ax) and f(cx)), this routine performs a
golden section search for the minimum, isolating it to a fractional precision of about tol. The
abscissa of the minimum is returned as xmin, and the minimum function value is returned as
golden, the returned function value.
{

float f1,f2,x0,x1,x2,x3;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

402 Chapter 10. Minimization or Maximization of Functions

x0=ax; At any given time we will keep track of four
x3=cx; points, x0,x1,x2,x3.
if (fabs(cx-bx) > fabs(bx-ax)) { Make x0 to x1 the smaller segment,
x1=bx;
X2=bx+C* (cx-bx) ; and fill in the new point to be tried.
} else {
x2=bx;
x1=bx-C* (bx-ax) ;
}
f£1=(*f) (x1); The initial function evaluations. Note that
£2=(*f) (x2); we never need to evaluate the function
while (fabs(x3-x0) > tol*(fabs(x1)+fabs(x2))) { at the original endpoints.
if (#2 < f1) { One possible outcome,
SHFT3 (x0,x1,x2,R*x1+C*x3) its housekeeping,
SHFT2(£1,£2, (*£) (x2)) and a new function evaluation.
} else { The other outcome,
SHFT3 (x3 ,x2,x1,R*x2+C*x0)
SHFT2(£2,£1, (*£) (x1)) and its new function evaluation.
}
+ Back to see if we are done.
if (f1 < £2) { We are done. Output the best of the two
*xmin=x1; current values.
return f1;
} else {
*xmin=x2;
return f2;
+

10.2 Parabolic Interpolation and Brent’s Method
in One Dimension

We already tipped our hand about the desirability of parabolic interpolation in
the previous section’s mnbrak routine, but it is now time to be more explicit. A
golden section search is designed to handle, in effect, the worst possible case of
function minimization, with the uncooperative minimum hunted down and cornered
like a scared rabbit. But why assume the worst? If the function is nicely parabolic
near to the minimum — surely the generic case for sufficiently smooth functions —
then the parabola fitted through any three points ought to take us in a single leap
to the minimum, or at least very near to it (see Figure 10.2.1). Since we want to
find an abscissa rather than an ordinate, the procedure is technically called inverse
parabolic interpolation.

The formula for the abscissa x that is the minimum of a parabola through three

points f(a), f(b), and f(c) is

1 (b= 4) If) = FO] = = PI) = f@))
2° @—a)[f() - flo} © — olf) — F(a] (10.2.1)

as you can easily derive. This formula fails only if the three points are collinear,
in which case the denominator is zero (minimum of the parabola is infinitely far

r=b

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.2 Parabolic Interpolation and Brent’s Method 403

------- parabola through ® @ ®
stteeeeeceeeees parabola through (1) 2) @)

Figure 10.2.1. Convergence to a minimum by inverse parabolic interpolation. A parabola (dashed line) is
drawn through the three original points 1 ,2,3 on the given function (solid line). The function is evaluated
at the parabola’s minimum, 4, which replaces point 3. A new parabola (dotted line) is drawn through
points 1,4,2. The minimum of this parabola is at 5, which is close to the minimum of the function.

away). Note, however, that (10.2.1) is as happy jumping to a parabolic maximum
as to a minimum. No minimization scheme that depends solely on (10.2.1) is likely
to succeed in practice.

The exacting task is to invent a scheme that relies on a sure-but-slow technique,
like golden section search, when the function is not cooperative, but that switches
over to (10.2.1) when the function allows. The task is nontrivial for several
reasons, including these: (i) The housekeeping needed to avoid unnecessary function
evaluations in switching between the two methods can be complicated. (ii) Careful
attention must be given to the “endgame,” where the function is being evaluated
very near to the roundoff limit of equation (10.1.2). (iii) The scheme for detecting a
cooperative versus noncooperative function must be very robust.

Brent’s method [1] is up to the task in all particulars. At any particular stage,
it is keeping track of six function points (not necessarily all distinct), a, b, u, v,
w and x, defined as follows: the minimum is bracketed between a and b; x is the
point with the very least function value found so far (or the most recent one in
case of a tie); w is the point with the second least function value; v is the previous
value of w; u is the point at which the function was evaluated most recently. Also
appearing in the algorithm is the point 2, the midpoint between a and b; however,
the function is not evaluated there.

You can read the code below to understand the method’s logical organization.
Mention of a few general principles here may, however, be helpful: Parabolic
interpolation is attempted, fitting through the points x, v, and w. To be acceptable,
the parabolic step must (i) fall within the bounding interval (a, b), and (ii) imply a
movement from the best current value x that is less than half the movement of the
step before last. This second criterion insures that the parabolic steps are actually
converging to something, rather than, say, bouncing around in some nonconvergent
limit cycle. In the worst possible case, where the parabolic steps are acceptable but

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

404 Chapter 10. Minimization or Maximization of Functions

useless, the method will approximately alternate between parabolic steps and golden
sections, converging in due course by virtue of the latter. The reason for comparing
to the step before last seems essentially heuristic: Experience shows that it is better
not to “punish” the algorithm for a single bad step if it can make it up on the next one.

Another principle exemplified in the code is never to evaluate the function less
than a distance tol from a point already evaluated (or from a known bracketing
point). The reason is that, as we saw in equation (10.1.2), there is simply no
information content in doing so: the function will differ from the value already
evaluated only by an amount of order the roundoff error. Therefore in the code below
you will find several tests and modifications of a potential new point, imposing this
restriction. This restriction also interacts subtly with the test for “doneness,” which
the method takes into account.

A typical ending configuration for Brent’s method is that a and b are 2 x x x tol
apart, with x (the best abscissa) at the midpoint of a and b, and therefore fractionally
accurate to +tol.

Indulge us a final reminder that tol should generally be no smaller than the
square root of your machine’s floating-point precision.

#include <math.h>

#include "nrutil.h"

#define ITMAX 100

#define CGOLD 0.3819660

#define ZEPS 1.0e-10

Here ITMAX is the maximum allowed number of iterations; CGOLD is the golden ratio; ZEPS is
a small number that protects against trying to achieve fractional accuracy for a minimum that
happens to be exactly zero.

#define SHFT(a,b,c,d) (a)=(b);(b)=(c) ; (c)=(d);

float brent(float ax, float bx, float cx, float (*f) (float), float tol,

float *xmin)
Given a function f£, and given a bracketing triplet of abscissas ax, bx, cx (such that bx is
between ax and cx, and f£(bx) is less than both f(ax) and f(cx)), this routine isolates
the minimum to a fractional precision of about tol using Brent's method. The abscissa of
the minimum is returned as xmin, and the minimum function value is returned as brent, the
returned function value.

{
int iter;
float a,b,d,etemp,fu,fv,fw,fx,p,q,r,tol1,tol2,u,v,w,x,xm;
float e=0.0; This will be the distance moved on
the step before last.
a=(ax < cx ? ax : cx); a and b must be in ascending order,
b=(ax > cx ? ax : cx); but input abscissas need not be.

; Initializations...

= (+f) (x) 5

for (iter=1;iter<=ITMAX;iter++) { Main program loop.
xm=0.5*(atb) ;
tol2=2.0*(tol1=tol*fabs (x) +ZEPS) ;
if (fabs(x-xm) <= (tol2-0.5*(b-a))) { Test for done here.

*xmin=x};
return fx;
}
if (fabs(e) > tol1) { Construct a trial parabolic fit.

r=(x-w) *(£x-fv) ;
q=(x-v) *(£x-fw) ;
p=(x-v) *q-(x-w) x;
q=2.0*(q-r);

if (q > 0.0) p = -p;
q=fabs(q) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.3 One-Dimensional Search with First Derivatives 405

etemp=e;
e=d;
if (fabs(p) >= fabs(0.5*q*etemp) || p <= q*(a-x) || p >= q*(b-x))
d=CGOLD*(e=(x >= xm ? a-x : b-x));
The above conditions determine the acceptability of the parabolic fit. Here we
take the golden section step into the larger of the two segments.
else {
d=p/q; Take the parabolic step.
u=xtd;
if (u-a < tol2 || b-u < tol2)
d=SIGN(tol1,xm-x) ;

3
} else {
d=CGOLD*(e=(x >= xm ? a-x : b-x));
}
u=(fabs(d) >= toll ? x+d : x+SIGN(tol1,d));
fu=(*£) (u) ;
This is the one function evaluation per iteration.
if (fu <= fx) { Now decide what to do with our func-
if (u >= x) a=x; else b=x; tion evaluation.
SHFT(v,w,x,u) Housekeeping follows:
SHFT(fv,fw,fx,fu)
} else {
if (u < x) a=u; else b=u;
if (fu <= fw || w == x) {
vw;
w=u;
fv=fw;
fw=fu;
} else if (fu <= fv || v==x || v==w) {
v=u;
fv=fu;
3
} Done with housekeeping. Back for
+ another iteration.
nrerror("Too many iterations in brent");
*xmin=x}; Never get here.
return fx;

CITED REFERENCES AND FURTHER READING:

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood Cliffs, NJ: Prentice-
Hall), Chapter 5. [1]

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NU: Prentice-Hall), §8.2.

10.3 One-Dimensional Search with First
Derivatives

Here we want to accomplish precisely the same goal as in the previous
section, namely to isolate a functional minimum that is bracketed by the triplet of
abscissas (a, b,c), but utilizing an additional capability to compute the function’s
first derivative as well as its value.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

406 Chapter 10. Minimization or Maximization of Functions

In principle, we might simply search for a zero of the derivative, ignoring the
function value information, using a root finder like rtflsp or zbrent (§§9.2-9.3).
It doesn’t take long to reject that idea: How do we distinguish maxima from minima?
Where do we go from initial conditions where the derivatives on one or both of
the outer bracketing points indicate that “downhill” is in the direction out of the
bracketed interval?

We don’t want to give up our strategy of maintaining a rigorous bracket on the
minimum at all times. The only way to keep such a bracket is to update it using
function (not derivative) information, with the central point in the bracketing triplet
always that with the lowest function value. Therefore the role of the derivatives can
only be to help us choose new trial points within the bracket.

One school of thought is to “use everything you’ve got”: Compute a polynomial
of relatively high order (cubic or above) that agrees with some number of previous
function and derivative evaluations. For example, there is a unique cubic that agrees
with function and derivative at two points, and one can jump to the interpolated
minimum of that cubic (if there is a minimum within the bracket). Suggested by
Davidon and others, formulas for this tactic are given in [1].

We like to be more conservative than this. Once superlinear convergence sets
in, it hardly matters whether its order is moderately lower or higher. In practical
problems that we have met, most function evaluations are spent in getting globally
close enough to the minimum for superlinear convergence to commence. So we are
more worried about all the funny “stiff” things that high-order polynomials can do
(cf. Figure 3.0.1b), and about their sensitivities to roundoff error.

This leads us to use derivative information only as follows: The sign of the
derivative at the central point of the bracketing triplet (a,b,c) indicates uniquely
whether the next test point should be taken in the interval (a,b) or in the interval
(b,c). The value of this derivative and of the derivative at the second-best-so-far
point are extrapolated to zero by the secant method (inverse linear interpolation),
which by itself is superlinear of order 1.618. (The golden mean again: see [1], p. 57.)
We impose the same sort of restrictions on this new trial point as in Brent’s method.
If the trial point must be rejected, we bisect the interval under scrutiny.

Yes, we are fuddy-duddies when it comes to making flamboyant use of derivative
information in one-dimensional minimization. But we have met too many functions
whose computed “derivatives” don’t integrate up to the function value and don’t
accurately point the way to the minimum, usually because of roundoff errors,
sometimes because of truncation error in the method of derivative evaluation.

You will see that the following routine is closely modeled on brent in the
previous section.

#include <math.h>

#include "nrutil.h"

#define ITMAX 100

#define ZEPS 1.0e-10

#define MOV3(a,b,c, d,e,f) (a)=(d);(b)=(e) ; (c)=(£) ;

float dbrent(float ax, float bx, float cx, float (*f) (float),

float (*df) (float), float tol, float *xmin)
Given a function f and its derivative function df, and given a bracketing triplet of abscissas ax,
bx, cx [such that bx is between ax and cx, and f (bx) is less than both f(ax) and f(cx)],
this routine isolates the minimum to a fractional precision of about tol using a modification of
Brent's method that uses derivatives. The abscissa of the minimum is returned as xmin, and

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.3 One-Dimensional Search with First Derivatives 407

the minimum function value is returned as dbrent, the returned function value.

{

int iter,ok1,ok2; Will be used as flags for whether pro-
float a,b,d,di,d2,du,dv,dw,dx,e=0.0; posed steps are acceptable or not.
float fu,fv,fw,fx,olde,tol1,tol2,u,u1,u2,v,w,x,xm;

Comments following will point out only differences from the routine brent. Read that

routine first.

a=(ax < cx ? ax : cx);

b=(ax > cx ? ax : cx);

X=W=v=bx;

= (+£) (x) 5

= (*df) (x); All our housekeeping chores are dou-

for (iter=1;iter<=ITMAX;iter++) { bled by the necessity of moving
xm=0.5*(atb) ; derivative values around as well
tol1=tol*fabs(x)+ZEPS; as function values.
to12=2.0*tol1;
if (fabs(x-xm) <= (tol2-0.5*(b-a))) {

*xmin=x;
return fx;
}
if (fabs(e) > tol1) {
d1=2.0*(b-a) ; Initialize these d's to an out-of-bracket
d2=d1; value.
if (dw != dx) d1=(w-x)*dx/(dx-dw) ; Secant method with one point.
if (dv != dx) d2=(v-x)*dx/(dx-dv) ; And the other.
Which of these two estimates of d shall we take? We will insist that they be within
the bracket, and on the side pointed to by the derivative at x:
uil=x+d1;
u2=x+d2;
oki = (a-ul)*(ul-b) > 0.0 && dx*d1 <= 0.0;
ok2 = (a-u2)*(u2-b) > 0.0 && dx*d2 <= 0.0;
olde=e; Movement on the step before last.
e=d;
if (ok1 || ok2) { Take only an acceptable d, and if
if (ok1 && ok2) both are acceptable, then take
d=(fabs(d1) < fabs(d2) ? dl : d2); the smallest one.
else if (ok1)
d=d1;
else
d=d2;
if (fabs(d) <= fabs(0.5*olde)) {
u=x+d;
if (u-a < tol2 || b-u < tol2)
d=SIGN(tol1,xm-x) ;
} else { Bisect, not golden section.
d=0.5*(e=(dx >= 0.0 ? a-x : b-x));
Decide which segment by the sign of the derivative.
3
} else {
d=0.5*(e=(dx >= 0.0 7 a-x : b-x));
3
} else {
d=0.5*(e=(dx >= 0.0 ? a-x : b-x));
}
if (fabs(d) >= tol1) {
u=xt+d;
fu=(*f) (u);
} else {
u=x+SIGN(tol1,d);
fu=(*f) (u);
if (fu > fx) { If the minimum step in the downhill
*xmin=x}; direction takes us uphill, then

return fx; we are done.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

408 Chapter 10. Minimization or Maximization of Functions

3
}
du=(*df) (u) ; Now all the housekeeping, sigh.
if (fu <= fx) {
if (u >= x) a=x; else b=x;
MOV3(v,fv,dv, w,fw,dw)
MOV3(w,fw,dw, x,fx,dx)
MOV3(x,fx,dx, u,fu,du)
} else {
if (u < x) a=u; else b=u;
if (fu <= fw || w== x) {
MOV3(v,fv,dv, w,fw,dw)
MOV3(w,fw,dw, u,fu,du)
} else if (fu< fv I] v==x Il v==w) {
MOV3(v,fv,dv, u,fu,du)

}
}
+
nrerror("Too many iterations in routine dbrent");
return 0.0; Never get here.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 55; 454-458. [1]

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood Cliffs, NJ: Prentice-
Hall), p. 78.

10.4 Downhill Simplex Method in
Multidimensions

With this section we begin consideration of multidimensional minimization,
that is, finding the minimum of a function of more than one independent variable.
This section stands apart from those which follow, however: All of the algorithms
after this section will make explicit use of a one-dimensional minimization algorithm
as a part of their computational strategy. This section implements an entirely
self-contained strategy, in which one-dimensional minimization does not figure.

The downhill simplex method is due to Nelder and Mead [1]. The method
requires only function evaluations, not derivatives. It is not very efficient in terms
of the number of function evaluations that it requires. Powell’s method (810.5) is
almost surely faster in all likely applications. However, the downhill simplex method
may frequently be the best method to use if the figure of merit is “get something
working quickly” for a problem whose computational burden is small.

The method has a geometrical naturalness about it which makes it delightful
to describe or work through:

A simplex is the geometrical figure consisting, in N dimensions, of N + 1
points (or vertices) and all their interconnecting line segments, polygonal faces, etc.
In two dimensions, a simplex is a triangle. In three dimensions it is a tetrahedron,
not necessarily the regular tetrahedron. (The simplex method of linear programming,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.4 Downhill Simplex Method in Multidimensions 409

simplex at beginning of step

high
low

reflection

(a)
reflection and expansion
(b)
contraction
(c)
multiple
contraction
(d)
Figure 10.4.1. Possible outcomes for a step in the downhill simplex method. The simplex at the

beginning of the step, here a tetrahedron, is shown, top. The simplex at the end of the step can be any one
of (a) a reflection away from the high point, (b) a reflection and expansion away from the high point, (c)
a contraction along one dimension from the high point, or (d) a contraction along all dimensions towards
the low point. An appropriate sequence of such steps will always converge to a minimum of the function.

described in § 10.8, also makes use of the geometrical concept of a simplex. Otherwise
it is completely unrelated to the algorithm that we are describing in this section.) In
general we are only interested in simplexes that are nondegenerate, i.e., that enclose
a finite inner N-dimensional volume. If any point of a nondegenerate simplex is
taken as the origin, then the N other points define vector directions that span the
N-dimensional vector space.

In one-dimensional minimization, it was possible to bracket a minimum, so that
the success of a subsequent isolation was guaranteed. Alas! There is no analogous
procedure in multidimensional space. For multidimensional minimization, the best
we can do is give our algorithm a starting guess, that is, an N-vector of independent
variables as the first point to try. The algorithm is then supposed to make its own way

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

410 Chapter 10. Minimization or Maximization of Functions

downhill through the unimaginable complexity of an N-dimensional topography,
until it encounters a (local, at least) minimum.

The downhill simplex method must be started not just with a single point, but
with N + 1 points, defining an initial simplex. If you think of one of these points
(it matters not which) as being your initial starting point Po, then you can take
the other N points to be

Pj = Po + dex (10.4.1)

where the e;’s are N unit vectors, and where \ is a constant which is your guess
of the problem’s characteristic length scale. (Or, you could have different \ ;’s for
each vector direction.)

The downhill simplex method now takes a series of steps, most steps just moving
the point of the simplex where the function is largest (“highest point”) through the
opposite face of the simplex to a lower point. These steps are called reflections,
and they are constructed to conserve the volume of the simplex (hence maintain
its nondegeneracy). When it can do so, the method expands the simplex in one or
another direction to take larger steps. When it reaches a “valley floor,” the method
contracts itself in the transverse direction and tries to ooze down the valley. If there
is a situation where the simplex is trying to “pass through the eye of a needle,” it
contracts itself in all directions, pulling itself in around its lowest (best) point. The
routine name amoeba is intended to be descriptive of this kind of behavior; the basic
moves are summarized in Figure 10.4.1.

Termination criteria can be delicate in any multidimensional minimization
routine. Without bracketing, and with more than one independent variable, we
no longer have the option of requiring a certain tolerance for a single independent
variable. We typically can identify one “cycle” or “step” of our multidimensional
algorithm. It is then possible to terminate when the vector distance moved in that
step is fractionally smaller in magnitude than some tolerance tol. Alternatively,
we could require that the decrease in the function value in the terminating step be
fractionally smaller than some tolerance ftol. Note that while tol should not
usually be smaller than the square root of the machine precision, it is perfectly
appropriate to let ftol be of order the machine precision (or perhaps slightly larger
so as not to be diddled by roundoff).

Note well that either of the above criteria might be fooled by a single anomalous
step that, for one reason or another, failed to get anywhere. Therefore, it is frequently
a good idea to restart a multidimensional minimization routine at a point where
it claims to have found a minimum. For this restart, you should reinitialize any
ancillary input quantities. In the downhill simplex method, for example, you should
reinitialize N of the N + 1 vertices of the simplex again by equation (10.4.1), with
Po being one of the vertices of the claimed minimum.

Restarts should never be very expensive; your algorithm did, after all, converge
to the restart point once, and now you are starting the algorithm already there.

Consider, then, our N-dimensional amoeba:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.4 Downhill Simplex Method in Multidimensions 411

#include <math.h>
#include "nrutil.h"

#define TINY 1.0e-10 A small number.
#define NMAX 5000 Maximum allowed number of function evalua-
#define GET_PSUM \ tions.

for (j j<=ndim; j++) {\

for (sum=0.0,i=1;i<=mpts;i++) sum += pli] [j];\
psum[(j]=sum;}
#define SWAP(a,b) {swap=(a) ; (a)=(b) ; (b)=swap;}

void amoeba(float **p, float y[], int ndim, float ftol,

float (*funk) (float []), int *nfunk)
Multidimensional minimization of the function funk (x) where x[1..ndim] is a vector in ndim
dimensions, by the downhill simplex method of Nelder and Mead. The matrix pli. -ndim+1]
(1. .ndim] is input. Its ndim+1 rows are ndim-dimensional vectors which are the vertices of
the starting simplex. Also input is the vector y[1..ndim+1], whose components must be pre-
initialized to the values of funk evaluated at the ndimt1 vertices (rows) of p; and ftol the
fractional convergence tolerance to be achieved in the function value (n.b.!). On output, p and
y will have been reset to ndim+1 new points all within ftol of a minimum function value, and
nfunk gives the number of function evaluations taken.
{

float amotry(float **p, float y[], float psum[], int ndim,

float (*funk)(float []), int ihi, float fac);
int i,ihi,ilo,inhi,j,mpts=ndim+1;
float rtol,sum,swap,ysave,ytry,*psum;

psum=vector(1,ndim) ;
+nfunk=0;
GET_PSUM
for Gi) f
ilo=1;
First we must determine which point is the highest (worst), next-highest, and lowest
(best), by looping over the points in the simplex.
ahi = y(1]>y(2] ? (inhi=2,1) : (inhi=1,2);
for (i=1;i<=mpts;it+) {
if (y[i] <= y[ilo]) ilo=i;
if (yfi] > ylini]) {

inh: hi;

ihi=i;
} else if (y[i] > y[inhi] && i != ihi) inhi=i;

}
rtol=2.0*fabs(y[ihi]-y[ilo])/(fabs(y[ihi])+fabs(y [ilo] )+TINY) ;
Compute the fractional range from highest to lowest and return if satisfactory.
if (rtol < ftol) { If returning, put best point and value in slot 1.
SWAP (y([1] ,y[ilo])
for (i=1;i<=ndim;it++) SWAP(p[1] [i] ,p[ilo] [i])
break;
}
if (#nfunk >= NMAX) nrerror("NMAX exceeded") ;
*nfunk += 2;
Begin a new iteration. First extrapolate by a factor —1 through the face of the simplex
across from the high point, i.e., reflect the simplex from the high point.
ytry=amotry(p,y,psum,ndim,funk,ihi,-1.0);
if (ytry <= y[ilo])
Gives a result better than the best point, so try an additional extrapolation by a
factor 2.
ytry=amotry(p,y,psum,ndim,funk,ihi,2.0);
else if (ytry >= y[inhi]) {
The reflected point is worse than the second-highest, so look for an intermediate
lower point, i.e., do a one-dimensional contraction.
ysave=y [ihi] ;
ytry=amotry(p,y,psum,ndim,funk,ihi,0.5);
if (ytry >= ysave) { Can't seem to get rid of that high point. Better
for (i=1;i<=mpts;it++) { contract around the lowest (best) point.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

412 Chapter 10. Minimization or Maximization of Functions

if (i != ilo) {
for (j=1;j<=ndim; j++)
pli] [j]=psum[j]=0.5* (p[i] [j]+plilo] [j]);
y [i] =(*funk) (psum) ;

+

}

*nfunk += ndim; Keep track of function evaluations.

GET_PSUM Recompute psum.

3
} else --(+*nfunk) ; Correct the evaluation count.

} Go back for the test of doneness and the next
free_vector(psum,1,ndim) ; iteration.

#include "nrutil.h"

float amotry(float **p, float y[], float psum[], int ndim,
float (*funk) (float []), int ihi, float fac)
Extrapolates by a factor fac through the face of the simplex across from the high point, tries
it, and replaces the high point if the new point is better.
{
int j;
float faci,fac2,ytry,*ptry;

ptry=vector(1,ndim) ;
faci=(1.0-fac)/ndim;
fac2=faci-fac;
for (j=1;j<=ndim;j++) ptry[j]=psum[j]*faci-p[ihi] [j]*fac2;
ytry=(*funk) (ptry) ; Evaluate the function at the trial point.
if (ytry < yl[ihi]) { If it’s better than the highest, then replace the highest.
ylihi]=ytry;
for (j=1;j<=ndim;j++) {
psum[j] += ptry(j]-plihi] [j];
plihi] [j]l=ptry[j];

}
}
free_vector(ptry,1,ndim) ;
return ytry;

CITED REFERENCES AND FURTHER READING:
Nelder, J.A., and Mead, R. 1965, Computer Journal, vol. 7, pp. 308-313. [1]
Yarbro, L.A., and Deming, S.N. 1974, Analytica Chimica Acta, vol. 73, pp. 391-398.

Jacoby, S.L.S, Kowalik, J.S., and Pizzo, J.T. 1972, Iterative Methods for Nonlinear Optimization
Problems (Englewood Cliffs, NJ: Prentice-Hall).

10.5 Direction Set (Powell’s) Methods in
Multidimensions

We know (§10.1-810.3) how to minimize a function of one variable. If we
start at a point P in N-dimensional space, and proceed from there in some vector

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.5 Direction Set (Powell's) Methods in Multidimensions 413

direction n, then any function of N variables f(P) can be minimized along the line
n by our one-dimensional methods. One can dream up various multidimensional
minimization methods that consist of sequences of such line minimizations. Different
methods will differ only by how, at each stage, they choose the next direction n to
try. All such methods presume the existence of a “black-box” sub-algorithm, which
we might call 1inmin (given as an explicit routine at the end of this section), whose
definition can be taken for now as

linmin: Given as input the vectors P and n, and the
function f, find the scalar \ that minimizes f(P + An).
Replace P by P + An. Replace n by An. Done.

All the minimization methods in this section and in the two sections following
fall under this general schema of successive line minimizations. (The algorithm
in $10.7 does not need very accurate line minimizations. Accordingly, it has its
own approximate line minimization routine, Insrch.) In this section we consider
a class of methods whose choice of successive directions does not involve explicit
computation of the function’s gradient; the next two sections do require such gradient
calculations. You will note that we need not specify whether 1inmin uses gradient
information or not. That choice is up to you, and its optimization depends on your
particular function. You would be crazy, however, to use gradients in linmin and
not use them in the choice of directions, since in this latter role they can drastically
reduce the total computational burden.

But what if, in your application, calculation of the gradient is out of the question.
You might first think of this simple method: Take the unit vectors e1,e2,...ey asa
set of directions. Using 1inmin, move along the first direction to its minimum, then
from there along the second direction to its minimum, and so on, cycling through the
whole set of directions as many times as necessary, until the function stops decreasing.

This simple method is actually not too bad for many functions. Even more
interesting is why it is bad, i.e. very inefficient, for some other functions. Consider
a function of two dimensions whose contour map (level lines) happens to define a
long, narrow valley at some angle to the coordinate basis vectors (see Figure 10.5.1).
Then the only way “down the length of the valley” going along the basis vectors at
each stage is by a series of many tiny steps. More generally, in N dimensions, if
the function’s second derivatives are much larger in magnitude in some directions
than in others, then many cycles through all N basis vectors will be required in
order to get anywhere. This condition is not all that unusual; according to Murphy’s
Law, you should count on it.

Obviously what we need is a better set of directions than the e;’s. All direction
set methods consist of prescriptions for updating the set of directions as the method
proceeds, attempting to come up with a set which either (i) includes some very
good directions that will take us far along narrow valleys, or else (more subtly)
(ii) includes some number of “‘non-interfering” directions with the special property
that minimization along one is not “spoiled” by subsequent minimization along
another, so that interminable cycling through the set of directions can be avoided.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

414 Chapter 10. Minimization or Maximization of Functions

start

Z
fy x

Figure 10.5.1. Successive minimizations along coordinate directions in a long, narrow “valley” (shown
as contour lines). Unless the valley is optimally oriented, this method is extremely inefficient, taking
many tiny steps to get to the minimum, crossing and re-crossing the principal axis.

Conjugate Directions

This concept of “non-interfering” directions, more conventionally called con-
jugate directions, is worth making mathematically explicit.

First, note that if we minimize a function along some direction u, then the
gradient of the function must be perpendicular to u at the line minimum; if not, then
there would still be a nonzero directional derivative along u.

Next take some particular point P as the origin of the coordinate system with
coordinates x. Then any function f can be approximated by its Taylor series

(a) of
i ij

(10.5.1)

where
Of

Ox,Ox;

c=f(P) b=-Vflp — [Aly (10.5.2)

P

The matrix A whose components are the second partial derivative matrix of the
function is called the Hessian matrix of the function at P.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.5 Direction Set (Powell's) Methods in Multidimensions 415

In the approximation of (10.5.1), the gradient of f is easily calculated as
Vf=A-x—b (10.5.3)

(This implies that the gradient will vanish — the function will be at an extremum —
at a value of x obtained by solving A - x = b. This idea we will return to in §10.7!)
How does the gradient V f change as we move along some direction? Evidently

6(Vf) =A: (5x) (10.5.4)

Suppose that we have moved along some direction u to a minimum and now
propose to move along some new direction v. The condition that motion along v not
spoil our minimization along u is just that the gradient stay perpendicular to u, ie.,
that the change in the gradient be perpendicular to u. By equation (10.5.4) this is just

O=u-d(Vf)=u-A-v (10.5.5)

When (10.5.5) holds for two vectors u and v, they are said to be conjugate.
When the relation holds pairwise for all members of a set of vectors, they are said
to be a conjugate set. If you do successive line minimization of a function along
a conjugate set of directions, then you don’t need to redo any of those directions
(unless, of course, you spoil things by minimizing along a direction that they are
not conjugate to).

A triumph for a direction set method is to come up with a set of N linearly
independent, mutually conjugate directions. Then, one pass of V line minimizations
will put it exactly at the minimum of a quadratic form like (10.5.1). For functions
f that are not exactly quadratic forms, it won’t be exactly at the minimum; but
repeated cycles of N line minimizations will in due course converge quadratically
to the minimum.

Powell’s Quadratically Convergent Method

Powell first discovered a direction set method that does produce N mutually
conjugate directions. Here is how it goes: Initialize the set of directions u; to
the basis vectors,

ua =e i=1,...,N (10.5.6)

Now repeat the following sequence of steps (“basic procedure”) until your function
stops decreasing:
e Save your starting position as Po.

e For i = 1,...,N, move P;_, to the minimum along direction u; and
call this point P;.
e Fori = 1,...,N —1, set uj — uj4i.

e Set uy — Py — Po.
e Move Py to the minimum along direction u y and call this point Po.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

416 Chapter 10. Minimization or Maximization of Functions

Powell, in 1964, showed that, for a quadratic form like (10.5.1), k iterations
of the above basic procedure produce a set of directions u; whose last k members
are mutually conjugate. Therefore, NV iterations of the basic procedure, amounting
to N(N + 1) line minimizations in all, will exactly minimize a quadratic form.
Brent [1] gives proofs of these statements in accessible form.

Unfortunately, there is a problem with Powell’s quadratically convergent al-
gorithm. The procedure of throwing away, at each stage, u; in favor of Py — Po
tends to produce sets of directions that “fold up on each other” and become linearly
dependent. Once this happens, then the procedure finds the minimum of the function
f only over a subspace of the full N-dimensional case; in other words, it gives the
wrong answer. Therefore, the algorithm must not be used in the form given above.

There are a number of ways to fix up the problem of linear dependence in
Powell’s algorithm, among them:

1. You can reinitialize the set of directions u ; to the basis vectors e; after every
N or N + 1 iterations of the basic procedure. This produces a serviceable method,
which we commend to you if quadratic convergence is important for your application
(i.e., if your functions are close to quadratic forms and if you desire high accuracy).

2. Brent points out that the set of directions can equally well be reset to
the columns of any orthogonal matrix. Rather than throw away the information
on conjugate directions already built up, he resets the direction set to calculated
principal directions of the matrix A (which he gives a procedure for determining).
The calculation is essentially a singular value decomposition algorithm (see §2.6).
Brent has a number of other cute tricks up his sleeve, and his modification of
Powell’s method is probably the best presently known. Consult [1] for a detailed
description and listing of the program. Unfortunately it is rather too elaborate for
us to include here.

3. You can give up the property of quadratic convergence in favor of a more
heuristic scheme (due to Powell) which tries to find a few good directions along
narrow valleys instead of N necessarily conjugate directions. This is the method
that we now implement. (It is also the version of Powell’s method given in Acton [2],
from which parts of the following discussion are drawn.)

Discarding the Direction of Largest Decrease

The fox and the grapes: Now that we are going to give up the property of
quadratic convergence, was it so important after all? That depends on the function
that you are minimizing. Some applications produce functions with long, twisty
valleys. Quadratic convergence is of no particular advantage to a program which
must slalom down the length of a valley floor that twists one way and another (and
another, and another, ... — there are N dimensions!). Along the long direction,
a quadratically convergent method is trying to extrapolate to the minimum of a
parabola which just isn’t (yet) there; while the conjugacy of the N — 1 transverse
directions keeps getting spoiled by the twists.

Sooner or later, however, we do arrive at an approximately ellipsoidal minimum
(cf. equation 10.5.1 when b, the gradient, is zero). Then, depending on how much
accuracy we require, a method with quadratic convergence can save us several times
N? extra line minimizations, since quadratic convergence doubles the number of
significant figures at each iteration.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.5 Direction Set (Powell's) Methods in Multidimensions 417

The basic idea of our now-modified Powell’s method is still to take P y — Po as
anew direction; it is, after all, the average direction moved after trying all NV possible
directions. For a valley whose long direction is twisting slowly, this direction is
likely to give us a good run along the new long direction. The change is to discard
the old direction along which the function f made its largest decrease. This seems
paradoxical, since that direction was the best of the previous iteration. However, it
is also likely to be a major component of the new direction that we are adding, so
dropping it gives us the best chance of avoiding a buildup of linear dependence.

There are a couple of exceptions to this basic idea. Sometimes it is better not
to add a new direction at all. Define

fo=f(Po) fv =f(P) fe = f(2Py ~ Po) (10.5.7)

Here fg is the function value at an “extrapolated” point somewhat further along
the proposed new direction. Also define Af to be the magnitude of the largest
decrease along one particular direction of the present basic procedure iteration. (Af
is a positive number.) Then:

1. If fz > fo, then keep the old set of directions for the next basic procedure,
because the average direction Py — Po is all played out.

2. 1f2 (fo —2fv + fw) [(fo — fw) — Af]? > (fo — fe)? fF, then keep the old
set of directions for the next basic procedure, because either (i) the decrease along
the average direction was not primarily due to any single direction’s decrease, or (ii)
there is a substantial second derivative along the average direction and we seem to
be near to the bottom of its minimum.

The following routine implements Powell’s method in the version just described.
In the routine, xi is the matrix whose columns are the set of directions n ;; otherwise
the correspondence of notation should be self-evident.

#include <math.h>

#include "nrutil.h"

#define TINY 1.0e-25 A small number.

#define ITMAX 200 Maximum allowed iterations.

void powell(float p[], float **xi, int n, float ftol, int *iter, float *fret,

float (*func) (float []))
Minimization of a function func of n variables. Input consists of an initial starting point
p[1..n]; an initial matrix xi[1..n][1..n], whose columns contain the initial set of di-
rections (usually the n unit vectors); and ftol, the fractional tolerance in the function value
such that failure to decrease by more than this amount on one iteration signals doneness. On
output, p is set to the best point found, xi is the then-current direction set, fret is the returned
function value at p, and iter is the number of iterations taken. The routine Linmin is used.
{

void linmin(float p[], float xif[], int n, float *fret,

float (*func) (float []));
int i,ibig,j;
float del,fp,fptt,t,*pt,*ptt,*xit;

pt=vector(1,n);
ptt=vector(1,n);
xit=vector(1,n);
*fret=(*func) (p);
for (j=1;j<-n; j++) pt(jl=pljl; Save the initial point.
for (*iter=1;;++(*iter)) {
fp=(*fret) ;
ibig=0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

418 Chapter 10. Minimization or Maximization of Functions

del=0.0; Will be the biggest function decrease.
for (i=1;i<=n;it+) { In each iteration, loop over all directions in the set.
for (j=1;j<=n;jt+) xit(jl=xilj] li]; Copy the direction,

fptt=(*fret) ;
linmin(p,xit,n,fret,func) ; minimize along it,
if (fptt-(*fret) > del) { and record it if it is the largest decrease
del=fptt-(*fret) ; so far.
ibig=i;
}
}
if (2.0*(fp-(*fret)) <= ftol*(fabs(fp)+fabs(*fret))+TINY) {
free_vector(xit,1,n); Termination criterion.
free_vector(ptt,1,n);
free_vector(pt,1,n);
return;
}
if (*iter == ITMAX) nrerror("powell exceeding maximum iterations.") ;
for (j=1;j<=n;jt+) { Construct the extrapolated point and the
ptt(j]=2.0*p(j]-pt [jl]; average direction moved. Save the
xit (j]=p(j]-pt (jl; old starting point.
ptljl=pljl;

fptt=(*func) (ptt) ;
if (fptt < fp) {
t=2.0*(fp-2.0*(*fret)+fptt) *SQR(fp-(*fret)-del)-del*SQR(fp-fptt) ;
if (t < 0.0) {
linmin(p,xit,n,fret,func) ; Move to the minimum of the new direc-
for (j=1;j<en;j++) { tion, and save the new direction.
xilj] [ibig]=xilj] In];
xilj] n)=xit[jl;

Function value at extrapolated point.

+ Back for another iteration.

Implementation of Line Minimization

Make no mistake, there is a right way to implement linmin: It is to use
the methods of one-dimensional minimization described in §10.1-§10.3, but to
rewrite the programs of those sections so that their bookkeeping is done on vector-
valued points P (all lying along a given direction n) rather than scalar-valued
abscissas x. That straightforward task produces long routines densely populated
with “for (k=1;k<=n;k++)” loops.

We do not have space to include such routines in this book. Our 1inmin, which
works just fine, is instead a kind of bookkeeping swindle. It constructs an “artificial”
function of one variable called f1dim, which is the value of your function, say,
func, along the line going through the point p in the direction xi. linmin calls our
familiar one-dimensional routines mnbrak (§10.1) and brent (§10.3) and instructs
them to minimize f1dim. linmin communicates with f1dim “over the head” of
mnbrak and brent, through global (external) variables. That is also how it passes
to f£1dim a pointer to your user-supplied function.

The only thing inefficient about 1inmin is this: Its use as an interface between a
multidimensional minimization strategy and a one-dimensional minimization routine
results in some unnecessary copying of vectors hither and yon. That should not

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.5 Direction Set (Powell's) Methods in Multidimensions 419

normally be a significant addition to the overall computational burden, but we cannot
disguise its inelegance.

#include "nrutil.h"
#define TOL 2.0e-4 Tolerance passed to brent.

int ncom; Global variables communicate with fidim.
float *pcom,*xicom, (*nrfunc) (float []);

void linmin(float p[], float xi[], int n, float *fret, float (*func) (float []))
Given an n-dimensional point p[1..n] and an n-dimensional direction xi[1..n], moves and
resets p to where the function func(p) takes on a minimum along the direction xi from p,
and replaces xi by the actual vector displacement that p was moved. Also returns as fret
the value of func at the returned location p. This is actually all accomplished by calling the
routines mnbrak and brent.

{
float brent(float ax, float bx, float cx,
float (*f) (float), float tol, float *xmin);
float fidim(float x);
void mnbrak(float *ax, float *bx, float *cx, float *fa, float *fb,
float *fc, float (*func) (float));
int j;
float xx,xmin,fx,fb,fa,bx,ax;
ncom=n; Define the global variables.
pcom=vector(1,n);
xicom=vector(1,n);
unc;
jj<-njjt+) f
peom[j]=p[j];
xicom[(j]=xilj];
+
ax=0.0; Initial guess for brackets.
xx=1.0;
mnbrak (&ax,&xx,&bx,&fa,&fx,&fb,fidim) ;
*fret=brent (ax,xx,bx,fidim,TOL, &xmin) ;
for ( j<en;jt+) { Construct the vector results to return.
xilj] *= xmin;
plj] += xiljl;
+
free_vector(xicom,1,n);
free_vector(pcom,1,n);
3

#include "nrutil.h"

extern int ncom; Defined in linmin.
extern float *pcom,*xicom,(*nrfunc) (float []);

float fidim(float x)
Must accompany linmin.
t

int j;

float f,*xt;

xt=vector(1,ncom) ;

for (j=1;j<=ncom; j++) xt[j]=pcom[j]+x*xicom[j];
f=(#nrfunc) (xt);

free_vector(xt,1,ncom) ;

return f;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

420 Chapter 10. Minimization or Maximization of Functions

CITED REFERENCES AND FURTHER READING:

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood Cliffs, NJ: Prentice-
Hall), Chapter 7. [1]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 464-467. [2]

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic
Press), pp. 259-262.

10.6 Conjugate Gradient Methods in
Multidimensions

We consider now the case where you are able to calculate, at a given N-
dimensional point P, not just the value of a function f(P) but also the gradient
(vector of first partial derivatives) Vf (P).

A rough counting argument will show how advantageous it is to use the gradient
information: Suppose that the function f is roughly approximated as a quadratic
form, as above in equation (10.5.1),

f(x) © 6 — box + SKAUX (10.6.1)

Then the number of unknown parameters in f is equal to the number of free
parameters in A and b, which is 3N(N + 1), which we see to be of order N?.
Changing any one of these parameters can move the location of the minimum.
Therefore, we should not expect to be able to find the minimum until we have
collected an equivalent information content, of order N ? numbers.

In the direction set methods of §10.5, we collected the necessary information by
making on the order of N? separate line minimizations, each requiring “a few” (but
sometimes a big few!) function evaluations. Now, each evaluation of the gradient
will bring us N new components of information. If we use them wisely, we should
need to make only of order N separate line minimizations. That is in fact the case
for the algorithms in this section and the next.

A factor of NV improvement in computational speed is not necessarily implied.
As a rough estimate, we might imagine that the calculation of each component of
the gradient takes about as long as evaluating the function itself. In that case there
will be of order N? equivalent function evaluations both with and without gradient
information. Even if the advantage is not of order N, however, it is nevertheless
quite substantial: (i) Each calculated component of the gradient will typically save
not just one function evaluation, but a number of them, equivalent to, say, a whole
line minimization. (ii) There is often a high degree of redundancy in the formulas
for the various components of a function’s gradient; when this is so, especially when
there is also redundancy with the calculation of the function, then the calculation of
the gradient may cost significantly less than N function evaluations.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.6 Conjugate Gradient Methods in Multidimensions 421

(a)

(b)

Figure 10.6.1. (a) Steepest descent method in a long, narrow “valley.” While more efficient than the
strategy of Figure 10.5.1, steepest descent is nonetheless an inefficient strategy, taking many steps to
reach the valley floor. (b) Magnified view of one step: A step starts off in the local gradient direction,
perpendicular to the contour lines, and travers aight line until a local minimum is reached, where
the traverse is parallel to the local contour lines.

Acommon beginner’s error is to assume that any reasonable way of incorporating
gradient information should be about as good as any other. This line of thought leads
to the following not very good algorithm, the steepest descent method:

Steepest Descent: Start at a point Po. As many times
as needed, move from point P; to the point P;,1 by
minimizing along the line from P; in the direction of
the local downhill gradient —V f (Pj).

The problem with the steepest descent method (which, incidentally, goes back
to Cauchy), is similar to the problem that was shown in Figure 10.5.1. The method
will perform many small steps in going down a long, narrow valley, even if the valley
is a perfect quadratic form. You might have hoped that, say in two dimensions,
your first step would take you to the valley floor, the second step directly down
the long axis; but remember that the new gradient at the minimum point of any
line minimization is perpendicular to the direction just traversed. Therefore, with
the steepest descent method, you must make a right angle turn, which does not, in
general, take you to the minimum. (See Figure 10.6.1.)

Just as in the discussion that led up to equation (10.5.5), we really want a way
of proceeding not down the new gradient, but rather in a direction that is somehow
constructed to be conjugate to the old gradient, and, insofar as possible, to all
previous directions traversed. Methods that accomplish this construction are called
conjugate gradient methods.

In §2.7 we discussed the conjugate gradient method as a technique for solving
linear algebraic equations by minimizing a quadratic form. That formalism can also
be applied to the problem of minimizing a function approximated by the quadratic

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

422 Chapter 10. Minimization or Maximization of Functions

form (10.6.1). Recall that, starting with an arbitrary initial vector gq and letting
ho = Qo. the conjugate gradient method constructs two sequences of vectors from
the recurrence
S41 = Si —AMA hE hig = Sigg +yihi = = 0,1,2,... (10.6.2)
The vectors satisfy the orthogonality and conjugacy conditions
g,°8; =0 h;-A-h; =0 g, hy =0 j<i (10.6.3)

The scalars \; and +; are given by

Si Si gi chi

i= ot _ _ fit 10.6.4

h;-A-h; h;-A-h; ( )

_ Sign Sign (10.6.5)
"8 8;

Equations (10.6.2)—(10.6.5) are simply equations (2.7.32)—(2.7.35) for a symmetric
A in a new notation. (A self-contained derivation of these results in the context of
function minimization is given by Polak [1].)

Now suppose that we knew the Hessian matrix A in equation (10.6.1). Then
we could use the construction (10.6.2) to find successively conjugate directions h ;
along which to line-minimize. After N such, we would efficiently have arrived at
the minimum of the quadratic form. But we don’t know A.

Here is a remarkable theorem to save the day: Suppose we happen to have
g, = —Vf(P;), for some point P;, where f is of the form (10.6.1). Suppose that we
proceed from P; along the direction h; to the local minimum of f located at some
point P;41 and then set g,,, = —Vf(Pi+1). Then, this g,,, is the same vector
as would have been constructed by equation (10.6.2). (And we have constructed
it without knowledge of A!)

Proof: By equation (10.5.3),g; = —A-P; +b, and

Sign = —A- (Pi + Ahi) +b = g, — A= hi (10.6.6)

with \ chosen to take us to the line minimum. But at the line minimum h;- Vf =
—h; - g;4, = 0. This latter condition is easily combined with (10.6.6) to solve for
X. The result is exactly the expression (10.6.4). But with this value of A, (10.6.6)
is the same as (10.6.2), q.e.d.

We have, then, the basis of an algorithm that requires neither knowledge of the
Hessian matrix A, nor even the storage necessary to store such a matrix. A sequence
of directions h; is constructed, using only line minimizations, evaluations of the
gradient vector, and an auxiliary vector to store the latest in the sequence of g’s.

The algorithm described so far is the original Fletcher-Reeves version of the
conjugate gradient algorithm. Later, Polak and Ribiere introduced one tiny, but
sometimes significant, change. They proposed using the form

ay; — (eet = Bi) “Big (10.6.7)

Si i

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

10.6 Conjugate Gradient Methods in Multidimensions 423

instead of equation (10.6.5). “Wait,” you say, “aren’t they equal by the orthogonality
conditions (10.6.3)?” They are equal for exact quadratic forms. In the real world,
however, your function is not exactly a quadratic form. Arriving at the supposed
minimum of the quadratic form, you may still need to proceed for another set of
iterations. There is some evidence [2] that the Polak-Ribiere formula accomplishes
the transition to further iterations more gracefully: When it runs out of steam, it
tends to reset h to be down the local gradient, which is equivalent to beginning the
conjugate-gradient procedure anew.

The following routine implements the Polak-Ribiere variant, which we recom-
mend; but changing one program line, as shown, will give you Fletcher-Reeves. The
routine presumes the existence of a function func (p), where p[1..n] is a vector
of length n, and also presumes the existence of a function dfunc(p,df) that sets
the vector gradient df[1..n] evaluated at the input point p.

The routine calls 1inmin to do the line minimizations. As already discussed,
you may wish to use a modified version of linmin that uses dbrent instead of
brent, i.e., that uses the gradient in doing the line minimizations. See note below.

#include <math.h>

#include "nrutil.h"

#define ITMAX 200

#define EPS 1.0e-10

Here ITMAX is the maximum allowed number of iterations, while EPS is a small number to
rectify the special case of converging to exactly zero function value.

#define FREEALL free_vector(xi,1,n) ;free_vector(h,1,n) ;free_vector(g,1,n);

void frprmn(float p[], int n, float ftol, int *iter, float *fret,

float (*func) (float []), void (*dfunc) (float [], float []))
Given a starting point p[1..n], Fletcher-Reeves-Polak-Ribiere minimization is performed on a
function func, using its gradient as calculated by a routine dfunc. The convergence tolerance
on the function value is input as ftol. Returned quantities are p (the location of the minimum),
iter (the number of iterations that were performed), and fret (the minimum value of the
function). The routine linmin is called to perform line minimizations.
{

void linmin(float p[], float xif[], int n, float *fret,

float (*func) (float []));

int j,its;

float gg,gam,fp,dgg;

float *g,*h,*xi;

g=vector(1,n);
h=vector(1,n);

xi=vector(1,n);
fp=(*func) (p); Initializations.
(*dfunc) (p,xi) ;
for (j= =n;jt+) {
glj] = -xiljl];
xiljl=h[jl=gljl;
}
for (its=1;its<=ITMAX;itst++) { Loop over iterations.

*iter=its;
linmin(p,xi,n,fret,func) ; Next statement is the normal return:
if (2.0*fabs(*fret-fp) <= ftol*(fabs(*fret)+fabs(fp)+EPS)) {
FREEALL
return;
}
fp= *fret;
(*dfunc) (p,xi) ;
dgg=gg=0.0;
for (j=1;j<=n;jt+) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

424 Chapter 10. Minimization or Maximization of Functions

ge += eljl*eljl;

/* dgg += xilj]*xiljl]; */ This statement for Fletcher-Reeves.

dgg t= (xiljl+g[j])*xiljl; This statement for Polak-Ribiere.

if (gg == 0.0) { Unlikely. If gradient is exactly zero then
FREEALL we are already done.
return;

}

gam=dgg/gg;

for (j=1;j<=n;jt+) {
glj] = -xiljl;

xilj]=h(jl=glj]+gam*h[j];
+
+

nrerror("Too many iterations in frprmn");

Note on Line Minimization Using Derivatives

Kindly reread the last part of §10.5. We here want to do the same thing, but
using derivative information in performing the line minimization.

The modified version of linmin, called dlinmin, and its required companion
routine dfidim follow:

#include "nrutil.h"
#define TOL 2.0e-4 Tolerance passed to dbrent.

int ncom; Global variables communicate with df1dim.
float *pcom,*xicom, (*nrfunc) (float []);
void (*nrdfun) (float [], float []);

void dlinmin(float p[], float xi[], int n, float *fret, float (*func)(float []),
void (*dfunc) (float [], float []))
Given an n-dimensional point p[1..n] and an n-dimensional direction xi[1..n], moves and
resets p to where the function func(p) takes on a minimum along the direction xi from p,
and replaces xi by the actual vector displacement that p was moved. Also returns as fret
the value of func at the returned location p. This is actually all accomplished by calling the
routines mnbrak and dbrent.
{
float dbrent(float ax, float bx, float cx,
float (*f) (float), float (*df) (float), float tol, float *xmin) ;
float fidim(float x);
float dfidim(float x);
void mnbrak(float *ax, float *bx, float *cx, float *fa, float *fb,
float *fc, float (*func) (float));
int j;
float xx,xmin,fx,fb,fa,bx,ax;

ncom=n; Define the global variables.
pcom=vector(1,n);

xicom=vector(1,n);

nrfunc=func;

peom[j]=pLj];
xicom[j]=xilj];

Initial guess for brackets.

xx=1.0;
mnbrak (&ax,&xx,&bx,&fa,&fx,&fb,f1dim) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.7 Variable Metric Methods in Multidimensions 425

*fret=dbrent (ax,xx,bx,fidim,dfidim,TOL,&xmin) ;

for (j=1;j<=n;j++) { Construct the vector results to return.
xilj] xmin;
plj] += xiljl;

}
free_vector(xicom,1,n);
free_vector(pcom,1,n);

#include "nrutil.h"

extern int ncom; Defined in dlinmin.
extern float *pcom,*xicom,(*nrfunc) (float []);
extern void (*nrdfun) (float [], float []);

float dfidim(float x)
t
int j;
float df1=0.0;
float *xt,*df;

xt=vector(1,ncom) ;

df=vector(1,ncom) ;

for (j=1;j<=ncom; j++) xt[j]=pcom[j]+x*xicom[j];
(*nrdfun) (xt df) ;

for (j=1;j<=ncom;j++) df1 += df[j]*xicom[j];
free_vector(df,1,ncom) ;

free_vector(xt,1,ncom) ;

return df1;

CITED REFERENCES AND FURTHER READING:
Polak, E. 1971, Computational Methods in Optimization (New York: Academic Press), §2.3. [1]

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic Press),
Chapter Ill.1.7 (by K.W. Brodlie). [2]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§8.7.

10.7 Variable Metric Methods in
Multidimensions

The goal of variable metric methods, which are sometimes called quasi-Newton
methods, is not different from the goal of conjugate gradient methods: to accumulate
information from successive line minimizations so that N such line minimizations
lead to the exact minimum of a quadratic form in N dimensions. In that case, the
method will also be quadratically convergent for more general smooth functions.

Both variable metric and conjugate gradient methods require that you are able to
compute your function’s gradient, or first partial derivatives, at arbitrary points. The
variable metric approach differs from the conjugate gradient in the way that it stores

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

426 Chapter 10. Minimization or Maximization of Functions

and updates the information that is accumulated. Instead of requiring intermediate
storage on the order of N, the number of dimensions, it requires a matrix of size
N x N. Generally, for any moderate JN, this is an entirely trivial disadvantage.

On the other hand, there is not, as far as we know, any overwhelming advantage
that the variable metric methods hold over the conjugate gradient techniques, except
perhaps a historical one. Developed somewhat earlier, and more widely propagated,
the variable metric methods have by now developed a wider constituency of satisfied
users. Likewise, some fancier implementations of variable metric methods (going
beyond the scope of this book, see below) have been developed to a greater level of
sophistication on issues like the minimization of roundoff error, handling of special
conditions, and so on. We tend to use variable metric rather than conjugate gradient,
but we have no reason to urge this habit on you.

Variable metric methods come in two main flavors. One is the Davidon-Fletcher-
Powell (DFP) algorithm (sometimes referred to as simply Fletcher-Powell). The
other goes by the name Broyden-Fletcher-Goldfarb-Shanno (BFGS). The BFGS and
DFP schemes differ only in details of their roundoff error, convergence tolerances,
and similar “dirty” issues which are outside of our scope [1,2]. However, it has
become generally recognized that, empirically, the BFGS scheme is superior in these
details. We will implement BFGS in this section.

As before, we imagine that our arbitrary function f(x) can be locally approx-
imated by the quadratic form of equation (10.6.1). We don’t, however, have any
information about the values of the quadratic form’s parameters A and b, except
insofar as we can glean such information from our function evaluations and line
minimizations.

The basic idea of the variable metric method is to build up, iteratively, a good
approximation to the inverse Hessian matrix A~', that is, to construct a sequence
of matrices H; with the property,

lim H; = A7t (10.7.1)
i-00
Even better if the limit is achieved after N iterations instead of oo.

The reason that variable metric methods are sometimes called quasi-Newton
methods can now be explained. Consider finding a minimum by using Newton’s
method to search for a zero of the gradient of the function. Near the current point
X;, we have to second order

f(x) = f (xi) + (x — xi) - VER) + 5(K — xi) “As (Kx) (10.7.2)
so
Vi (x) = Vi (xi) +A: (x — X;) (10.7.3)

In Newton’s method we set V f(x) = 0 to determine the next iteration point:
x—x;=—-A™)-Vf(x:) (10.7.4)

The left-hand side is the finite step we need take to get to the exact minimum; the

right-hand side is known once we have accumulated an accurate H ~ A~!.

The “quasi” in quasi-Newton is because we don’t use the actual Hessian matrix
of f, but instead use our current approximation of it. This is often better than

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.7 Variable Metric Methods in Multidimensions 427

using the true Hessian. We can understand this paradoxical result by considering the
descent directions of f at x;. These are the directions p along which f decreases:
Vf +p < 0. For the Newton direction (10.7.4) to be a descent direction, we must have

Vf (Xi) (kK — xi) = —(k— xi) - A+ (k— xi) <0 (10.7.5)

which is true if A is positive definite. In general, far from a minimum, we have no
guarantee that the Hessian is positive definite. Taking the actual Newton step with
the real Hessian can move us to points where the function is increasing in value.
The idea behind quasi-Newton methods is to start with a positive definite, symmetric
approximation to A (usually the unit matrix) and build up the approximating H ;’s
in such a way that the matrix H; remains positive definite and symmetric. Far from
the minimum, this guarantees that we always move in a downhill direction. Close
to the minimum, the updating formula approaches the true Hessian and we enjoy
the quadratic convergence of Newton’s method.

When we are not close enough to the minimum, taking the full Newton step
p even with a positive definite A need not decrease the function; we may move
too far for the quadratic approximation to be valid. All we are guaranteed is that
initially f decreases as we move in the Newton direction. Once again we can use
the backtracking strategy described in §9.7 to choose a step along the direction of
the Newton step p, but not necessarily all the way.

We won’t rigorously derive the DFP algorithm for taking H ; into H;,1; you
can consult [3] for clear derivations. Following Brodlie (in [2]), we will give the
following heuristic motivation of the procedure.

Subtracting equation (10.7.4) at x;41 from that same equation at x; gives

X41 —X; = A’) (Vig — VA) (10.7.6)

where Vf; = Vf (x;). Having made the step from x; to x;1, we might reasonably
want to require that the new approximation H;+1 satisfy (10.7.6) as if it were
actually Act, that is,

Xiga — Xi = Higa - (Viiga — VSi) (10.7.7)

We might also imagine that the updating formula should be of the form H j41 =
H; + correction.

What “objects” are around out of which to construct a correction term? Most
notable are the two vectors x;,1 — x; and Vfizi — V/fi; and there is also H;.
There are not infinitely many natural ways of making a matrix out of these objects,
especially if (10.7.7) must hold! One such way, the DFP updating formula, is

_ _ (Xi41 — Xi) @ (Kind — Xi)
Hist = Hi + Qo) Wh = Vf
_ [Hi (V finn — VA) @ [Hi (Vf — Val
(Vfi+1 — Vfi) Hi: (Vfisa — Vfi)

(10.7.8)

where ® denotes the “outer” or “direct” product of two vectors, a matrix: The ij
component of u®v is u;v;. (You might want to verify that 10.7.8 does satisfy 10.7.7.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

428 Chapter 10. Minimization or Maximization of Functions

The BFGS updating formula is exactly the same, but with one additional term,

vo + [(V fir — VF) Hi (Vf — Vf] uu (10.7.9)
where u is defined as the vector
v= (Xig1 = Xi)
Gr — xi) - (Vf — Vf)
Hi (Vfizi — VSi) (10.7.10)

(Via — Wh) Hi (Vora — VS)

(You might also verify that this satisfies 10.7.7.)

You will have to take on faith — or else consult [3] for details of — the “deep”
result that equation (10.7.8), with or without (10.7.9), does in fact converge to A 1
in N steps, if f is a quadratic form.

Here now is the routine df pmin that implements the quasi-Newton method, and
uses Insrch from §9.7. As mentioned at the end of newt in §9.7, this algorithm
can fail if your variables are badly scaled.

#include <math.h>
#include "nrutil.h"

#define ITMAX 200 Maximum allowed number of iterations.
#define EPS 3.0e-8 Machine precision.

#define TOLX (4*EPS) Convergence criterion on x values.
#define STPMX 100.0 Scaled maximum step length allowed in

line searches.
#tdefine FREEALL free_vector(xi,1,n);free_vector(pnew,1,n); \
free_matrix(hessin,1,n,1,n) ;free_vector(hdg,1,n);free_vector(g,1,n); \
free_vector(dg,1,n);

void dfpmin(float p[], int n, float gtol, int *iter, float *fret,

float(*func) (float []), void (*dfunc) (float [], float []))
Given a starting point p[1..n] that is a vector of length n, the Broyden-Fletcher-Goldfarb-
Shanno variant of Davidon-Fletcher-Powell minimization is performed on a function func, using
its gradient as calculated by a routine dfunc. The convergence requirement on zeroing the
gradient is input as gtol. Returned quantities are p[1..n] (the location of the minimum),
iter (the number of iterations that were performed), and fret (the minimum value of the
function). The routine Insrch is called to perform approximate line minimizations.
{

void Insrch(int n, float xold[], float fold, float g[], float p[], float x[],

float *f, float stpmax, int *check, float (*func) (float []));

int check,i,its,j;

float den,fac,fad,fae,fp,stpmax,sum=0.0,sumdg,sumxi,temp,test;

float *dg,*g,*hdg,**hessin, *pnew, *xi;

dg=vector(1,n);
g=vector(1,n);
hdg=vector(1,n);
hessin=matrix(1,n,1,n);
pnew=vector(1,n) ;
xi=vector(1,n);

fp=(*func) (p); Calculate starting function value and gra-
(*dfunc) (p,g); dient,
for (i=1;i<=n;i++) { and initialize the inverse Hessian to the
for (j=1;j<=n;jt++) hessin[i] [j]=0.0; unit matrix.
hessin[i] [i]=1.0;
xili] = -gfil; Initial line direction.

sum += p[i]*p[i];
+
stpmax=STPMX*FMAX (sqrt (sum) , (float)n) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.7 Variable Metric Methods in Multidimensions 429

for (its=1;its<=ITMAX;its++) {

*iter=its;

lnsrch(n,p,fp,g,xi,pnew,fret ,stpmax ,&check,func) ;

The new function evaluation occurs in Insrch; save the function value in fp for the

next line search. It is usually safe to ignore the value of check.

fp = *fret;

for (i=1;i<=n;it+) {
xi[i]=pnew[i]-p(i];
pli]=pnew [il ;

Main loop over the iterations.

Update the line direction,
and the current point.

doa,

test=0.0;

for (i=1;i<=n;it+) {
temp=fabs (xi [i] )/FMAX(fabs(p[i]) ,1.0);
if (temp > test) test=temp;

Test for convergence on Az.

Buipnjoul) say ajqepeos

+

if (test < TOLX) {
FREEALL
return;

+

for (i=1;i<=n;it++) dglil=glil;
(*dfunc) (p,g);
test=0.0;

Save the old gradient,
and get the new gradient.
Test for convergence on zero gradient.

den=FMAX(*fret 1.0);

for (i=1;i<=n;it+) {
temp=fabs (g[i] ) *FMAX (fabs (p[i]) ,1.0)/den;
if (temp > test) test=temp;

}
if (test < gtol) {
FREEALL
return;
}
for (i=1;i<=n;i++) dglil=gli]-dgli]; Compute difference of gradients,
for (i=1;i<=n;it++) { and difference times current matrix.
hdg [i] 0;
for ( =n;j++) hdgli] += hessin[i] [j]*dg[j];

}

fac=fae=sumdg=sumxi=0.0;

for (i=1;i<=n;it+) {
fac += dg[i]*xilil;
fae += dg[i]*hdg[il];
sumdg += SQR(dg[il);
sumxi += SQR(xilil]);

Calculate dot products for the denomi-
nators.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg
“aleMYOS Sedioay JeouEWNN Aq 2661-886} (O) 1BUAdoD suresBolg “ssald AysienluN eGpuquied Aq 2661-8861 (O) 14

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

}
if (fac > sqrt(EPS*sumdg*sumxi)) { Skip update if fac not sufficiently posi-
fac=1.0/fac; tive.
fad=1.0/fae;
The vector that makes BFGS different from DFP:
for (i i jit+) dgli]=fac*xi[i]-fad*hdg[il;
for ( pitt) { The BFGS updating formula:
for (j=i;j<=n;jt+) {
hessin[i] [j] += fac*xi[i]*xi[j]
-fad*hdg [i] *hdg[j]+fae*dg[i]*dg[j];
hessin[j] [i]=hessin[i] [j];
}
}
}
for (i=1;i<=n;it+) { Now calculate the next direction to go,
xi[i]=0.0
for (j ‘=n;j++) xili] -= hessin[i] [(j]+*gljl];
}
} and go back for another iteration.

nrerror("too many iterations in dfpmin");
FREEALL

430 Chapter 10. Minimization or Maximization of Functions

Quasi-Newton methods like dfpmin work well with the approximate line
minimization done by lnsrch. The routines powell (§10.5) and frprmn (§10.6),
however, need more accurate line minimization, which is carried out by the routine
linmin.

Advanced Implementations of Variable Metric Methods

Although rare, it can conceivably happen that roundoff errors cause the matrix H; to
become nearly singular or non-positive-definite. This can be serious, because the supposed
search directions might then not lead downhill, and because nearly singular H;’s tend to give
subsequent H,’s that are also nearly singular.

There is a simple fix for this rare problem, the same as was mentioned in §10.4: In case
of any doubt, you should restart the algorithm at the claimed minimum point, and see if it
goes anywhere. Simple, but not very elegant. Modern implementations of variable metric
methods deal with the problem in a more sophisticated way.

Instead of building up an approximation to AW, itis possible to build up an approximation
of A itself. Then, instead of calculating the left-hand side of (10.7.4) directly, one solves
the set of linear equations

A+ (x—xi) = -Vf (xi) (10.7.11)

At first glance this seems like a bad idea, since solving (10.7.11) is a process of order
NN? — and anyway, how does this help the roundoff problem? The trick is not to store A but
rather a triangular decomposition of A, its Cholesky decomposition (cf. §2.9). The updating
formula used for the Cholesky decomposition of A is of order N? and can be arranged to
guarantee that the matrix remains positive definite and nonsingular, even in the presence of
finite roundoff. This method is due to Gill and Murray [1,2].

CITED REFERENCES AND FURTHER READING:

Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Englewood Cliffs, NJ: Prentice-Hall). [1]

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic Press),
Chapter Ill.1, §83-6 (by K. W. Brodlie). [2]

Polak, E. 1971, Computational Methods in Optimization (New York: Academic Press), pp. 56ff. [3]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 467-468.

10.8 Linear Programming and the Simplex
Method

The subject of linear programming, sometimes called linear optimization,
concerns itself with the following problem: For N independent variables x 1,..., 27,
maximize the function

z= 9121 + agg%q +--+: + aonzN (10.8.1)

subject to the primary constraints

m>0, w>0, ... ty >0 (10.8.2)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.8 Linear Programming and the Simplex Method 431

and simultaneously subject to M@ = m, + m2 + mg additional constraints, m1 of
them of the form

GX, + ajg%q +++ +aintn < dj (b; > 0) i=1,..., my, (10.8.3)
mg of them of the form

aj 21 + aj2%2 +++: +ajnrn > b; >0 j=mt1,...,mi+mz (10.8.4)

and m3 of them of the form

Git + Gpot2 +--+ +apntn = dp > 0

(10.8.5)

The various aj;’s can have either sign, or be zero. The fact that the b’s must all be
nonnegative (as indicated by the final inequality in the above three equations) is a
matter of convention only, since you can multiply any contrary inequality by —1.
There is no particular significance in the number of constraints M being less than,
equal to, or greater than the number of unknowns NV.

A set of values x1 ...2y that satisfies the constraints (10.8.2)—(10.8.5) is called
a feasible vector. The function that we are trying to maximize is called the objective
function. The feasible vector that maximizes the objective function is called the
optimal feasible vector. An optimal feasible vector can fail to exist for two distinct
reasons: (i) there are no feasible vectors, i.e., the given constraints are incompatible,
or (ii) there is no maximum, i.e., there is a direction in N space where one or more
of the variables can be taken to infinity while still satisfying the constraints, giving
an unbounded value for the objective function.

As you see, the subject of linear programming is surrounded by notational and
terminological thickets. Both of these thorny defenses are lovingly cultivated by a
coterie of stern acolytes who have devoted themselves to the field. Actually, the
basic ideas of linear programming are quite simple. Avoiding the shrubbery, we
want to teach you the basics by means of a couple of specific examples; it should
then be quite obvious how to generalize.

Why is linear programming so important? (i) Because “nonnegativity” is the
usual constraint on any variable 7; that represents the tangible amount of some
physical commodity, like guns, butter, dollars, units of vitamin E, food calories,
kilowatt hours, mass, etc. Hence equation (10.8.2). (ii) Because one is often
interested in additive (linear) limitations or bounds imposed by man or nature:
minimum nutritional requirement, maximum affordable cost, maximum on available
labor or capital, minimum tolerable level of voter approval, etc. Hence equations
(10.8.3)-(10.8.5). (iii) Because the function that one wants to optimize may be
linear, or else may at least be approximated by a linear function — since that is the
problem that linear programming can solve. Hence equation (10.8.1). For a short,
semipopular survey of linear programming applications, see Bland [1].

Here is a specific example of a problem in linear programming, which has
N = 4, mj, = 2, mz = m3 = 1, hence M = 4:

Maximize z=2,+ 29+ 323 - ar4 (10.8.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

432 Chapter 10. Minimization or Maximization of Functions

xX
XN

a feasible basic vector

(not optimal)

J),

primary constraint

N
27
a) >
x2
es, 7
Ly
Figure 10.8.1. Basic concepts of linear programming. The case of only two independent variables,

@1, £2, is shown. The linear function z, to be maximized, is represented by its contour lines. Primary
constraints require 2; and 22 to be positive. Additional constraints may restrict the solution to regions
(inequality constraints) or to surfaces of lower dimensionality (equality constraints). Feasible vectors
satisfy all constraints. Feasible basic vectors also lie on the boundary of the allowed region. The simplex
method steps among feasible basic vectors until the optimal feasible vector is found.

with all the x’s nonnegative and also with
@1 +223 < 740

2x2 — 7x4 <0
(10.8.7)

x2 — 23 +204 > 5
9

®+2%2+9%34+%4 =

The answer turns out to be (to 2 decimals) 71 = 0,22 = 3.33,x73 = 4.73, x44 = 0.95.
In the rest of this section we will learn how this answer is obtained. Figure 10.8.1
summarizes some of the terminology thus far.

Fundamental Theorem of Linear Optimization

Imagine that we start with a full V-dimensional space of candidate vectors. Then
(in mind’s eye, at least) we carve away the regions that are eliminated in turn by each
imposed constraint. Since the constraints are linear, every boundary introduced by
this process is a plane, or rather hyperplane. Equality constraints of the form (10.8.5)

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

10.8 Linear Programming and the Simplex Method 433

force the feasible region onto hyperplanes of smaller dimension, while inequalities
simply divide the then-feasible region into allowed and nonallowed pieces.

When all the constraints are imposed, either we are left with some feasible
region or else there are no feasible vectors. Since the feasible region is bounded by
hyperplanes, it is geometrically a kind of convex polyhedron or simplex (cf. §10.4).
If there is a feasible region, can the optimal feasible vector be somewhere in its
interior, away from the boundaries? No, because the objective function is linear.
This means that it always has a nonzero vector gradient. This, in turn, means that
we could always increase the objective function by running up the gradient until
we hit a boundary wall.

The boundary of any geometrical region has one less dimension than its interior.
Therefore, we can now run up the gradient projected into the boundary wall until we
reach an edge of that wall. We can then run up that edge, and so on, down through
whatever number of dimensions, until we finally arrive at a point, a vertex of the
original simplex. Since this point has all N of its coordinates defined, it must be
the solution of NV simultaneous equalities drawn from the original set of equalities
and inequalities (10.8.2)-(10.8.5).

Points that are feasible vectors and that satisfy N of the original constraints
as equalities, are termed feasible basic vectors. If N > M, then a feasible basic
vector has at least N — M of its components equal to zero, since at least that many
of the constraints (10.8.2) will be needed to make up the total of NV. Put the other
way, at most 7 components of a feasible basic vector are nonzero. In the example
(10.8.6)-(10.8.7), you can check that the solution as given satisfies as equalities the
last three constraints of (10.8.7) and the constraint x; > 0, for the required total of 4.

Put together the two preceding paragraphs and you have the Fundamental
Theorem of Linear Optimization: If an optimal feasible vector exists, then there is a
feasible basic vector that is optimal. (Didn’t we warn you about the terminological
thicket?)

The importance of the fundamental theorem is that it reduces the optimization
problem to a “combinatorial” problem, that of determining which N constraints
(out of the MM + N constraints in 10.8.2-10.8.5) should be satisfied by the optimal
feasible vector. We have only to keep trying different combinations, and computing
the objective function for each trial, until we find the best.

Doing this blindly would take halfway to forever. The simplex method, first
published by Dantzig in 1948 (see [2]), is a way of organizing the procedure so that
(i) a series of combinations is tried for which the objective function increases at each
step, and (ii) the optimal feasible vector is reached after a number of iterations that
is almost always no larger than of order M or N, whichever is larger. An interesting
mathematical sidelight is that this second property, although known empirically ever
since the simplex method was devised, was not proved to be true until the 1982 work
of Stephen Smale. (For a contemporary account, see [3].)

Simplex Method for a Restricted Normal Form

A linear programming problem is said to be in normal form if it has no
constraints in the form (10.8.3) or (10.8.4), but rather only equality constraints of the
form (10.8.5) and nonnegativity constraints of the form (10.8.2).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

434 Chapter 10. Minimization or Maximization of Functions

For our purposes it will be useful to consider an even more restricted set of cases,
with this additional property: Each equality constraint of the form (10.8.5) must
have at least one variable that has a positive coefficient and that appears uniquely in
that one constraint only. We can then choose one such variable in each constraint
equation, and solve that constraint equation for it. The variables thus chosen are
called left-hand variables or basic variables, and there are exactly M/ (= m3) of
them. The remaining NV — M variables are called right-hand variables or nonbasic
variables. Obviously this restricted normal form can be achieved only in the case
M <_N, so that is the case that we will consider.

You may be thinking that our restricted normal form is so specialized that
it is unlikely to include the linear programming problem that you wish to solve.
Not at all! We will presently show how any linear programming problem can be
transformed into restricted normal form. Therefore bear with us and learn how to
apply the simplex method to a restricted normal form.

Here is an example of a problem in restricted normal form:

Maximize z = 2x2 — 423 (10.8.8)
with x1, %2, x3, and x4 all nonnegative and also with

ay = 2-672 4+ 73

(10.8.9)
t4 = 8+ 3x2 — 4x3

This example has N = 4, M = 2; the left-hand variables are x; and x4; the
right-hand variables are x2 and 73. The objective function (10.8.8) is written so
as to depend only on right-hand variables; note, however, that this is not an actual
restriction on objective functions in restricted normal form, since any left-hand
variables appearing in the objective function could be eliminated algebraically by
use of (10.8.9) or its analogs.

For any problem in restricted normal form, we can instantly read off a feasible
basic vector (although not necessarily the optimal feasible basic vector). Simply set
all right-hand variables equal to zero, and equation (10.8.9) then gives the values of
the left-hand variables for which the constraints are satisfied. The idea of the simplex
method is to proceed by a series of exchanges. In each exchange, a right-hand
variable and a left-hand variable change places. At each stage we maintain a problem
in restricted normal form that is equivalent to the original problem.

It is notationally convenient to record the information content of equations
(10.8.8) and (10.8.9) in a so-called tableau, as follows:

Zz 0 2 —4
xy 2 —6 1
v4 8 3 -4 (10.8.10)

You should study (10.8.10) to be sure that you understand where each entry comes
from, and how to translate back and forth between the tableau and equation formats
of a problem in restricted normal form.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.8 Linear Programming and the Simplex Method 435

The first step in the simplex method is to examine the top row of the tableau,
which we will call the “z-row.” Look at the entries in columns labeled by right-hand
variables (we will call these “right-columns”). We want to imagine in turn the effect
of increasing each right-hand variable from its present value of zero, while leaving
all the other right-hand variables at zero. Will the objective function increase or
decrease? The answer is given by the sign of the entry in the z-row. Since we want
to increase the objective function, only right columns having positive z-row entries
are of interest. In (10.8.10) there is only one such column, whose z-row entry is 2.

The second step is to examine the column entries below each z-row entry that
was selected by step one. We want to ask how much we can increase the right-hand
variable before one of the left-hand variables is driven negative, which is not allowed.
If the tableau element at the intersection of the right-hand column and the left-hand
variable’s row is positive, then it poses no restriction: the corresponding left-hand
variable will just be driven more and more positive. If all the entries in any right-hand
column are positive, then there is no bound on the objective function and (having
said so) we are done with the problem.

If one or more entries below a positive z-row entry are negative, then we have
to figure out which such entry first limits the increase of that column’s right-hand
variable. Evidently the limiting increase is given by dividing the element in the right-
hand column (which is called the pivot element) into the element in the “constant
column” (leftmost column) of the pivot element’s row. A value that is small in
magnitude is most restrictive. The increase in the objective function for this choice
of pivot element is then that value multiplied by the z-row entry of that column. We
repeat this procedure on all possible right-hand columns to find the pivot element
with the largest such increase. That completes our “choice of a pivot element.”

In the above example, the only positive z-row entry is 2. There is only one
negative entry below it, namely —6, so this is the pivot element. Its constant-column
entry is 2. This pivot will therefore allow x2 to be increased by 2 + |6|, which results
in an increase of the objective function by an amount (2 x 2) + |6].

The third step is to do the increase of the selected right-hand variable, thus
making it a left-hand variable; and simultaneously to modify the left-hand variables,
reducing the pivot-row element to zero and thus making it a right-hand variable. For
our above example let’s do this first by hand: We begin by solving the pivot-row
equation for the new left-hand variable x 2 in favor of the old one x;, namely

v1 =2-622 +43 x2 «1+ 223 (10.8.11)

We then substitute this into the old z-row,

2 = 2x —4r3 =2 [5 - art + 423] — 423 = 3 — 321 —~Hgz, (10.8.12)

and into all other left-variable rows, in this case only x4,

v4 =8+3[5— $01 + $23] — 403 =9- $21 — fer (10.8.13)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

436 Chapter 10. Minimization or Maximization of Functions

Equations (10.8.11)—-(10.8.13) form the new tableau

z 2 i i

& 3 3 3

1 1 1

v2 3 ~6 6

1 7

x. 9 5 5
4 2 2 (10.8.14)

The fourth step is to go back and repeat the first step, looking for another possible
increase of the objective function. We do this as many times as possible, that is, until
all the right-hand entries in the z-row are negative, signaling that no further increase
is possible. In the present example, this already occurs in (10.8.14), so we are done.

The answer can now be read from the constant column of the final tableau. In
(10.8.14) we see that the objective function is maximized to a value of 2/3 for the
solution vector x2 = 1/3, 74 = 9,271 = 73 = 0.

Now look back over the procedure that led from (10.8.10) to (10.8.14). You will
find that it could be summarized entirely in tableau format as a series of prescribed
elementary matrix operations:

e Locate the pivot element and save it.

e Save the whole pivot column.

e Replace each row, except the pivot row, by that linear combination of itself

and the pivot row which makes its pivot-column entry zero.

e Divide the pivot row by the negative of the pivot.

e Replace the pivot element by the reciprocal of its saved value.

e Replace the rest of the pivot column by its saved values divided by the

saved pivot element.
This is the sequence of operations actually performed by a linear programming
routine, such as the one that we will presently give.

You should now be able to solve almost any linear programming problem that
starts in restricted normal form. The only special case that might stump you is
if an entry in the constant column turns out to be zero at some stage, so that a
left-hand variable is zero at the same time as all the right-hand variables are zero.
This is called a degenerate feasible vector. To proceed, you may need to exchange
the degenerate left-hand variable for one of the right-hand variables, perhaps even
making several such exchanges.

Writing the General Problem in Restricted Normal Form

Here is a pleasant surprise. There exist a couple of clever tricks that render
trivial the task of translating a general linear programming problem into restricted
normal form!

First, we need to get rid of the inequalities of the form (10.8.3) or (10.8.4), for
example, the first three constraints in (10.8.7). We do this by adding to the problem
so-called slack variables which, when their nonnegativity is required, convert the
inequalities to equalities. We will denote slack variables as y;. There will be
my, + mg of them. Once they are introduced, you treat them on an equal footing
with the original variables x;; then, at the very end, you simply ignore them.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.8 Linear Programming and the Simplex Method 437

For example, introducing slack variables leaves (10.8.6) unchanged but turns
(10.8.7) into

a, + 243 + y1 = 740

2x2 — Tra + yo =0

(10.8.15)
XQ — 23 + 244 — y3 =

Oo we

@+2%2+234+ 24 =

(Notice how the sign of the coefficient of the slack variable is determined by which
sense of inequality it is replacing.)

Second, we need to insure that there is a set of M left-hand vectors, so that we
can set up a starting tableau in restricted normal form. (In other words, we need to
find a “feasible basic starting vector:’) The trick is again to invent new variables!
There are IM of these, and they are called artificial variables; we denote them by z;.
You put exactly one artificial variable into each constraint equation on the following
model for the example (10.8.15):

z = 740 — 2 — 243 —-y

2g = —242 + Tr4 — yo

1 (10.8.16)
23 = 3 —%2+%3-— 2244+ ys

24 = 9-41 —XQ-— 43-44

Our example is now in restricted normal form.

Now you may object that (10.8.16) is not the same problem as (10.8.15) or
(10.8.7) unless all the z;’s are zero. Right you are! There is some subtlety here!
We must proceed to solve our problem in two phases. First phase: We replace our
objective function (10.8.6) by a so-called auxiliary objective function
= 2-2-2 -z= — (7495 — 2a, — Ax — 2x3 + 4x4 — y — yo + ys)

(10.8.17)
(where the last equality follows from using 10.8.16). We now perform the simplex
method on the auxiliary objective function (10.8.17) with the constraints (10.8.16).
Obviously the auxiliary objective function will be maximized for nonnegative z ;’s if
all the z;’s are zero. We therefore expect the simplex method in this first phase to
produce a set of left-hand variables drawn from the x ;’s and y;’s only, with all the
24’s being right-hand variables. Aha! We then cross out the z;’s, leaving a problem
involving only x;’s and y;,’s in restricted normal form. In other words, the first phase
produces an initial feasible basic vector. Second phase: Solve the problem produced
by the first phase, using the original objective function, not the auxiliary.

And what if the first phase doesn’t produce zero values for all the z;’s? That
signals that there is no initial feasible basic vector, i.e., that the constraints given to
us are inconsistent among themselves. Report that fact, and you are done.

Here is how to translate into tableau format the information needed for both the
first and second phases of the overall method. As before, the underlying problem

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

438 Chapter 10. Minimization or Maximization of Functions
to be solved is as posed in equations (10.8.6)—-(10.8.7).
Ban r2 v3 Ta YL y2 ¥3
z 0 1 1 3 | -3 0 0 0
zy 740 —l 0 —2 0 -1 0 0
22 0 0 —2 0 7 0 -1 0
23 3 0 | -1 1 | -2 0 0 1
ZA 9 -1 -1 -1 -1 0 0 0
2! || —7493 2 4 2 | -4 1 1 | -1
(10.8.18)
This is not as daunting as it may, at first sight, appear. The table entries inside
the box of double lines are no more than the coefficients of the original problem

(10.8.6)-(10.8.7) organized into a tabular form. In fact, these entries, along with
the values of N, M, m1, m2, and ms, are the only input that is needed by the
simplex method routine below. The columns under the slack variables y; simply
record whether each of the WV constraints is of the form <, >, or =; this is redundant
information with the values m1, m2, mz, as long as we are sure to enter the rows of
the tableau in the correct respective order. The coefficients of the auxiliary objective
function (bottom row) are just the negatives of the column sums of the rows above,
so these are easily calculated automatically.

The output from a simplex routine will be (i) a flag telling whether a finite

solution, no solution, or an unbounded solution was found, and (ii) an updated tableau.
The output tableau that derives from (10.8.18), given to two significant figures, is
v1 Y2 Y3
z 17.03 —.95 —.05 —1.05
x2 3.33 —.35 —.15 35
3 4.73 —.55 05 —.45
v4 95 —.10 10 10
Yi 730.55 10 —.10 90
(10.8.19)
A little counting of the x,’s and y,’s will convince you that there are M +1

rows (including the z-row) in both the input and the output tableaux, but that only
N +1 — mz columns of the output tableau (including the constant column) contain
any useful information, the other columns belonging to now-discarded artificial
variables. In the output, the first numerical column contains the solution vector,
along with the maximum value of the objective function. Where a slack variable (y i)
appears on the left, the corresponding value is the amount by which its inequality
is safely satisfied. Variables that are not left-hand variables in the output tableau
have zero values. Slack variables with zero values represent constraints that are
satisfied as equalities.

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD
(S-801€b-1ZS-0 NES!) DNILAdNOO OISILNSIOS 4O LHW SHL !O NI SAdIO4Y TWOIMSWNN Woy aBed ejdwes

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

10.8 Linear Programming and the Simplex Method 439

Routine Implementing the Simplex Method

The following routine is based algorithmically on the implementation of Kuenzi,
Tzschach, and Zehnder [4]. Aside from input values of M, N, m1, m2, mz, the
principal input to the routine is a two-dimensional array a containing the portion of
the tableau (10.8.18) that is contained between the double lines. This input occupies
the 7 + 1 rows and N + 1 columns of a[1..m+1] [1..n+1]. Note, however, that
reference is made internally to row M + 2 of a (used for the auxiliary objective
function, just as in 10.8.18). Therefore the variable declared as float **a, must
point to allocated memory allowing references in the subrange

ali] [kl], i=1...m+2,k=1...n+4 (10.8.20)

You will suffer endless agonies if you fail to understand this simple point. Also do
not neglect to order the rows of a in the same order as equations (10.8.1), (10.8.3),
(10.8.4), and (10.8.5), that is, objective function, <-constraints, >-constraints,
=-constraints.

On output, the tableau a is indexed by two returned arrays of integers. iposv[j]
contains, for j= 1... 1, the number 7 whose original variable x ; is now represented
by row j+1 of a. These are thus the left-hand variables in the solution. (The first row
of a is of course the z-row.) A value i > N indicates that the variable is a y; rather
than an x;,%v +4; = yj. Likewise, izrov[j] contains, for j= 1... N, the number 7
whose original variable x; is now a right-hand variable, represented by column j+1
of a. These variables are all zero in the solution. The meaning of i > N is the same
as above, except that i > N +m 1 + m2 denotes an artificial or slack variable which
was used only internally and should now be entirely ignored.

The flag icase is set to zero if a finite solution is found, +1 if the objective
function is unbounded, —1 if no solution satisfies the given constraints.

The routine treats the case of degenerate feasible vectors, so don’t worry about
them. You may also wish to admire the fact that the routine does not require storage
for the columns of the tableau (10.8.18) that are to the right of the double line; it
keeps track of slack variables by more efficient bookkeeping.

Please note that, as given, the routine is only “semi-sophisticated” in its tests
for convergence. While the routine properly implements tests for inequality with
zero as tests against some small parameter EPS, it does not adjust this parameter to
reflect the scale of the input data. This is adequate for many problems, where the
input data do not differ from unity by too many orders of magnitude. If, however,
you encounter endless cycling, then you should modify EPS in the routines simp1x
and simp2. Permuting your variables can also help. Finally, consult [5].

#include "nrutil.h"

#define EPS 1.0e-6

Here EPS is the absolute precision, which should be adjusted to the scale of your variables.
#define FREEALL free_ivector(13,1,m) ;free_ivector(11,1,n+1);

void simplx(float **a, int m, int n, int m1, int m2, int m3, int *icase,
int izrov[], int iposv[])
Simplex method for linear programming. Input parameters a, m, n, mp, np, m1, m2, and m3,
and output parameters a, icase, izrov, and iposv are described above.
{

void simp1(float **a, int mm, int 11[], int nll, int iabf, int *kp,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

440 Chapter 10. Minimization or Maximization of Functions

float *bmax) ;
void simp2(float **a, int m, int n, int *ip, int kp);
void simp3(float **a, int il, int k1, int ip, int kp);
int i,ip,is,k,kh,kp,nl1;
int *11,*13;
float qi,bmax;

if (m != (mi+m2+m3)) nrerror("Bad input constraint counts in simplx"); > on
1i=ivector(1,n+1); s §8
13=ivector(1,m) ; > Se
nli=n; = Ss
for (k=1;k<=n;k++) 11[k]=izrov[k]=k; z 58
Initialize index list of columns admissible for exchange, and make all variables initially bacius
right-hand. gs asses
for (i=1;i<=m;i++) { esses
if (a[i+1][1] < 0.0) nrerror("Bad input tableau in simplx"); gas8S
Constants b; must be nonnegative. as 5 . q
iposv[i]=nti; S655
Initial left-hand variables. m1 type constraints are represented by having their slack %2a8 P
variable initially left-hand, with no artificial variable. m2 type constraints have their uogsz
slack variable initially left-hand, with a minus sign, and their artificial variable handled x 2@ fame)
implicitly during their first exchange. m3 type constraints have their artificial variable NS S c mn
initially left-hand. 33322
+ SoRce
if (m2+m3) { Origin is not a feasible starting so- 8 of 2
for (i=1;i<=m2;it+) 13[i]=1; lution: we must do phase one. 333 D Z
Initialize list of m2 constraints whose slack variables have never been exchanged out 253 g z
of the initial basis. S23 ‘3
(nt+1) ;k++) { Compute the auxiliary objective func- 2 a Bs0
tion. Seeaoa
for (i=mi+1;i<=m;it+) qi += aliti] Ik]; ° <o2 3°
; a(m+2] [k] = -q1; 28294
for (;;) { 22285
simp1(a,m+1,11,n11,0,&kp,&bmax) ; Find max. coeff. of auxiliary objec- =P es Exe)
if (bmax <= EPS && alm+2][1] < -EPS) {__ tive fn. S386
Auxiliary objective function is still negative and can’t be improved, hence no s223ea
feasible solution exists. & Z& 3 6
FREEALL return; 82586
} else if (bmax <= EPS && alm+2][1] <= EPS) { assge
Auxiliary objective function is zero and can't be improved; we have a feasible 3 atc Zo
starting vector. Clean out the artificial variables corresponding to any remaining 283 Fy a
equality constraints by goto one and then move on to phase two. 2953 ak
for (ip=mi+m2+1;ip<=m;ipt+) { Soeko
if (iposv[ip] == (iptn)) { Found an artificial variable for an & 3828
simp1(a,ip,11,nl1,1,&kp,&bmax) ; equality constraint. S G25g
if (bmax > EPS) Exchange with column correspond- A 2o8
goto one; ing to maximum pivot element 363 g
, } in row. 388 5
®
for (i=mi+1;i<=m1+m2; i++) Change sign of row for any m2 con- Se 3°
if (13[i-m1] 1) straints still present from the ini- Z20
for (k=1;k<=n+1;k++) tial basis. e238
alitt] [x] = -ali+t] Ox]; Ses
break; Go to phase two. as Zz
} ?
simp2(a,m,n,&ip,kp) ; Locate a pivot element (phase one).
if (ip == 0) { Maximum of auxiliary objective func-
*icase = -1; tion is unbounded, so no feasi-
FREEALL return; ble solution exists.
}
one: simp3(a,m+1,n,ip,kp);

Exchange a left- and a right-hand variable (phase one), then update lists.

10.8 Linear Programming and the Simplex Method 441

if (iposv[ip] >= (n+mi+m2+1)) {
for (k=1;k<=n11;k++)
if (11[k] == kp) break;

Exchanged out an artificial variable
for an equality constraint. Make
sure it stays out by removing it

--nli; from the 11 list.
for (is=k;is<=nl1;ist++) 11[is]=11[is+1];
} else {

kh=iposv [ip] -m1-n;

if (kh >= 1 && 13[kh]) { Exchanged out an m2 type constraint
13 [kh] =0; for the first time. Correct the
++a[m+2] [kp+1] ; pivot column for the minus sign
for (i=1;i<=m+2;i++) and the implicit artificial vari-

ali] (kpti] = -ali] Uep+i]; able.
}
}
is=izrov [kp] ; Update lists of left- and right-hand
izrov[kp]=iposv [ip] ; variables.

iposv[ip]=is;

} Still in phase one, go back to the
+ for(;;).
End of phase one code for finding an initial feasible solution. Now, in phase two, optimize
it.
for G3) f
simpi(a,0,11,n11,0,&kp,&bmax) ; Test the z-row for doneness.
if (bmax <= EPS) { Done. Solution found. Return with
*icase=0; the good news.
FREEALL return;
}
simp2(a,m,n,&ip,kp); Locate a pivot element (phase two).
if (ip == 0) { Objective function is unbounded. Re-
*icase=1; port and return.
FREEALL return;
}
simp3(a,m,n,ip,kp) ; Exchange a left- and a right-hand
is=izrov [kp] ; variable (phase two),
izrov[kp]=iposv[ip] ;
+ and return for another iteration.

The preceding routine makes use of the following utility functions.

#include <math.h>

void simp1(float **a, int mm, int 11[], int nll, int iabf, int *kp,

float *bmax)

Determines the maximum of those elements whose index is contained in the supplied list 11,
either with or without taking the absolute value, as flagged by iabf.

{

int k;

float test;

if (nll <= 0) No eligible columns.
*bmax=0.0;

else {

*kp=11[1];
*bmax=a [mm+1] [*kp+1] ;
for (k=2;k<=nll;k++) {

if (iabf == 0)
test=a[mm+1] [11 [k]+1]-(*bmax) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

442

Chapter 10. Minimization or Maximization of Functions

else

test=fabs (a[mm+1] [11[k]+1] )-fabs(*bmax) ;
if (test > 0.0) {

*bmax=a [mm+1] [11 [k] +1] ;

*kp=11([k];

#define EPS 1.0e-6

void simp2(float **a, int m, int n, int *ip, int kp)
Locate a pivot element, taking degeneracy into account.

{

int k,i;
float qp,q0,q,q1;

*ip=0;
for (i=1;i<=m;it++)
aif (al[it1] [kpt1] < -EPS) break;
if (i>m) return;
qi = -ali+1] [1] /ali+1] [kp+1];
*ip=i;
for (i=*ipt1;i<=m;it+) {
if (a[i+1] [kp+i] < -EPS) {
q = ~-alit1] [1] /alit1] [kpt+1];
if (q< qi) {
*ip=i;
qi=q;
} else if (q == qi) {
for (k=1;k<=n;k++) {
gp = -al*ip+1] [k+1]/a[*ip+1] [kp+1] ;
qo = -alitd] [k+1]/ali+1] [kp+1];
if (q0 != gp) break;

Any possible pivots?

We have a degeneracy.

if (q0 < qp) *ip=i;

void simp3(float **a, int i1, int ki, int ip, int kp)

Matrix operations to exchange a left-hand and right-hand variable (see text).

{

int kk,ii;
float piv;

piv=1.0/alip+1] [kp+1] ;
for (ii=1;ii<=i1+1;ii++)
if (4i-1 != ip) {
alii] [kp+1] *= piv;
for (kk=1;kk<=k1+1;kkt++)
if (kk-1 != kp)
alii] [kk] -= alip+1] [kk] *a[ii] [kp+1];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.8 Linear Programming and the Simplex Method 443

for (kk=1;kk<=k1i+1;kk++)
if (kk-1 != kp) aliptt] [kk] *= -piv;
alip+1] [kp+1]=piv;

Other Topics Briefly Mentioned

Every linear programming problem in normal form with N variables and Mf
constraints has a corresponding dual problem with / variables and N constraints.
The tableau of the dual problem is, in essence, the transpose of the tableau of the
original (sometimes called primal) problem. It is possible to go from a solution
of the dual to a solution of the primal. This can occasionally be computationally
useful, but generally it is no big deal.

The revised simplex method is exactly equivalent to the simplex method in its
choice of which left-hand and right-hand variables are exchanged. Its computational
effort is not significantly less than that of the simplex method. It does differ in
the organization of its storage, requiring only a matrix of size MM x M, rather than
M x N, in its intermediate stages. If you have a lot of constraints, and memory
size is one of them, then you should look into it.

The primal-dual algorithm and the composite simplex algorithm are two dif-
‘erent methods for avoiding the two phases of the usual simplex method: Progress
is made simultaneously towards finding a feasible solution and finding an optimal
solution. There seems to be no clearcut evidence that these methods are superior
to the usual method by any factor substantially larger than the “tender-loving-care
actor” (which reflects the programming effort of the proponents).

Problems where the objective function and/or one or more of the constraints are
replaced by expressions nonlinear in the variables are called nonlinear programming
problems. The literature on such problems is vast, but outside our scope. The special
case of quadratic expressions is called quadratic programming. Optimization prob-
lems where the variables take on only integer values are called integer programming
problems, a special case of discrete optimization generally. The next section looks
at a particular kind of discrete optimization problem.

CITED REFERENCES AND FURTHER READING:

Bland, R.G. 1981, Scientific American, vol. 244 (June), pp. 126-144. [1]

Dantzig, G.B. 1963, Linear Programming and Extensions (Princeton, NJ: Princeton University
Press). [2]

Kolata, G. 1982, Science, vol. 217, p. 39. [3]

Gill, P.E., Murray, W., and Wright, M.H. 1991, Numerical Linear Algebra and Optimization, vol. 1
(Redwood City, CA: Addison-Wesley), Chapters 7-8.

Cooper, L., and Steinberg, D. 1970, Introduction to Methods of Optimization (Philadelphia: Saun-
ders).

Gass, S.T. 1969, Linear Programming, 3rd ed. (New York: McGraw-Hill).

Murty, K.G. 1976, Linear and Combinatorial Programming (New York: Wiley).

Land, A.H., and Powell, S. 1973, Fortran Codes for Mathematical Programming (London: Wiley-
Interscience).

Kuenzi, H.P., Tzschach, H.G., and Zehnder, C.A. 1971, Numerical Methods of Mathematical
Optimization (New York: Academic Press). [4]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

444 Chapter 10. Minimization or Maximization of Functions

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§4.10.

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [5]

10.9 Simulated Annealing Methods

The method of simulated annealing [1,2] is a technique that has attracted signif-
icant attention as suitable for optimization problems of large scale, especially ones
where a desired global extremum is hidden among many, poorer, local extrema. For
practical purposes, simulated annealing has effectively “solved” the famous traveling
salesman problem of finding the shortest cyclical itinerary for a traveling salesman
who must visit each of N cities in turn. (Other practical methods have also been
found.) The method has also been used successfully for designing complex integrated
circuits: The arrangement of several hundred thousand circuit elements on a tiny
silicon substrate is optimized so as to minimize interference among their connecting
wires [3,4]. Surprisingly, the implementation of the algorithm is relatively simple.

Notice that the two applications cited are both examples of combinatorial
minimization. There is an objective function to be minimized, as usual; but the space
over which that function is defined is not simply the N-dimensional space of N
continuously variable parameters. Rather, it is a discrete, but very large, configuration
space, like the set of possible orders of cities, or the set of possible allocations of
silicon “real estate” blocks to circuit elements. The number of elements in the
configuration space is factorially large, so that they cannot be explored exhaustively.
Furthermore, since the set is discrete, we are deprived of any notion of “continuing
downhill in a favorable direction’ The concept of “direction” may not have any
meaning in the configuration space.

Below, we will also discuss how to use simulated annealing methods for spaces
with continuous control parameters, like those of §§10.4-10.7. This application is
actually more complicated than the combinatorial one, since the familiar problem of
“long, narrow valleys” again asserts itself. Simulated annealing, as we will see, tries
“random” steps; but in a long, narrow valley, almost all random steps are uphill!
Some additional finesse is therefore required.

At the heart of the method of simulated annealing is an analogy with thermody-
namics, specifically with the way that liquids freeze and crystallize, or metals cool
and anneal. At high temperatures, the molecules of a liquid move freely with respect
to one another. If the liquid is cooled slowly, thermal mobility is lost. The atoms are
often able to line themselves up and form a pure crystal that is completely ordered
over a distance up to billions of times the size of an individual atom in all directions.
This crystal is the state of minimum energy for this system. The amazing fact is that,
for slowly cooled systems, nature is able to find this minimum energy state. In fact, if
a liquid metal is cooled quickly or “quenched,” it does not reach this state but rather
ends up in a polycrystalline or amorphous state having somewhat higher energy.

So the essence of the process is slow cooling, allowing ample time for
redistribution of the atoms as they lose mobility. This is the technical definition of
annealing, and it is essential for ensuring that a low energy state will be achieved.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.9 Simulated Annealing Methods 445

Although the analogy is not perfect, there is a sense in which all of the
minimization algorithms thus far in this chapter correspond to rapid cooling or
quenching. In all cases, we have gone greedily for the quick, nearby solution: From
the starting point, go immediately downhill as far as you can go. This, as often
remarked above, leads to a local, but not necessarily a global, minimum. Nature’s
own minimization algorithm is based on quite a different procedure. The so-called
Boltzmann probability distribution,

Prob (EZ) ~ exp(—E/kT) (10.9.1)

expresses the idea that a system in thermal equilibrium at temperature T’ has its
energy probabilistically distributed among all different energy states E. Even at
low temperature, there is a chance, albeit very small, of a system being in a high
energy state. Therefore, there is a corresponding chance for the system to get out of
a local energy minimum in favor of finding a better, more global, one. The quantity
k (Boltzmann’s constant) is a constant of nature that relates temperature to energy.
In other words, the system sometimes goes uphill as well as downhill; but the lower
the temperature, the less likely is any significant uphill excursion.

In 1953, Metropolis and coworkers [5] first incorporated these kinds of prin-
ciples into numerical calculations. Offered a succession of options, a simulated
thermodynamic system was assumed to change its configuration from energy E’; to
energy E>» with probability p = exp[—(E2 — E)/kT). Notice that if £2 < E\, this
probability is greater than unity; in such cases the change is arbitrarily assigned a
probability p = 1, i.e., the system always took such an option. This general scheme,
of always taking a downhill step while sometimes taking an uphill step, has come
to be known as the Metropolis algorithm.

To make use of the Metropolis algorithm for other than thermodynamic systems,
one must provide the following elements:

1. A description of possible system configurations.

2. A generator of random changes in the configuration; these changes are the
“options” presented to the system.

3. An objective function E (analog of energy) whose minimization is the
goal of the procedure.

4. A control parameter T’ (analog of temperature) and an annealing schedule
which tells how it is lowered from high to low values, e.g., after how many random
changes in configuration is each downward step in T taken, and how large is that
step. The meaning of “high” and “low” in this context, and the assignment of a
schedule, may require physical insight and/or trial-and-error experiments.

Combinatorial Minimization: The Traveling Salesman

A concrete illustration is provided by the traveling salesman problem. The
proverbial seller visits N cities with given positions (x;, y;), returning finally to his or
her city of origin. Each city is to be visited only once, and the route is to be made as
short as possible. This problem belongs to a class known as NP-complete problems,
whose computation time for an exact solution increases with N as exp(const. x N),
becoming rapidly prohibitive in cost as N increases. The traveling salesman problem
also belongs to a class of minimization problems for which the objective function E

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

446 Chapter 10. Minimization or Maximization of Functions

has many local minima. In practical cases, it is often enough to be able to choose
from these a minimum which, even if not absolute, cannot be significantly improved
upon. The annealing method manages to achieve this, while limiting its calculations
to scale as a small power of N.

As a problem in simulated annealing, the traveling salesman problem is handled
as follows:

1. Configuration. The cities are numbered i = 1... N and each has coordinates
(xi, ys). A configuration is a permutation of the number 1... .N, interpreted as the
order in which the cities are visited.

2. Rearrangements. An efficient set of moves has been suggested by Lin [6].
The moves consist of two types: (a) A section of path is removed and then replaced
with the same cities running in the opposite order; or (b) a section of path is removed
and then replaced in between two cities on another, randomly chosen, part of the path.

3. Objective Function. In the simplest form of the problem, EF is taken just
as the total length of journey,

N
E=L= > V (wi = i414)? + (Yi — Yor)? (10.9.2)
i=1

with the convention that point NV + 1 is identified with point 1. To illustrate the
flexibility of the method, however, we can add the following additional wrinkle:
Suppose that the salesman has an irrational fear of flying over the Mississippi River.
In that case, we would assign each city a parameter jv;, equal to +1 if it is east of the
Mississippi, —1 if it is west, and take the objective function to be

N
£B=y- [Ve = tins)? + (Yi — irr)? + ACHE — wist)?| (10.9.3)

i=1

A penalty 4. is thereby assigned to any river crossing. The algorithm now finds
the shortest path that avoids crossings. The relative importance that it assigns to
length of path versus river crossings is determined by our choice of \. Figure 10.9.1
shows the results obtained. Clearly, this technique can be generalized to include
many conflicting goals in the minimization.

4. Annealing schedule. This requires experimentation. We first generate some
random rearrangements, and use them to determine the range of values of AF that
will be encountered from move to move. Choosing a starting value for the parameter
T which is considerably larger than the largest AF normally encountered, we
proceed downward in multiplicative steps each amounting to a 10 percent decrease
in T. We hold each new value of T' constant for, say, 100.N reconfigurations, or for
10N successful reconfigurations, whichever comes first. When efforts to reduce F
further become sufficiently discouraging, we stop.

The following traveling salesman program, using the Metropolis algorithm,
illustrates the main aspects of the simulated annealing technique for combinatorial
problems.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

447

10.9 Simulated Annealing Methods

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer: Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

TT—T_T_T_T] ee es ee ee
du L 44 L 44
qo oF GO oF 4°

[ee oe a a Pop pp pp

val o _ val o _ val o

3S Ss cS
s 2 2

Traveling salesman problem solved by simulated annealing. The (nearly) shortest path

Figure 10.9.1.

among 100 randomly positioned cities

own in (a). The dotted line is a river, but there is no penalty in

self to the minimum

number of crossings, two. In (c) the penalty has been made negative: the salesman is actually a smuggler

crossing. In (b) the river-crossing penalty is made large, and the solution restricts
who crosses the river on the flimsiest excuse!


448 Chapter 10. Minimization or Maximization of Functions

#include <stdio.h>

#include <math.h>

#define TFACTR 0.9 Annealing schedule: reduce t by this factor on each step.
#define ALEN(a,b,c,d) sqrt(((b)-(a))*((b)-(a))+((d)-(c)) *((d)-(c)))

void anneal(float x[], float y[], int iorder[], int ncity)
This algorithm finds the shortest round-trip path to ncity cities whose coordinates are in the
arrays x[1..ncity],y[1..ncity]. The array iorder[1..ncity] specifies the order in
which the cities are visited. On input, the elements of iorder may be set to any permutation
of the numbers 1 to ncity. This routine will return the best alternative path it can find.
{

int irbiti(unsigned long *iseed) ;

int metrop(float de, float t);

float ran3(long *idum) ;

float revcst(float x[], float y[], int iorder[], int ncity, int n[]);

void reverse(int iorder[], int ncity, int n[]);

float trncst(float x[], float y[], int iorder[], int ncity, int n[]);

void trnspt(int iorder[], int ncity, int n[]);

int ans,nover,nlimit,i1,i2;

int i,j,k,nsucc,nn,idec;

static int n[7];

long idum;

unsigned long iseed;

float path,de,t;

nover=100*ncity; Maximum number of paths tried at any temperature.
nlimit=10«ncity; Maximum number of successful path changes before con-
path=0.0; tinuing.

t=0.5;

for (i=1;i<ncity;it+) { Calculate initial path length.

il=iorder [i] ;
i2=iorder [it+1] ;
path += ALEN(x(i1],x(i2],y[i1],y[i2]);
+
i1=iorder [ncity] ; Close the loop by tying path ends together.
i2=iorder [1];
path += ALEN(x[i1] ,x[i2] ,y[i1],y[i2]);

idum -1;
iseed=111;
for (j=1;j<=100;j++) { Try up to 100 temperature steps.
nsucc=0;
for (k=1;k<=snover;kt+) {
do {
n[i]=1+(int) (ncity*ran3(&idum)) ; Choose beginning of segment
n(2]=1+(int) ((ncity-1)*ran3(&idum)) ; and. end of segment.
if (n(2] >= n[1]) ++n[2];
nn=1+((n[1]-n[2]+ncity-1) % ncity); mn is the number of cities
} while (nn<3); not on the segment.

idec=irbit1(&iseed) ;

Decide whether to do a segment reversal or transport.

if (idec == 0) { Do a transport.
n[3]=n[2]+(int) (abs (nn-2)*ran3(kidum))+1;
n(3]=1+((n[3]-1) % ncity);
Transport to a location not on the path.

de=trncst (x,y,iorder,ncity,n) ; Calculate cost.
ans=metrop(de,t) ; Consult the oracle.
if (ans) {

+tsucc;

path += de;

trnspt (iorder ,ncity,n) ; Carry out the transport.
}

} else { Do a path reversal.

de=revest (x,y,iorder,ncity,n) ; Calculate cost.

ans=metrop(de,t) ; Consult the oracle.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.9 Simulated Annealing Methods 449

if (ans) {
+tsucc;
path += de;
reverse(iorder ,ncity,n) ; Carry out the reversal.
3
3
if (msucc >= nlimit) break; Finish early if we have enough suc-
} cessful changes. _
printf("\n %s %10.6f 712.6f \n","T =",t, 8 g
" Path Length =",path) ; Ey
printf("Successful Moves: %6d\n",nsucc) ; o
t *= TFACTR; Annealing schedule. =
if (msucc == 0) return; If no success, we are done. 2
t a
+ 5.
a

#include <math.h>
#define ALEN(a,b,c,d) sqrt(((b)-(a))*((b)-(a))+((d)-(c) )*((d)-(c)))

float revcst(float x[], float y[], int iorder[], int ncity, int n[])
This function returns the value of the cost function for a proposed path reversal. ncity is the
number of cities, and arrays x[1..ncity],y[1..ncity] give the coordinates of these cities.
iorder[1..ncity] holds the present itinerary. The first two values n[1] and n[2] of array
n give the starting and ending cities along the path segment which is to be reversed. On output,
de is the cost of making the reversal. The actual reversal is not performed by this routine.
{

float xx[5],yy[5],de;

int j,ii;

n[3]=1 + ((n[1]+ncity-2) % ncity); Find the city before n[1] ..
n[4]=1 + (n[2] % ncity); .. and the city after n[2].
for ( 35<=4;5++) {

iisiorder[n[j]]; Find coordinates for the four cities in-
xx(j]=x (iil; volved.
yy (jl=y (iil;

de = -ALEN(xx[1] ,xx[3],yy([1],yy[3]); Calculate cost of disconnecting the seg-
de -= ALEN(xx(2] ,xx[4] ,yy([2],yy[4]); ment at both ends and reconnecting
de += ALEN(xx[1] ,xx(4],yy[1],yy[4]); in the opposite order.

de += ALEN(xx([2] ,xx(3] ,yy[2],yy[3]);

return de;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

void reverse(int iorder[], int ncity, int n[])

This routine performs a path segment reversal. iorder[1..ncity] is an input array giving the
present itinerary. The vector n has as its first four elements the first and last cities n[1] ,n[2]
of the path segment to be reversed, and the two cities n[3] and n[4] that immediately
precede and follow this segment. n[3] and n[4] are found by function revcst. On output,
iorder[1..ncity] contains the segment from n[1] to n[2] in reversed order.

{

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg
“aleMYOS Sedioay JeouEWNN Aq 2661-886} (O) 1BUAdoD suresBolg “ssald AysienluN eGpuquied Aq 2661-8861 (O) 14

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

int nn,j,k,1,itmp;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

nn=(1+((n[2]-n[i]+ncity) % ncity))/2; This many cities must be swapped to
for (j=1;j<=nn;j++) { effect the reversal.
k=1 + ((n[1]+j-2) % ncity); Start at the ends of the segment and
1=1 + ((n[2]-j+ncity) % ncity); swap pairs of cities, moving toward
itmp=iorder [k] ; the center.

iorder [k]=iorder[1] ;
iorder [1]=itmp;

450 Chapter 10. Minimization or Maximization of Functions

#include <math.h>
#define ALEN(a,b,c,d) sqrt(((b)-(a))*((b)-(a))+((a)-(c) )*((d)-(c)))

float trncst(float x[], float y[], int iorder[], int ncity, int n[])

This routine returns the value of the cost function for a proposed path segment transport. ncity
is the number of cities, and arrays x[1..ncity] and y[1..ncity] give the city coordinates.
iorder[1..ncity] is an array giving the present itinerary. The first three elements of array
n give the starting and ending cities of the path to be transported, and the point among the
remaining cities after which it is to be inserted. On output, de is the cost of the change. The
actual transport is not performed by this routine.

{
float xx[7],yy[7],de;
int j,ii;
n{4]=1 + (n[3] % ncity); Find the city following n[3]..
n(5]=1 + ((n[1]+ncity-2) % ncity); and the one preceding n[1]..
n{6]=1 + (n[2] % ncity); ..and the one following n[2].
for ( 3J<=6; j++) {
ii=iorder [n[j]]; Determine coordinates for the six cities
xx(j]=x (iil; involved.
yy (j]=y (iil;
3
de = -ALEN(xx([2] ,xx[6] ,yy([2],yy[6]); Calculate the cost of disconnecting the
de -= ALEN(xx(1] ,xx(5],yy([1],yy[5]); path segment from n[1] to n[2],
de -= ALEN(xx(3] ,xx[4] ,yy(3],yy[4]); opening a space between n[3] and
de += ALEN(xx(1] ,xx(3] ,yy([1],yy[3]); n[4], connecting the segment in the
de += ALEN(xx(2] ,xx[4] ,yy([2],yy[4]); space, and connecting n[5] ton[6].
de += ALEN(xx([5] ,xx(6] ,yy[5],yy[6]);
return de;
}

#include "nrutil.h"

void trnspt(int iorder[], int ncity, int n[])

This routine does the actual path transport, once metrop has approved. iorder[1..ncity]
is an input array giving the present itinerary. The array n has as its six elements the beginning
n[1] and end n[2] of the path to be transported, the adjacent cities n[3] and n[4] between
which the path is to be placed, and the cities n[5] and n[6] that precede and follow the path.
n[4], n[5], and n[6] are calculated by function trncst. On output, iorder is modified to
reflect the movement of the path segment.

{

int m1,m2,m3,nn,j,jj,*jorder;

jorder=ivector(1,ncity) ;

mi=1 + ((n[2]-n[1]+ncity) % ncity); Find number of cities from n[1] to n[2]
m2=1 + ((n[5]-n[4]+ncity) % ncity); ..and the number from n[4] to n[5]
m3=1 + ((n[3]-n[6]+ncity) % ncity); ..and the number from n[6] to n[3].
nn=1;
for (j=1;j<=m1;j++) {
jj=1 + ((j+n[1]-2) % ncity); Copy the chosen segment.
jorder [nn++]=iorder[jj];
}
for (j=1;j<=m2;j++) { Then copy the segment from n[4] to
jj=it+((j+n[4]-2) % ncity); n{5].
jorder [nn++]=iorder [jj];
}
for (j=1;j<=m3;j++) { Finally, the segment from n[6] to n[3].
jj=1 + ((j+n[6]-2) % ncity);
jorder [nn++]=iorder [jj];
}
for (j=1;j<=ncity; j++) Copy jorder back into iorder.

iorder [j]=jorder[j];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.9 Simulated Annealing Methods 451

free_ivector(jorder,1,ncity);

#include <math.h>

int metrop(float de, float t)

Metropolis algorithm. metrop returns a boolean variable that issues a verdict on whether
to accept a reconfiguration that leads to a change de in the objective function e. If de<0,
metrop = 1 (true), while if de>0, metrop is only true with probability exp(-de/t), where
t is a temperature determined by the annealing schedule.

t

float ran3(long *idum) ;

static long gljdum=1;

return de < 0.0 || ran3(&gljdum) < exp(-de/t);
}

Continuous Minimization by Simulated Annealing

The basic ideas of simulated annealing are also applicable to optimization
problems with continuous N-dimensional control spaces, e.g., finding the (ideally,
global) minimum of some function f(x), in the presence of many local minima,
where x is an N-dimensional vector. The four elements required by the Metropolis
procedure are now as follows: The value of f is the objective function. The
system state is the point x. The control parameter T is, as before, something like a
temperature, with an annealing schedule by which it is gradually reduced. And there
must be a generator of random changes in the configuration, that is, a procedure for
taking a random step from x to x + Ax.

The last of these elements is the most problematical. The literature to date [7-10]
describes several different schemes for choosing Ax, none of which, in our view,
inspire complete confidence. The problem is one of efficiency: A generator of
random changes is inefficient if, when local downhill moves exist, it nevertheless
almost always proposes an uphill move. A good generator, we think, should not
become inefficient in narrow valleys; nor should it become more and more inefficient
as convergence to a minimum is approached. Except possibly for [7], all of the
schemes that we have seen are inefficient in one or both of these situations.

Our own way of doing simulated annealing minimization on continuous control
spaces is to use a modification of the downhill simplex method (§10.4). This amounts
to replacing the single point x as a description of the system state by a simplex of
N +1 points. The “moves” are the same as described in §10.4, namely reflections,
expansions, and contractions of the simplex. The implementation of the Metropolis
procedure is slightly subtle: We add a positive, logarithmically distributed random
variable, proportional to the temperature 7’, to the stored function value associated
with every vertex of the simplex, and we subtract a similar random variable from
the function value of every new point that is tried as a replacement point. Like the
ordinary Metropolis procedure, this method always accepts a true downhill step, but

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

452 Chapter 10. Minimization or Maximization of Functions

sometimes accepts an uphill one. In the limit 7’ — 0, this algorithm reduces exactly
to the downhill simplex method and converges to a local minimum.

At a finite value of 7’, the simplex expands to a scale that approximates the size
of the region that can be reached at this temperature, and then executes a stochastic,
tumbling Brownian motion within that region, sampling new, approximately random,
points as it does so. The efficiency with which a region is explored is independent
of its narrowness (for an ellipsoidal valley, the ratio of its principal axes) and
orientation. If the temperature is reduced sufficiently slowly, it becomes highly
likely that the simplex will shrink into that region containing the lowest relative
minimum encountered.

As in all applications of simulated annealing, there can be quite a lot of
problem-dependent subtlety in the phrase “sufficiently slowly”; success or failure
is quite often determined by the choice of annealing schedule. Here are some
possibilities worth trying:

e Reduce T to (1 — €)T after every m moves, where €/m is determined

by experiment.

e Budget a total of kK moves, and reduce T after every m moves to a value
T =To(1 —k/K)*, where k is the cumulative number of moves thus far,
and a is a constant, say 1,2, or 4. The optimal value for a depends on the
statistical distribution of relative minima of various depths. Larger values
of a spend more iterations at lower temperature.

e Afterevery m moves, set T' to 3 times f1— fy, where 3 is an experimentally
determined constant of order 1, f1 is the smallest function value currently
represented in the simplex, and f» is the best function ever encountered.
However, never reduce T by more than some fraction ¥ at a time.

Another strategic question is whether to do an occasional restart, where a vertex
of the simplex is discarded in favor of the “best-ever” point. (You must be sure that
the best-ever point is not currently in the simplex when you do this!) We have found
problems for which restarts — every time the temperature has decreased by a factor
of 3, say — are highly beneficial; we have found other problems for which restarts
have no positive, or a somewhat negative, effect.

You should compare the following routine, amebsa, with its counterpart anoeba
in §10.4. Note that the argument iter is used in a somewhat different manner.

#include <math.h>
#include "nrutil.h"
#tdefine GET_PSUM \
for (n=1;n<=ndim;n++) {\

for (sum=0.0,m=1;m<=mpts;m++) sum += p[m] [n];\
psum[n]=sum ;}
extern long idum; Defined and initialized in main.
float tt; Communicates with amotsa.

void amebsa(float **p, float y[], int ndim, float pb[], float *yb, float ftol,
float (*funk) (float []), int *iter, float temptr)

Multidimensional minimization of the function funk(x) where x[1..ndim] is a vector in
ndim dimensions, by simulated annealing combined with the downhill simplex method of Nelder
and Mead. The input matrix p[1..ndim+1][1..ndim] has ndimt+1 rows, each an ndim-
dimensional vector which is a vertex of the starting simplex. Also input are the following: the
vector y[1..ndimt+1], whose components must be pre-initialized to the values of funk eval-
uated at the ndim+1 vertices (rows) of p; ftol, the fractional convergence tolerance to be
achieved in the function value for an early return; iter, and temptr. The routine makes iter
function evaluations at an annealing temperature temptr, then returns. You should then de-

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.9 Simulated Annealing Methods 453

crease temptr according to your annealing schedule, reset iter, and call the routine again
(leaving other arguments unaltered between calls). If iter is returned with a positive value,
then early convergence and return occurred. If you initialize yb to a very large value on the first
call, then yb and pb[1. .ndim] will subsequently return the best function value and point ever
encountered (even if it is no longer a point in the simplex).

{

float amotsa(float **p, float y[], float psum[], int ndim, float pb[],
float *yb, float (*funk)(float []), int ihi, float *yhi, float fac);

float rani(long *idum) ;

int i,ihi,ilo,j,m,n,mpts=ndim+1;

float rtol,sum,swap,yhi,ylo,ynhi,ysave,yt,ytry,*psum;

psum=vector(1,ndim) ;

tt = -temptr;

GET_PSUM

for Gi) f
ilo=1; Determine which point is the highest (worst),
ihi=2; next-highest, and lowest (best).

ynhi=ylo=y[1]+tt*log(rani(&idum)) ; Whenever we “look at” a vertex, it gets
yhi=y[2]+tt*log (rani (&idum)) ; a random thermal fluctuation.
if (ylo > yhi) {

ihi=1;

ynhi=yhi;
yhi=ylo;
ylo=ynhi;

for (i=3;i<=mpts;it+) { Loop over the points in the simplex.
yt=y [i] +tt*log (rani (&idum)) ; More thermal fluctuations.
if (yt <= ylo) {
ilo=i;
ylo=yt;

if (yt > yhi) {
ynhi=yhi;

} else if (yt > ynhi) {
ynhi=yt ;
3
}
rtol=2.0*fabs(yhi-ylo)/ (fabs (yhi)+fabs(ylo)) ;
Compute the fractional range from highest to lowest and return if satisfactory.
if (rtol < ftol || *iter < 0) f{ If returning, put best point and value in
swap=y [1]; slot 1.
ylt]=y [ilo];
y[ilo]=swap;
for (n=1;n<=ndim;n++) {
swap=p[1] [n] ;
p([1] [In] =p [ilo] [n];
pLilo] [n]=swap;

}
break;
}
*iter -= 2;

Begin a new iteration. First extrapolate by a factor —1 through the face of the simplex
across from the high point, i.e., reflect the simplex from the high point.
ytry=amotsa(p,y,psum,ndim,pb,yb,funk,ihi,&yhi,-1.0);
if (ytry <= ylo) {
Gives a result better than the best point, so try an additional extrapolation by a
factor of 2.
ytry=amotsa(p,y,psum,ndim,pb,yb,funk,ihi,&yhi,2.0);
} else if (ytry >= ynhi) {
The reflected point is worse than the second-highest, so look for an intermediate

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

454 Chapter 10. Minimization or Maximization of Functions

lower point, i.e., do a one-dimensional contraction.

ysave=yhi;
ytry=amotsa(p,y,psum,ndim,pb,yb,funk,ihi,&yhi,0.5);
if (ytry >= ysave) { Can't seem to get rid of that high point.

for (i=1;i<=mpts;it+) { Better contract around the lowest
if Gi != ilo) { (best) point.
for (j=1;j<=ndim;j++) {
psum[j]=0.5* (p[i] [j]+p[lilo] [j]);
pli] [j]=psum[j] ;

y [i] =(*funk) (psum) ;

+
}
*iter -= ndim;
GET_PSUM Recompute psum.
}
} else ++(*iter); Correct the evaluation count.

+

free_vector(psum,1,ndim) ;

#include <math.h>
#include "nrutil.h"

extern long idum; Defined and initialized in main.
extern float tt; Defined in amebsa.

float amotsa(float **p, float y[], float psum[], int ndim, float pb[],

float *yb, float (*funk)(float []), int ihi, float *yhi, float fac)
Extrapolates by a factor fac through the face of the simplex across from the high point, tries
it, and replaces the high point if the new point is better.
{

float rani(long *idum) ;

int j;

float faci,fac2,yflu,ytry,*ptry;

ptry=vector(1,ndim) ;
faci=(1.0-fac)/ndim;
fac2=faci-fac;
for (j=1;j<=ndim; j++)
ptry [j]=psum[j]*faci-p[ihi] [j]*fac2;
ytry=(*funk) (ptry) ;
if (ytry <= *yb) { Save the best-ever.
for (j=1;j<=ndim; j++) pb[jl=ptry[j];
*yb=ytry;

yflu=ytry-tt*log (ran1 (&idum)) ; We added a thermal fluctuation to all the current
if (yflu < *yhi) { vertices, but we subtract it here, so as to give
ylihi]=ytry; the simplex a thermal Brownian motion: It
*yhi=yflu; likes to accept any suggested change.
for (j=1;j<=ndim;j++) {
psum[j] += ptry(j]-plihi] [j];
plihi] [jl]=ptry[jl;

}
}
free_vector(ptry,1,ndim) ;
return yflu;

There is not yet enough practical experience with the method of simulated
annealing to say definitively what its future place among optimization methods

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

10.9 Simulated Annealing Methods 455

will be. The method has several extremely attractive features, rather unique when
compared with other optimization techniques.

First, it is not “greedy,” in the sense that it is not easily fooled by the quick
payoff achieved by falling into unfavorable local minima. Provided that sufficiently
general reconfigurations are given, it wanders freely among local minima of depth
less than about T. As T is lowered, the number of such minima qualifying for
frequent visits is gradually reduced.

Second, configuration decisions tend to proceed in a logical order. Changes
that cause the greatest energy differences are sifted over when the control parameter
T is large. These decisions become more permanent as T is lowered, and attention
then shifts more to smaller refinements in the solution. For example, in the traveling
salesman problem with the Mississippi River twist, if A is large, a decision to cross
the Mississippi only twice is made at high T’, while the specific routes on each side
of the river are determined only at later stages.

The analogies to thermodynamics may be pursued to a greater extent than we
have done here. Quantities analogous to specific heat and entropy may be defined,
and these can be useful in monitoring the progress of the algorithm towards an
acceptable solution. Information on this subject is found in [1].

CITED REFERENCES AND FURTHER READING:

Kirkpatrick, S., Gelatt, C.D., and Vecchi, M.P. 1983, Science, vol. 220, pp. 671-680. [1]

Kirkpatrick, S. 1984, Journal of Statistical Physics, vol. 34, pp. 975-986. [2]

Vecchi, M.P. and Kirkpatrick, S. 1983, IEEE Transactions on Computer Aided Design, vol. CAD-
2, pp. 215-222. [3]

Otten, R.H.J.M., and van Ginneken, L.P.P.P. 1989, The Annealing Algorithm (Boston: Kluwer)
[contains many references to the literature]. [4]

Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller A., and Teller, E. 1953, Journal of Chemical
Physics, vol. 21, pp. 1087-1092. [5]

Lin, S. 1965, Bell System Technical Journal, vol. 44, pp. 2245-2269. [6]

Vanderbilt, D., and Louie, S.G. 1984, Journal of Computational Physics, vol. 56, pp. 259-271. [7]

Bohachevsky, I.0., Johnson, M.E., and Stein, M.L. 1986, Technometrics, vol. 28, pp. 209-217. [8]

Corana, A., Marchesi, M., Martini, C., and Ridella, S. 1987, ACM Transactions on Mathematical
Software, vol. 13, pp. 262-280. [9]

Bélisle, C.J.P., Romeijn, H.E., and Smith, R.L. 1990, Technical Report 90-25, Department of
Industrial and Operations Engineering, University of Michigan, submitted to Mathematical
Programming. [10]

Christofides, N., Mingozzi, A., Toth, P., and Sandi, C. (eds.) 1979, Combinatorial Optimization
(London and New York: Wiley-Interscience) [not simulated annealing, but other topics and
algorithms].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 11. Eigensystems

11.0 Introduction

An N x N matrix A is said to have an eigenvector x and corresponding
eigenvalue A if

A-x=Xx (11.0.1)

Obviously any multiple of an eigenvector x will also be an eigenvector, but we
won’t consider such multiples as being distinct eigenvectors. (The zero vector is not
considered to be an eigenvector at all.) Evidently (11.0.1) can hold only if

det |A — 1] =0 (11.0.2)

which, if expanded out, is an Nth degree polynomial in \ whose roots are the eigen-
values. This proves that there are always N (not necessarily distinct) eigenvalues.
Equal eigenvalues coming from multiple roots are called degenerate. Root-searching
in the characteristic equation (11.0.2) is usually a very poor computational method
for finding eigenvalues. We will learn much better ways in this chapter, as well as
efficient ways for finding corresponding eigenvectors.

The above two equations also prove that every one of the N eigenvalues has
a (not necessarily distinct) corresponding eigenvector: If \ is set to an eigenvalue,
then the matrix A — A1 is singular, and we know that every singular matrix has at
least one nonzero vector in its nullspace (see §2.6 on singular value decomposition).

If you add rx to both sides of (11.0.1), you will easily see that the eigenvalues
of any matrix can be changed or shifted by an additive constant 7 by adding to the
matrix that constant times the identity matrix. The eigenvectors are unchanged by
this shift. Shifting, as we will see, is an important part of many algorithms for
computing eigenvalues. We see also that there is no special significance to a zero
eigenvalue. Any eigenvalue can be shifted to zero, or any zero eigenvalue can be
shifted away from zero.

456

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

11.0 Introduction 457

Definitions and Basic Facts
A matrix is called symmetric if it is equal to its transpose,
A=AT or aij = yu (11.0.3)

It is called Hermitian or self-adjoint if it equals the complex-conjugate of its transpose
(its Hermitian conjugate, denoted by “{”)

A=Al oor ai = aj* (11.0.4)
It is termed orthogonal if its transpose equals its inverse,
AT.A=A-AT=1 (11.0.5)

and unitary if its Hermitian conjugate equals its inverse. Finally, a matrix is called
normal if it commutes with its Hermitian conjugate,

A-Al=at-a (11.0.6)

For real matrices, Hermitian means the same as symmetric, unitary means the
same as orthogonal, and both of these distinct classes are normal.

The reason that “Hermitian” is an important concept has to do with eigenvalues.
The eigenvalues of a Hermitian matrix are all real. In particular, the eigenvalues
of a real symmetric matrix are all real. Contrariwise, the eigenvalues of a real
nonsymmetric matrix may include real values, but may also include pairs of complex
conjugate values; and the eigenvalues of a complex matrix that is not Hermitian
will in general be complex.

The reason that “normal” is an important concept has to do with the eigen-
vectors. The eigenvectors of a normal matrix with nondegenerate (i.e., distinct)
eigenvalues are complete and orthogonal, spanning the N-dimensional vector space.
For a normal matrix with degenerate eigenvalues, we have the additional freedom of
replacing the eigenvectors corresponding to a degenerate eigenvalue by linear com-
binations of themselves. Using this freedom, we can always perform Gram-Schmidt
orthogonalization (consult any linear algebra text) and find a set of eigenvectors that
are complete and orthogonal, just as in the nondegenerate case. The matrix whose
columns are an orthonormal set of eigenvectors is evidently unitary. A special case
is that the matrix of eigenvectors of a real, symmetric matrix is orthogonal, since
the eigenvectors of that matrix are all real.

When a matrix is not normal, as typified by any random, nonsymmetric, real
matrix, then in general we cannot find any orthonormal set of eigenvectors, nor even
any pairs of eigenvectors that are orthogonal (except perhaps by rare chance). While
the NV non-orthonormal eigenvectors will “usually” span the N-dimensional vector
space, they do not always do so; that is, the eigenvectors are not always complete.
Such a matrix is said to be defective.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

458 Chapter 11. Eigensystems

Left and Right Eigenvectors

While the eigenvectors of a non-normal matrix are not particularly orthogonal
among themselves, they do have an orthogonality relation with a different set of
vectors, which we must now define. Up to now our eigenvectors have been column
vectors that are multiplied to the right of a matrix A, as in (11.0.1). These, more
explicitly, are termed right eigenvectors. We could also, however, try to find row
vectors, which multiply A to the left and satisfy

x-A=Xx (11.0.7)

These are called left eigenvectors. By taking the transpose of equation (11.0.7), we
see that every left eigenvector is the transpose of a right eigenvector of the transpose
of A. Now by comparing to (11.0.2), and using the fact that the determinant of a
matrix equals the determinant of its transpose, we also see that the left and right
eigenvalues of A are identical.

If the matrix A is symmetric, then the left and right eigenvectors are just
transposes of each other, that is, have the same numerical values as components.
Likewise, if the matrix is self-adjoint, the left and right eigenvectors are Hermitian
conjugates of each other. For the general nonnormal case, however, we have the
following calculation: Let Xz be the matrix formed by columns from the right
eigenvectors, and X ;, be the matrix formed by rows from the left eigenvectors. Then
(11.0.1) and (11.0.7) can be rewritten as

A-Xp=Xpr-diag(\1...Av) = -X_-A=diag(y...An)-Xz_ (11.0.8)

Multiplying the first of these equations on the left by X,,, the second on the right
by Xp, and subtracting the two, gives

(Xr -XpR)-diag(Ay...An) = diag(A... Aw) - (Xi + XR) (11.0.9)

This says that the matrix of dot products of the left and right eigenvectors commutes
with the diagonal matrix of eigenvalues. But the only matrices that commute with a
diagonal matrix of distinct elements are themselves diagonal. Thus, if the eigenvalues
are nondegenerate, each left eigenvector is orthogonal to all right eigenvectors except
its corresponding one, and vice versa. By choice of normalization, the dot products
of corresponding left and right eigenvectors can always be made unity for any matrix
with nondegenerate eigenvalues.

If some eigenvalues are degenerate, then either the left or the right eigenvec-
tors corresponding to a degenerate eigenvalue must be linearly combined among
themselves to achieve orthogonality with the right or left ones, respectively. This
can always be done by a procedure akin to Gram-Schmidt orthogonalization. The
normalization can then be adjusted to give unity for the nonzero dot products between
corresponding left and right eigenvectors. If the dot product of corresponding left and
right eigenvectors is zero at this stage, then you have a case where the eigenvectors
are incomplete! Note that incomplete eigenvectors can occur only where there are
degenerate eigenvalues, but do not always occur in such cases (in fact, never occur
for the class of “normal” matrices). See [1] for a clear discussion.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.0 Introduction 459

In both the degenerate and nondegenerate cases, the final normalization to
unity of all nonzero dot products produces the result: The matrix whose rows
are left eigenvectors is the inverse matrix of the matrix whose columns are right
eigenvectors, if the inverse exists.

Diagonalization of a Matrix

Multiplying the first equation in (11.0.8) by X_, and using the fact that X;,
and Xp are matrix inverses, we get

Xp -A- Xp =diag(Ar... A) (11.0.10)
This is a particular case of a similarity transform of the matrix A,
A > Z!-A-Z (11.0.1)

for some transformation matrix Z. Similarity transformations play a crucial role
in the computation of eigenvalues, because they leave the eigenvalues of a matrix
unchanged. This is easily seen from

det |Z~'- A+ Z — Al] = det|Z~! - (A— 1) -Z|

= det |Z| det|A — Al] det |Z~*| (11.0.12)
= det |A — Al|

Equation (11.0.10) shows that any matrix with complete eigenvectors (which includes
all normal matrices and “most” random nonnormal ones) can be diagonalized by a
similarity transformation, that the columns of the transformation matrix that effects
the diagonalization are the right eigenvectors, and that the rows of its inverse are
the left eigenvectors.

For real, symmetric matrices, the eigenvectors are real and orthonormal, so the
transformation matrix is orthogonal. The similarity transformation is then also an
orthogonal transformation of the form

A = Z™.A-Z (11.0.13)

While real nonsymmetric matrices can be diagonalized in their usual case of complete
eigenvectors, the transformation matrix is not necessarily real. It turns out, however,
that a real similarity transformation can “almost” do the job. It can reduce the matrix
down to a form with little two-by-two blocks along the diagonal, all other elements
zero. Each two-by-two block corresponds to a complex-conjugate pair of complex
eigenvalues. We will see this idea exploited in some routines given later in the chapter.
The “grand strategy” of virtually all modern eigensystem routines is to nudge
the matrix A towards diagonal form by a sequence of similarity transformations,

A > Py!-A-Py => P5!-Py!-A-Py- Po

11.0.14
> P31-P5-Py!-A-Pi-P2-P3 > etc. ( )

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

460 Chapter 11. Eigensystems

If we get all the way to diagonal form, then the eigenvectors are the columns of
the accumulated transformation

Xp =P1-Po-P3-... (11.0.15)

Sometimes we do not want to go all the way to diagonal form. For example, if we are
interested only in eigenvalues, not eigenvectors, it is enough to transform the matrix
A to be triangular, with all elements below (or above) the diagonal zero. In this
case the diagonal elements are already the eigenvalues, as you can see by mentally
evaluating (11.0.2) using expansion by minors.

There are two rather different sets of techniques for implementing the grand
strategy (11.0.14). It turns out that they work rather well in combination, so most
modern eigensystem routines use both. The first set of techniques constructs individ-
ual P,’s as explicit “atomic” transformations designed to perform specific tasks, for
example zeroing a particular off-diagonal element (Jacobi transformation, §11.1), or
a whole particular row or column (Householder transformation, §11.2; elimination
method, §11.5). In general, a finite sequence of these simple transformations cannot
completely diagonalize a matrix. There are then two choices: either use the finite
sequence of transformations to go most of the way (e.g., to some special form like
tridiagonal or Hessenberg, see §11.2 and §11.5 below) and follow up with the second
set of techniques about to be mentioned; or else iterate the finite sequence of simple
transformations over and over until the deviation of the matrix from diagonal is
negligibly small. This latter approach is conceptually simplest, so we will discuss
it in the next section; however, for N greater than ~ 10, it is computationally
inefficient by a roughly constant factor ~ 5.

The second set of techniques, called factorization methods, is more subtle.
Suppose that the matrix A can be factored into a left factor F ; and a right factor
Fr. Then

A=F,-Fr or equivalently F,'-A=Fr (11.0.16)

If we now multiply back together the factors in the reverse order, and use the second
equation in (11.0.16) we get

Fr-F, =F,)-A-Fr (11.0.17)

which we recognize as having effected a similarity transformation on A with the
transformation matrix being F,! In §11.3 and $11.6 we will discuss the QR method
which exploits this idea.

Factorization methods also do not converge exactly in a finite number of
transformations. But the better ones do converge rapidly and reliably, and, when
following an appropriate initial reduction by simple similarity transformations, they
are the methods of choice.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.0 Introduction 461

“Eigenpackages of Canned Eigenroutines”

You have probably gathered by now that the solution of eigensystems is a fairly
complicated business. It is. It is one of the few subjects covered in this book for
which we do not recommend that you avoid canned routines. On the contrary, the
purpose of this chapter is precisely to give you some appreciation of what is going
on inside such canned routines, so that you can make intelligent choices about using
them, and intelligent diagnoses when something goes wrong.

You will find that almost all canned routines in use nowadays trace their ancestry
back to routines published in Wilkinson and Reinsch’s Handbook for Automatic
Computation, Vol. II, Linear Algebra [2]. This excellent reference, containing papers
by a number of authors, is the Bible of the field. A public-domain implementation
of the Handbook routines in FORTRAN is the EISPACK set of programs [3]. The
routines in this chapter are translations of either the Handbook or EISPACK routines,
so understanding these will take you a lot of the way towards understanding those
canonical packages.

IMSL [4] and NAG [5] each provide proprietary implementations, in FORTRAN,
of what are essentially the Handbook routines.

A good “eigenpackage” will provide separate routines, or separate paths through
sequences of routines, for the following desired calculations:

e all eigenvalues and no eigenvectors

e all eigenvalues and some corresponding eigenvectors

e all eigenvalues and all corresponding eigenvectors
The purpose of these distinctions is to save compute time and storage; it is wasteful
to calculate eigenvectors that you don’t need. Often one is interested only in
the eigenvectors corresponding to the largest few eigenvalues, or largest few in
magnitude, or few that are negative. The method usually used to calculate “some”
eigenvectors is typically more efficient than calculating all eigenvectors if you desire
fewer than about a quarter of the eigenvectors.

A good eigenpackage also provides separate paths for each of the above
calculations for each of the following special forms of the matrix:

e real, symmetric, tridiagonal

e real, symmetric, banded (only a small number of sub- and superdiagonals

are nonzero)

e real, symmetric

e real, nonsymmetric

e complex, Hermitian

e complex, non-Hermitian
Again, the purpose of these distinctions is to save time and storage by using the least
general routine that will serve in any particular application.

In this chapter, as a bare introduction, we give good routines for the following
paths:

e all eigenvalues and eigenvectors of a real, symmetric, tridiagonal matrix

(§11.3)
e all eigenvalues and eigenvectors of a real, symmetric, matrix (§11.1-811.3)
e all eigenvalues and eigenvectors of a complex, Hermitian matrix
(§11.4)

e all eigenvalues and no eigenvectors of a real, nonsymmetric matrix (§11.5—

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

462 Chapter 11. Eigensystems

§11.6)
We also discuss, in $11.7, how to obtain some eigenvectors of nonsymmetric
matrices by the method of inverse iteration.

Generalized and Nonlinear Eigenvalue Problems
Many eigenpackages also deal with the so-called generalized eigenproblem, [6]
A-x=B-x (11.0.18)

where A and B are both matrices. Most such problems, where B is nonsingular,
can be handled by the equivalent

(B-!-A)-x =x (11.0.19)

Often A and B are symmetric and B is positive definite. The matrix B~!- A in
(11.0.19) is not symmetric, but we can recover a symmetric eigenvalue problem
by using the Cholesky decomposition B = L - L7 of §2.9. Multiplying equation
(11.0.18) by L~!, we get

C-(L7 -x) = X(L* - x) (11.0.20)

where
C=Lt-A-(L)? (11.0.21)
The matrix C is symmetric and its eigenvalues are the same as those of the original

problem (11.0.18); its eigenfunctions are L” - x. The efficient way to form C is
first to solve the equation

Y-LT=A (11.0.2)
for the lower triangle of the matrix Y. Then solve
L-C=Y (11.0.23)
for the lower triangle of the symmetric matrix C.
Another generalization of the standard eigenvalue problem is to problems
nonlinear in the eigenvalue A, for example,

(AM +BA+C)-x=0 (11.0.24)

This can be turned into a linear problem by introducing an additional unknown
eigenvector y and solving the 2NV x 2N eigensystem,

Ger oa) G)-a8) (1.0.25)

This technique generalizes to higher-order polynomials in A. A polynomial of degree
M produces a linear MN x MN eigensystem (see [7]).

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

11.1 Jacobi Transformations of a Symmetric Matrix 463

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 6. [1]

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [2]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [3]

IMSL Math/Library Users Manual (IMSL Inc., 2500 CityWest Boulevard, Houston TX 77042). [4]

NAG Fortran Library (Numerical Algorithms Group, 256 Banbury Road, Oxford OX27DE, U.K.),
Chapter FO2. [5]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), §7.7. [6]
Wilkinson, J.H. 1965, The Algebraic Eigenvalue Problem (New York: Oxford University Press). [7]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 13.

Horn, R.A., and Johnson, C.R. 1985, Matrix Analysis (Cambridge: Cambridge University Press).

11.1 Jacobi Transformations of a Symmetric
Matrix

The Jacobi method consists of a sequence of orthogonal similarity transforma-
tions of the form of equation (11.0.14). Each transformation (a Jacobi rotation) is
just a plane rotation designed to annihilate one of the off-diagonal matrix elements.
Successive transformations undo previously set zeros, but the off-diagonal elements
nevertheless get smaller and smaller, until the matrix is diagonal to machine preci-
sion. Accumulating the product of the transformations as you go gives the matrix
of eigenvectors, equation (11.0.15), while the elements of the final diagonal matrix
are the eigenvalues.

The Jacobi method is absolutely foolproof for all real symmetric matrices. For
matrices of order greater than about 10, say, the algorithm is slower, by a significant
constant factor, than the QR method we shall give in §11.3. However, the Jacobi
algorithm is much simpler than the more efficient methods. We thus recommend it
for matrices of moderate order, where expense is not a major consideration.

The basic Jacobi rotation Pp, is a matrix of the form

Pog = : 1: (11.1.1)

1

Here all the diagonal elements are unity except for the two elements c in rows (and
columns) p and q. All off-diagonal elements are zero except the two elements s and
—s. The numbers c and s are the cosine and sine of a rotation angle ¢, so ce+s2=1.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

464 Chapter 11. Eigensystems

A plane rotation such as (11.1.1) is used to transform the matrix A according to

! T
A= Pig

»A- Pog (11.1.2)
Now, Pra - A changes only rows p and q of A, while A - P,,, changes only columns
p and q. Notice that the subscripts p and q do not denote components of P p,q, but
rather label which kind of rotation the matrix is, i.e., which rows and columns it

affects. Thus the changed elements of A in (11.1.2) are only in the p and q rows

and columns indicated below:
r t t
Ap Aq
t t ui t
G1 Opp © pq Gn
A= : : (11.1.3)
t t t t
on Tap © Aq Gn
al .. a

t ue np nq

Multiplying out equation (11.1.2) and using the symmetry of A, we get the explicit
formulas

frp ye répr#éq (11.14)

Ang = CArg + 8Arp

yp =~ Capp + 8 qy — 28CApq (11.1.5)

Og = 8° Opp + C7 dgq + 28CApq (11.1.6)
2 2

Aq =(c-s )apq + 8C(App _ qq) (11.1.7)

The idea of the Jacobi method is to try to zero the off-diagonal elements by a
series of plane rotations. Accordingly, to set ang = 0, equation (11.1.7) gives the
following expression for the rotation angle @

2 62 _
9 =cot2¢ = “> = Sat op (11.1.8)
2sc 2apq
If we let t = s/c, the definition of # can be rewritten
?+2t0-1=0 (11.1.9)

The smaller root of this equation corresponds to a rotation angle less than 7/4
in magnitude; this choice at each stage gives the most stable reduction. Using the
form of the quadratic formula with the discriminant in the denominator, we can
write this smaller root as

sgn(0)

= ver (1.1.10)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.1 Jacobi Transformations of a Symmetric Matrix 465

If 6 is so large that 6? would overflow on the computer, we set t = 1/(20). It
now follows that

(1.1.11)

(11.1.12)
When we actually use equations (11.1.4)-(11.1.7) numerically, we rewrite them

to minimize roundoff error. Equation (11.1.7) is replaced by
ay, = 0 (11.1.13)
The idea in the remaining equations is to set the new quantity equal to the old

quantity plus a small correction. Thus we can use (11.1.7) and (11.1.13) to eliminate
Qgq from (11.1.5), giving

ayy = App — tapg (11.1.14)
Similarly,
yg = Agq + tpg (11.1.15)
Gp = Arp — 8(Arq + Trp) (11.1.16)
Gig = Arg + 8(Arp — TArq) (11.1.17)
where 7 (= tan ¢/2) is defined by
T= (11.1.18)

l+e

One can see the convergence of the Jacobi method by considering the sum of
the squares of the off-diagonal elements

S= So lars)? (11.1.19)
rs
Equations (11.1.4)-(11.1.7) imply that

S! = S — apg? (11.1.20)

(Since the transformation is orthogonal, the sum of the squares of the diagonal
elements increases correspondingly by 2|apq|”.) The sequence of S’s thus decreases
monotonically. Since the sequence is bounded below by zero, and since we can
choose ay, to be whatever element we want, the sequence can be made to converge
to zero.

Eventually one obtains a matrix D that is diagonal to machine precision. The
diagonal elements give the eigenvalues of the original matrix A, since

D=V’-A-V (11.1.21)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

466 Chapter 11. Eigensystems

where
V=P,-P2-P3--: (11.1.22)

the P;’s being the successive Jacobi rotation matrices. The columns of V are the
eigenvectors (since A- V = V - D). They can be computed by applying

Vi =V-P; (11.1.23)

at each stage of calculation, where initially V is the identity matrix. In detail,
equation (11.1.23) is

Ung = Urs (s Ap, s#q)

J

Upp = CUrp — SUrq (11.1.24)
Us

Urg = SUrp + CUrg

We rewrite these equations in terms of 7 as in equations (11.1.16) and (11.1.17)
to minimize roundoff.

The only remaining question is the strategy one should adopt for the order in
which the elements are to be annihilated. Jacobi’s original algorithm of 1846 searched
the whole upper triangle at each stage and set the largest off-diagonal element to zero.
This is a reasonable strategy for hand calculation, but it is prohibitive on a computer
since the search alone makes each Jacobi rotation a process of order N ? instead of N.

A better strategy for our purposes is the cyclic Jacobi method, where one
annihilates elements in strict order. For example, one can simply proceed down
the rows: Pj2,Pi3,...,Pin; then P23,P24, etc. One can show that convergence
is generally quadratic for both the original or the cyclic Jacobi methods, for
nondegenerate eigenvalues. One such set of n(n — 1)/2 Jacobi rotations is called
a sweep.

The program below, based on the implementations in [1,2], uses two further
refinements:
e In the first three sweeps, we carry out the pq rotation only if |ap,y| > €
for some threshold value
1S
e= i (1.1.25)

5 n?

where So is the sum of the off-diagonal moduli,

$0 = So lars (1.1.26)

r<s

e After four sweeps, if |apg| < |app| and |apg| < |agg|, we set |apg| = 0
and skip the rotation. The criterion used in the comparison is |apg| <
10-(?+?)/a,,|, where D is the number of significant decimal digits on the
machine, and similarly for |agq|.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.1 Jacobi Transformations of a Symmetric Matrix 467

In the following routine the nxn symmetric matrix a is stored as a[1..n]
[{1..n]. On output, the superdiagonal elements of a are destroyed, but the diagonal
and subdiagonal are unchanged and give full information on the original symmetric
matrix a. The vector d[1..n] returns the eigenvalues of a. During the computation,
it contains the current diagonal of a. The matrix v[1..n][1..n] outputs the
normalized eigenvector belonging to d[k] in its kth column. The parameter nrot is
the number of Jacobi rotations that were needed to achieve convergence.

Typical matrices require 6 to 10 sweeps to achieve convergence, or 3n to 5n?
Jacobi rotations. Each rotation requires of order 4n operations, each consisting
of a multiply and an add, so the total labor is of order 12n? to 20n° operations.
Calculation of the eigenvectors as well as the eigenvalues changes the operation
count from 4n to 6n per rotation, which is only a 50 percent overhead.

#include <math.h>

#include "nrutil.h"

#define ROTATE(a,i,j,k,1) g=ali] [j] ;h=alk] [1] ;alil] [j]=g-s*(htg*tau) ;\
a(k] [1]=h+s*(g-h*tau) ;

void jacobi(float **a, int n, float a[], float **v, int *nrot)
Computes all eigenvalues and eigenvectors of a real symmetric matrix a[1..n] [1..n]. On
output, elements of a above the diagonal are destroyed. d[1..n] returns the eigenvalues of a.
v(1..n] [1..n] is a matrix whose columns contain, on output, the normalized eigenvectors of
a. nrot returns the number of Jacobi rotations that were required.
t

int j,iq,ip,i;

float tresh,theta,tau,t,sm,s,h,g,c,*b,¥*z;

b=vector(1,n);

z=vector(1,n);

for (ip=1;ip<=n;ip++) {
for (iq=1;iq<=n;iqt+) vip] [iq]=0.0;
vip] Cip]=1.0;

Initialize to the identity matrix.

}

for (ip=1;ip<=n;ip++) { Initialize b and d to the diagonal
blip]=d[ip]=alip] [ip]; of a.
z[ip]=0.0; This vector will accumulate terms

of the form tap, as in equa-
tion (11.1.14).

for (ip=1;ip<=n-1;ipt++) {
for (igq=ip+1;ig<=n;igt+)
sm += fabs(alip] [iq]);

Sum off-diagonal elements.

}

if (sm == 0.0) { The normal return, which relies
free_vector(z,1,n); on quadratic convergence to
free_vector(b,1,n); machine underflow.
return;

}

if (i < 4)
tresh=0.2*sm/(n*n) ; ...on the first three sweeps.

else
tresh=0.0; ... thereafter.

for (ip=1; m-1;ipt+) {

for (ig=ipt1;igq<=n;igt+) {
g=100.0*fabs(al[ip] [iq]);
After four sweeps, skip the rotation if the off-diagonal element is small.
if (i > 4 && (float) (fabs(d[ip])+g) == (float)fabs(d[ip])
&& (float) (fabs(d[iq])+g) == (float)fabs(d[iq]))
alip] [iq]=0.0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

468

Chapter11. Eigensystems

else if (fabs(alip][iq]) > tresh) {

3
}

h=d[iq]-d[ip] ;
if ((float)(fabs(h)+g) == (float) fabs(h))

t=(alip] Liq])/h; t = 1/(20)
else {
theta=0.5*h/(alip] [iq]); Equation (11.1.10).

t=1.0/(fabs(theta)+tsqrt (1.0+theta*theta)) ;
if (theta < 0.0) t = -t;

+

c=1.0/sqrt(1+t*t) ;

s=t*c;

tau=s/(1.0+c);

h=t*a[ip] [iq] ;

zlip] -= h;

zliq] += h;

d[ip] -= h;

d[iq] += h;

a[ip] [iq]=0.0;

for (j=1;j<=ip-1;j++) {
ROTATE(a,j,ip,j,iq)

Case of rotations 1 < j <p.

+
for (j=ipt1;j<=iq-1;j++) { Case of rotations p < j < q.
ROTATE(a,ip,j,j,iq)

}
for (j=iqt1;j<-n;j++) {
ROTATE(a,ip,j,iq,j)

Case of rotations q < j <n.

}

for (j=1;j<=n;j++) {
ROTATE(v,j,ip,j,iq)

}

++(*nrot) ;

for (ip=1;ip<=n;ipt+) {
blip] += zlip];
dlip]=b[ip] ; Update d with the sum of tapg,
z[ip]=0.0; and reinitialize z.

}
+

nrerror("Too many iterations in routine jacobi");

Note that the above routine assumes that underflows are set to zero.

machines where this is not true, the program must be modified.

The eigenvalues are not ordered on output. If sorting is desired, the following
routine can be invoked to reorder the output of jacobi or of later routines in this
chapter. (The method, straight insertion, is N? rather than N log N; but since you
have just done an N® procedure to get the eigenvalues, you can afford yourself

this little indulgence.)

void eigsrt(float d[], float **v, int n)

Given the eigenvalues d[1..n] and eigenvectors v[1..n][1..n] as output from jacobi
(§11.1) or tqli (§11.3), this routine sorts the eigenvalues into descending order, and rearranges

the columns of v correspondingly. The method is straight insertion.

t
int k,j,i;
float p;

for (i=
p=d[k=i] ;

jisn;i++) f

On

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.2 Reduction of a Symmetric Matrix to Tridiagonal Form 469

for (j=it1;j<=n; j++)
if (d[j] >= p) p=d[k=j];
if (k != i) {
d(k]=d [i] ;
alil=p;
for (j=1;j<=n;jt++) {
p=v(j] (41;
v(j] Ci]=v(j] Oc ;
vj] [k]=p;

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), $8.4.

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [1]

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [2]

11.2 Reduction of a Symmetric Matrix
to Tridiagonal Form: Givens and
Householder Reductions

As already mentioned, the optimum strategy for finding eigenvalues and
eigenvectors is, first, to reduce the matrix to a simple form, only then beginning an
iterative procedure. For symmetric matrices, the preferred simple form is tridiagonal.
The Givens reduction is a modification of the Jacobi method. Instead of trying to
reduce the matrix all the way to diagonal form, we are content to stop when the
matrix is tridiagonal. This allows the procedure to be carried out in a finite number
of steps, unlike the Jacobi method, which requires iteration to convergence.

Givens Method

For the Givens method, we choose the rotation angle in equation (11.1.1) so
as to zero an element that is not at one of the four “corners,” i.e., not @pp, Apgs
OF @gq in equation (11.1.3). Specifically, we first choose P23 to annihilate a3,
(and, by symmetry, a13). Then we choose P24 to annihilate a4;. In general, we
choose the sequence

Pog, Poa,.--,Pan;Paa,---,Pani---3Pn—1n

where P ;;, annihilates a;,;_;. The method works because elements such as ay and

ngs with r 4 pr # q, are linear combinations of the old quantities a,» and aq, by

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

470 Chapter 11. Eigensystems

equation (11.1.4). Thus, if a,» and a,q have already been set to zero, they remain
zero as the reduction proceeds. Evidently, of order n?/2 rotations are required,
and the number of multiplications in a straightforward implementation is of order
4n°/3, not counting those for keeping track of the product of the transformation
matrices, required for the eigenvectors.

The Householder method, to be discussed next, is just as stable as the Givens
reduction and it is a factor of 2 more efficient, so the Givens method is not generally
used. Recent work (see [1]) has shown that the Givens reduction can be reformulated
to reduce the number of operations by a factor of 2, and also avoid the necessity
of taking square roots. This appears to make the algorithm competitive with the
Householder reduction. However, this “fast Givens” reduction has to be monitored
to avoid overflows, and the variables have to be periodically rescaled. There does
not seem to be any compelling reason to prefer the Givens reduction over the
Householder method.

Householder Method

The Householder algorithm reduces an n x n symmetric matrix A to tridiagonal
form by n — 2 orthogonal transformations. Each transformation annihilates the
required part of a whole column and whole corresponding row. The basic ingredient
is a Householder matrix P, which has the form

P=1-2w-w? (11.2.1)

where w is a real vector with |w|? = 1. (In the present notation, the outer or matrix
product of two vectors, a and b is written a - b” , while the inner or scalar product of
the vectors is written as a7 - b.) The matrix P is orthogonal, because
P? = (1—2w-w’)-(1—2w-w’)
= 1-—4w-w! + 4w-(w?-w)-w? (11.2.2)
=1

Therefore P = P~!. But P? = P, and so P? = P~!, proving orthogonality.
Rewrite P as

T
u-u
P=1- 11.2.3
x (11.23)
where the scalar H is
1.
Hs jul? (11.2.4)

and u can now be any vector. Suppose x is the vector composed of the first column
of A. Choose

(11.2.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.2 Reduction of a Symmetric Matrix to Tridiagonal Form 471

where e is the unit vector [1,0,...,0]7, and the choice of signs will be made
later. Then

(11.2.6)

This shows that the Householder matrix P acts on a given vector x to zero all its
elements except the first one.

To reduce a symmetric matrix A to tridiagonal form, we choose the vector x
for the first Householder matrix to be the lower n — 1 elements of the first column.
Then the lower n — 2 elements will be zeroed:

1 0 0 vee 0 au a2 413 tee Qin
0 a2i
P;-A=|0 a31
(n—-1)p, : irrelevant
LO Ant
au 412 4130 *** Gin
k
= 0 (11.2.7)
: irrelevant
0

Here we have written the matrices in partitioned form, with ‘"—1)P denoting a
Householder matrix with dimensions (n — 1) x (n — 1). The quantity & is simply
plus or minus the magnitude of the vector [a21,...,@ni]?.

The complete orthogonal transformation is now

an. | k 0 + 0
k
A'=P-A-P=| 0 (11.2.8)

irrelevant

We have used the fact that P? = P.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

472 Chapter 11. Eigensystems

Now choose the vector x for the second Householder matrix to be the bottom
n — 2 elements of the second column, and from it construct

1 0 | 0 0
01/0 0

Po =/0 0 (11.2.9)
: (n—2)p,
0 0

The identity block in the upper left corner insures that the tridiagonalization achieved
in the first step will not be spoiled by this one, while the (n — 2)-dimensional
Householder matrix ‘"~?) Ps creates one additional row and column of the tridiagonal
output. Clearly, a sequence of nm — 2 such transformations will reduce the matrix
A to tridiagonal form.

Instead of actually carrying out the matrix multiplications in P- A-P, we
compute a vector

A-u

P= (11.2.10)
Then
u-u? r
A-P=A-(1— )=A-—p-u
A’=P-A-P=A-—p-ul —u-p? +2Ku-ut
where the scalar K is defined by
ul -p
K= 11.2.11
Ti ( )
If we write
q=p-Ku (11.2.12)
then we have
A’ =A-—q-u? -—u-q? (11.2.13)

This is the computationally useful formula.
Following [2], the routine for Householder reduction given below actually starts
in the nth column of A, not the first as in the explanation above. In detail, the

equations are as follows: At stage m (m = 1,2,...,m—2) the vector u has the form
ul = [aa, ai2,..-, 5-2, @is-1 + Vo, 0,..-, 0] (11.2.14)
Here
(11.2.15)
and the quantity o (|x|? in our earlier notation) is
o = (aia)? +++ + (a1)? (11.2.16)

We choose the sign of o in (11.2.14) to be the same as the sign of a;;—-1 to lessen
roundoff error.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.2 Reduction of a Symmetric Matrix to Tridiagonal Form 473

Variables are thus computed in the following order: o,u, H,p, K,q, A’. At any
stage m, A is tridiagonal in its last m — 1 rows and columns.

If the eigenvectors of the final tridiagonal matrix are found (for example, by the
routine in the next section), then the eigenvectors of A can be obtained by applying
the accumulated transformation

Q=Pi-P2---Pn-2 (11.2.17)

to those eigenvectors. We therefore form Q by recursion after all the P’s have
been determined:

Qn-2 = Pn—2
Q; =P3- Qui j=n-3,..., 1 (11.2.18)
Q=Q

Input for the routine below is the real, symmetric matrix a[1..n] [1..n]. On
output, a contains the elements of the orthogonal matrix q. The vector d[1..n] is
set to the diagonal elements of the tridiagonal matrix A’, while the vector e[1..n]
is set to the off-diagonal elements in its components 2 through n, with e[1]=0.
Note that since a is overwritten, you should copy it before calling the routine, if it
is required for subsequent computations.

No extra storage arrays are needed for the intermediate results. At stage m, the

vectors p and q are nonzero only in elements 1,...,i (recall that i = n —-m+1),
while u is nonzero only in elements 1,...,2 — 1. The elements of the vector e are
being determined in the order n,n — 1,...,s0 we can store p in the elements of e

not already determined. The vector q can overwrite p once p is no longer needed.
We store u in the ith row of a and u/H in the ith column of a. Once the reduction
is complete, we compute the matrices Q ; using the quantities u and u /H that have
been stored in a. Since Q; is an identity matrix in the last n — 7 + 1 rows and
columns, we only need compute its elements up to row and column n — j. These
can overwrite the u’s and u/H’s in the corresponding rows and columns of a, which
are no longer required for subsequent Q’s.

The routine tred2, given below, includes one further refinement. If the quantity
o is zero or “small” at any stage, one can skip the corresponding transformation.
A simple criterion, such as

smallest positive number representable on machine

o< 7 ra
machine precision

would be fine most of the time. A more careful criterion is actually used. Define
the quantity

i-l1
e=)) la| (11.2.19)
k=1

If € = 0 to machine precision, we skip the transformation. Otherwise we redefine

Giz becomes ajx/€ (11.2.20)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

474 Chapter11. Eigensystems

and use the scaled variables for the transformation. (A Householder transformation
depends only on the ratios of the elements.)

Note that when dealing with a matrix whose elements vary over many orders of
magnitude, it is important that the matrix be permuted, insofar as possible, so that
the smaller elements are in the top left-hand corner. This is because the reduction
is performed starting from the bottom right-hand corner, and a mixture of small and
large elements there can lead to considerable rounding errors.

The routine tred2 is designed for use with the routine tq1i of the next section.
tqli finds the eigenvalues and eigenvectors of a symmetric, tridiagonal matrix.
The combination of tred2 and tqli is the most efficient known technique for
finding all the eigenvalues and eigenvectors (or just all the eigenvalues) of a real,
symmetric matrix.

In the listing below, the statements indicated by comments are required only for
subsequent computation of eigenvectors. If only eigenvalues are required, omission
of the commented statements speeds up the execution time of tred2 by a factor of 2
for large n. In the limit of large n, the operation count of the Householder reduction
is 2n3/3 for eigenvalues only, and 4n°/3 for both eigenvalues and eigenvectors.

#include <math.h>

void tred2(float **a, int n, float d[], float e[])
Householder reduction of a real, symmetric matrix a[1..n] [1..n]. On output, a is replaced
by the orthogonal matrix Q effecting the transformation. d[1..n] returns the diagonal ele-
ments of the tridiagonal matrix, and e[1..n] the off-diagonal elements, with e[1]=0. Several
statements, as noted in comments, can be omitted if only eigenvalues are to be found, in which
case a contains no useful information on output. Otherwise they are to be included.
{

int 1,k,j,i;

float scale,hh,h,g,f;

ji>=2;i--) {
i-1;

h=scale=0.0;
if (> 4) {
for (k=1;k<=1;k++)
scale fabs (a([i] [k]);
if (scale 0.0) Skip transformation.
e(i]=ali] (11;
else {
for (k=1;k<=1;k++) {
ali] [k] /= scale; Use scaled a's for transformation.
h += a[i] [k] *a[i] [k]; Form o in h.
3
f=ali] 1;
f >= 0.0 ? -sqrt(h) : sqrt(h));
scale*g;
£*g; Now h is equation (11.2.4).
ali] (1]=f-g; Store u in the ith row of a.

£=0.0;

for (j=1;j<=1;jt+) {

/* Next statement can be omitted if eigenvectors not wanted */
alj] Cil=alil (j]/h; Store u/H in ith column of a.

Form an element of A-u ing.

g t= alj] (k] *a[i] [k];
for (k=j+1;k<=1;k++)
g t= alk] (j]*alil [k];
e[jl]=g/h; Form element of p in temporarily unused
element of e.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.3 Eigenvalues and Eigenvectors of a Tridiagonal Matrix 475

f += e[j]*alil [j];

}
hh=f/(h+h) ; Form K, equation (11.2.11).
for (j=1;j<=1;jt+) { Form q and store in e overwriting p.
f=ali] [j];
e[j]=g=e[j]-hh+f ;
for (k=1;k<=j;k++) Reduce a, equation (11.2.13).
alj] Ik] (f*e [k]+g*a [il [k]);
}
}
} else
e(i]=ali] (11;
d[i]=h;

+

/* Next statement can be omitted if eigenvectors not wanted */

da[1]=0.0;

e[1]=0.0;

/* Contents of this loop can be omitted if eigenvectors not
wanted except for statement d[i]=ali] [i]; */

for (i=1;i<=n;it+) { Begin accumulation of transformation ma-
1=i-1; trices.
if (d[i]) { This block skipped when i=1.
for (j=1;j<=1;j++) {
g=0.0;
for (k=1;k<=1;k++) Use u and u/H stored in a to form P-Q.

g t= ali] [k] *alk] [j];
for (k=1;k<=1;k++)
alk] (j] -= g*alk] [i];
}

d(i]=alil Ci]; This statement remains.
ala] [i 0; Reset row and column of a to identity
for (j=1;j<=1; j++) alj] [il=alil] [j]=0.0; matrix for next iteration.

CITED REFERENCES AND FURTHER READING:

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), §5.1. [1]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag).

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [2]

11.3 Eigenvalues and Eigenvectors of a
Tridiagonal Matrix

Evaluation of the Characteristic Polynomial

Once our original, real, symmetric matrix has been reduced to tridiagonal form,
one possible way to determine its eigenvalues is to find the roots of the characteristic
polynomial p,,(\) directly. The characteristic polynomial of a tridiagonal matrix can
be evaluated for any trial value of \ by an efficient recursion relation (see [1], for
example). The polynomials of lower degree produced during the recurrence form a

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

476 Chapter 11. Eigensystems

Sturmian sequence that can be used to localize the eigenvalues to intervals on the
real axis. A root-finding method such as bisection or Newton’s method can then
be employed to refine the intervals. The corresponding eigenvectors can then be
found by inverse iteration (see §11.7).

Procedures based on these ideas can be found in [2,3]. If, however, more
than a small fraction of all the eigenvalues and eigenvectors are required, then the
factorization method next considered is much more efficient.

The QR and QL Algorithms

The basic idea behind the QR algorithm is that any real matrix can be
decomposed in the form

A=Q:R (11.3.1)

where Q is orthogonal and R is upper triangular. For a general matrix, the
decomposition is constructed by applying Householder transformations to annihilate
successive columns of A below the diagonal (see §2.10).

Now consider the matrix formed by writing the factors in (11.3.1) in the
opposite order:

A’=R-Q (11.3.2)

Since Q is orthogonal, equation (11.3.1) gives R = Q TA. Thus equation (11.3.2)
becomes

A’=Q7-A-Q (11.3.3)

We see that A’ is an orthogonal transformation of A.
You can verify that a QR transformation preserves the following properties of
a matrix: symmetry, tridiagonal form, and Hessenberg form (to be defined in $11.5).
There is nothing special about choosing one of the factors of A to be upper
triangular; one could equally well make it lower triangular. This is called the QL
algorithm, since

A=Q-L (11.3.4)

where L is lower triangular. (The standard, but confusing, nomenclature R and L
stands for whether the right or left of the matrix is nonzero.)

Recall that in the Householder reduction to tridiagonal form in §11.2, we started
in the nth (last) column of the original matrix. To minimize roundoff, we then
exhorted you to put the biggest elements of the matrix in the lower right-hand
corner, if you can. If we now wish to diagonalize the resulting tridiagonal matrix,
the QL algorithm will have smaller roundoff than the QR algorithm, so we shall
use QL henceforth.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.3 Eigenvalues and Eigenvectors of a Tridiagonal Matrix 477

The QL algorithm consists of a sequence of orthogonal transformations:
As =Q,:Ls

Tr (11.3.5)

As41 = Ls + Qs (=Q, -As -Qs)
The following (nonobvious!) theorem is the basis of the algorithm for a general
matrix A: (i) If A has eigenvalues of different absolute value |\ ;|, then A, — [lower
triangular form] as s — oo. The eigenvalues appear on the diagonal in increasing
order of absolute magnitude. (ii) If A has an eigenvalue | ;| of multiplicity p,
A, — [lower triangular form] as s — oo, except for a diagonal block matrix
of order p, whose eigenvalues — ;. The proof of this theorem is fairly lengthy;
see, for example, [4].

The workload in the QL algorithm is O(n°) per iteration for a general matrix,
which is prohibitive. However, the workload is only O(n) per iteration for a
tridiagonal matrix and O(n?) for a Hessenberg matrix, which makes it highly
efficient on these forms.

In this section we are concerned only with the case where A is a real, symmetric,
tridiagonal matrix. All the eigenvalues ; are thus real. According to the theorem,
if any A; has a multiplicity p, then there must be at least p — 1 zeros on the
sub- and superdiagonal. Thus the matrix can be split into submatrices that can be
diagonalized separately, and the complication of diagonal blocks that can arise in
the general case is irrelevant.

In the proof of the theorem quoted above, one finds that in general a super-
diagonal element converges to zero like

al) ~ (*) (11.3.6)

Although A; < Aj, convergence can be slow if A; is close to \;. Convergence can
be accelerated by the technique of shifting: If k is any constant, then A — kl has
eigenvalues A; — k. If we decompose

As —ks1 = Q,-Ls (11.3.7)
so that
As41 =Ls-:Q, +ksl
(11.3.8)
= QF As Q,
then the convergence is determined by the ratio
Ni — ks
—— 11.3:
dj —ks ( 9)

The idea is to choose the shift k, at each stage to maximize the rate of
convergence. A good choice for the shift initially would be k, close to A,, the
smallest eigenvalue. Then the first row of off-diagonal elements would tend rapidly
to zero. However, ; is not usually known a priori. A very effective strategy in
practice (although there is no proof that it is optimal) is to compute the eigenvalues

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

478 Chapter 11. Eigensystems

of the leading 2 x 2 diagonal submatrix of A. Then set k, equal to the eigenvalue
closer to ay .
More generally, suppose you have already found r — 1 eigenvalues of A. Then

you can deflate the matrix by crossing out the first r — 1 rows and columns, leaving
ro bee eae 0
0
d,
A= ro & (11.3.10)
er dpa
0
dn—1 €n-1
LO aa 0 en-1 dn

Choose k, equal to the eigenvalue of the leading 2 x 2 submatrix that is closer to d,..
One can show that the convergence of the algorithm with this strategy is generally
cubic (and at worst quadratic for degenerate eigenvalues). This rapid convergence
is what makes the algorithm so attractive.

Note that with shifting, the eigenvalues no longer necessarily appear on the
diagonal in order of increasing absolute magnitude. The routine eigsrt ($11.1)
can be used if required.

As we mentioned earlier, the QL decomposition of a general matrix is effected
by a sequence of Householder transformations. For a tridiagonal matrix ,however, it is
more efficient to use plane rotations P ,,. One uses the sequence P 12, Po3,... , Pn—1.n
to annihilate the elements a12,a23,...,@n—1,n. By symmetry, the subdiagonal
elements a21,432,.--,@n,n—1 Will be annihilated too. Thus each Q, is a product
of plane rotations:

Q? =p). p&p

s n-1

(113.11)

where P; annihilates a; ;1. Note that it is Qr in equation (11.3.11), not Q, because
we defined L = Q7- A.

QL Algorithm with Implicit Shifts

The algorithm as described so far can be very successful. However, when
the elements of A differ widely in order of magnitude, subtracting a large k,
from the diagonal elements can lead to loss of accuracy for the small eigenvalues.
This difficulty is avoided by the QL algorithm with implicit shifts. The implicit
QL algorithm is mathematically equivalent to the original QL algorithm, but the
computation does not require k,1 to be actually subtracted from A.

The algorithm is based on the following lemma: If A is a symmetric nonsingular matrix
and B = Q° -A-Q, where Q is orthogonal and B is tridiagonal with positive off-diagonal
elements, then Q and B are fully determined when the last row of Q” is specified. Proof:
Let q7 denote the ith row vector of the matrix Q". Then q, is the ith column vector of the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.3 Eigenvalues and Eigenvectors of a Tridiagonal Matrix 479

matrix Q. The relation B- Q7 = Q7 - A can be written

Bova ay at
a2 f2 72 qo G2
: = : “A (11.3.12)
Qn—1 Bn—1 Yn-1 qr, qr,
Qn Bn qr qr
The nth row of this matrix equation is
anq,_1+Bng, =qn-A (113.13)
Since Q is orthogonal,
qe dm = Onm (11.3.14)
Thus if we postmultiply equation (11.3.13) by q,,, we find
Bn =q,-A-dn (11.3.15)
which is known since q,, is known. Then equation (11.3.13) gives
OnGn—1 = Zn (1.3.16)
where
Zn-1 =4,°A- Bnd (113.17)
is known. Therefore
an = Zn-12n-1, (113.18)
or
on = [ena (113.19)
and
Gna = 2n—1/On (113.20)

(where a is nonzero by hypothesi
Gn In—19+++>In—z and the a’s
quantities at level n — (j + 1).

To apply the lemma in practice, suppose one can somehow find a tridiagonal matrix
As+1 such that

. Similarly, one can show by induction that if we know
B's, and y’s up to level n — 7, one can determine the

Asi =@. - A, -, (113.21)

where oe is orthogonal and has the same last row as Qr in the original QL algorithm.
Then Q, = Q, and As41 = Asqi-

Now, in the original algorithm, from equation (11.3.11) we see that the last row of Q
is the same as the last row of po. But recall that po), is a plane rotation designed to
annihilate the (rn — 1,n) element of A, — ks1. A simple calculation using the expression
(11.1.1) shows that it has parameters

dn —ks —en-1

; 8 113.22
e2 + (dn — ks)? Ve? + (dn — ks)? ( )
The matrix po), + As: pot is tridiagonal with 2 extra elements:
x xX x
x x xX X (11.3.23)
x xX x
x xX xX

We must now reduce this to tridiagonal form with an orthogonal matrix whose last row is
[0,0,...,0,1] so that the last row of Q) will stay equal to p‘). This can be done by

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

480 Chapter11. Eigensystems

a sequence of Householder or Givens transformations. For the special form of the matrix
(11.3.23), Givens is better. We rotate in the plane (n — 2,n — 1) to annihilate the (n — 2,n)
element. [By symmetry, the (n,n — 2) element will also be zeroed.] This leaves us with
tridiagonal form except for extra elements (n — 3, — 1) and (n — 1,n — 3). We annihilate
these with a rotation in the (n — 3,n — 2) plane, and so on. Thus a sequence of n — 2
Givens rotations is required. The result is that

Qh =U. =P). ph... PO, - PS), (1.3.24)

where the P’s are the Givens rotations and P,,_1 is the same plane rotation as in the original
algorithm. Then equation (11.3.21) gives the next iterate of A. Note that the shift k; enters
implicitly through the parameters (11.3.22).

The following routine tq1i (“Tridiagonal QL Implicit’), based algorithmically
on the implementations in [2,3], works extremely well in practice. The number of
iterations for the first few eigenvalues might be 4 or 5, say, but meanwhile the
off-diagonal elements in the lower right-hand corner have been reduced too. The
later eigenvalues are liberated with very little work. The average number of iterations
per eigenvalue is typically 1.3 — 1.6. The operation count per iteration is O(n),
with a fairly large effective coefficient, say, ~ 20n. The total operation count for
the diagonalization is then ~ 20n x (1.3 — 1.6)n ~ 30n?. If the eigenvectors
are required, the statements indicated by comments are included and there is an
additional, much larger, workload of about 3n3 operations.

#include <math.h>
#include "nrutil.h"

void tqli(float d[], float e[], int n, float **z)
QL algorithm with implicit shifts, to determine the eigenvalues and eigenvectors of a real, sym-
metric, tridiagonal matrix, or of a real, symmetric matrix previously reduced by tred2 §11.2. On
input, d[1..n] contains the diagonal elements of the tridiagonal matrix. On output, it returns
the eigenvalues. The vector e[1..n] inputs the subdiagonal elements of the tridiagonal matrix,
with e[1] arbitrary. On output e is destroyed. When finding only the eigenvalues, several lines
may be omitted, as noted in the comments. If the eigenvectors of a tridiagonal matrix are de-
sired, the matrix z[1..n] [1..n] is input as the identity matrix. If the eigenvectors of a matrix
that has been reduced by tred2 are required, then z is input as the matrix output by tred2.
In either case, the kth column of z returns the normalized eigenvector corresponding to d[k].
{

float pythag(float a, float b);

int m,l,iter,i,k;

float s,r,p,g,f,dd,c,b;

for (i=2;i<=n;it++) e[i-1]=e[i]; Convenient to renumber the el-

e[n]=0.0; ements of e.
for (1=1;1<=n;1++) {
iter=0;
do {
for (m=1;m<=n-1;m++) { Look for a single small subdi-
dd=fabs (d[m] )+fabs(d[m+1]) ; agonal element to split
if ((float)(fabs(e[m])+dd) == dd) break; the matrix.
3
if @!=1) {
if (iter++ == 30) nrerror("Too many iterations in tqli");
g=(d[1+1]-d[1])/(2.0*e[1]); Form shift.
r=pythag(g,1.0);

({m]-d[1]+e[1]/(g+SIGN(r,g)); This is dm — ks.
.0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.4 Hermitian Matrices 481

for (i=m-1;i>=1;i--) { A plane rotation as in the origi-
f=s*e[i]; nal QL, followed by Givens
b=c*e [i]; rotations to restore tridiag-
e(it1]=(r=pythag(f,g)); onal form.
0.0) { Recover from underflow.

d{i+1] -= p;

e[m]=0.0;

break;

& “Pp;
r=(d[i]-g)*s+2.0*c*b;
d[it+1]=g+ (p=s*r) ;

g=c*r-b;

/* Next loop can be omitted if eigenvectors not wanted*/

for (k=1;k<=n;k++) { Form eigenvectors.
f=z([k] (it+1];

z[k] [i+1] =s*z[k] [i]+c*f ;
z[k] [i] =c*z[k] [i] -s*f;

0.0 && i >= 1) continue;
P;

3
} while (m != 1);

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), pp. 331-335. [1]

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [2]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [3]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
86.6.6. [4]

11.4 Hermitian Matrices

The complex analog of a real, symmetric matrix is a Hermitian matrix,
satisfying equation (11.0.4). Jacobi transformations can be used to find eigenvalues
and eigenvectors, as also can Householder reduction to tridiagonal form followed by
QL iteration. Complex versions of the previous routines jacobi, tred2, and tqli
are quite analogous to their real counterparts. For working routines, consult [1,2].

An alternative, using the routines in this book, is to convert the Hermitian
problem to a real, symmetric one: If C = A + 7B is a Hermitian matrix, then the
n X n complex eigenvalue problem

(A + iB) - (u + iv) = X(u + iv) (11.4.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

482 Chapter 11. Eigensystems

is equivalent to the 2n x 2n real problem
A —B u u
Ss )-[ Jaf] ana

Note that the 2n x 2n matrix in (11.4.2) is symmetric: AT = A and B? = -B
if C is Hermitian.
Corresponding to a given eigenvalue ., the vector

| (11.4.3)

u

is also an eigenvector, as you can verify by writing out the two matrix equa-
tions implied by (11.4.2). Thus if \1, A2,...,An are the eigenvalues of C, then
the 2n eigenvalues of the augmented problem (11.4.2) are 1,1, A2, A2,.--,
Xn: An; each, in other words, is repeated twice. The eigenvectors are pairs of the
form u + iv and i(u + iv); that is, they are the same up to an inessential phase. Thus
we solve the augmented problem (11.4.2), and choose one eigenvalue and eigenvector
from each pair. These give the eigenvalues and eigenvectors of the original matrix C.

Working with the augmented matrix requires a factor of 2 more storage than the
original complex matrix. In principle, a complex algorithm is also a factor of 2 more
efficient in computer time than is the solution of the augmented problem.

CITED REFERENCES AND FURTHER READING:

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [1]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [2]

11.5 Reduction of a General Matrix to
Hessenberg Form

The algorithms for symmetric matrices, given in the preceding sections, are
highly satisfactory in practice. By contrast, it is impossible to design equally
satisfactory algorithms for the nonsymmetric case. There are two reasons for this.
First, the eigenvalues of a nonsymmetric matrix can be very sensitive to small changes
in the matrix elements. Second, the matrix itself can be defective, so that there is
no complete set of eigenvectors. We emphasize that these difficulties are intrinsic
properties of certain nonsymmetric matrices, and no numerical procedure can “cure”
them. The best we can hope for are procedures that don’t exacerbate such problems.

The presence of rounding error can only make the situation worse. With finite-
precision arithmetic, one cannot even design a foolproof algorithm to determine
whether a given matrix is defective or not. Thus current algorithms generally try to
find a complete set of eigenvectors, and rely on the user to inspect the results. If any
eigenvectors are almost parallel, the matrix is probably defective.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.5 Reduction of a General Matrix to Hessenberg Form 483

Apart from referring you to the literature, and to the collected routines in [1,2], we
are going to sidestep the problem of eigenvectors, giving algorithms for eigenvalues
only. If you require just a few eigenvectors, you can read $11.7 and consider finding
them by inverse iteration. We consider the problem of finding all eigenvectors of a
nonsymmetric matrix as lying beyond the scope of this book.

Balancing

The sensitivity of eigenvalues to rounding errors during the execution of
some algorithms can be reduced by the procedure of balancing. The errors in
the eigensystem found by a numerical procedure are generally proportional to the
Euclidean norm of the matrix, that is, to the square root of the sum of the squares
of the elements. The idea of balancing is to use similarity transformations to
make corresponding rows and columns of the matrix have comparable norms, thus
reducing the overall norm of the matrix while leaving the eigenvalues unchanged.
A symmetric matrix is already balanced.

Balancing is a procedure with of order N? operations. Thus, the time taken
by the procedure balanc, given below, should never be more than a few percent
of the total time required to find the eigenvalues. It is therefore recommended that
you always balance nonsymmetric matrices. It never hurts, and it can substantially
improve the accuracy of the eigenvalues computed for a badly balanced matrix.

The actual algorithm used is due to Osborne, as discussed in [1]. It consists of a
sequence of similarity transformations by diagonal matrices D. To avoid introducing
rounding errors during the balancing process, the elements of D are restricted to be
exact powers of the radix base employed for floating-point arithmetic (i.e., 2 for most
machines, but 16 for IBM mainframe architectures). The output is a matrix that
is balanced in the norm given by summing the absolute magnitudes of the matrix
elements. This is more efficient than using the Euclidean norm, and equally effective:
A large reduction in one norm implies a large reduction in the other.

Note that if the off-diagonal elements of any row or column of a matrix are
all zero, then the diagonal element is an eigenvalue. If the eigenvalue happens to
be ill-conditioned (sensitive to small changes in the matrix elements), it will have
relatively large errors when determined by the routine hqr (§11.6). Had we merely
inspected the matrix beforehand, we could have determined the isolated eigenvalue
exactly and then deleted the corresponding row and column from the matrix. You
should consider whether such a pre-inspection might be useful in your application.
(For symmetric matrices, the routines we gave will determine isolated eigenvalues
accurately in all cases.)

The routine balanc does not keep track of the accumulated similarity trans-
formation of the original matrix, since we will only be concerned with finding
eigenvalues of nonsymmetric matrices, not eigenvectors. Consult [1-3] if you want
to keep track of the transformation.

#include <math.h>
#define RADIX 2.0

void balanc(float **a, int n)

Given a matrix a[1..n][1..n], this routine replaces it by a balanced matrix with identical
eigenvalues. A symmetric matrix is already balanced and is unaffected by this procedure. The
parameter RADIX should be the machine's floating-point radix.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

484 Chapter 11. Eigensystems

{
int last,j,i;
float s,r,g,f,c,sqrdx;
sqrdx=RADIX*RADIX;
last=0;
while (last == 0) {
last=1;
for (i Calculate row and column norms.
r=c=0.
for (j= 3jt++)
i if
fabs(alj] [i]);
fabs(alil [j]);
3
if (c er) { If both are nonzero,
g=r/RADIX;
f=1.0;
s=ctr;
while (c<g) { find the integer power of the machine radix that
f *= RADIX; comes closest to balancing the matrix.
c *= sqrdx;
}
g=r*RADIX;
while (c>g) {
f£ /= RADIX;
ce /= sqrdx;
}
if ((ctr)/f < 0.95*s) {
last=0;
g=1.0/f;
j++) ali] [j] Apply similarity transforma-
3j++) alj] (il tion.
}
}
}
+
}

Reduction to Hessenberg Form

The strategy for finding the eigensystem of a general matrix parallels that of the
symmetric case. First we reduce the matrix to a simpler form, and then we perform
an iterative procedure on the simplified matrix. The simpler structure we use here is
called Hessenberg form. An upper Hessenberg matrix has zeros everywhere below
the diagonal except for the first subdiagonal row. For example, in the 6 x 6 case,
the nonzero elements are:

x ™® X& xX XK X
x ™® X& xX XK X
xX Kk &K XK xX

x kK xX xX

x -® xX

x x

By now you should be able to tell at a glance that such a structure can
be achieved by a sequence of Householder transformations, each one zeroing the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.5 Reduction of a General Matrix to Hessenberg Form 485

required elements in a column of the matrix. Householder reduction to Hessenberg
form is in fact an accepted technique. An alternative, however, is a procedure
analogous to Gaussian elimination with pivoting. We will use this elimination
procedure since it is about a factor of 2 more efficient than the Householder method,
and also since we want to teach you the method. It is possible to construct matrices
for which the Householder reduction, being orthogonal, is stable and elimination is
not, but such matrices are extremely rare in practice.

Straight Gaussian elimination is not a similarity transformation of the matrix.
Accordingly, the actual elimination procedure used is slightly different. Before the
rth stage, the original matrix A = A, has become A,., which is upper Hessenberg
in its first r — 1 rows and columns. The rth stage then consists of the following
sequence of operations:

e Find the element of maximum magnitude in the rth column below the
diagonal. If it is zero, skip the next two “bullets” and the stage is done.
Otherwise, suppose the maximum element was in row r’.

e Interchange rows r’ and r + 1. This is the pivoting procedure. To make
the permutation a similarity transformation, also interchange columns r’
and r+ 1.

e Forti =r+2,r+3,...,N, compute the multiplier

dir

Nir+1 =
Or+iyr
Subtract n;-+1 times row r + 1 from row i. To make the elimination a
similarity transformation, also add n;,,41 times column i to column r + 1.
A total of N — 2 such stages are required.

When the magnitudes of the matrix elements vary over many orders, you should
try to rearrange the matrix so that the largest elements are in the top left-hand corner.
This reduces the roundoff error, since the reduction proceeds from left to right.

Since we are concerned only with eigenvalues, the routine elmhes does not
keep track of the accumulated similarity transformation. The operation count is
about 5N°/6 for large N.

#include <math.h>
#define SWAP(g,h) {y=(g);(g)=(h) ; (h)=y3}

void elmhes(float **a, int n)

Reduction to Hessenberg form by the elimination method. The real, nonsymmetric matrix
a(1..n][1..n] is replaced by an upper Hessenberg matrix with identical eigenvalues. Rec-
ommended, but not required, is that this routine be preceded by balanc. On output, the
Hessenberg matrix is in elements ali] [j] with i < j+1. Elements with i > j+1 are to be
thought of as zero, but are returned with random values.

{

int m,j,i;

float y,x;

for (m=2;m<n;mt++) { m is called r + 1 in the text.
x=0.0;
ism;
for (j=m;j<=n;jt+) { Find the pivot.

if (fabs(a[j][m-1]) > fabs(x)) {

x=a(j] [m-1];
ji

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

486 Chapter 11. Eigensystems

3
}
if G !=m) { Interchange rows and columns.
for (j=m-1;j<=n; j++) SWAP(a[i] [j],alm] (j])
for (j=1;j<=n; j++) SWAP(a[j] [i] ,alj] [m])
}
if @ ft Carry out the elimination.
for (i=m+1;i<=n;it+) {
if ((y=ali][m-1]) != 0.0) {
y /= x;
ali] [m-1]=y;
ij
a[j] {m]
}
3
}

CITED REFERENCES AND FURTHER READING:

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [1]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [2]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§6.5.4. [3]

11.6 The QR Algorithm for Real Hessenberg
Matrices

Recall the following relations for the QR algorithm with shifts:
Q, (As —ks1) =Rs (11.6.1)
where Q is orthogonal and R is upper triangular, and
Asi =Ry- Ql + kl

11.6.2
=Q,-As QF ne)
The QR transformation preserves the upper Hessenberg form of the original matrix
A = Aj, and the workload on such a matrix is O(n?) per iteration as opposed
to O(n) on a general matrix. As s — 00, As converges to a form where the
eigenvalues are either isolated on the diagonal or are eigenvalues of a 2 x 2 submatrix
on the diagonal.

As we pointed out in §11.3, shifting is essential for rapid convergence. A key
difference here is that a nonsymmetric real matrix can have complex eigenvalues. This

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.6 The QR Algorithm for Real Hessenberg Matrices 487

means that good choices for the shifts /,, may be complex, apparently necessitating
complex arithmetic.

Complex arithmetic can be avoided, however, by a clever trick. The trick
depends on a result analogous to the lemma we used for implicit shifts in §11.3. The
lemma we need here states that if B is a nonsingular matrix such that

B-Q=Q-H (11.6.3)

where Q is orthogonal and H is upper Hessenberg, then Q and H are fully determined
by the first column of Q. (The determination is unique if H has positive subdiagonal
elements.) The lemma can be proved by induction analogously to the proof given
for tridiagonal matrices in §11.3.

The lemma is used in practice by taking two steps of the QR algorithm,
either with two real shifts k, and k,11, or with complex conjugate values k, and
ks41 = ky*. This gives a real matrix A,+2, where

Asi2 = Q541°Q,°As -QF Qh: (11.6.4)

The Q’s are determined by

Ag — kl =QT-Rs (11.6.5)
Aci = Q,+ As QP (11.6.6)
Asai — spl = Qh, Rev (11.6.7)

Using (11.6.6), equation (11.6.7) can be rewritten
Ag kewl = QS Qvy1 Rett Qs (11.68)
Hence, if we define
M = (Ag — ksi l): (As — ks!) (11.6.9)

equations (11.6.5) and (11.6.8) give

R=Q:-M (11.6.10)
where

Q=Qs41 Qs (11.6.11)

R=Ry4i-Rs (11.6.12)

Equation (11.6.4) can be rewritten
Ag: QT =Q?- Asya (1.6.13)
Thus suppose we can somehow find an upper Hessenberg matrix H such that

A,-Q’ =Q'-H (11.6.14)

where Q is orthogonal. If fol has the same first column as Q7 (i.e., Q has the same
first row as Q), then Q = Q and Aging = H.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

488 Chapter 11. Eigensystems

The first row of Q is found as follows. Equation (11.6.10) shows that Q is
the orthogonal matrix that triangularizes the real matrix M. Any real matrix can
be triangularized by premultiplying it by a sequence of Householder matrices P ;
(acting on the first column), P» (acting on the second column), ..., P»—1. Thus
Q = P,-1:::P2- Pi, and the first row of Q is the first row of P; since P; is an
(i — 1) x (i — 1) identity matrix in the top left-hand corner. We now must find Q
satisfying (11.6.14) whose first row is that of P,.

The Householder matrix P; is determined by the first column of M. Since As
is upper Hessenberg, equation (11.6.9) shows that the first column of M has the
form [p1,q1,71,0, ...,0]7, where

pi = ah, — aulks + ksi) + ksksy1 + aizaar
Gi = 421 (aq1 + a22 — ks — kis 41) (11.6.15)

Ty = 421432
Hence
Py =1-2w,-w) (11.6.16)

where wy has only its first 3 elements nonzero (cf. equation 11.2.5). The matrix
P, - Ag: PT is therefore upper Hessenberg with 3 extra elements:

x
x
XxX
XxX

~ x xX xX
x xX xX X

P,-A,-PP = (11.6.17)

x xX X X &X

x xX XK XK XK
x X X KX XK XK XK
xX KX XX XK XK X

This matrix can be restored to upper Hessenberg form without affecting the first row
by a sequence of Householder similarity transformations. The first such Householder
matrix, P2, acts on elements 2, 3, and 4 in the first column, annihilating elements
3 and 4. This produces a matrix of the same form as (11.6.17), with the 3 extra
elements appearing one column over:

x
x

(11.6.18)

~*~ «eX XK XK

~ xX xX XK Xx

x XX X xX

x X xX KX XK X
x XX KX XK XK xX
x XX KX XK XK xX

Proceeding in this way up to P,,1, we see that at each stage the Householder
matrix P, has a vector w, that is nonzero only in elements r, r + 1, and r + 2.
These elements are determined by the elements r, r + 1, and r + 2 in the (r — 1)st

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.6 The QR Algorithm for Real Hessenberg Matrices 489

column of the current matrix. Note that the preliminary matrix P; has the same
structure as Po,...,Py-1.
The result is that

Ppi-::Po-Py-A,:PP-P2.-.P? | =H (11.6.19)
where H is upper Hessenberg. Thus

Q=Q=P,-1---Po- Pi (11.6.20)
and
Asia =H (11.6.21)

The shifts of origin at each stage are taken to be the eigenvalues of the 2 x 2
matrix in the bottom right-hand corner of the current A. This gives

ks + kis42 = Qn-1n-1 + Gnn
(11.6.2)

Ksks41 = Qn-1,n-14nn — An—1,nGn,n=1
Substituting (11.6.22) in (11.6.15), we get

pi = a21 {{(dnn — @11)(@n—1yn—1 — G11) — An—1,nAnyn—1|/a21 + a12}
1 = a21[a22 — a11 — (nn — G11) — (An—1yn—1 — @11)]
ry = G21432 (11.6.23)

We have judiciously grouped terms to reduce possible roundoff when there are
small off-diagonal elements. Since only the ratios of elements are relevant for a
Householder transformation, we can omit the factor a2; from (11.6.23).

In summary, to carry out a double QR step we construct the Householder
matrices P,., r=1,...,n—1. For P; we use pi, gi, and r; given by (11.6.23). For
the remaining matrices, p,., q,, andr, are determined by the (r,r —1),(r+1,r—1),
and (r + 2,r — 1) elements of the current matrix. The number of arithmetic
operations can be reduced by writing the nonzero elements of the 2w - w7 part of
the Householder matrix in the form

-[1 g/(pts) r/(p+s)] (11.6.24)

where

Sv apt+et+r (11.6.25)

(We have simply divided each element by a piece of the normalizing factor; cf.
the equations in $11.2.)

If we proceed in this way, convergence is usually very fast. There are two
possible ways of terminating the iteration for an eigenvalue. First, if a p,—1 becomes
“negligible, then ay» is an eigenvalue. We can then delete the nth row and column

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

490 Chapter11. Eigensystems

of the matrix and look for the next eigenvalue. Alternatively, a ,—1,,—2 may become
negligible. In this case the eigenvalues of the 2 x 2 matrix in the lower right-hand
corner may be taken to be eigenvalues. We delete the nth and (n — 1)st rows and
columns of the matrix and continue.

The test for convergence to an eigenvalue is combined with a test for negligible
subdiagonal elements that allows splitting of the matrix into submatrices. We find the
largest i such that a;,;_1 is negligible. If i = n, we have found a single eigenvalue. If
i = n—1,we have found two eigenvalues. Otherwise we continue the iteration on the
submatrix in rows 7 to n (7 being set to unity if there is no small subdiagonal element).

After determining 7, the submatrix in rows 7 to n is examined to see if the product
of any two consecutive subdiagonal elements is small enough that we can work with
an even smaller submatrix, starting say in row m. We start with m = n — 2 and
decrement it down to i + 1, computing p, q, and r according to equations (11.6.23)
with 1 replaced by m and 2 by m+ 1. If these were indeed the elements of the special
“first” Householder matrix in a double QR step, then applying the Householder
matrix would lead to nonzero elements in positions (m+ 1,m—1),(m+2,m-—1),
and (m + 2,m). We require that the first two of these elements be small compared
with the local diagonal elements dm—1,m—1; mm and Gm+41,m+1. A Satisfactory
approximate criterion is

Jamym—1\(la| + |r|) < |p|(lam-+-1jm41| + |amm| + |am—1;m—1]) (11.6.26)

Very rarely, the procedure described so far will fail to converge. On such
matrices, experience shows that if one double step is performed with any shifts
that are of order the norm of the matrix, convergence is subsequently very rapid.
Accordingly, if ten iterations occur without determining an eigenvalue, the usual
shifts are replaced for the next iteration by shifts defined by

keg + kisg1 = 1.5 & (l@nn—1| + |@n—1,n—2])
5 (116.27)
ksks41 = (|@n,n—1| + |@n—1,n—2])

The factor 1.5 was arbitrarily chosen to lessen the likelihood of an “unfortunate”
choice of shifts. This strategy is repeated after 20 unsuccessful iterations. After 30
unsuccessful iterations, the routine reports failure.

The operation count for the QR algorithm described here is ~ 5k per iteration,
where k; is the current size of the matrix. The typical average number of iterations per
eigenvalue is ~ 1.8, so the total operation count for all the eigenvalues is ~ 3n°. This
estimate neglects any possible efficiency due to splitting or sparseness of the matrix.

The following routine hqr is based algorithmically on the above description,
in turn following the implementations in [1,2].

#include <math.h>
#include "nrutil.h"

void hqr(float **a, int n, float wr[], float wif])

Finds all eigenvalues of an upper Hessenberg matrix a[1..n][1..n]. On input a can be
exactly as output from elmhes §11.5; on output it is destroyed. The real and imaginary parts
of the eigenvalues are returned in wr[1..n] and wil[1..n], respectively.

{

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.6 The QR Algorithm for Real Hessenberg Matrices 491

int nn,m,1,k,j,its,i,mmin;
float z,y,x,w,v,u,t,s,r,q,p,anorm;

anorm=0.0; Compute matrix norm for possible use in lo-
for (i=1;i<=n;i++) cating single small subdiagonal element.
for (j=IMAX(i-1,1);j<=n; j++)
anorm += fabs(alil] [j]);

nn=n;
t=0.0; Gets changed only by an exceptional shift.
while (nn >= 1) { Begin search for next eigenvalue.
its=0;
do {
for (l=nn;1>=2;1--) { Begin iteration: look for single small subdi-
s=fabs(a[1-1] [1-1])+fabs(a[1] [1]); agonal element.
if (s == 0.0) s=anorm;

if ((float)(fabs(a[1][1-1]) + s) == s) {
a[1] [1-1]=0.0;

break;
}
}
x=a[nn] [nn] ;
if (1 == nn) { One root found.
wr [nn] =x+t;
wi[nn--]=0.0;
} else {

y=a[nn-1] [nn-1] ;
w=a [nn] [nn-1] *a[nn-1] [nn] ;
if (1 == (mn-1)) { Two roots found...
p=0.5*(y-x) ;
q=p*ptw;
z=sqrt (fabs (q));
x += t;
if (q >= 0.0) { ..-a real pair.
z=p+SIGN(z,p);
wr (nn-1]=wr [nn] =x+z;
if (z) wr[nn]=x-w/z;
wi {nn-1]=wi [nn]=0.0;

} else { ...a complex pair.
wr [nn-1]=wr [nn] =x+p;
wifnn-1]= -(wi{nn]=z) ;
+
mn -= 2;
} else { No roots found. Continue iteration.
if (its == 30) nrerror("Too many iterations in hqr");
if (its 10 || its == 20) { Form exceptional shift.
t += x;
for (i=1;i<=nn;it+) afi] [i] -= x;
s=fabs(a[nn] [nn-1])+fabs(a[nn-1] [nn-2]) ;
y' .75*s;
w = -0.4375*s*s;
+
++its;
for (m=(nn-2);m>=1;m--) { Form shift and then look for
{m] [mn]; 2 consecutive small sub-
Y=X-Z; diagonal elements.
-2z;

r*s-w) /a[mt+1] [m]+a[m] [m+1]; Equation (11.6.23).
q=a[m+1] [m+1]-z-r-s;
(m+2] [m+1] ;

s=fabs(p)+fabs(q)+fabs(r) ; Scale to prevent overflow or
p /= 3s; underflow.

q /= s;

r/=s;

if (m == 1) break;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

492

Chapter 11. Eigensystems

u=fabs (a[m] [m-1])*(fabs(q)+fabs(r)) ;
v=fabs (p) *(fabs(a[m-1] [m-1])+fabs(z)+fabs(a[m+1] [m+1]));

if ((float)(utv) == v) break;

+

Equation (11.6.26).

for (i=m+2;i<=nn;i++) {

a(il]

[i-2]=0.0;

aif (i != (m+2)) ali] [i-3]=0.0;

+

for (k=m;k<=nn-1;k++) {
Double QR step on rows 1 to nn and columns m to nn.
if (k !=m) {

p=a([k] [k-1]; Begin setup of Householder
q=alk+1] [k-1]; vector.
r=0.0;
if (k != (nn-1)) r=a[k+2] [k-1];
if ((x=fabs(p)+fabs(q)+fabs(r)) != 0.0) {
p /= x; Scale to prevent overflow or
q /= x; underflow.
r /= x;
}
}
if ((s=SIGN(sqrt (p*ptq*qtr+r),p)) != 0.0) {
if (k ==m) {
if (1 != m)
alk] [k-1] = -alk] [k-1];
} else
alk] [k-1] = -s*x;
p t= s; Equations (11.6.24).
x=p/s;
y=q/s;
z=r/s;
q/= PB
xr /= p;

}
}
} while (1 < nn-1);

for (j=k;j<=nn;j++) { Row modification.
p=alk] [j]+q*alk+1] [j];
if (k != (mn-1)) {

p t= r*a[k+2] [j];

a[k+2][j] -= p*z;
+
alk+1] [j
alk] (j]

“= pty;
-= ptx;
}
mmin = nn<k+3 ? nn : k+3;
for (i=1;i<=mmin;i++) {
p=x*a[i] [k] +y*a[i] [k+1];
if (k != (nn-1)) {
p += zx*a[i] [k+2];
ali] [k+2] -= pxr;

Column modification.

+
ali] (k+1.
ali] (k]

= p¥qi
“= Ps

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.7 Eigenvalues or Eigenvectors by Inverse Iteration 493

CITED REFERENCES AND FURTHER READING:

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag). [1]

Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed. (Baltimore: Johns Hopkins
University Press), $7.5.

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [2]

11.7 Improving Eigenvalues and/or Finding
Eigenvectors by Inverse Iteration

The basic idea behind inverse iteration is quite simple. Let y be the solution
of the linear system

(A—Tl)-y=b (11.7.1)

where b is a random vector and 7 is close to some eigenvalue \ of A. Then the
solution y will be close to the eigenvector corresponding to 4. The procedure can
be iterated: Replace b by y and solve for a new y, which will be even closer to
the true eigenvector.

We can see why this works by expanding both y and b as linear combinations
of the eigenvectors x; of A:

y => 5x; b => Bix; (11.7.2)
j

J

Then (11.7.1) gives

Se aj(Aj — 7)x5 = SO 85x; (11.7.3)
j
so that
(11.7.4)
and

(11.7.5)

If 7 is close to A,,, say, then provided @,, is not accidentally too small, y will be
approximately x,,, up to a normalization. Moreover, each iteration of this procedure
gives another power of A; — 7 in the denominator of (11.7.5). Thus the convergence
is rapid for well-separated eigenvalues.

Suppose at the kth stage of iteration we are solving the equation

(A — Trl) -y = be (11.7.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

494 Chapter 11. Eigensystems

where b;, and 7, are our current guesses for some eigenvector and eigenvalue of
interest (let’s say, X, and A,,). Normalize b;, so that by, -b, = 1. The exact
eigenvector and eigenvalue satisfy

A+Xn = AnXn (11.7.7)
so
(A = TK1) + Xn = (An — Tk)Xn (11.7.8)
Since y of (11.7.6) is an improved approximation to x ,,, we normalize it and set
y
bea = (11.7.9)
lyl

We get an improved estimate of the eigenvalue by substituting our improved guess
y for x, in (11.7.8). By (11.7.6), the left-hand side is b;, so calling A, our new
value 7,41, we find

Tk+1 = Tk + (11.7.10)

bey

While the above formulas look simple enough, in practice the implementation
can be quite tricky. The first question to be resolved is when to use inverse iteration.
Most of the computational load occurs in solving the linear system (11.7.6). Thus
a possible strategy is first to reduce the matrix A to a special form that allows easy
solution of (11.7.6). Tridiagonal form for symmetric matrices or Hessenberg for
nonsymmetric are the obvious choices. Then apply inverse iteration to generate
all the eigenvectors. While this is an O(N?) method for symmetric matrices, it
is many times less efficient than the QL method given earlier. In fact, even the
best inverse iteration packages are less efficient than the QL method as soon as
more than about 25 percent of the eigenvectors are required. Accordingly, inverse
iteration is generally used when one already has good eigenvalues and wants only
a few selected eigenvectors.

You can write a simple inverse iteration routine yourself using LU decompo-
sition to solve (11.7.6). You can decide whether to use the general LU algorithm
we gave in Chapter 2 or whether to take advantage of tridiagonal or Hessenberg
form. Note that, since the linear system (11.7.6) is nearly singular, you must be
careful to use a version of LU decomposition like that in §2.3 which replaces a zero
pivot with a very small number.

We have chosen not to give a general inverse iteration routine in this book,
because it is quite cumbersome to take account of all the cases that can arise.
Routines are given, for example, in [1,2]. If you use these, or write your own routine,
you may appreciate the following pointers.

One starts by supplying an initial value 79 for the eigenvalue X,, of interest.
Choose a random normalized vector bo as the initial guess for the eigenvector x»,
and solve (11.7.6). The new vector y is bigger than bo by a “growth factor” ly],
which ideally should be large. Equivalently, the change in the eigenvalue, which by
(11.7.10) is essentially 1/ |y|, should be small. The following cases can arise:

e If the growth factor is too small initially, then we assume we have made

a “bad” choice of random vector. This can happen not just because of
a small 3, in (11.7.5), but also in the case of a defective matrix, when
(11.7.5) does not even apply (see, e.g., [1] or [3] for details). We go back
to the beginning and choose a new initial vector.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

11.7 Eigenvalues or Eigenvectors by Inverse Iteration 495

e The change |b; — bo| might be less than some tolerance €. We can use this
as a criterion for stopping, iterating until it is satisfied, with a maximum
of 5 — 10 iterations, say.

e After a few iterations, if |bx+1 — bx| is not decreasing rapidly enough,
we can try updating the eigenvalue according to (11.7.10). If Ty41 = Tr
to machine accuracy, we are not going to improve the eigenvector much
more and can quit. Otherwise start another cycle of iterations with the
new eigenvalue.

The reason we do not update the eigenvalue at every step is that when we solve
the linear system (11.7.6) by LU decomposition, we can save the decomposition
if 7), is fixed. We only need do the backsubstitution step each time we update b ;.
The number of iterations we decide to do with a fixed rT; is a trade-off between the
quadratic convergence but O(N) workload for updating 7, at each step and the
linear convergence but O(N”) load for keeping 7;, fixed. If you have determined the
eigenvalue by one of the routines given earlier in the chapter, it is probably correct
to machine accuracy anyway, and you can omit updating it.

There are two different pathologies that can arise during inverse iteration. The
first is multiple or closely spaced roots. This is more often a problem with symmetric
matrices. Inverse iteration will find only one eigenvector for a given initial guess To.
A good strategy is to perturb the last few significant digits in 79 and then repeat the
iteration. Usually this provides an independent eigenvector. Special steps generally
have to be taken to ensure orthogonality of the linearly independent eigenvectors,
whereas the Jacobi and QL algorithms automatically yield orthogonal eigenvectors
even in the case of multiple eigenvalues.

The second problem, peculiar to nonsymmetric matrices, is the defective case.
Unless one makes a “good” initial guess, the growth factor is small. Moreover,
iteration does not improve matters. In this case, the remedy is to choose random
initial vectors, solve (11.7.6) once, and quit as soon as any vector gives an acceptably
large growth factor. Typically only a few trials are necessary.

One further complication in the nonsymmetric case is that a real matrix can
have complex-conjugate pairs of eigenvalues. You will then have to use complex
arithmetic to solve (11.7.6) for the complex eigenvectors. For any moderate-sized
(or larger) nonsymmetric matrix, our recommendation is to avoid inverse iteration
in favor of a QR method that includes the eigenvector computation in complex
arithmetic. You will find routines for this in [1,2] and other places.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America).

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook for Automatic Com-
putation (New York: Springer-Verlag), p. 418. [1]

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed., vol. 6 of
Lecture Notes in Computer Science (New York: Springer-Verlag). [2]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
p. 356. [3]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 12. Fast Fourier Transform

12.0 Introduction

A very large class of important computational problems falls under the general
rubric of “Fourier transform methods” or “spectral methods.” For some of these
problems, the Fourier transform is simply an efficient computational tool for
accomplishing certain common manipulations of data. In other cases, we have
problems for which the Fourier transform (or the related “power spectrum’) is itself
of intrinsic interest. These two kinds of problems share a common methodology.

Largely for historical reasons the literature on Fourier and spectral methods has
been disjoint from the literature on “classical” numerical analysis. Nowadays there
is no justification for such a split. Fourier methods are commonplace in research and
we shall not treat them as specialized or arcane. At the same time, we realize that
many computer users have had relatively less experience with this field than with, say,
differential equations or numerical integration. Therefore our summary of analytical
results will be more complete. Numerical algorithms, per se, begin in §12.2. Various
applications of Fourier transform methods are discussed in Chapter 13.

A physical process can be described either in the time domain, by the values of
some quantity h as a function of time t, e.g., h(t), or else in the frequency domain,
where the process is specified by giving its amplitude H (generally a complex
number indicating phase also) as a function of frequency f, that is H(f), with
—oo < f < ov. For many purposes it is useful to think of h(t) and H(f) as being
two different representations of the same function. One goes back and forth between
these two representations by means of the Fourier transform equations,

H(f)= [. h(t)e2™*Ftdt
(12.0.1)
ny = [ane ?"ar

If t is measured in seconds, then f in equation (12.0.1) is in cycles per second,
or Hertz (the unit of frequency). However, the equations work with other units too. If
his a function of position x (in meters), H will be a function of inverse wavelength
(cycles per meter), and so on. If you are trained as a physicist or mathematician, you
are probably more used to using angular frequency w, which is given in radians per
sec. The relation between w and f, H(w) and H(f) is

w=2af Aw) =[A(f)| pouyen (12.0.2)

496

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

12.0 Introduction 497

and equation (12.0.1) looks like this

H(w) = [. h(t)e™* dt

1 —iwt
=— H(w)e "dw
27 Jo

We were raised on the w-convention, but we changed! There are fewer factors of
27 to remember if you use the f-convention, especially when we get to discretely
sampled data in §12.1.

From equation (12.0.1) it is evident at once that Fourier transformation is a
linear operation. The transform of the sum of two functions is equal to the sum of
the transforms. The transform of a constant times a function is that same constant
times the transform of the function.

In the time domain, function h(t) may happen to have one or more special
symmetries It might be purely real or purely imaginary or it might be even,
h(t) = h(-t), or odd, h(t) = —h(—t). In the frequency domain, these symmetries
lead to relationships between H(f) and H(—f). The following table gives the
correspondence between symmetries in the two domains:

(12.0.3)

If... then..

h(t) is real (f= [H( fl

h(t) is imaginary (-f) = HUN f)\*

h(t) is even HL fy= Huy [ie., H(f) is even]
h(t) is odd H(-f)=-H(f)  [ie., H(f) is odd]
h(t) is real and even A(f) is real and even

h(t) is real and odd HI(f) is imaginary and odd

h(t) is imaginary and even H(f) is imaginary and even

h(t) is imaginary and odd H(f) is real and odd

In subsequent sections we shall see how to use these symmetries to increase
computational efficiency.

Here are some other elementary properties of the Fourier transform. (We’ll use
the “<>” symbol to indicate transform pairs.) If

h(t) —> H(f)

is such a pair, then other transform pairs are

h(at) —= qh “time scaling” (12.0.4)
a a
1st
i —h(- 3) <> H(bf) “frequency scaling” (12.0.5)
A(t — to) <> H(f) P"/'° “time shifting” (12.0.6)

h(t) e~274fot > A(f — fo) “frequency shifting” (12.0.7)

Jeo JO Woo suMMy/:dy

“(Aluo eouewy YON) €%pZ-228-008-
“sayndwiod Jansas Aue 0} (aU0 S

4 J@YLNY “asn jeuosied u:

n
Ey
3
3
o
3
Ey
&
3
fei
3
Zz
Cc
=
m
2
fe)
>
=
D
m
is}
v0
m
n
2
2
4
x
m
>
D
4
fe)
a
n
Q
m
Zz
a
a
5
Q
fe}
=
vu
Cc
a
2
fo)
@D
jes)
Zz
2
a
8
k
g
3
i
2

oS
2
2
o
3S
nn
os
<
i?)
2
3
Ey
g
a
o
Cc
Es
Fe
g
g
z
3
3
S
9
2
3
Q
3
3
a
9
S
z
E
2
3S
g
2
o
3S
nn
z
<
Zz
é
3
g
3.
Ea
DD
2
Q.
3
3
8
n
g
a
8

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

3
Qa
2
2.
ce}
3
Q
2
eI
2
3
3
2g
5
2
3
3B
8
Ea
5
°

a
=
oO
fog
g
oe

498 Chapter 12. Fast Fourier Transform

With two functions h(t) and g(t), and their corresponding Fourier transforms
H(f) and G(f), we can form two combinations of special interest. The convolution
of the two functions, denoted g * h, is defined by

g*x*h= [. g(t)h(t — 7) dr (12.0.8)

co

Note that g « / is a function in the time domain and that g * h = h * g. It turns out
that the function g * h is one member of a simple transform pair

gxh <= G(f)A(f) “Convolution Theorem” (12.0.9)

In other words, the Fourier transform of the convolution is just the product of the
individual Fourier transforms.
The correlation of two functions, denoted Corr(g, 2), is defined by

00

Corr(g, h) = / g(t + t)h(r) dr (12.0.10)

The correlation is a function of t, which is called the lag. It therefore lies in the time
domain, and it turns out to be one member of the transform pair:

Corr(g, h) => G(f)A*(f) “Correlation Theorem” (12.0.11)

[More generally, the second member of the pair is G(f) H (— f), but we are restricting
ourselves to the usual case in which g and / are real functions, so we take the liberty of
setting H(—f) = H*(f).] This result shows that multiplying the Fourier transform
of one function by the complex conjugate of the Fourier transform of the other gives
the Fourier transform of their correlation. The correlation of a function with itself is
called its autocorrelation. In this case (12.0.11) becomes the transform pair

Corr(g, 9) => |G(f)|? “Wiener-Khinchin Theorem” (12.0.12)

The total power in a signal is the same whether we compute it in the time
domain or in the frequency domain. This result is known as Parseval’s theorem:

Total Power = [. \n(t)|? dt = [. |H(f)P af (12.0.13)

Frequently one wants to know “how much power” is contained in the frequency
interval between f and f + df. In such circumstances one does not usually
distinguish between positive and negative f, but rather regards f as varying from 0
(“zero frequency” or D.C.) to +00. In such cases, one defines the one-sided power
spectral density (PSD) of the function h as

Pf) S|AA)P+|A(-f)P? 0< f <0 (12.0.14)

so that the total power is just the integral of P,(f) from f = 0 to f = oo. When the
function h(t) is real, then the two terms in (12.0.14) are equal, so Pn(f) = 2|H(f)|°.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

12.0 Introduction 499

| h(t) ?

(a

Pi,( f) (one-sided)

=)
”

(b)

P,( f)
(two-sided)

-f 0 f
(c)

Figure 12.0.1. Normalizations of one- and two-sided power spectra. The area under the square of the
function, (a), equals the area under its one-sided power spectrum at positive frequencies, (b), and also
equals the area under its two-sided power spectrum at positive and negative frequencies, (c).

Be warned that one occasionally sees PSDs defined without this factor two. These,
strictly speaking, are called two-sided power spectral densities, but some books
are not careful about stating whether one- or two-sided is to be assumed. We
will always use the one-sided density given by equation (12.0.14). Figure 12.0.1
contrasts the two conventions.

If the function h(t) goes endlessly from —oo < t < ov, then its total power
and power spectral density will, in general, be infinite. Of interest then is the (one-
or two-sided) power spectral density per unit time. This is computed by taking a
long, but finite, stretch of the function h(t), computing its PSD [that is, the PSD
of a function that equals h(t) in the finite stretch but is zero everywhere else], and
then dividing the resulting PSD by the length of the stretch used. Parseval’s theorem
in this case states that the integral of the one-sided PSD-per-unit-time over positive
frequency is equal to the mean square amplitude of the signal h(t).

You might well worry about how the PSD-per-unit-time, which is a function of
frequency f, converges as one evaluates it using longer and longer stretches of data.
This interesting question is the content of the subject of “power spectrum estimation,”
and will be considered below in §13.4-§13.7. A crude answer for now is: The

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

500 Chapter 12. Fast Fourier Transform

PSD-per-unit-time converges to finite values at all frequencies except those where
h(t) has a discrete sine-wave (or cosine-wave) component of finite amplitude. At
those frequencies, it becomes a delta-function, i.e., a sharp spike, whose width gets
narrower and narrower, but whose area converges to be the mean square amplitude
of the discrete sine or cosine component at that frequency.

We have by now stated all of the analytical formalism that we will need in this
chapter with one exception: In computational work, especially with experimental
data, we are almost never given a continuous function h(t) to work with, but are
given, rather, a list of measurements of h(ta) for a discrete set of t;’s. The profound
implications of this seemingly unimportant fact are the subject of the next section.

CITED REFERENCES AND FURTHER READING:

Champeney, D.C. 1973, Fourier Transforms and Their Physical Applications (New York: Academic
Press).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:
Academic Press).

12.1 Fourier Transform of Discretely Sampled
Data

In the most common situations, function h(t) is sampled (i.e., its value is
recorded) at evenly spaced intervals in time. Let A denote the time interval between
consecutive samples, so that the sequence of sampled values is

hn = h(nA) n=...,—3,—-2,—-1,0,1,2,3,... (12.1.1)
The reciprocal of the time interval A is called the sampling rate; if A is measured
in seconds, for example, then the sampling rate is the number of samples recorded
per second.

Sampling Theorem and Aliasing

For any sampling interval A, there is also a special frequency f,, called the
Nyquist critical frequency, given by

(12.1.2)

If a sine wave of the Nyquist critical frequency is sampled at its positive peak value,
then the next sample will be at its negative trough value, the sample after that at
the positive peak again, and so on. Expressed otherwise: Critical sampling of a
sine wave is two sample points per cycle. One frequently chooses to measure time
in units of the sampling interval A. In this case the Nyquist critical frequency is
just the constant 1/2.

The Nyquist critical frequency is important for two related, but distinct, reasons.
One is good news, and the other bad news. First the good news. It is the remarkable

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.1 Fourier Transform of Discretely Sampled Data 501

fact known as the sampling theorem: If a continuous function h(t), sampled at an
interval A, happens to be bandwidth limited to frequencies smaller in magnitude than
fc ie., if H(f) = 0 forall |f| > f-, then the function h(t) is completely determined
by its samples h,,. In fact, h(t) is given explicitly by the formula

aan sin[27 f-(t — nA)]
A(t) =A a> hn at nd) (12.1.3)
This is a remarkable theorem for many reasons, among them that it shows that the
“information content” of a bandwidth limited function is, in some sense, infinitely
smaller than that of a general continuous function. Fairly often, one is dealing
with a signal that is known on physical grounds to be bandwidth limited (or at
least approximately bandwidth limited). For example, the signal may have passed
through an amplifier with a known, finite frequency response. In this case, the
sampling theorem tells us that the entire information content of the signal can be
recorded by sampling it at a rate A~! equal to twice the maximum frequency passed
by the amplifier (cf. 12.1.2).

Now the bad news. The bad news concerns the effect of sampling a continuous
function that is not bandwidth limited to less than the Nyquist critical frequency.
In that case, it turns out that all of the power spectral density that lies outside of
the frequency range —f. < f < f¢ is spuriously moved into that range. This
phenomenon is called aliasing. Any frequency component outside of the frequency
range (—f, fc) is aliased (falsely translated) into that range by the very act of
discrete sampling. You can readily convince yourself that two waves exp(277f it)
and exp(2mi ft) give the same samples at an interval A if and only if f; and
f2 differ by a multiple of 1/A, which is just the width in frequency of the range
(—fes fe). There is little that you can do to remove aliased power once you have
discretely sampled a signal. The way to overcome aliasing is to (i) know the natural
bandwidth limit of the signal — or else enforce a known limit by analog filtering
of the continuous signal, and then (ii) sample at a rate sufficiently rapid to give at
least two points per cycle of the highest frequency present. Figure 12.1.1 illustrates
these considerations.

To put the best face on this, we can take the alternative point of view: If a
continuous function has been competently sampled, then, when we come to estimate
its Fourier transform from the discrete samples, we can assume (or rather we might
as well assume) that its Fourier transform is equal to zero outside of the frequency
range in between — f, and f,. Then we look to the Fourier transform to tell whether
the continuous function has been competently sampled (aliasing effects minimized).
We do this by looking to see whether the Fourier transform is already approaching
zero as the frequency approaches f,. from below, or —f. from above. If, on the
contrary, the transform is going towards some finite value, then chances are that
components outside of the range have been folded back over onto the critical range.

Discrete Fourier Transform

We now estimate the Fourier transform of a function from a finite number of its
sampled points. Suppose that we have N consecutive sampled values

hy =h(ty),  te=kA, k=0,1,2,...,N—1 (12.1.4)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

502 Chapter 12. Fast Fourier Transform

A
H(f)

(b)

(c)

Figure 12.1.1. | The continuous function shown in (a) is nonzero only for a finite interval of time T.
It follows that its Fourier transform, whose modulus is shown schematically in (b), is not bandwidth
limited but has finite amplitude for all frequencies. If the original function is sampled with a sampling
interval A, as in (a), then the Fourier transform (c) is defined only between plus and minus the Nyquist
critical frequency. Power outside that range is folded over or “aliased” into the range. The effect can be
eliminated only by low-pass filtering the original function before sampling.

so that the sampling interval is A. To make things simpler, let us also suppose that
N is even. If the function h(t) is nonzero only in a finite interval of time, then
that whole interval of time is supposed to be contained in the range of the V points
given. Alternatively, if the function h(t) goes on forever, then the sampled points are
supposed to be at least “typical” of what h(t) looks like at all other times.

With N numbers of input, we will evidently be able to produce no more than
N independent numbers of output. So, instead of trying to estimate the Fourier
transform H(f) at all values of f in the range —f, to f-, let us seek estimates
only at the discrete values

on N N

In = NR DET Qe g

The extreme values of n in (12.1.5) correspond exactly to the lower and upper limits

of the Nyquist critical frequency range. If you are really on the ball, you will have

noticed that there are N + 1, not N, values of n in (12.1.5); it will turn out that

the two extreme values of n are not independent (in fact they are equal), but all the
others are. This reduces the count to NV.

(12.1.5)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.1 Fourier Transform of Discretely Sampled Data 503

The remaining step is to approximate the integral in (12.0.1) by a discrete sum:

N-1 N-1

oo
H(fn) = | (ter n't & > hy 27 fnthA = A > hy e2mikr/N

7° k=0 k=0 ( )

12.1.6

Here equations (12.1.4) and (12.1.5) have been used in the final equality. The final
summation in equation (12.1.6) is called the discrete Fourier transform of the Nv
points h;,. Let us denote it by Hy,

Hy, = hy e27ihr/N (12.1.7)
k=0

The discrete Fourier transform maps N complex numbers (the h ;.’s) into N complex
numbers (the H,,’s). It does not depend on any dimensional parameter, such as the
time scale A. The relation (12.1.6) between the discrete Fourier transform of a set
of numbers and their continuous Fourier transform when they are viewed as samples
of a continuous function sampled at an interval A can be rewritten as

HA (fn) * AB (12.1.8)

where f,, is given by (12.1.5).

Up to now we have taken the view that the index n in (12.1.7) varies from — N/2
to N/2 (cf. 12.1.5). You can easily see, however, that (12.1.7) is periodic in n, with
period NV. Therefore, H_, = Hny_n n=1,2,.... With this conversion in mind,
one generally lets the n in H,, vary from 0 to N — 1 (one complete period). Then n
and k (in h;,) vary exactly over the same range, so the mapping of N numbers into
N numbers is manifest. When this convention is followed, you must remember that
zero frequency corresponds to n = 0, positive frequencies 0 < f < f, correspond
to values 1 < n < N/2-—1, while negative frequencies —f. < f <0 correspond to
N/2+1<n< N-1. The valuen = N/2 corresponds to both f = f. and f = —fe.

The discrete Fourier transform has symmetry properties almost exactly the same
as the continuous Fourier transform. For example, all the symmetries in the table
following equation (12.0.3) hold if we read h,;, for h(t), Hy, for H(f), and Hy—n
for H(—f). (Likewise, “even” and “odd” in time refer to whether the values h ;, at k
and N — k are identical or the negative of each other.)

The formula for the discrete inverse Fourier transform, which recovers the set
of h,’s exactly from the H,,,’s is:

N-1
k= S Hy, e727tkr/N (12.1.9)

n=0

Notice that the only differences between (12.1.9) and (12.1.7) are (i) changing the
sign in the exponential, and (ii) dividing the answer by N. This means that a
routine for calculating discrete Fourier transforms can also, with slight modification,
calculate the inverse transforms.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

504 Chapter 12. Fast Fourier Transform

The discrete form of Parseval’s theorem is

N-1 1 N-1
Se |hel? = W Se |Anl? (12.1.10)
k=0 n=0

There are also discrete analogs to the convolution and correlation theorems (equations
12.0.9 and 12.0.11), but we shall defer them to §13.1 and §13.2, respectively.

CITED REFERENCES AND FURTHER READING:
Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:

Academic Press).

12.2 Fast Fourier Transform (FFT)

How much computation is involved in computing the discrete Fourier transform
(12.1.7) of N points? For many years, until the mid-1960s, the standard answer
was this: Define W as the complex number

W =e2t/N (12.2.1)
Then (12.1.7) can be written as
N-1
Hy, = >> W"* hy (12.2.2)
k=0

In other words, the vector of h,’s is multiplied by a matrix whose (n, k)th element
is the constant W to the power n x k. The matrix multiplication produces a vector
result whose components are the H,,’s. This matrix multiplication evidently requires
N? complex multiplications, plus a smaller number of operations to generate the
required powers of W. So, the discrete Fourier transform appears to be an O(N ”)
process. These appearances are deceiving! The discrete Fourier transform can,
in fact, be computed in O(N log, NV) operations with an algorithm called the fast
Fourier transform, or FFT. The difference between N log, N and N? is immense.
With N = 10°, for example, it is the difference between, roughly, 30 seconds of CPU
time and 2 weeks of CPU time on a microsecond cycle time computer. The existence
of an FFT algorithm became generally known only in the mid-1960s, from the work
of J.W. Cooley and J.W. Tukey. Retrospectively, we now know (see [1]) that efficient
methods for computing the DFT had been independently discovered, and in some
cases implemented, by as many as a dozen individuals, starting with Gauss in 1805!

One “rediscovery” of the FFT, that of Danielson and Lanczos in 1942, provides
one of the clearest derivations of the algorithm. Danielson and Lanczos showed
that a discrete Fourier transform of length N can be rewritten as the sum of two
discrete Fourier transforms, each of length N/2. One of the two is formed from the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.2 Fast Fourier Transform (FFT) 505

even-numbered points of the original N, the other from the odd-numbered points.
The proof is simply this:

N-1
F.= > e2miskIN ¢,
j=0
N/2-1 N/2-1
_ S e2TiR2DIN fF SS e27iR(2I+N)IN fy
j=0 j (12.2.3)
N/2-1 N/2-1
_ S e2rikil(N/2) 4 S e27thi/(N/2) fy
j=0 j=0

Fe +W* Fe

In the last line, W is the same complex constant as in (12.2.1), Ff denotes the kth
component of the Fourier transform of length N/2 formed from the even components
of the original f;’s, while F/? is the corresponding transform of length N/2 formed
from the odd components. Notice also that & in the last line of (12.2.3) varies from
0 to N, not just to N/2. Nevertheless, the transforms F'f and F? are periodic in k
with length N/2. So each is repeated through two cycles to obtain F',.

The wonderful thing about the Danielson-Lanczos Lemma is that it can be used
recursively. Having reduced the problem of computing F, to that of computing
Fg and F?, we can do the same reduction of F¢ to the problem of computing
the transform of its N/4 even-numbered input data and N/4 odd-numbered data.
In other words, we can define Fy;° and F'f° to be the discrete Fourier transforms
of the points which are respectively even-even and even-odd on the successive
subdivisions of the data.

Although there are ways of treating other cases, by far the easiest case is the
one in which the original N is an integer power of 2. In fact, we categorically
recommend that you only use FFTs with NV a power of two. If the length of your data
set is not a power of two, pad it with zeros up to the next power of two. (We will give
more sophisticated suggestions in subsequent sections below.) With this restriction
on N, it is evident that we can continue applying the Danielson-Lanczos Lemma
until we have subdivided the data all the way down to transforms of length 1. What
is the Fourier transform of length one? It is just the identity operation that copies its
one input number into its one output slot! In other words, for every pattern of log » N
e’s and o’s, there is a one-point transform that is just one of the input numbers f ,,

Feeevoro~-oee — fF for some n (12.2.4)

(Of course this one-point transform actually does not depend on k, since it is periodic
in k with period 1.)

The next trick is to figure out which value of n corresponds to which pattern of
e’s and o’s in equation (12.2.4). The answer is: Reverse the pattern of e’s and o’s,
then let e = 0 and o = 1, and you will have, in binary the value of n. Do you see
why it works? It is because the successive subdivisions of the data into even and odd
are tests of successive low-order (least significant) bits of n. This idea of bit reversal
can be exploited in a very clever way which, along with the Danielson-Lanczos

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

506 Chapter 12. Fast Fourier Transform

000 /» 000 000
001 00 001
010 010 010
oul 01 oul
100 100 100
101 10 101
110 110 110
11. VL 11 11

(a) (b)

Figure 12.2.1. Reordering an array (here of length 8) by bit reversal, (a) between two arrays, versus (b)
in place. Bit reversal reordering is a necessary part of the fast Fourier transform (FFT) algorithm.

Lemma, makes FFTs practical: Suppose we take the original vector of data f;
and rearrange it into bit-reversed order (see Figure 12.2.1), so that the individual
numbers are in the order not of 7, but of the number obtained by bit-reversing 7.
Then the bookkeeping on the recursive application of the Danielson-Lanczos Lemma
becomes extraordinarily simple. The points as given are the one-point transforms.
We combine adjacent pairs to get two-point transforms, then combine adjacent pairs
of pairs to get 4-point transforms, and so on, until the first and second halves of
the whole data set are combined into the final transform. Each combination takes
of order N operations, and there are evidently log, N combinations, so the whole
algorithm is of order N log, N (assuming, as is the case, that the process of sorting
into bit-reversed order is no greater in order than N log, NV).

This, then, is the structure of an FFT algorithm: It has two sections. The first
section sorts the data into bit-reversed order. Luckily this takes no additional storage,
since it involves only swapping pairs of elements. (If k, is the bit reverse of k2, then
kg is the bit reverse of k,.) The second section has an outer loop that is executed
logy N times and calculates, in turn, transforms of length 2,4,8,...,.N. For each
stage of this process, two nested inner loops range over the subtransforms already
computed and the elements of each transform, implementing the Danielson-Lanczos
Lemma. The operation is made more efficient by restricting external calls for
trigonometric sines and cosines to the outer loop, where they are made only log y NV
times. Computation of the sines and cosines of multiple angles is through simple
recurrence relations in the inner loops (cf. 5.5.6).

The FFT routine given below is based on one originally written by N. M.
Brenner. The input quantities are the number of complex data points (nn), the data
array (data[1..2*nn]), and isign, which should be set to either +1 and is the sign
of 7 in the exponential of equation (12.1.7). When isign is set to —1, the routine
thus calculates the inverse transform (12.1.9) — except that it does not multiply by
the normalizing factor 1/N that appears in that equation. You can do that yourself.

Notice that the argument nn is the number of complex data points. The actual

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.2 Fast Fourier Transform (FFT) 507

length of the real array (data[1..2*nn]) is 2 times nn, with each complex value
occupying two consecutive locations. In other words, data[1] is the real part of
fo, data[2] is the imaginary part of fo, and so on up to data[2*nn-1], which
is the real part of fy—1, and data[2*nn], which is the imaginary part of fy—1.
The FFT routine gives back the F’,’s packed in exactly the same fashion, as nn
complex numbers.

The real and imaginary parts of the zero frequency component F'p are in data [1]
and data [2]; the smallest nonzero positive frequency has real and imaginary parts in
data [3] and data[4]; the smallest (in magnitude) nonzero negative frequency has
real and imaginary parts in data[2*nn-1] and data[2*nn]. Positive frequencies
increasing in magnitude are stored in the real-imaginary pairs data[5], data[6]
up to data[nn-1], data[nn]. Negative frequencies of increasing magnitude are
stored in data[2*nn-3], data[2*nn-2] down to data[nn+3], data[nn+4].
Finally, the pair data[nn+1], data[nn+2] contain the real and imaginary parts of
the one aliased point that contains the most positive and the most negative frequency.
You should try to develop a familiarity with this storage arrangement of complex
spectra, also shown in Figure 12.2.2, since it is the practical standard.

This is a good place to remind you that you can also use a routine like four1
without modification even if your input data array is zero-offset, that is has the range
data[0..2*nn-1]. In this case, simply decrement the pointer to data by one when
fourt is invoked, e.g., four1 (data-1,1024,1);. The real part of fo will now be
returned in data[0], the imaginary part in data[1], and so on. See §1.2.

#include <math.h>
#define SWAP(a,b) tempr=(a) ; (a)=(b) ; (b)=tempr

void four1(float data[], unsigned long nn, int isign)
Replaces data[1..2*nn] by its discrete Fourier transform, if isign is input as 1; or replaces
data[1..2*nn] by nn times its inverse discrete Fourier transform, if isign is input as —1.
data is a complex array of length nn or, equivalently, a real array of length 2*nn. nn MUST
be an integer power of 2 (this is not checked for!).
{
unsigned long n,mmax,m,j,istep,i;
double wtemp,wr,wpr,wpi,wi, theta; Double precision for the trigonomet-
float tempr,tempi; ric recurrences.

for (i=1;i<n;it=2) { This is the bit-reversal section of the
if Goad routine.
SWAP (data[j] ,data[i]) ; Exchange the two complex numbers.
SWAP (data[j+1] ,data[i+1]) ;
}
m=nn;
while (m >= 2 & j >m) {
ja
m >>= 1;
}
j += m;
+
Here begins the Danielson-Lanczos section of the routine.
mmax=2;
while (n > mmax) { Outer loop executed logs nn times.
istep=mmax << 1;
theta=isign* (6 .28318530717959/mmax) ; Initialize the trigonometric recurrence.

wtemp=sin(0.5*theta) ;
wpr = -2.0*wtemp*wtemp;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

508 Chapter 12. Fast Fourier Transform

@® real @® real

———_— t=0 b=--==-----| f= 0

@ imag @ imag

© real ® real 1

po t= pot Lp

@® imag @® imag N

= 2
zie 2 z\|¢é =
Nn a
% % | [SD real N2-1
E 2 | paren saaga| ({ “N
5 ba ® imag
2 = | |GsD real 1
5 5 prssct co7c7| ¢ f=+>5— (combination)
q ‘ 5 F GD imag 2
: ~ | |\eD
real
GED imag
real

@® _ imag

(a) (b)

Figure 12.2.2. Input and output arrays for FFT. (a) The input array contains N (a power of 2)
complex time samples in a real array of length 2.V, with real and imaginary parts alternating. (b) The
output array contains the complex Fourier spectrum at N values of frequency. Real and imaginary parts
again alternate. The array starts with zero frequency, works up to the most positive frequency (which
is ambiguous with the most negative frequency). Negative frequencies follow, from the second-most
negative up to the frequency just below zero.

wpi=sin(theta) ;

wr=1.0;
wi=0.0;
for (m=1;m<mmax;m- > { Here are the two nested inner loops.
for (i=m;i<=n;it=istep) {
j=itmmax; This is the Danielson-Lanczos for-
tempr=wrxdata[j]-wi*data[j+1] ; mula:
tempi=wr*data[j+1]+wi*data[j];
data[j]=data[i]-tempr;
data[j+1]=data[i+1]-tempi;
data[i] += tempr;
data[it+1] += tempi;
3
wr=(wtemp=wr) *wpr-witwpitwr ; Trigonometric recurrence.

wi=wi*wpr+wtemp*wpi+wi;
}

mmax=istep;

(A double precision version of four1, named dfour1, is used by the routine mpmul
in $20.6. You can easily make the conversion, or else get the converted routine
from the Numerical Recipes diskette.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.2 Fast Fourier Transform (FFT) 509

Other FFT Algorithms

We should mention that there are a number of variants on the basic FFT algorithm
given above. As we have seen, that algorithm first rearranges the input elements
into bit-reverse order, then builds up the output transform in log, N iterations. In
the literature, this sequence is called a decimation-in-time or Cooley-Tukey FFT
algorithm. It is also possible to derive FFT algorithms that first go through a set of
log, N iterations on the input data, and rearrange the output values into bit-reverse
order. These are called decimation-in-frequency or Sande-Tukey FFT algorithms. For
some applications, such as convolution (§13.1), one takes a data set into the Fourier
domain and then, after some manipulation, back out again. In these cases it is possible
to avoid all bit reversing. You use a decimation-in-frequency algorithm (without its
bit reversing) to get into the “scrambled” Fourier domain, do your operations there,
and then use an inverse algorithm (without its bit reversing) to get back to the time
domain. While elegant in principle, this procedure does not in practice save much
computation time, since the bit reversals represent only a small fraction of an FFT’s
operations count, and since most useful operations in the frequency domain require
a knowledge of which points correspond to which frequencies.

Another class of FFTs subdivides the initial data set of length N not all the
way down to the trivial transform of length 1, but rather only down to some other
small power of 2, for example N = 4, base-4 FFTs, or N = 8, base-8 FFTs. These
small transforms are then done by small sections of highly optimized coding which
take advantage of special symmetries of that particular small NV. For example, for
N = 4, the trigonometric sines and cosines that enter are all +1 or 0, so many
multiplications are eliminated, leaving largely additions and subtractions. These
can be faster than simpler FFTs by some significant, but not overwhelming, factor,
e.g., 20 or 30 percent.

There are also FFT algorithms for data sets of length N not a power of two. They
work by using relations analogous to the Danielson-Lanczos Lemma to subdivide
the initial problem into successively smaller problems, not by factors of 2, but by
whatever small prime factors happen to divide NV. The larger that the largest prime
factor of N is, the worse this method works. If N is prime, then no subdivision
is possible, and the user (whether he knows it or not) is taking a slow Fourier
transform, of order N? instead of order Nlog, N. Our advice is to stay clear
of such FFT implementations, with perhaps one class of exceptions, the Winograd
Fourier transform algorithms. Winograd algorithms are in some ways analogous to
the base-4 and base-8 FFTs. Winograd has derived highly optimized codings for
taking small-N discrete Fourier transforms, e.g., for N = 2,3, 4,5, 7,8, 11, 13, 16.
The algorithms also use a new and clever way of combining the subfactors. The
method involves a reordering of the data both before the hierarchical processing and
after it, but it allows a significant reduction in the number of multiplications in the
algorithm. For some especially favorable values of NV, the Winograd algorithms can
be significantly (e.g., up to a factor of 2) faster than the simpler FFT algorithms
of the nearest integer power of 2. This advantage in speed, however, must be
weighed against the considerably more complicated data indexing involved in these
transforms, and the fact that the Winograd transform cannot be done “in place.”

Finally, an interesting class of transforms for doing convolutions quickly are
number theoretic transforms. These schemes replace floating-point arithmetic with

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

510 Chapter 12. Fast Fourier Transform

integer arithmetic modulo some large prime N+1, and the Nth root of 1 by the
modulo arithmetic equivalent. Strictly speaking, these are not Fourier transforms
at all, but the properties are quite similar and computational speed can be far
superior. On the other hand, their use is somewhat restricted to quantities like
correlations and convolutions since the transform itself is not easily interpretable
as a “frequency” spectrum.

CITED REFERENCES AND FURTHER READING:

Nussbaumer, H.J. 1982, Fast Fourier Transform and Convolution Algorithms (New York: Springer-
Verlag).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:
Academic Press).

Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall). [1]
Bloomfield, P. 1976, Fourier Analysis of Time Series — An Introduction (New York: Wiley).

Van Loan, C. 1992, Computational Frameworks for the Fast Fourier Transform (Philadelphia:
S.LA.M.).

Beauchamp, K.G. 1984, Applications of Walsh Functions and Related Functions (New York:
Academic Press) [non-Fourier transforms].

Heideman, M.T., Johnson, D.H., and Burris, C.S. 1984, IEEE ASSP Magazine, pp. 14-21 (Oc-
tober).

12.3 FFT of Real Functions, Sine and Cosine
Transforms

It happens frequently that the data whose FFT is desired consist of real-valued
samples f;, 7 = 0...N—1. To use four1, we put these into a complex array
with all imaginary parts set to zero. The resulting transform F,,, n =0...N—1
satisfies Fy_,* = F,,. Since this complex-valued array has real values for Fo
and F'y/2, and (N/2) — 1 other independent values F, ... Fy 2-1, it has the same
2(N/2—1)+2 = N “degrees of freedom” as the original, real data set. However,
the use of the full complex FFT algorithm for real data is inefficient, both in execution
time and in storage required. You would think that there is a better way.

There are two better ways. The first is “mass production”: Pack two separate
real functions into the input array in such a way that their individual transforms can
be separated from the result. This is implemented in the program twofft below.
This may remind you of a one-cent sale, at which you are coerced to purchase
two of an item when you only need one. However, remember that for correlations
and convolutions the Fourier transforms of two functions are involved, and this is a
handy way to do them both at once. The second method is to pack the real input
array cleverly, without extra zeros, into a complex array of half its length. One then
performs a complex FFT on this shorter length; the trick is then to get the required
answer out of the result. This is done in the program realft below.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.3 FFT of Real Functions, Sine and Cosine Transforms 511

Transform of Two Real Functions Simultaneously

First we show how to exploit the symmetry of the transform F’,, to handle
two real functions at once: Since the input data f; are real, the components of the
discrete Fourier transform satisfy

Fy-n = (Fn)* (12.3.1)

where the asterisk denotes complex conjugation. By the same token, the discrete
Fourier transform of a purely imaginary set of g;’s has the opposite symmetry.

Gn—n = —(Gn)* (12.3.2)

Therefore we can take the discrete Fourier transform of two real functions each of
length N simultaneously by packing the two data arrays as the real and imaginary
parts, respectively, of the complex input array of four1. Then the resulting transform
array can be unpacked into two complex arrays with the aid of the two symmetries.
Routine twofft works out these ideas.

void twofft(float datai{], float data2[], float fftif], float fft2[],

unsigned long n)
Given two real input arrays datai[1..n] and data2[1..n], this routine calls four1 and
returns two complex output arrays, fft1[1..2n] and f£t2[1..2n], each of complex length
n (i.e., real length 2*n), which contain the discrete Fourier transforms of the respective data
arrays. n MUST be an integer power of 2.
{

void fouri(float data[], unsigned long nn, int isign);

unsigned long nn3,nn2,jj,j;

float rep,rem,aip,aim;

nn3=1+ (nn2=2+n+n) ;

for (j=1,jj=2;j<=n;j++,jjt=2) { Pack the two real arrays into one com-

fft1[jj-1]=datai[j]; plex array.
ffti[jjl=data2[j];

}

fourl(ffti,n,1); Transform the complex array.

f£t2[1]=fft1[2];
fft1[2]=fft2[2]=0.0;

for (j= =n+1;j+=2) {
rep=0.5*(fft1[j]+fft1[nn2-j]); Use symmetries to separate the two trans-
rem=0.5*(fft1[j]-fft1[nn2-j]); forms.

aip=0.5*(fft1[j+1]+f£ft1[nn3-j]);

aim=0.5*(fft1[j+1]-fft1[nn3-j]) ;

fft1[j]=rep; Ship them out in two complex arrays.
ffti[j+t]=aim;

fft1[nn2-j]=rep;

fft1i[{nn3-j] = -aim;

ff£t2[j]=aip;

f£t2[j+1] -rem;

f£t2[nn2-j]=aip;

f£t2[nn3-j]=rem;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

512 Chapter 12. Fast Fourier Transform

What about the reverse process? Suppose you have two complex transform
arrays, each of which has the symmetry (12.3.1), so that you know that the inverses
of both transforms are real functions. Can you invert both in a single FFT? This is
even easier than the other direction. Use the fact that the FFT is linear and form
the sum of the first transform plus i times the second. Invert using four1 with
isign = —1. The real and imaginary parts of the resulting complex array are the
two desired real functions.

FFT of Single Real Function

To implement the second method, which allows us to perform the FFT of
a single real function without redundancy, we split the data set in half, thereby
forming two real arrays of half the size. We can apply the program above to these
two, but of course the result will not be the transform of the original data. It will
be a schizophrenic combination of two transforms, each of which has half of the
information we need. Fortunately, this schizophrenia is treatable. It works like this:

The right way to split the original data is to take the even-numbered fj as
one data set, and the odd-numbered f,; as the other. The beauty of this is that
we can take the original real array and treat it as a complex array h; of half the
length. The first data set is the real part of this array, and the second is the
imaginary part, as prescribed for twofft. No repacking is required. In other words
hy = fog +ifoju1, J =0,..., N/2—1. We submit this to four1, and it will give

back a complex array H, = F° +iF°, n=0,...,N/2—1 with

N/2-1
Fe= S fog e271 (N/2)

k=0

an (12.3.3)
Fo = S forur e2mihn/(N/2)

k=0

The discussion of program twof ft tells you how to separate the two transforms
F* and F°° out of H,,. How do you work them into the transform F’, of the original
data set f;? Simply glance back at equation (12.2.3):

F,=Fe+e7™/N Po n=0,...,.N-1 (12.3.4)

Expressed directly in terms of the transform H,, of our real (masquerading as
complex) data set, the result is

Fy, = 3 (An + Hyj2-n*) — 3 An - Ayn PrN n=0,...,N-1
(12.3.5)

A few remarks:

e Since Fy_,,* = F;, there is no point in saving the entire spectrum. The
positive frequency half is sufficient and can be stored in the same array as
the original data. The operation can, in fact, be done in place.

e Even so, we need values H,,, n = 0,..., N/2 whereas four gives only
the values n = 0,..., N/2 — 1. Symmetry to the rescue, Hyj2 = Ao.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.3 FFT of Real Functions, Sine and Cosine Transforms 513

e The values Fo and F'y,2 are real and independent. In order to actually
get the entire F;, in the original array space, it is convenient to put F'n 2
into the imaginary part of Fo.

e Despite its complicated form, the process above is invertible. First peel
Fy 2 out of Fo. Then construct

FL = (Fn + Fyy2-n)

f= xo Fn — Fyjo-n)

n=0,...,N/2-1 (12.3.6)

and use four1 to find the inverse transform of H, = FD + iF,
Surprisingly, the actual algebraic steps are virtually identical to those of
the forward transform.

Here is a representation of what we have said:

#include <math.h>

void realft(float data[], unsigned long n, int isign)

Calculates the Fourier transform of a set of n real-valued data points. Replaces this data (which
is stored in array data[1. .n]) by the positive frequency half of its complex Fourier transform.
The real-valued first and last components of the complex transform are returned as elements
data[1] and data[2], respectively. n must be a power of 2. This routine also calculates the
inverse transform of a complex data array if it is the transform of real data. (Result in this case
must be multiplied by 2/n.)

{

void fouri(float data[], unsigned long nn, int isign);

unsigned long i,i1,i2,i13,i4,np3;

float c1=0.5,c2,hir,hii,h2r,h2i;

double wr,wi,wpr,wpi,wtemp, theta; Double precision for the trigonomet-
ric recurrences.

theta=3.141592653589793/ (double) (n>>1); Initialize the recurrence.
if (isign == 1) {
c2 = -0.5;
fouri(data,n>>1,1); The forward transform is here.
} else {
c2=0.5; Otherwise set up for an inverse trans-
theta = -theta; form.
+
wtemp=sin(0.5*theta) ;
wpr = -2.0*wtemp*wtemp;

in(theta) ;

.O+wpr 5
wi=wpi;
np3=n+3
for (i=2;i<=(n>>2);i++) { Case i=1 done separately below.

i4=1+(i3=np3-(i2=1+(i1=iti-1)));
hir=c1*(data[i1]+data[i3]) ; The two separate transforms are sep-
hii=c1*(data[i2]-data[i4]); arated out of data.

h2r = -c2*(data[i2]+data[i4]);
h2i=c2*(data[i1]-data[i3]);
data[ii]=hir+wr+*h2r-wi*h2i;
data[i2]=hli+wr+*h2it+wi*h2r;

Here they are recombined to form
the true transform of the origi-

data[i3]=hir-wr*h2r+wi*h2i ; nal real data.
data[i4] -hiitwr*h2i+wi*h2r;
wr=(wtemp=wr) *wpr-wi*wpitwr; The recurrence.

wi=wixwprt+wtemp*wpitwi ;
+
if (isign == 1) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

514 Chapter 12. Fast Fourier Transform

data[1] = (hir=data[1])+data[2] ; Squeeze the first and last data to-
data[2] = hir-data[2] ; gether to get them all within the
} else { original array.

data[1]=c1*((hir=data[1])+data[2]) ;

data[2]=c1*(hir-data[2]) ;

fouri(data,n>>1,-1); This is the inverse transform for the
} case isign=-1.

Fast Sine and Cosine Transforms

Among their other uses, the Fourier transforms of functions can be used to solve
differential equations (see $19.4). The most common boundary conditions for the
solutions are 1) they have the value zero at the boundaries, or 2) their derivatives
are zero at the boundaries. In the first instance, the natural transform to use is the
sine transform, given by

N-1
Fy = > fj sin(ajk/N) sine transform (12.3.7)
j=l

where f;, 7 = 0,...,.N —1 is the data array, and fo = 0.

At first blush this appears to be simply the imaginary part of the discrete Fourier
transform. However, the argument of the sine differs by a factor of two from the
value that would make this so. The sine transform uses sines only as a complete set
of functions in the interval from 0 to 27, and, as we shall see, the cosine transform
uses cosines only. By contrast, the normal FFT uses both sines and cosines, but only
half as many of each. (See Figure 12.3.1.)

The expression (12.3.7) can be “force-fit” into a form that allows its calculation
via the FFT. The idea is to extend the given function rightward past its last tabulated
value. We extend the data to twice their length in such a way as to make them an
odd function about 7 = N, with fy = 0,

fon; =—-fy) jf =0,...,N-1 (12.3.8)

Consider the FFT of this extended function:

2N-1
Fe = So fe? 9k/ON) (12.3.9)

j=0

The half of this sum from 7 = N to 7 = 2N — 1 can be rewritten with the
substitution j/ = 2N — j

2N-1 N
fye2tak/2N) _ fon jr e2M2N-J)K/QN)
=N j=l

7

(12.3.10)

N-1

__ fre 2m’ B/N)

j/=0

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.3 FFT of Real Functions, Sine and Cosine Transforms 515

(a)

(b)

0 2a

Figure 12.3.1. Basis functions used by the Fourier transform (a), sine transform (b), and cosine transform
(c), are plotted. The first five basis functions are shown in each case. (For the Fourier transform, the real
and imaginary parts of the basis functions are both shown.) While some basis functions occur in more
than one transform, the basis sets are distinct. For example, the sine transform functions labeled (1), (3),
(5) are not present in the Fourier basis. Any of the three sets can expand any function in the interval
shown; however, the sine or cosine transform best expands functions matching the boundary conditions
of the respective basis functions, namely zero function values for sine, zero derivatives for cosine.

so that

N-1
r= SO fi [eariaesen) _
j=0

N-1

21 > fy sin(wjk/N)
j=0

2nisk/(2N))

(12.3.11)

Thus, up to a factor 27 we get the sine transform from the FFT of the extended function.

This method introduces a factor of two inefficiency into the computation by
extending the data. This inefficiency shows up in the FFT output, which has
zeros for the real part of every element of the transform. For a one-dimensional
problem, the factor of two may be bearable, especially in view of the simplicity
of the method. When we work with partial differential equations in two or three
dimensions, though, the factor becomes four or eight, so efforts to eliminate the
inefficiency are well rewarded.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

516 Chapter 12. Fast Fourier Transform

From the original real data array f; we will construct an auxiliary array y; and
apply to it the routine realft. The output will then be used to construct the desired
transform. For the sine transform of data f;, 7 = 1,...,.N —1, the auxiliary array is

yo =0
yj = sinGin/N)(fj + fj) + ahi — fn—;) J=1,...,.N-1
This array is of the same dimension as the original. Notice that the first term is

symmetric about 7 = N/2 and the second is antisymmetric. Consequently, when
realft is applied to y;, the result has real parts R;, and imaginary parts I, given by

N-1
R= y, cos(2rjk/N)
j=0
N-1
=) (fi + fiv—5) sin(ir/N) cos(2njk/N)
j=l
N-1
= Ss 2f; sin(ja/N) cos(2rjk/N)
j=0
Nf. Qk+ Din. Qk-Din
= fi E W —s W
j=0
= Poppi — Fop-1 (12.3.13)
N-1
Ih = y, sin(2ajk/N)
j=0
N-1
= )_ (fi — fr-3)5 sin(2rjk/N)
j=l
N-1
= f; sin(2ajk/N)
j=0
= Fo (12.3.14)

Therefore F;, can be determined as follows:
Fo, = Ip Fope1 = For-1 + Re k=0,...,(N/2-1) (12.3.15)
The even terms of Fy are thus determined very directly. The odd terms require

a recursion, the starting point of which follows from setting k = 0 in equation
(12.3.15) and using Fy = —F_):

Fi= 5Ro (123.16)

The implementing program is

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

12.3 FFT of Real Functions, Sine and Cosine Transforms 517

#include <math.h>

void sinft(float y[], int n)

Calculates the sine transform of a set of n real-valued data points stored in array y[1..n].
The number n must be a power of 2. On exit y is replaced by its transform. This program,
without changes, also calculates the inverse sine transform, but in this case the output array
should be multiplied by 2/n.

{

void realft(float data[], unsigned long n, int isign);

int j,n2=n+2;

float sum,y1,y2;

double theta,wi=0.0,wr=1.0,wpi,wpr,wtemp; Double precision in the trigono-

metric recurrences.

theta=3.14159265358979/(double) n; Initialize the recurrence.

wtemp=sin(0.5*theta) ;

wpr = -2.0*wtemp*wtemp;

wpi=sin(theta) ;

y[1]=0.0;

jjs=(m>>1) +1; j++) {

wr=(wtemp=wr) *wpr-wi*wpitwr; Calculate the sine for the auxiliary array.
wi=witwpr+wtemp*wpitwi ; The cosine is needed to continue the recurrence.
yt=wix(y(j]l+y{n2-j]); Construct the auxiliary array.
y2=0.5*(y[j]-y[n2-j]);
yljl=yity2; Terms j and N — j are related.
y(n2-j]=y1-y2;

+

realft(y,n,1); Transform the auxiliary array.

y(1]*=0.5; Initialize the sum used for odd terms below.

sum=y [2]=0.0;
yljl=y(j+11; Even terms determined directly.
yl[j+1]=sum; Odd terms determined by this running sum.

+

3

The sine transform, curiously, is its own inverse. If you apply it twice, you get the
original data, but multiplied by a factor of N/2.

The other common boundary condition for differential equations is that the
derivative of the function is zero at the boundary. In this case the natural transform
is the cosine transform. There are several possible ways of defining the transform.
Each can be thought of as resulting from a different way of extending a given array
to create an even array of double the length, and/or from whether the extended array
contains 2/V — 1, 2, or some other number of points. In practice, only two of the
numerous possibilities are useful so we will restrict ourselves to just these two.

The first form of the cosine transform uses N + 1 data points:

N-1

w]+ © fj cos(njk/N) (12.3.17)

j=l

Fy = 5lfo + (-1)"

It results from extending the given array to an even array about 7 = N, with

fon-j = fy, j=0,.

If you substitute this extended array into equation (12.3.9), and follow steps analogous
to those leading up to equation (12.3.11), you will find that the Fourier transform is

\N-1 (12.3.18)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

518 Chapter 12. Fast Fourier Transform

just twice the cosine transform (12.3.17). Another way of thinking about the formula
(12.3.17) is to notice that it is the Chebyshev Gauss-Lobatto quadrature formula (see
§4.5), often used in Clenshaw-Curtis adaptive quadrature (see §5.9, equation 5.9.4).

Once again the transform can be computed without the factor of two inefficiency.
In this case the auxiliary function is

95 = 5 (fi + fv) — sini /N)Fj — fiv-3) § = 0,---N 1 (123.19)
Instead of equation (12.3.15), realft now gives
Fo, = Ry Ferg = For-1 + Ik k=0,...,(N/2-1) (12.3.20)

The starting value for the recursion for odd k in this case is

N-1

Fy = 5(fo— fix) +S ficostin/N) (123.21)

j=l

This sum does not appear naturally among the R;, and J;,, and so we accumulate it
during the generation of the array yj.

Once again this transform is its own inverse, and so the following routine
works for both directions of the transformation. Note that although this form of
the cosine transform has N + 1 input and output values, it passes an array only
of length N to realft.

#include <math.h>
#define PI 3.141592653589793

void cosfti(float y[], int n)
Calculates the cosine transform of a set y[1. .n+1] of real-valued data points. The transformed
data replace the original data in array y. n must be a power of 2. This program, without
changes, also calculates the inverse cosine transform, but in this case the output array should
be multiplied by 2/n.
t

void realft(float data[], unsigned long n, int isign);

int j,n2;

float sum,y1,y2;

double theta,wi=0.0,wpi,wpr,wr=1.0,wtemp;

Double precision for the trigonometric recurrences.

PI/n; Initialize the recurrence.
in(0.5*theta) ;

wpr = -2.0*wtemp*wtemp;

wpi=sin(theta) ;

sum=0.5*(y[1]-y[n+1]);

y(1]=0.5*(y[1]+y Int+1]) ;

n2=n+2;

3js=(m>>1) j++) £
temp=wr) *wpr-wi*wpitwr ; Carry out the recurrence.
wi=wixwprt+wtemp*wpitwi ;
yi=0.5*(y[j]+y[n2-j]);

Calculate the auxiliary function.

y2=(y[j]-y[n2-j]);

yljl=y1-wixy2; The values for 7 and N — j are related.

y [n2-j]=yit+wity2;

sum += wr*y2; Carry along this sum for later use in unfold-

} ing the transform.

j=n/2+1 unnecessary since y [n/2+1] unchanged.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.3 FFT of Real Functions, Sine and Cosine Transforms 519

realft(y,n,1); Calculate the transform of the auxiliary func-
y(nt1]=y [2]; tion.
y(2]=sum; sum is the value of F) in equation (12.3.21).

for (j=4;j<=n;j+=2) {
Equation (12.3.20).

yl[j]=sum;

The second important form of the cosine transform is defined by

N-1 .
. thi +4)
Fy = dX fy cos (12.3.22)
with inverse
N-1
2a mk(j + 3)
fi => Feeos — (12.3.23)
N k=0 N

Here the prime on the summation symbol means that the term for k = 0 has a
coefficient of 3 in front. This form arises by extending the given data, defined for
j=0,...,N—1,toj = N,...,2N—1insucha way that it is even about the point
N- 4 and periodic. (It is therefore also even about 7 = — 3) The form (12.3.23)
is related to Gauss-Chebyshev quadrature (see equation 4.5.19), to Chebyshev
approximation (§5.8, equation 5.8.7), and Clenshaw-Curtis quadrature (§5.9).

This form of the cosine transform is useful when solving differential equations
on “staggered” grids, where the variables are centered midway between mesh points.
It is also the standard form in the field of data compression and image processing.

The auxiliary function used in this case is similar to equation (12.3.19):

1. . i+) . .
95 = 3 (fi + fy-ja) t8m (fi — fy-ja) 9 = 0,0, N= 1

(12.3.24)
Carrying out the steps similar to those used to get from (12.3.12) to (12.3.15), we find

wk _ wk
Fo = cos wie —sin Wik (12.3.25)

. 1k tk
Fop_1 = sin wee + cos wit + Pops (12.3.26)

Note that equation (12.3.26) gives
1

Fy-1 = sRn 2 (12.3.27)

2

Thus the even components are found directly from (12.3.25), while the odd com-
ponents are found by recursing (12.3.26) down from k = N/2 — 1, using (12.3.27)
to start.

Since the transform is not self-inverting, we have to reverse the above steps to
find the inverse. Here is the routine:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

520 Chapter 12. Fast Fourier Transform

#include <math.h>
#define PI 3.141592653589793

void cosft2(float y[], int n, int isign)

Calculates the “staggered” cosine transform of a set y[1..n] of real-valued data points. The
transformed data replace the original data in array y. n must be a power of 2. Set isign to
+1 for a transform, and to —1 for an inverse transform. For an inverse transform, the output
array should be multiplied by 2/n.

{

doa,

void realft(float data[], unsigned long n, int isign);
int i;

float sum,sumi,y1,y2,ytemp;

double theta,wi=0.0,wil,wpi,wpr,wr=1.0,wr1,wtemp;
Double precision for the trigonometric recurrences.

Buipnjoul) say ajqepeos

theta=0.5*PI/n; Initialize the recurrences.
wri=cos (theta) ;

ZSSoP
oes
58252
o
22923
28599
Q@sa uF
8se
32885
29
oes geage
wil=sin(theta) ; BOGS
wpr = -2.0*wil*wil; ea3g8 P
wpi=sin(2.0*theta) ; Neeee
if (isign == 1) { Forward transform. we S20
for (i=1;i<=n/2;it+) { Seen
y1=0.5*(y [i]+y [n-i+1]); Calculate the auxiliary function. 33322
anys Weelnnd , S55R52
y2=wil*(y[i]-y(n-it1]); gogo
ylil=y1+y2; 86e°
y(n-it1]=y1-y2; 333 0r
wr1=(wtemp=wr1) *wpr-wil*wpitwrt; Carry out the recurrence. 253 $ z
wil=wil*wpr+wtemp*wpit+wil; BS ° E}
+ Se e8o0
<8
realft(y,n,1); Transform the auxiliary function. = a2 $ o
for (i=3;i<=n;it=2) { Even terms. Se2e3 le}
wtemp=wr) *wpr-wixwpitwr ; g 3 sod
is*wpr+wtemp+wpitwi; oz 3 =
yt=y [i] *wr-y [i+1] *wi; geosa
y2-y litt] «wry [i] +wi; BSE 5
ylil=y1; S386
i =y2: FO =
yli+1]=y2; ages oe
} s28235
sum=0.5*y [2] ; Initialize recurrence for odd terms & Z& 3 6
for (i=n;i>=2;i-=2) { with $Ry/2. 82586
sumi=sum; Carry out recurrence for odd terms. FA aggo
sum += y[il]; es szs
[i]=sumt ; 23358
mes z8Ser
to avson
} else if (isign == -1) { Inverse transform. 8 oeke
ytemp=y [n] ; eseag
r agoge
for (i=n;i>=4;i-=2) y[i]=yli-2]-y[il; Form difference of odd terms. Sa20o
y[2]=2.0*ytemp; Bag 8
for (i=3;i<=n;it=2) { Calculate Ry and I;,. 363 g
wr=(wtemp=wr) *wpr-wixwpitwr; zdoz
wi=wi*wpr+wtemp*wpitwi; ges 3
yi=y (il *wrty [it+1] wi; ges
y2=y [i+1] *wr-y [i] *wi; 35S
ylil=y1; 523
yliti]=y2; Be D
}
realft(y,n,-1);
for (i=1;i<=n/2;it+) { Invert auxiliary array.
yt=y [i]+y[n-i+1];

y2=(0.5/wit)*(y [i]-y [n-i+1]);
y[i]=0.5*(y1+y2) ;
y(n-it1]=0.5*(y1-y2) ;
wri=(wtemp=wr1) *wpr-wil*wpitwr1;
wil=wil*wpr+wtemp*wpi+wil ;

12.4 FFT in Two or More Dimensions 521

An alternative way of implementing this algorithm is to form an auxiliary
function by copying the even elements of f; into the first N/2 locations, and the
odd elements into the next N/2 elements in reverse order. However, it is not easy
to implement the alternative algorithm without a temporary storage array and we
prefer the above in-place algorithm.

Finally, we mention that there exist fast cosine transforms for small N that do
not rely on an auxiliary function or use an FFT routine. Instead, they carry out the
transform directly, often coded in hardware for fixed N of small dimension [1].

CITED REFERENCES AND FURTHER READING:
Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall), §10-10.

Sorensen, H.V., Jones, D.L., Heideman, M.T., and Burris, C.S. 1987, IEEE Transactions on
Acoustics, Speech, and Signal Processing, vol. ASSP-35, pp. 849-863.

Hou, H.S. 1987, IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. ASSP-35,
pp. 1455-1461 [see for additional references].

Hockney, R.W. 1971, in Methods in Computational Physics, vol. 9 (New York: Academic Press).
Temperton, C. 1980, Journal of Computational Physics, vol. 34, pp. 314-329.

Clarke, R.J. 1985, Transform Coding of Images, (Reading, MA: Addison-Wesley).

Gonzalez, R.C., and Wintz, P. 1987, Digital Image Processing, (Reading, MA: Addison-Wesley).

Chen, W., Smith, C.H., and Fralick, S.C. 1977, IEEE Transactions on Communications, vol. COM-
25, pp. 1004-1009. [1]

12.4 FFT in Two or More Dimensions

Given a complex function h(k;,k2) defined over the two-dimensional grid
O0< ky < Ni -—1, 0 < ko < No —1, we can define its two-dimensional discrete
Fourier transform as a complex function H(n1,n2), defined over the same grid,

No-1N,-1
A(ni,n2) = > > exp(2mikgn2/N2) exp(2rikin1/N1) h(k1, ke)
ka=0 ky =0

(12.4.1)

By pulling the “subscripts 2” exponential outside of the sum over k 1, or by reversing
the order of summation and pulling the “subscripts 1” outside of the sum over k 2,
we can see instantly that the two-dimensional FFT can be computed by taking one-
dimensional FFTs sequentially on each index of the original function. Symbolically,

HI(n1,n2) = FFT-on-index-1 (FFT-on-index-2 [h(k1, k2)])

. . (12.4.2)
= FFT-on-index-2 (FFT-on-index-1 [h(k1, k2)})

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

522 Chapter 12. Fast Fourier Transform

For this to be practical, of course, both NV; and No should be some efficient length
for an FFT, usually a power of 2. Programming a two-dimensional FFT, using
(12.4.2) with a one-dimensional FFT routine, is a bit clumsier than it seems at first.
Because the one-dimensional routine requires that its input be in consecutive order
as a one-dimensional complex array, you find that you are endlessly copying things
out of the multidimensional input array and then copying things back into it. This
is not recommended technique. Rather, you should use a multidimensional FFT
routine, such as the one we give below.

The generalization of (12.4.1) to more than two dimensions, say to L-
dimensions, is evidently

Np-1 Ny-1
A(ny,...,n~)= > tee > exp(2mikpnz/Nz) x ++:
kp=0 ke =0 (12.4.3)
x exp(2mikyn1/N1) h(ki,..., kt)

where n; and ky range from 0 to. N; —1,... , nz and ky range from 0 to Nz — 1.
How many calls to a one-dimensional FFT are in (12.4.3)? Quite a few! For each
value of ky, k2,...,k,—1 you FFT to transform the L index. Then for each value of
ky, ko,...,k,—2 and ny you FFT to transform the L — 1 index. And so on. It is
best to rely on someone else having done the bookkeeping for once and for all.

The inverse transforms of (12.4.1) or (12.4.3) are just what you would expect
them to be: Change the i’s in the exponentials to —i’s, and put an overall
factor of 1/(N, x --- x Nz) in front of the whole thing. Most other features
of multidimensional FFTs are also analogous to features already discussed in the
one-dimensional case:

e Frequencies are arranged in wrap-around order in the transform, but now

for each separate dimension.

e The input data are also treated as if they were wrapped around. If they are
discontinuous across this periodic identification (in any dimension) then
the spectrum will have some excess power at high frequencies because
of the discontinuity. The fix, if you care, is to remove multidimensional
linear trends.

e Ifyou are doing spatial filtering and are worried about wrap-around effects,
then you need to zero-pad all around the border of the multidimensional
array. However, be sure to notice how costly zero-padding is in multidi-
mensional transforms. If you use too thick a zero-pad, you are going to
waste a lot of storage, especially in 3 or more dimensions!

e Aliasing occurs as always if sufficient bandwidth limiting does not exist
along one or more of the dimensions of the transform.

The routine f ourn that we furnish herewith is a descendant of one written by N.
M. Brenner. It requires as input (i) a scalar, telling the number of dimensions, e.g.,
2; (ii) a vector, telling the length of the array in each dimension, e.g., (32,64). Note
that these lengths must all be powers of 2, and are the numbers of complex values
in each direction; (iii) the usual scalar equal to +1 indicating whether you want the
transform or its inverse; and, finally (iv) the array of data.

A few words about the data array: fourn accesses it as a one-dimensional array
of real numbers, that is, data[1..(2N1N2...Nz)], of length equal to twice the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.4 FFT in Two or More Dimensions 523

data [1] row of 2N, float numbers
row | f, =0
f= x
11
row N,/2 f= aaa =!
11

row N,/2+ 1

row N,/2 +2

data [2N,N,]---7"

Figure 12.4.1. Storage arrangement of frequencies in the output H (fi, f2) of a two-dimensional FFT.
The input data is a two-dimensional Ny x No array h(t1,t2) (stored by rows of complex numbers).
The output is also stored by complex rows. Each row corresponds to a particular value of fi, as shown
in the figure. Within each row, the arrangement of frequencies £ is exactly as shown in Figure 12.2.2.
A and Ag are the sampling intervals in the 1 and 2 directions, respectively. The total number of (real)
array elements is N2. The program fourn can also do more than two dimensions, and the storage
arrangement generalizes in the obvious way.

product of the lengths of the L dimensions. It assumes that the array represents
an L-dimensional complex array, with individual components ordered as follows:
(i) each complex value occupies two sequential locations, real part followed by
imaginary; (ii) the first subscript changes least rapidly as one goes through the array;
the last subscript changes most rapidly (that is, “store by rows,” the C norm); (ili)
subscripts range from | to their maximum values (Nj, No,..., Nz, respectively),
rather than from 0 to. N; —1, No—1,..., Nz —1. Almost all failures to get fourn
to work result from improper understanding of the above ordering of the data array,
so take care! (Figure 12.4.1 illustrates the format of the output array.)

#include <math.h>
#define SWAP(a,b) tempr=(a) ; (a)=(b) ; (b)=tempr

void fourn(float data[], unsigned long nn[], int ndim, int isign)

Replaces data by its ndim-dimensional discrete Fourier transform, if isign is input as 1.
nn[1..ndim] is an integer array containing the lengths of each dimension (number of complex
values), which MUST all be powers of 2. data is a real array of length twice the product of
these lengths, in which the data are stored as in a multidimensional complex array: real and
imaginary parts of each element are in consecutive locations, and the rightmost index of the
array increases most rapidly as one proceeds along data. For a two-dimensional array, this is
equivalent to storing the array by rows. If isign is input as —1, data is replaced by its inverse
transform times the product of the lengths of all dimensions.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

524 Chapter 12. Fast Fourier Transform

int idim;

unsigned long i1,i12,i13,i2rev,i3rev,ip1,ip2,ip3,ifp1,ifp2;
unsigned long ibit,k1,k2,n,nprev,nrem,ntot;

float tempi,tempr;

double theta,wi,wpi,wpr,wr,wtemp; Double precision for trigonometric recur-
rences.
for (ntot=1,idim=1;idim<=ndim; idim++) Compute total number of complex val-
ntot *= nn[idim]; ues.
nprev=1;
for (idim=ndim;idim>=1;idim--) { Main loop over the dimensions.

n=nn[idim] ;
nrem=ntot/(n*nprev) ;
ipi=nprev << 1;

ip2=ip1*n;
ip3=ip2*nrem;
i2rev=1;
for (12=1;i2<=ip2;i2t+=ip1) { This is the bit-reversal section of the
if (i2 < i2rev) { routine.
for (i1=i2;i1<=i2tip1-2;i1+=2) {

p2) {

SWAP (data[i3] ,data[i3rev]) ;
SWAP (data [i3+1] ,data[i3rev+1]) ;

}
}
ibit=ip2 >> 1;
while (ibit ipl && i2rev > ibit) {
i2rev -= ibit;
ibit >>= 1;

}
i2rev += ibit;
}
ifpi=ip1; Here begins the Danielson-Lanczos sec-
while (ifpi < ip2) { tion of the routine.
ifp2=ifp1 << 1;
theta=isign*6 .28318530717959/ (ifp2/ip1) ; Initialize for the trig. recur-
wtemp=sin(0.5*theta) ; rence.
wpr = -2.0*wtemp*wtemp;
wpi=sin(theta) ;
wr=1.
wi=0.
for (i3=1;i3<=ifp1;i3+=ip1) {
for (i1=i13;i1<=i3+ip1-2;i1+=2) {
for (i2=i1;i2<=ip3;i2+=ifp2) {
k1=i2; Danielson-Lanczos formula:
kQ=ki+ifpt;
tempr=(float)wr*data[k2] -(float)witdata[k2+1] ;
tempi=(float)wr*data[k2+1]+(float) wixdata[k2] ;
data[k2]=data[k1]-tempr;
data [k2+1]=data[ki+1]-tempi;
data[ki] += tempr;
data[ki+1] += tempi;

+

wtemp=wr)*wpr-wi*wpitwr;  Trigonometric recurrence.
wi=wi*wpr+wtemp*wpitwi ;
}
ifpl=ifp2;
}

nprev *= n;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

12.5 Fourier Transforms of Real Data in Two and Three Dimensions 525

CITED REFERENCES AND FURTHER READING:

Nussbaumer, H.J. 1982, Fast Fourier Transform and Convolution Algorithms (New York: Springer-
Verlag).

12.5 Fourier Transforms of Real Data in Two
and Three Dimensions

Two-dimensional FFTs are particularly important in the field of image process-
ing. An image is usually represented as a two-dimensional array of pixel intensities,
real (and usually positive) numbers. One commonly desires to filter high, or low,
frequency spatial components from an image; or to convolve or deconvolve the
image with some instrumental point spread function. Use of the FFT is by far the
most efficient technique.

In three dimensions, a common use of the FFT is to solve Poisson’s equation
for a potential (e.g., electromagnetic or gravitational) on a three-dimensional lattice
that represents the discretization of three-dimensional space. Here the source terms
(mass or charge distribution) and the desired potentials are also real. In two and
three dimensions, with large arrays, memory is often at a premium. It is therefore
important to perform the FFTs, insofar as possible, on the data “in place.” We
want a routine with functionality similar to the multidimensional FFT routine fourn
(812.4), but which operates on real, not complex, input data. We give such a
routine in this section. The development is analogous to that of §12.3 leading to the
one-dimensional routine realft. (You might wish to review that material at this
point, particularly equation 12.3.5.)

It is convenient to think of the independent variables n1,...,nz in equation
(12.4.3) as representing an L-dimensional vector 7 in wave-number space, with
values on the lattice of integers. The transform H(nj,...,nz) is then denoted

It is easy to see that the transform H (7) is periodic in each of its L dimensions.
Specifically, if P,, Py, Ps, ... denote the vectors (Ni,0,0,...), (0,.N2,0,...),
(0,0,.N3,...), and so forth, then

H(a@+Pj) =H) j=l,...,L (12.5.1)

Equation (12.5.1) holds for any input data, real or complex. When the data is real,
we have the additional symmetry

A(-it) = H(7i)* (12.5.2)
Equations (12.5.1) and (12.5.2) imply that the full transform can be trivially obtained
from the subset of lattice values 7i that have

O0<m<N,-1

O<ng<N2-1

(12.5.3)
Ni

Osms>

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

526 Chapter 12. Fast Fourier Transform

In fact, this set of values is overcomplete, because there are additional symmetry
relations among the transform values that have n, = 0 and nz, = N,,/2. However
these symmetries are complicated and their use becomes extremely confusing.
Therefore, we will compute our FFT on the lattice subset of equation (12.5.3),
even though this requires a small amount of extra storage for the answer, i.e., the
transform is not quite “in place.” (Although an in-place transform is in fact possible,
we have found it virtually impossible to explain to any user how to unscramble its
output, i.e., where to find the real and imaginary components of the transform at
some particular frequency!)

We will implement the multidimensional real Fourier transform for the three
dimensional case L = 3, with the input data stored as a real, three-dimensional array
data[1..nn1] [1..nn2] [1..nn3]. This scheme will allow two-dimensional data
to be processed with effectively no loss of efficiency simply by choosing nn1 = 1.
(Note that it must be the first dimension that is set to 1.) The output spectrum comes
back packaged, logically at least, as a complex, three-dimensional array that we can
call SPEC[1. .nni] (1. .nn2] [1..nn3/2+1] (cf. equation 12.5.3). In the first two
of its three dimensions, the respective frequency values f1 or f2 are stored in wrap-
around order, that is with zero frequency in the first index value, the smallest positive
frequency in the second index value, the smallest negative frequency in the last index
value, and so on (cf. the discussion leading up to routines four1 and fourn). The
third of the three dimensions returns only the positive half of the frequency spectrum.
Figure 12.5.1 shows the logical storage scheme. The returned portion of the complex
output spectrum is shown as the unshaded part of the lower figure.

The physical, as opposed to logical, packaging of the output spectrum is neces-
sarily a bit different from the logical packaging, because C does not have a convenient,
portable mechanism for equivalencing real and complex arrays. The subscript range
SPEC[1..nn1] [1..nn2] [1..nn3/2] is returned in the input array data[1. .nn1]
(1. .nn2] [1..nn3], with the correspondence

Re(SPEC[i1] [12] [i3]) = data[i1] [12] [2*i3-1]

(12.5.4)
Im(SPEC[i1] [12] [i3]) = data[i1] [i2] [2*i3]

The remaining “plane” of values, SPEC[1. .nn1] (1. .nn2] [nn3/2+1], is returned
in the two-dimensional float array speq[1..nn1] [1..2*nn2],with the corre-
spondence

Re(SPEC[i1] [12] [nn3/2+1]) = speqli1] [2*i12-1]

(12.5.5)
Im(SPEC [i1] [42] [nn3/2+1]) = speq[i1] [2*i2]

Note that speq contains frequency components whose third component f3 is at
the Nyquist critical frequency +f,. In some applications these values will in fact
be ignored or set to zero, since they are intrinsically aliased between positive and
negative frequencies.

With this much introduction, the implementing procedure, called r1ft3, is
something of an anticlimax. Look in the innermost loop in the procedure, and you
will see equation (12.3.5) implemented on the last transform index. The case of
i3=1 is coded separately, to account for the fact that speq is to be filled instead of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.5 Fourier Transforms of Real Data in Two and Three Dimensions 527

nn2, 1 nn2, nn3

Input data array

float data[1..nn1][1..nn2][1..nn3]

Output spectrum
arrays (complex)

nn2,1 nn2,nn3/2
_ ry
9 a
FI x
1,1 1, nn3 . ~
a a
x a
g
a a
h =f : :
qd a
q 8
§ for
Ss a
in e &S £
2 es 2g 3
pay ~ =a fz}
gs E
29 3
1,1 f =0 1,nn3/2
h=-f
Figure 12.5.1. Input and output data arrangement for rlft3. All arrays shown are presumed

to have a first (leftmost) dimension of range [1..nn1], coming out of the page. The input data
array is a real, three-dimensional array data[1..nn1][1..nn2][1..nn3]. (For two-dimensional
data, one sets nni = 1.) The output data can be viewed as a single complex array with dimensions
(1..nn1] [1..nn2] [1..nn3/2+1] (cf. equation 12.5.3), corresponding to the frequency components
fi and fz being stored in wrap-around order, but only positive fg values being stored (others being
obtainable by symmetry). The output data is actually returned mostly in the input array data, but partly
stored in the real array speq[1..nn1][1..2*nn2]. See text for details.

“(eoueWY YON apis}no) Bio abpuquieo@ Masjsno}9auIp 0} jee PUaS JO ‘(AJUO BOVEWY YON) EZ7Z-Z28-008- I I/29 40 Woo" U'MMy/:d}Y

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

528 Chapter 12. Fast Fourier Transform

overwriting the input array of data. The three enclosing for loops (indices i2, i3,
and i1, from inside to outside) could in fact be done in any order — their actions all
commute. We chose the order shown because of the following considerations: (i) i3
should not be the inner loop, because if it is, then the recurrence relations on wr and
wi become burdensome. (ii) On virtual-memory machines, i1 should be the outer
loop, because (with C order of array storage) this results in the array data, which
might be very large, being accessed in block sequential order.

Note that the work done in r1ft3 is quite (logarithmically) small, compared to
the associated complex FFT, fourn. Since C does not have a convenient complex
type, the operations are carried out explicitly below in terms of real and imaginary
parts. The routine r1ft3 is based on an earlier routine by G.B. Rybicki.

#include <math.h>

void rlft3(float ***data, float **speq, unsigned long nni, unsigned long nn2,

unsigned long nn3, int isign)
Given a three-dimensional real array data[1..nn1][1..nn2][1..nn3] (where nni = 1 for
the case of a logically two-dimensional array), this routine returns (for isign=1) the complex
fast Fourier transform as two complex arrays: On output, data contains the zero and positive
frequency values of the third frequency component, while speq[1. .nn1] [1. .2*nn2] contains
the Nyquist critical frequency values of the third frequency component. First (and second)
frequency components are stored for zero, positive, and negative frequencies, in standard wrap-
around order. See text for description of how complex values are arranged. For isign=-1, the
inverse transform (times nni*nn2*nn3/2 as a constant multiplicative factor) is performed,
with output data (viewed as a real array) deriving from input data (viewed as complex) and
speq. For inverse transforms on data not generated first by a forward transform, make sure
the complex input data array satisfies property (12.5.2). The dimensions nni, nn2, nn3 must
always be integer powers of 2.
{

void fourn(float data[], unsigned long nn[], int ndim, int isign);

void nrerror(char error_text[]);

unsigned long i1,i2,i3,j1,j2,j3,nn[4] ,ii3;

double theta,wi,wpi,wpr,wr,wtemp;

float c1,c2,hir,hli,h2r,h2i;

if (1+&data[nn1] [nn2] [nn3]-&data[1] [1] [1] != nni+*nn2*nn3)
nrerror("rlft3: problem with dimensions or contiguity of data array\n");

c1=0.5;
c2 = -0.5*isign;
thet: sign*(6.28318530717959/nn3) ;

wtemp=sin(0.5*theta) ;

wpr = -2.0*wtemp*wtemp;
wpi=sin(theta) ;
nn[1]=nn1;
nn[2]=nn2;
nn[3]=nn3 >> 1;
if (isign == 1) { Case of forward transform.
fourn(&data[1] [1] [1]-1,nn,3,isign) ; Here is where most all of the com-
for (i1=1;il<=nn1;i1++) pute time is spent.
for (i2=1,j2=0;i2<=nn2;i2t++) { Extend data periodically into speq.
speq[i1] [++j2]=data[i1] [i2] [1];
speq[i1] [++j2]=data[i1] [i2] [2];
}

for (i1=1;i1<=nni;ii++) {
ji=(i1 '!= 1 7 mni-i1+2 : 1);
Zero frequency is its own reflection, otherwise locate corresponding negative frequency
in wrap-around order.
wr=1.0; Initialize trigonometric recurrence.
wi=0.0;
for (113=1,13=1;i3<=(nn3>>2)+1;i3++,ii3+=2) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.5 Fourier Transforms of Real Data in Two and Three Dimensions 529

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

Figure 12.5.2. (a) A two-dimensional image with intensities either purely black or purely white. (b) The
same image, after it has been low-pass filtered using r1ft3. Regions with fine-scale features become gray.

for (i2=1;i2<=nn2;i2t++) {

if (43 == 1) { Equation (12.3.5).
j2=(i2 '= 1 ? ((mn2-i2)<<1)+3 : 1);
hir=c1*(data[i1] [12] [1]+speq[j1][j2]);
hii=c1*(data[it] [42] [2]-speqlj1] [j2+1]);
h2i=c2*(data[i1] [i2] [1]-speq[j1][j2]);
h2r= -c2*(data[i1] [12] [2]+speq[j1] [j2+1]);
data[it1] [i2] (1]=hir+h2r;
data[it1] [i2] [2]=h1i+h2i;
speq(ji1] [j2]=hir-h2r;
speq[ji] [j2+1]=h2i-h1i;

} else {
j2=(i2 != 1 ? nn2-i2+2 : 1);
j3=nn3+3-(i3<<1);
hir=ci*(data[i1] [12] [113] +data[j1] [j2] [j3]);
hii=c1*(data[it1] [12] [ii3+1]-data[j1] [j2] [(j3+1]);
h2i=c2*(data[i1] [i2] [113] -data[j1] [j2] [j3]);
h2r= -c2«(data[i1] [i2] [ii3+1]+data[j1] [j2] [(j3+1]);
data[i1] [12] [113] =hirt+twr*h2r-wi*h2i;
data[it1] [42] [ii3+1]=h1itwr*h2i+wi*h2r ;
data[j1) (j2] (j3] =hir-wr*h2r+wi*h2i;
data(j1] [j2] [(j3+1]= -hiitwr*h2i+wi*h2r;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

3
3
wr=(wtemp=wr) *wpr-witwpitwr ; Do the recurrence.
wi=wi*wpr+wtemp*wpi+wi;
}
+
if (isign == -1) Case of reverse transform.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

fourn(&data[1] [1] [1]-1,nn,3,isign) ;

We now give some fragments from notional calling programs, to clarify the use
of r1£t3 for two- and three-dimensional data. Note again that the routine does not
actually distinguish between two and three dimensions; two is treated like three, but
with the first dimension having length 1. Since the first dimension is the outer loop,
virtually no inefficiency is introduced.

The first program fragment FFTs a two-dimensional data array, allows for some

530 Chapter 12. Fast Fourier Transform

processing on it, e.g., filtering, and then takes the inverse transform. Figure 12.5.2
shows an example of the use of this kind of code: A sharp image becomes blurry
when its high-frequency spatial components are suppressed by the factor (here)
max (1 — 6f?/f?,0). The second program example illustrates a three-dimensional
transform, where the three dimensions have different lengths. The third program
example is an example of convolution, as it might occur in a program to compute the
potential generated by a three-dimensional distribution of sources.

#include <stdlib.h>
#include "nrutil.h"
#define N2 256

#define N3 256 Note that the first component must be set to 1.
int main(void) /* example1 */

This fragment shows how one might filter a 256 by 256 digital image.

{

void rlft3(float ***data, float **speq, unsigned long nn1,
unsigned long nn2, unsigned long nn3, int isign);
float ***data, **speq;

data=f3tensor(1,1,1,N2,1,N3);
speq=matrix(1,1,1,2*N2) ;

/* 7A Here the image would be loaded into data.
rlft3(data,speq,1,N2,N3,1);

/* 7A Here the arrays data and speq would be multiplied by a
rlft3(data,speq,1,N2,N3,-1); suitable filter function (of frequency).

/* oe k/ Here the filtered image would be unloaded from data.

free_matrix(speq,1,1,1,2*N2);
free_f3tensor(data,1,1,1,N2,1,N3);
return 0;

}

#define N1 32
#define N2 64
#define N3 16

int main(void) /* example2 */
This fragment shows how one might FFT a real three-dimensional array of size 32 by 64 by 16.
t
void rlft3(float ***data, float **speq, unsigned long nn1,
unsigned long nn2, unsigned long nn3, int isign);
int j;
float ***data,**speq;

data=f3tensor(1,N1,1,N2,1,N3);
speq=matrix(1,N1,1,2*N2) ;

/* oe Here load data.
rlft3(data,speq,N1,N2,N3,1);
/* 7A Here unload data and speq.

free_matrix(speq,1,N1,1,2*N2) ;
free_f3tensor(data,1,N1,1,N2,1,N3);
return 0;

}
#define N 32

int main(void) /* example3 */
This fragment shows how one might convolve two real, three-dimensional arrays of size 32 by
32 by 32, replacing the first array by the result.
t
void rlft3(float ***data, float **speq, unsigned long nn1,
unsigned long nn2, unsigned long nn3, int isign);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.5 Fourier Transforms of Real Data in Two and Three Dimensions 531

int j;
float fac,r,i,***datal,***data2,**speq1,**speq2,*sp1,*sp2;

/* oe

rlft3(datal,speq1,N,N,N,1); FFT both input arrays.

rlft3(data2,speq2,N,N,N,1);

fac=2.0/(N*N#N) ; Factor needed to get normalized inverse.

spi = &datat([1] [1] [1];

sp2 = &data2[1] [1] [1];

for (j=1;j<=N*N*N/2; j++) { Note how this can be made a single for-loop in-
rv = sp1[0]*sp2[0] - spi[1]*sp2(1]; stead of three nested ones by using
i = sp1[0]*sp2[1] + spi[1]*sp2[0]; the pointers spi and sp2.
sp1(0] = fac*r;
spi[1] = fac*i;
spl += 2;
sp2 += 2;

+
spl = &speqi [1] [1];
sp2 = &speq2(1] [1];
for (j=1;j<=N*N; j++) {
rv = sp1[0]*sp2[0] - spi[1]*sp2(1];
i = spi1[0]*sp2[1] + sp1[1]*sp2[0] ;
sp1(0] = fac*r;
spi[1] = fac*i;
spl += 2;
sp2 += 2;
+
rlft3(datal,speq1,N,N,N,-1); Inverse FFT the product of the two FFTs.
/* oe
free_matrix(speq2,1,
free_matrix(speqi,1,
free_f3tensor(data2,
free_f3tensor(datal,
return 0;

rPRES

To extend r1ft3 to four dimensions, you simply add an additional (outer) nested
for loop in i0, analogous to the present i1. (Modifying the routine to do an arbitrary
number of dimensions, as in fourn, is a good programming exercise for the reader.)

CITED REFERENCES AND FURTHER READING:
Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall).
Swartztrauber, P. N. 1986, Mathematics of Computation, vol. 47, pp. 323-346.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

532 Chapter 12. Fast Fourier Transform

12.6 External Storage or Memory-Local FFTs

Sometime in your life, you might have to compute the Fourier transform of a really
large data set, larger than the size of your computer’s physical memory. In such a case,
the data will be stored on some external medium, such as magnetic or optical tape or disk.
Needed is an algorithm that makes some manageable number of sequential passes through
the external data, processing it on the fly and outputting intermediate results to other external
media, which can be read on subsequent passes.

In fact, an algorithm of just this description was developed by Singleton [1] very soon
after the discovery of the FFT. The algorithm requires four sequential storage devices, each
capable of holding half of the input data. The first half of the input data is initially on one
device, the second half on another.

Singleton’s algorithm is based on the observation that it is possible to bit-reverse QM
values by the following sequence of operations: On the first pass, values are read alternately
from the two input devices, and written to a single output device (until it holds half the data),
and then to the other output device. On the second pass, the output devices become input
devices, and vice versa. Now, we copy two values from the first device, then two values
from the second, writing them (as before) first to fill one output device, then to fill a second.
Subsequent passes read 4, 8, etc., input values at a time. After completion of pass M — 1,
the data are in bit-reverse order.

Singleton’s next observation is that it is possible to alternate the passes of essentially
this bit-reversal technique with passes that implement one stage of the Danielson-Lanczos
combination formula (12.2.3). The scheme, roughly, is this: One starts as before with half
the input data on one device, half on another. In the first pass, one complex value is read
from each input device. Two combinations are formed, and one is written to each of two
output devices. After this “computing” pass, the devices are rewound, and a “permutation”
pass is performed, where groups of values are read from the first input device and alternately
written to the first and second output devices; when the first input device is exhausted, the
second is similarly processed. This sequence of computing and permutation passes is repeated
M — K — 1 times, where 2” is the size of internal buffer available to the program. The
second phase of the computation consists of a final K computation passes. What distinguishes
the second phase from the first is that, now, the permutations are local enough to do in place
during the computation. There are thus no separate permutation passes in the second phase.
In all, there are 24 — K — 2 passes through the data.

Here is an implementation of Singleton’s algorithm, based on [1]:

#include <stdio.h>
#include <math.h>
#include "nrutil.h"
#define KBF 128

void fourfs(FILE *file[5], unsigned long nn[], int ndim, int isign)
One- or multi-dimensional Fourier transform of a large data set stored on external media. On
input, ndim is the number of dimensions, and nn[1..ndim] contains the lengths of each di-
mension (number of real and imaginary value pairs), which must be powers of two. file[1..4]
contains the stream pointers to 4 temporary files, each large enough to hold half of the data.
The four streams must be opened in the system's “binary” (as opposed to “text”) mode. The
input data must be in C normal order, with its first half stored in file file[1], its second
half in f£ile[2], in native floating point form. KBF real numbers are processed per buffered
read or write. isign should be set to 1 for the Fourier transform, to —1 for its inverse. On
output, values in the array file may have been permuted; the first half of the result is stored in
file[3], the second half in file[4]. N.B.: For ndim > 1, the output is stored by columns,
ie., not in C normal order; in other words, the output is the transpose of that which would have
been produced by routine fourn.
{

void fourew(FILE *file(5], int *na, int *nb, int *nc, int *nd);

unsigned long j,j12,jk,k,kk,n=1,mm,kc=0,kd,ks,kr,nr,ns,nv;

int cc,na,nb,nc,nd;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.6 External Storage or Memory-Local FFTs 533

float tempr,tempi,*afa,*afb,*afc;
double wr,wi,wpr,wpi,wtemp,theta;
static int mate[5] = {0,2,1,4,3};

afa=vector(1,KBF) ;
afb=vector(1,KBF) ;
afc=vector(1,KBF) ;
for (j=1;j<=ndim;j++) {
n *= nn[j];
if (nn[j] <= 1) nrerror("invalid float or wrong ndim in fourfs");

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

nr=ns >> 1;

kd=KBF >> 1;

ks=n;

fourew(file,&na,&nb,&nc, &nd) ;

The first phase of the transform starts here.

for (;;) { Start of the computing pass.
theta=isign*3.141592653589793/ (n/mm) ;
wtemp=sin(0.5+theta) ;

wpr = -2.0*wtemp*wtemp;
wpi=sin(theta) ;
wr=1.0;
wi=0.0;
mm >>= 1;
for (j12=1;j12<=2;j12++) {
kr=0;
do {
cc=fread(afa[1] , sizeof (float) ,KBF,file[na]);
if (cc != KBF) nrerror("read error in fourfs");
cc=fread(&afb[1] ,sizeof (float) ,KBF,file[nb] ) ;
if (cc != KBF) nrerror("read error in fourfs");

for (j=1;j<=KBF;j+=2) {
tempr=((float)wr)*afb[j]-( (float) wi) *afb[j+1] ;
tempi=((float)wi)*afb[j]+( (float) wr) *afb[j+1] ;
afb[j]=afa[j]-tempr;
afa[j] += tempr;
afb[j+1]=afa[j+1]-tempi;
afa[j+1] += tempi;

mn) {

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

wtemp=wr) *wpr-wi*wpitwr ;
wi=wi*wprtwtemp*wpitwi ;

cc=fwrite(&afa[1i] ,sizeof (float) ,KBF,file[nc]);
if (cc != KBF) nrerror("write error in fourfs");
cc=fwrite(&afb[1] , sizeof (float) ,KBF,file[nd]);
if (cc != KBF) nrerror("write error in fourfs");
} while (++kr < nr);
if (j12 == 1 && ks != n && ks == KBF) {
na=mate [na] ;

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

nb=na;
3
if (mr == 0) break;
}
fourew(file,&na,&nb,&nc,&nd) ; Start of the permutation pass.
jk >>= 1;

while (jk == 1) {
mm=n;

534

Chapter 12. Fast Fourier Transform

+

jk=nn[+tnv] ;
}
ks >>= 1;
if (ks > KBF) {

for (j12=1;j12<=2;j12++) {

for (kr=1;kr<=ns;kr+=ks/KBF) {
for (k=1;k<=ks;k+=KBF) {
cec=fread(&afa[1] ,sizeof (float) ,KBF,file[na]);

if (cc != KBF) nrerror("read error in fourfs");
cec=fwrite(&%afa[1] ,sizeof (float) ,KBF,file[nc]);
if (cc != KBF) nrerror("write error in fourfs");

+
nc=mate[nc] ;
}
na=mate [na] ;
3
fourew(file,&na,&nb,&nc,&nd) ;
} else if (ks == KBF) nb=na;
else break;

j=1;

The second phase of the transform starts here. Now, the remaining permutations are suf-
ficiently local to be done in place.

for G3) f

theta=isign*3.141592653589793/ (n/mm) ;
wtemp=sin(0.5+theta) ;

wpr = -2.0*wtemp*wtemp;
wpi=sin(theta) ;

wr=1.0;

wi=0.0

mm >>= 1;

ks=kd;

kd >>= 1;

for (j12=1;j12<=2;j12++) {
for (kr=1;kr<=ns;kr++) {
cc=fread(&afc[1] ,sizeof (float) ,KBF,file[na]) ;
if (cc != KBF) nrerror("read error in fourfs");
kk=1;
k=ks+1;
for (;;) {
tempr=((float)wr) *afc[kktks]-((float)wi)*afc[kktks+1] ;
tempi=( (float) wi) *afc[kktks]+( (float) wr) *afc [kk+ks+1] ;
afa[j]=afc[kk]+tempr;
afb[j]=afc[kk]-tempr;
afa[++j]=afc[++kk]+tempi;
afb[j++]=afc[kk++]-tempi;
if (kk < k) continue;
ke += kd;
if (ke
kc=0;
wr=(wtemp=wr) *wpr-wi*wpitwr;
wi=wixwprt+wtemp*wpitwi ;

mm) {

3
kk += ks;
if (kk > KBF) break;
else k=kk+ks;
}
if (j > KBF) {
cc=fwrite(&afa[1] ,sizeof (float) ,KBF,file[nc]);

if (cc != KBF) nrerror("write error in fourfs");
cc=fwrite(&%afb[1] ,sizeof (float) ,KBF,file[nd]) ;
if (cc != KBF) nrerror("write error in fourfs");
j=t;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

12.6 External Storage or Memory-Local FFTs 535

3
na=mate [na] ;
}
fourew(file,&na,&nb,&nc, &nd) ;
jk >>= 1;
if (jk > 1) continue;
mm=n;
do {
if (nv < ndim) jk=nn[++nv];
else {

free_vector(afc,1,KBF) ;
free_vector(afb,1,KBF) ;
free_vector(afa,1,KBF) ;
return;
3
} while (jk == 1);

#include <stdio.h>
#define SWAP(a,b) ftemp=(a) ; (a)=(b) ; (b)=ftemp

void fourew(FILE *file[5], int *na, int *nb, int *nc, int *nd)
Utility used by fourfs. Rewinds and renumbers the four files.

t
int i;
FILE *ftemp;
for (i=1;i<=4;i++) rewind(file[i]);
SWAP(file([2] ,file[4]);
SWAP(file[1] ,file[3]);
xna=3;
}

For one-dimensional data, Singleton’s algorithm produces output in exactly the same
order as a standard FFT (e.g.,four1). For multidimensional data, the output is the transpose of
the conventional arrangement (e.g., the output of fourn). This peculiarity, which is intrinsic to
the method, is generally only a minor inconvenience. For convolutions, one simply computes
the component-by-component product of two transforms in their nonstandard arrangement,
and then does an inverse transform on the result. Note that, if the lengths of the different
dimensions are not all the same, then you must reverse the order of the values in nn[1. .ndim]
(thus giving the transpose dimensions) before performing the inverse transform. Note also
that, just like fourn, performing a transform and then an inverse results in multiplying the
original data by the product of the lengths of all dimensions.

We leave it as an exercise for the reader to figure out how to reorder fourfs’s output
into normal order, taking additional passes through the externally stored data. We doubt that
such reordering is ever really needed.

You will likely want to modify fourfs to fit your particular application. For example,
as written, KBF = 2 plays the dual role of being the size of the internal buffers, and the
record size of the unformatted reads and writes. The latter role limits its size to that allowed
by your machine’s I/O facility. It is a simple matter to perform multiple reads for a much
larger KBF, thus reducing the number of passes by a few.

Another modification of fourfs would be for the case where your virtual memory
machine has sufficient address space, but not sufficient physical memory, to do an efficient
FFT by the conventional algorithm (whose memory references are extremely nonlocal). In
that case, you will need to replace the reads, writes, and rewinds by mappings of the arrays

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

536 Chapter 12. Fast Fourier Transform

afa, afb, and afc into your address space. In other words, these arrays are replaced by
references to a single data array, with offsets that get modified wherever fourfs performs an
I/O operation. The resulting algorithm will have its memory references local within blocks
of size KBF. Execution speed is thereby sometimes increased enormously, albeit at the cost
of requiring twice as much virtual memory as an in-place FFT.

CITED REFERENCES AND FURTHER READING:

Singleton, R.C. 1967, IEEE Transactions on Audio and Electroacoustics, vol. AU-15, pp. 91-97.
(1)

Oppenheim, A.V., and Schafer, R.W. 1989, Discrete-Time Signal Processing (Englewood Cliffs,
NJ: Prentice-Hall), Chapter 9.

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


Chapter 13. Fourier and Spectral
Applications

13.0 Introduction

Fourier methods have revolutionized fields of science and engineering, from
radio astronomy to medical imaging, from seismology to spectroscopy. In this
chapter, we present some of the basic applications of Fourier and spectral methods
that have made these revolutions possible.

Say the word “Fourier” to a numericist, and the response, as if by Pavlovian
conditioning, will likely be “FFT.” Indeed, the wide application of Fourier methods
must be credited principally to the existence of the fast Fourier transform. Better
mousetraps stand aside: If you speed up any nontrivial algorithm by a factor of a
million or so, the world will beat a path towards finding useful applications for it.
The most direct applications of the FFT are to the convolution or deconvolution of
data (§13.1), correlation and autocorrelation (§13.2), optimal filtering (§13.3), power
spectrum estimation (§13.4), and the computation of Fourier integrals (§13.9).

As important as they are, however, FFT methods are not the be-all and end-all
of spectral analysis. Section 13.5 is a brief introduction to the field of time-domain
digital filters. In the spectral domain, one limitation of the FFT is that it always
represents a function’s Fourier transform as a polynomial in z = exp(2mifA)
(cf. equation 12.1.7). Sometimes, processes have spectra whose shapes are not
well represented by this form. An alternative form, which allows the spectrum to
have poles in z, is used in the techniques of linear prediction (§13.6) and maximum
entropy spectral estimation (§13.7).

Another significant limitation of all FFT methods is that they require the input
data to be sampled at evenly spaced intervals. For irregularly or incompletely
sampled data, other (albeit slower) methods are available, as discussed in §13.8.

So-called wavelet methods inhabit a representation of function space that is
neither in the temporal, nor in the spectral, domain, but rather something in-between.
Section 13.10 is an introduction to this subject. Finally §13.11 is an excursion into
numerical use of the Fourier sampling theorem.

537

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

538 Chapter 13. Fourier and Spectral Applications

13.1 Convolution and Deconvolution Using
the FFT

We have defined the convolution of two functions for the continuous case in
equation (12.0.8), and have given the convolution theorem as equation (12.0.9). The
theorem says that the Fourier transform of the convolution of two functions is equal
to the product of their individual Fourier transforms. Now, we want to deal with
the discrete case. We will mention first the context in which convolution is a useful
procedure, and then discuss how to compute it efficiently using the FFT.

The convolution of two functions r(t) and s(t), denoted r « s, is mathematically
equal to their convolution in the opposite order, s * 7. Nevertheless, in most
applications the two functions have quite different meanings and characters. One of
the functions, say s, is typically a signal or data stream, which goes on indefinitely
in time (or in whatever the appropriate independent variable may be). The other
function r is a “response function,” typically a peaked function that falls to zero in
both directions from its maximum. The effect of convolution is to smear the signal
s(t) in time according to the recipe provided by the response function r(t), as shown
in Figure 13.1.1. In particular, a spike or delta-function of unit area in s which occurs
at some time to is supposed to be smeared into the shape of the response function
itself, but translated from time 0 to time to as r(t — to).

In the discrete case, the signal s(t) is represented by its sampled values at equal
time intervals s;. The response function is also a discrete set of numbers r;,, with the
following interpretation: ro tells what multiple of the input signal in one channel (one
particular value of j) is copied into the identical output channel (same value of 7);
r, tells what multiple of input signal in channel 7 is additionally copied into output
channel j + 1; r_, tells the multiple that is copied into channel j — 1; and so on for
both positive and negative values of k in r;,. Figure 13.1.2 illustrates the situation.

Example: a response function with ro = 1 and all other rx’s equal to zero
is just the identity filter: convolution of a signal with this response function gives
identically the signal. Another example is the response function with r 14 = 1.5 and
all other r;,’s equal to zero. This produces convolved output that is the input signal
multiplied by 1.5 and delayed by 14 sample intervals.

Evidently, we have just described in words the following definition of discrete
convolution with a response function of finite duration M:

M/2

(r*s); = > Sj—k Tk (13.1.1)

k=—M/241

If a discrete response function is nonzero only in some range —M/2 < k < M/2,
where I is a sufficiently large even integer, then the response function is called a
finite impulse response (FIR), and its duration is 7. (Notice that we are defining
as the number of nonzero values of r;,; these values span a time interval of M — 1
sampling times.) In most practical circumstances the case of finite 1/ is the case of
interest, either because the response really has a finite duration, or because we choose
to truncate it at some point and approximate it by a finite-duration response function.

The discrete convolution theorem is this: If a signal s; is periodic with period
N, so that it is completely determined by the N values so,...,5y_1, then its

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.1 Convolution and Deconvolution Using the FFT 539

s(t
(t) i
r(t JAIN
() i
res(t
0) i
Figure 13.1.1. Example of the convolution of two functions. A signal s(t) is convolved with a

response function r(t). Since the response function is broader than some features in the original signal,
these are “washed out” in the convolution. In the absence of any additional noise, the process can be
reversed by deconvolution.

jo tow. LIE...

°
Zz
I

byl

-1

Figure 13.1.2. Convolution of discretely sampled functions. Note how the response function for negative
times is wrapped around and stored at the extreme right end of the array n,.

LUE PUAS JO ‘(KJUO BOVAWY YON) EZP2-ZZ8-008-|. 129 10 WOOJU-MMMy/: diay

*(eoveWY YON episyno) Bio‘eBpuquied® as|sno}oauIp 0} |

alqepeor

‘SIIOUD 10 Syoog sadioay jeoeLUNN Jepso Of “payiqiyoud Ajjouys si ‘ueyndwod JeAses Aue 0} (auO si) Bulpnjoul) si

-aulyoew jo BulAdoo Aue Jo ‘uojonpoides sayjin4 “asn euossed UMO 4194} 10} Adoo seded auo eyeW 0} suasn yeUsE}U! 10} payUesH

aysqem is

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

540 Chapter 13. Fourier and Spectral Applications

discrete convolution with a response function of finite duration NV is a member of
the discrete Fourier transform pair,

N/2
ee (13.1.2)
k=—N/241
Here S;,, (n = 0,...,N —1) is the discrete Fourier transform of the values
sj, (j = 0,...,N —1), while R,, (n = 0,...,N — 1) is the discrete Fourier
transform of the values r;,, (k = 0,...,.N —1). These values of r;, are the same

ones as for the range k = —N/2+1,.
as was described at the end of §12.2.

., N/2, but in wrap-around order, exactly

Treatment of End Effects by Zero Padding

The discrete convolution theorem presumes a set of two circumstances that
are not universal. First, it assumes that the input signal is periodic, whereas real
data often either go forever without repetition or else consist of one nonperiodic
stretch of finite length. Second, the convolution theorem takes the duration of the
response to be the same as the period of the data; they are both NV. We need to
work around these two constraints.

The second is very straightforward. Almost always, one is interested in a
response function whose duration M is much shorter than the length of the data
set N. In this case, you simply extend the response function to length N by
padding it with zeros, ie., define r, = 0 for M/2 < k < N/2 and also for
—N/24+1<k <—M/2+ 1. Dealing with the first constraint is more challenging.
Since the convolution theorem rashly assumes that the data are periodic, it will
falsely “pollute” the first output channel (7 * s)g with some wrapped-around data
from the far end of the data stream sy_1,5y_2, etc. (See Figure 13.1.3.) So,
we need to set up a buffer zone of zero-padded values at the end of the s ; vector,
in order to make this pollution zero. How many zero values do we need in this
buffer? Exactly as many as the most negative index for which the response function
is nonzero. For example, if r_3 is nonzero, while r_4,r_s5,... are all zero, then we
need three zero pads at the end of the data: sj)—3 = sy_2 = sn_1 = 0. These
zeros will protect the first output channel (r * s)9 from wrap-around pollution. It
should be obvious that the second output channel (r * s) 1 and subsequent ones will
also be protected by these same zeros. Let A denote the number of padding zeros,
so that the last actual input data point is sjy_—K_1.

What now about pollution of the very last output channel? Since the data
now end with s y—«-—1, the last output channel of interest is (r * s) v_«—1. This
channel can be polluted by wrap-around from input channel s 9 unless the number
XX is also large enough to take care of the most positive index k for which the
response function r;, is nonzero. For example, if ro through rg are nonzero, while
r7,Tg... are all zero, then we need at least K = 6 padding zeros at the end of the
data: sy_¢6 =... = Sny-1 = 0.

To summarize — we need to pad the data with a number of zeros on one
end equal to the maximum positive duration or maximum negative duration of the
response function, whichever is larger. (For a symmetric response function of
duration /, you will need only 14/2 zero pads.) Combining this operation with the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.1 Convolution and Deconvolution Using the FFT 541

— response function ——._,

<—m,—> <— m_—>|

| sample of original function

| convolution |

spoiled < unspoiled > spoiled

Figure 13.1.3. The wrap-around problem in convolving finite segments of a function. Not only must
the response function wrap be viewed as cyclic, but so must the sampled original function. Therefore
a portion at each end of the original function is erroneously wrapped around by convolution with the
response function.

™ response function —_——,

<— m,—> << m._—>

original function zero padding

jk m_———> <m,—|

not spoiled because zero

<—m | +—"——

| TT
unspoiled >| k spoiled >|
but irrelevant

Figure 13.1.4. Zero padding as solution to the wrap-around problem. The original function is extended
by zeros, serving a dual purpose: When the zeros wrap around, they do not disturb the true convolution;
and while the original function wraps around onto the zero region, that region can be discarded.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

yealur) eBpuqued Aq 2661-8861 (9) 1yBUAdoD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aIEMYJOS sadioay jeoUOWNN Aq Z661-886l (D) JUBUAdOD swesBOlg “SSAlq

542 Chapter 13. Fourier and Spectral Applications

padding of the response r;, described above, we effectively insulate the data from
artifacts of undesired periodicity. Figure 13.1.4 illustrates matters.

Use of FFT for Convolution

The data, complete with zero padding, are now a set of real numbers sj, j =
0,...,.N — 1, and the response function is zero padded out to duration N and
arranged in wrap-around order. (Generally this means that a large contiguous section
of the r;,’s, in the middle of that array, is zero, with nonzero values clustered at
the two extreme ends of the array.) You now compute the discrete convolution as
follows: Use the FFT algorithm to compute the discrete Fourier transform of s and of
r. Multiply the two transforms together component by component, remembering that
the transforms consist of complex numbers. Then use the FFT algorithm to take the
inverse discrete Fourier transform of the products. The answer is the convolution r« s.

What about deconvolution? Deconvolution is the process of undoing the
smearing in a data set that has occurred under the influence of a known response
function, for example, because of the known effect of a less-than-perfect measuring
apparatus. The defining equation of deconvolution is the same as that for convolution,
namely (13.1.1), except now the left-hand side is taken to be known, and (13.1.1) is
to be considered as a set of NV linear equations for the unknown quantities s ;. Solving
these simultaneous linear equations in the time domain of (13.1.1) is unrealistic in
most cases, but the FFT renders the problem almost trivial. Instead of multiplying
the transform of the signal and response to get the transform of the convolution, we
just divide the transform of the (known) convolution by the transform of the response
to get the transform of the deconvolved signal.

This procedure can go wrong mathematically if the transform of the response
function is exactly zero for some value R,,, so that we can’t divide by it. This
indicates that the original convolution has truly lost all information at that one
frequency, so that a reconstruction of that frequency component is not possible.
You should be aware, however, that apart from mathematical problems, the process
of deconvolution has other practical shortcomings. The process is generally quite
sensitive to noise in the input data, and to the accuracy to which the response function
rz is known. Perfectly reasonable attempts at deconvolution can sometimes produce
nonsense for these reasons. In such cases you may want to make use of the additional
process of optimal filtering, which is discussed in §13.3.

Here is our routine for convolution and deconvolution, using the FFT as
implemented in four1 of §12.2. Since the data and response functions are real,
not complex, both of their transforms can be taken simultaneously using twofft.
Note, however, that two calls to realft should be substituted if data and respns
have very different magnitudes, to minimize roundoff. The data are assumed to be
stored in a float array data[1..n], with n an integer power of two. The response
function is assumed to be stored in wrap-around order in a sub-array respns [1. .m]
of the array respns[1..n]. The value of m can be any odd integer less than or equal
to n, since the first thing the program does is to recopy the response function into the
appropriate wrap-around order in respns [1. .n]. The answer is provided in ans.

#include "nrutil.h"

void convlv(float data[], unsigned long n, float respns[], unsigned long m,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.1 Convolution and Deconvolution Using the FFT 543

int isign, float ans[])

Convolves or deconvolves a real data set data[1. .n] (including any user-supplied zero padding)
with a response function respns[1..n]. The response function must be stored in wrap-around
order in the first m elements of respns, where m is an odd integer <n. Wrap-around order
means that the first half of the array respns contains the impulse response function at positive
times, while the second half of the array contains the impulse response function at negative times,
counting down from the highest element respns[m]. On input isign is +1 for convolution,
—1 for deconvolution. The answer is returned in the first mn components of ans. However,
ans must be supplied in the calling program with dimensions [1..2*n], for consistency with
twofft. n MUST be an integer power of two.

{
void realft(float data[], unsigned long n, int isign);
void twofft(float datai{], float data2[], float ffti[], float fft2[],
unsigned long n);
unsigned long i,no2;
float dum,mag2,*fft;
fft=vector(1,n<<1);
for (i=1;i<=(m-1)/2;i++) Put respns in array of length n.
respns [n+1-: espns [m+1-i] ;
for (i=(m+3) /2;i<=n-(m-1)/2;it+) Pad with zeros.
respns [i]=0.0;
twofft (data,respns,fft,ans,n) ; FFT both at once.
no2=n>>1;
for (i= =nt+2;i+=2) {
if (isign == 1) {
ans [i-1]=(££t [i-1] * (dum=ans [i-1] )-fft [i] *ans[i])/no2; Multiply FFTs
ans [i] ft [i] *dum+fft [i-1] *ans[i]) /no2; to convolve.
} else if (isign == -1) {
if ((mag2=SQR(ans[i-1])+SQR(ans[i])) == 0.0)
nrerror("Deconvolving at response zero in convlv");
ans [i-1]=(£ft [i-1] * (dum=ans [i-1] )+f£ft [i] *ans [i] ) /mag2/no2; Divide FFTs
ans [i]=(fft [i] *dum-fft [i-1] *ans [i] ) /mag2/no2; to deconvolve.
} else nrerror("No meaning for isign in convlv");
+
ans [2]=ans[n+1] ; Pack last element with first for realft.
realft(ans,n,-1); Inverse transform back to time domain.
free_vector(fft,1,n<<1);
}

Convolving or Deconvolving Very Large Data Sets

If your data set is so long that you do not want to fit it into memory all at
once, then you must break it up into sections and convolve each section separately.
Now, however, the treatment of end effects is a bit different. You have to worry
not only about spurious wrap-around effects, but also about the fact that the ends of
each section of data should have been influenced by data at the nearby ends of the
immediately preceding and following sections of data, but were not so influenced
since only one section of data is in the machine at a time.

There are two, related, standard solutions to this problem. Both are fairly
obvious, so with a few words of description here, you ought to be able to implement
them for yourself. The first solution is called the overlap-save method. In this
technique you pad only the very beginning of the data with enough zeros to avoid
wrap-around pollution. After this initial padding, you forget about zero padding
altogether. Bring in a section of data and convolve or deconvolve it. Then throw out
the points at each end that are polluted by wrap-around end effects. Output only the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

544 Chapter 13. Fourier and Spectral Applications

|
|
|
|
|
|
|
|
|
|
|
A yj A+B) yBy Bec 4 ¥ Cc

{ | convolution (out)

Figure 13.1.5. The overlap-add method for convolving a response with a very long signal. The signal
data is broken up into smaller pieces. Each is zero padded at both ends and convolved (denoted by
bold arrows in the figure). Finally the pieces are added back together, including the overlapping regions
formed by the zero pads.

remaining good points in the middle. Now bring in the next section of data, but not
all new data. The first points in each new section overlap the last points from the
preceding section of data. The sections must be overlapped sufficiently so that the
polluted output points at the end of one section are recomputed as the first of the
unpolluted output points from the subsequent section. With a bit of thought you can
easily determine how many points to overlap and save.

The second solution, called the overlap-add method, is illustrated in Figure
13.1.5. Here you don’t overlap the input data. Each section of data is disjoint from
the others and is used exactly once. However, you carefully zero-pad it at both ends
so that there is no wrap-around ambiguity in the output convolution or deconvolution.
Now you overlap and add these sections of output. Thus, an output point near the
end of one section will have the response due to the input points at the beginning
of the next section of data properly added in to it, and likewise for an output point
near the beginning of a section, mutatis mutandis.

Even when computer memory is available, there is some slight gain in computing
speed in segmenting a long data set, since the FFTs’ N log, N is slightly slower than
linear in N.. However, the log term is so slowly varying that you will often be much
happier to avoid the bookkeeping complexities of the overlap-add or overlap-save
methods: If it is practical to do so, just cram the whole data set into memory and
FFT away. Then you will have more time for the finer things in life, some of which
are described in succeeding sections of this chapter.

CITED REFERENCES AND FURTHER READING:

Nussbaumer, H.J. 1982, Fast Fourier Transform and Convolution Algorithms (New York: Springer-
Verlag).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:
Academic Press).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.2 Correlation and Autocorrelation Using the FFT 545

Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall), Chap-
ter 13.

13.2 Correlation and Autocorrelation Using
the FFT

Correlation is the close mathematical cousin of convolution. It is in some
ways simpler, however, because the two functions that go into a correlation are not
as conceptually distinct as were the data and response functions that entered into
convolution. Rather, in correlation, the functions are represented by different, but
generally similar, data sets. We investigate their “correlation,” by comparing them
both directly superposed, and with one of them shifted left or right.

We have already defined in equation (12.0.10) the correlation between two
continuous functions g(t) and h(t), which is denoted Corr(g, h), and is a function
of lag t. We will occasionally show this time dependence explicitly, with the rather
awkward notation Corr(g, h)(t). The correlation will be large at some value of t if the
first function (g) is a close copy of the second (/) but lags it in time by t, ie., if the first
function is shifted to the right of the second. Likewise, the correlation will be large
for some negative value of t if the first function leads the second, i.e., is shifted to the
left of the second. The relation that holds when the two functions are interchanged is

Corr(g, h)(t) = Corr(h, g)(—t) (13.2.1)

The discrete correlation of two sampled functions gx and hx, each periodic
with period N, is defined by

2

=1
Corr(g, hh); = Gj+khe (13.2.2)
0

>
IL

The discrete correlation theorem says that this discrete correlation of two real
functions g and h is one member of the discrete Fourier transform pair

Corr(g, h); <> Ge H* (13.2.3)

where G', and H;, are the discrete Fourier transforms of g; and h,;, and the asterisk
denotes complex conjugation. This theorem makes the same presumptions about the
functions as those encountered for the discrete convolution theorem.

We can compute correlations using the FFT as follows: FFT the two data sets,
multiply one resulting transform by the complex conjugate of the other, and inverse
transform the product. The result (call it r;) will formally be a complex vector
of length N. However, it will turn out to have all its imaginary parts zero since
the original data sets were both real. The components of r;, are the values of the
correlation at different lags, with positive and negative lags stored in the by now
familiar wrap-around order: The correlation at zero lag is in ro, the first component;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

546 Chapter 13. Fourier and Spectral Applications

the correlation at lag 1 is in r;, the second component; the correlation at lag —1
is in ry—y, the last component; etc.

Just as in the case of convolution we have to consider end effects, since our
data will not, in general, be periodic as intended by the correlation theorem. Here
again, we can use zero padding. If you are interested in the correlation for lags as
large as +, then you must append a buffer zone of K zeros at the end of both
input data sets. If you want all possible lags from N data points (not a usual thing),
then you will need to pad the data with an equal number of zeros; this is the extreme
case. So here is the program:

#include "nrutil.h"

void correl(float datai[], float data2[], unsigned long n, float ans[])
Computes the correlation of two real data sets data1[1..n] and data2[1..n] (including any
user-supplied zero padding). n MUST be an integer power of two. The answer is returned as
the first n points in ans[1..2*n] stored in wrap-around order, i.e., correlations at increasingly
negative lags are in ans [n] on down to ans [n/2+1], while correlations at increasingly positive
lags are in ans[1] (zero lag) on up to ans[n/2]. Note that ans must be supplied in the calling
program with length at least 2*n, since it is also used as working space. Sign convention of
this routine: if data lags data2, i.e., is shifted to the right of it, then ans will show a peak
at positive lags.

{
void realft(float data[], unsigned long n, int isign);
void twofft(float datai{], float data2[], float ffti[], float fft2[],
unsigned long n);
unsigned long no2,i;
float dum,*fft;
fft=vector(1,n<<1);
twofft(datal,data2,fft,ans,n); Transform both data vectors at once.
no2=n>>1; Normalization for inverse FFT.
for (i=2;i<=n+2;it+=2) {
ans [i-1]=(fft [i-1] * (dum=ans [i-1])+£ft [i] *ans [i] ) /no2; Multiply to find
ans [i] =(fft [i] *dum-fft [i-1] *ans [i] ) /no2; FFT of their cor-
+ relation.
ans [2]=ans[n+1] ; Pack first and last into one element.
realft(ans,n,-1); Inverse transform gives correlation.
free_vector(fft,1,n<<1);
}

As in convlv, it would be better to substitute two calls to realft for the one
call to twofft, if data1 and data2 have very different magnitudes, to minimize
roundoff error.

The discrete autocorrelation of a sampled function g; is just the discrete
correlation of the function with itself. Obviously this is always symmetric with
respect to positive and negative lags. Feel free to use the above routine correl
to obtain autocorrelations, simply calling it with the same data vector in both
arguments. If the inefficiency bothers you, routine realft can, of course, be used
to transform the data vector instead.

CITED REFERENCES AND FURTHER READING:
Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-Hall), §13-2.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.3 Optimal (Wiener) Filtering with the FFT 547

13.3 Optimal (Wiener) Filtering with the FFT

There are a number of other tasks in numerical processing that are routinely
handled with Fourier techniques. One of these is filtering for the removal of noise
from a “corrupted” signal. The particular situation we consider is this: There is some
underlying, uncorrupted signal u(t) that we want to measure. The measurement
process is imperfect, however, and what comes out of our measurement device is a
corrupted signal c(t). The signal c(t) may be less than perfect in either or both of
two respects. First, the apparatus may not have a perfect “delta-function” response,
so that the true signal u(t) is convolved with (smeared out by) some known response
function r(t) to give a smeared signal s(t),

00
—0o
where S,R,U are the Fourier transforms of s,r,u, respectively. Second, the
measured signal c(t) may contain an additional component of noise n(t),

c(t) = s(t) + n(t) (13.3.2)

We already know how to deconvolve the effects of the response function r in
the absence of any noise (§13.1); we just divide C(f) by R(f) to get a deconvolved
signal. We now want to treat the analogous problem when noise is present. Our
task is to find the optimal filter, ¢(¢) or ®(f), which, when applied to the measured
signal c(t) or C(f), and then deconvolved by r(t) or R(f), produces a signal u(t)
or U(f) that is as close as possible to the uncorrupted signal u(t) or U(f). In other
words we will estimate the true signal U by

Gp) = LOE (13.3.3)

Rf)

In what sense is U to be close to U? We ask that they be close in the
least-square sense

00 oo 2
| \a(t) — u(t) 2 a= | J) - UD) a isminimized. (13.3.4)

—oo co

Substituting equations (13.3.3) and (13.3.2), the right-hand side of (13.3.4) becomes

2

ia [e t NAGA) SUA!” ay
= RUF) RP) (13355)
= / IR)? {ISCNP = BNP +IN)P LP} af

The signal S and the noise N are uncorrelated, so their cross product, when
integrated over frequency f, gave zero. (This is practically the definition of what we
mean by noise!) Obviously (13.3.5) will be a minimum if and only if the integrand
is minimized with respect to ®(f) at every value of f. Let us search for such a

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

548 Chapter 13. Fourier and Spectral Applications

solution where ®(f)) is a real function. Differentiating with respect to ®, and setting
the result equal to zero gives

Isc? 1336
SAP + INGA? (B89)

This is the formula for the optimal filter ®(f).

Notice that equation (13.3.6) involves S, the smeared signal, and N, the noise.
The two of these add up to be C, the measured signal. Equation (13.3.6) does not
contain U, the “true” signal. This makes for an important simplification: The optimal
filter can be determined independently of the determination of the deconvolution
function that relates S' and U.

To determine the optimal filter from equation (13.3.6) we need some way
of separately estimating S|? and IN/?. There is no way to do this from the
measured signal C’ alone without some other information, or some assumption or
guess. Luckily, the extra information is often easy to obtain. For example, we
can sample a long stretch of data c(t) and plot its power spectral density using
equations (12.0.14), (12.1.8), and (12.1.5). This quantity is proportional to the sum
|S)’ + |N/?, so we have

af) =

IS(AP+IN(NP & P(A =ICNP OS F< fe (13.3.7)

(More sophisticated methods of estimating the power spectral density will be
discussed in §13.4 and §13.7, but the estimation above is almost always good enough
for the optimal filter problem.) The resulting plot (see Figure 13.3.1) will often
immediately show the spectral signature of a signal sticking up above a continuous
noise spectrum. The noise spectrum may be flat, or tilted, or smoothly varying; it
doesn’t matter, as long as we can guess a reasonable hypothesis as to what it is.
Draw a smooth curve through the noise spectrum, extrapolating it into the region
dominated by the signal as well. Now draw a smooth curve through the signal plus
noise power. The difference between these two curves is your smooth “model” of the
signal power. The quotient of your model of signal power to your model of signal
plus noise power is the optimal filter ®(f). [Extend it to negative values of f by the
formula ®(—f) = ®(f).] Notice that &(f) will be close to unity where the noise
is negligible, and close to zero where the noise is dominant. That is how it does its
job! The intermediate dependence given by equation (13.3.6) just turns out to be the
optimal way of going in between these two extremes.

Because the optimal filter results from a minimization problem, the quality of
the results obtained by optimal filtering differs from the true optimum by an amount
that is second order in the precision to which the optimal filter is determined. In other
words, even a fairly crudely determined optimal filter (sloppy, say, at the 10 percent
level) can give excellent results when it is applied to data. That is why the separation
of the measured signal C into signal and noise components S and N can usefully be
done “by eye” from a crude plot of power spectral density. All of this may give you
thoughts about iterating the procedure we have just described. For example, after
designing a filter with response (f) and using it to make a respectable guess at the
signal U(f) = ®(f)C(f)/R(f), you might turn about and regard U(f) as a fresh
new signal which you could improve even further with the same filtering technique.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

13.4 Power Spectrum Estimation Using the FFT 549

a | C|? (measured)

| N[? (extrapolated)

log scale

5 _— |S]? (deduced)

f

Figure 13.3.1. Optimal (Wiener) filtering. The power spectrum of signal plus noise shows a signal peak
added to a noise tail. The tail is extrapolated back into the signal region as a “noise model.” Subtracting
gives the “signal model.” The models need not be accurate for the method to be useful. A simple algebraic
combination of the models gives the optimal filter (see text).

Don’t waste your time on this line of thought. The scheme converges to a signal of
S(f) = 0. Converging iterative methods do exist; this just isn’t one of them.

You can use the routine four1 (§12.2) or realft (§12.3) to FFT your data
when you are constructing an optimal filter. To apply the filter to your data, you
can use the methods described in §13.1. The specific routine conv1v is not needed
for optimal filtering, since your filter is constructed in the frequency domain to
begin with. If you are also deconvolving your data with a known response function,
however, you can modify convlv to multiply by your optimal filter just before it
takes the inverse Fourier transform.

CITED REFERENCES AND FURTHER READING:

Rabiner, L.R., and Gold, B. 1975, Theory and Application of Digital Signal Processing (Englewood
Cliffs, NJ: Prentice-Hall).

Nussbaumer, H.J. 1982, Fast Fourier Transform and Convolution Algorithms (New York: Springer-
Verlag).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:
Academic Press).

13.4 Power Spectrum Estimation Using the FFT

In the previous section we “informally” estimated the power spectral density of a
function c(t) by taking the modulus-squared of the discrete Fourier transform of some

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

550 Chapter 13. Fourier and Spectral Applications

finite, sampled stretch of it. In this section we’ll do roughly the same thing, but with
considerably greater attention to details. Our attention will uncover some surprises.

The first detail is power spectrum (also called a power spectral density or
PSD) normalization. In general there is some relation of proportionality between a
measure of the squared amplitude of the function and a measure of the amplitude
of the PSD. Unfortunately there are several different conventions for describing
the normalization in each domain, and many opportunities for getting wrong the
relationship between the two domains. Suppose that our function c(t) is sampled at
N points to produce values co ...cn-—1, and that these points span a range of time
T, that is T = (N — 1)A, where A is the sampling interval. Then here are several
different descriptions of the total power:

N-1
> \cj ? = “sum squared amplitude” (13.4.1)
j=0

1 yt ; 182
a c(t) |" dt = — 13.4.2
pf or ax x (13.42)
T N-1
[ c(t)? dt~ A >>| (13.4.3)
0 jo

PSD estimators, as we shall see, have an even greater variety. In this section,
we consider a class of them that give estimates at discrete values of frequency f;,
where i will range over integer values. In the next section, we will learn about
a different class of estimators that produce estimates that are continuous functions
of frequency f. Even if it is agreed always to relate the PSD normalization to a
particular description of the function normalization (e.g., 13.4.2), there are at least
the following possibilities: The PSD is

e defined for discrete positive, zero, and negative frequencies, and its sum

over these is the function mean squared amplitude

e defined for zero and discrete positive frequencies only, and its sum over

these is the function mean squared amplitude

e defined in the Nyquist interval from —f, to fc, and its integral over this

range is the function mean squared amplitude

e defined from 0 to f,, and its integral over this range is the function mean

squared amplitude

It never makes sense to integrate the PSD of a sampled function outside of the
Nyquist interval —f, and f, since, according to the sampling theorem, power there
will have been aliased into the Nyquist interval.

It is hopeless to define enough notation to distinguish all possible combinations
of normalizations. In what follows, we use the notation P(f) to mean any of the
above PSDs, stating in each instance how the particular P(f) is normalized. Beware
the inconsistent notation in the literature.

The method of power spectrum estimation used in the previous section is a
simple version of an estimator called, historically, the periodogram. If we take an
N-point sample of the function c(t) at equal intervals and use the FFT to compute

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.4 Power Spectrum Estimation Using the FFT 551

its discrete Fourier transform
N-1

Ce= So ej PPUN kh =0,...,N-1 (13.4.4)

j=0

then the periodogram estimate of the power spectrum is defined at N/2 +1
frequencies as

2

P(0) = Plfo) = 5 IO

Plt) = sya [ICel? +1Cw-uP] k= 12....(F-1) 345)

P(fe) = P( fia) = i |Cwyol

where f;, is defined only for the zero and positive frequencies

k

; k N
Sk = WK = Ale

2

k=0,1,... (13.4.6)
By Parseval’s theorem, equation (12.1.10), we see immediately that equation (13.4.5)
is normalized so that the sum of the N/2 + 1 values of P is equal to the mean
squared amplitude of the function c;.

We must now ask this question. In what sense is the periodogram estimate
(13.4.5) a “true” estimator of the power spectrum of the underlying function c(t)?
You can find the answer treated in considerable detail in the literature cited (see,
e.g.,[1] for an introduction). Here is a summary.

First, is the expectation value of the periodogram estimate equal to the power
spectrum, i.e., is the estimator correct on average? Well, yes and no. We wouldn’t
really expect one of the P(f,)’s to equal the continuous P(f) at exactly f,, since fi,
is supposed to be representative of a whole frequency “bin” extending from halfway
from the preceding discrete frequency to halfway to the next one. We should be
expecting the P(f,) to be some kind of average of P(f) over a narrow window
function centered on its f,;. For the periodogram estimate (13.4.6) that window
function, as a function of s the frequency offset in bins, is

1 sin(rs) ]?
; (13.4.7)

W(s) = N? [at

Notice that W(s) has oscillatory lobes but, apart from these, falls off only about as
W(s) © (7s) ~?. This is not a very rapid fall-off, and it results in significant leakage
(that is the technical term) from one frequency to another in the periodogram estimate.
Notice also that W(s) happens to be zero for s equal to a nonzero integer. This means
that if the function c(t) is a pure sine wave of frequency exactly equal to one of the
fx’s, then there will be no leakage to adjacent f;,’s. But this is not the characteristic
case! If the frequency is, say, one-third of the way between two adjacent f ;,’s, then
the leakage will extend well beyond those two adjacent bins. The solution to the
problem of leakage is called data windowing, and we will discuss it below.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

552 Chapter 13. Fourier and Spectral Applications

Turn now to another question about the periodogram estimate. What is the
variance of that estimate as N goes to infinity? In other words, as we take more
sampled points from the original function (either sampling a longer stretch of data at
the same sampling rate, or else by resampling the same stretch of data with a faster
sampling rate), then how much more accurate do the estimates P; become? The
unpleasant answer is that the periodogram estimates do not become more accurate
at all! In fact, the variance of the periodogram estimate at a frequency f is always
equal to the square of its expectation value at that frequency. In other words, the
standard deviation is always 100 percent of the value, independent of N! How can
this be? Where did all the information go as we added points? It all went into
producing estimates at a greater number of discrete frequencies f;,. If we sample a
longer run of data using the same sampling rate, then the Nyquist critical frequency
fc is unchanged, but we now have finer frequency resolution (more f ,’s) within the
Nyquist frequency interval; alternatively, if we sample the same length of data with a
finer sampling interval, then our frequency resolution is unchanged, but the Nyquist
range now extends up to a higher frequency. In neither case do the additional samples
reduce the variance of any one particular frequency’s estimated PSD.

You don’t have to live with PSD estimates with 100 percent standard deviations,
however. You simply have to know some techniques for reducing the variance of
the estimates. Here are two techniques that are very nearly identical mathematically,
though different in implementation. The first is to compute a periodogram estimate
with finer discrete frequency spacing than you really need, and then to sum the
periodogram estimates at A’ consecutive discrete frequencies to get one “smoother”
estimate at the mid frequency of those kK’. The variance of that summed estimate
will be smaller than the estimate itself by a factor of exactly 1/K, ie., the standard
deviation will be smaller than 100 percent by a factor 1/ /K. Thus, to estimate the
power spectrum at / + 1 discrete frequencies between 0 and f, inclusive, you begin
by taking the FFT of 2M K points (which number had better be an integer power of
two!). You then take the modulus square of the resulting coefficients, add positive
and negative frequency pairs, and divide by (2M K‘)?, all according to equation
(13.4.5) with N = 2M K. Finally, you “bin” the results into summed (not averaged)
groups of kK’. This procedure is very easy to program, so we will not bother to give
a routine for it. The reason that you sum, rather than average, K consecutive points
is so that your final PSD estimate will preserve the normalization property that the
sum of its WZ + 1 values equals the mean square value of the function.

A second technique for estimating the PSD at M/Z + 1 discrete frequencies in
the range 0 to f, is to partition the original sampled data into K segments each of
2M consecutive sampled points. Each segment is separately FFT’d to produce a
periodogram estimate (equation 13.4.5 with N = 2M). Finally, the kK periodogram
estimates are averaged at each frequency. It is this final averaging that reduces the
variance of the estimate by a factor K (standard deviation by VK). This second
technique is computationally more efficient than the first technique above by a modest
factor, since it is logarithmically more efficient to take many shorter FFTs than one
longer one. The principal advantage of the second technique, however, is that only
2M data points are manipulated at a single time, not 2/ as in the first technique.
This means that the second technique is the natural choice for processing long runs
of data, as from a magnetic tape or other data record. We will give a routine later
for implementing this second technique, but we need first to return to the matters of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.4 Power Spectrum Estimation Using the FFT 553

leakage and data windowing which were brought up after equation (13.4.7) above.

Data Windowing

The purpose of data windowing is to modify equation (13.4.7), which expresses
the relation between the spectral estimate P;, at a discrete frequency and the actual
underlying continuous spectrum P(f)) at nearby frequencies. In general, the spectral
power in one “bin” & contains leakage from frequency components that are actually
s bins away, where s is the independent variable in equation (13.4.7). There is, as
we pointed out, quite substantial leakage even from moderately large values of s.

When we select a run of NV sampled points for periodogram spectral estimation,
we are in effect multiplying an infinite run of sampled data c ; by a window function
in time, one that is zero except during the total sampling time N A, and is unity during
that time. In other words, the data are windowed by a square window function. By
the convolution theorem (12.0.9; but interchanging the roles of f and t), the Fourier
transform of the product of the data with this square window function is equal to the
convolution of the data’s Fourier transform with the window’s Fourier transform. In
fact, we determined equation (13.4.7) as nothing more than the square of the discrete
Fourier transform of the unity window function.

1 sin(7s) 2 1 Nal 2
- - Qrisk/N
W(s)= 3 Ss We d° (13.4.8)

The reason for the leakage at large values of s, is that the square window function
turns on and off so rapidly. Its Fourier transform has substantial components
at high frequencies. To remedy this situation, we can multiply the input data
cj, J = 0,...,.N — 1 by a window function w,; that changes more gradually from
zero to a maximum and then back to zero as j ranges from 0 to NV. In this case, the
equations for the periodogram estimator (13.4.4-13.4.5) become

N-1
Dp = Yo cjw; PPUV/N — k= 0,...,N-1 (13.4.9)
j=0
5 1 2
P(0) = P(fo) =a |Do|
Wes
1 2 2 N
P(fr) = —— ||Dr Dn-x k=1,2,...,{—-1
(fi) Wa f | + |Dy—x| | : (3 )
. . 1 2
P(fc) = P(fxy2) = Ww. |Dn;2| (13.4.10)
ss
where W,, stands for “window squared and summed,”
N-1
Wss = NSO ujZ (13.4.11)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

554 Chapter 13. Fourier and Spectral Applications

and f;, is given by (13.4.6). The more general form of (13.4.7) can now be written
in terms of the window function w, as

2

W(s) =

N-1
y e2misk/N ay,
k=0

1
Wss

; (13.4.12)
1

Wss

N/2
| cos(2rsk/N)w(k — N/2) dk
-N/2

Here the approximate equality is useful for practical estimates, and holds for any
window that is left-right symmetric (the usual case), and for s < N (the case of
interest for estimating leakage into nearby bins). The continuous function w(k—N/2)

in the integral is meant to be some smooth function that passes through the points w ;,.

There is a lot of perhaps unnecessary lore about choice of a window function, and
practically every function that rises from zero to a peak and then falls again has been
named after someone. A few of the more common (also shown in Figure 13.4.1) are:
j-3N

i
2

wy =1- | | = “Bartlett window” (13.4.13)
(The “Parzen window” is very similar to this.)

1 2nj
was ! — cos ()] = “Hann window” (13.4.14)

(The “Hamming window” is similar but does not go exactly to zero at the ends.)

/_ tn?
wy =1- ( 2 ) = “Welch window” (13.4.15)

We are inclined to follow Welch in recommending that you use either (13.4.13)
or (13.4.15) in practical work. However, at the level of this book, there is
effectively no difference between any of these (or similar) window functions. Their
difference lies in subtle trade-offs among the various figures of merit that can be
used to describe the narrowness or peakedness of the spectral leakage functions
computed by (13.4.12). These figures of merit have such names as: highest sidelobe
level (dB), sidelobe fall-off (dB per octave), equivalent noise bandwidth (bins), 3-dB
bandwidth (bins), scallop loss (dB), worst case process loss (dB). Roughly speaking,
the principal trade-off is between making the central peak as narrow as possible
versus making the tails of the distribution fall off as rapidly as possible. For
details, see (e.g.) [2]. Figure 13.4.2 plots the leakage amplitudes for several windows
already discussed.

There is particularly a lore about window functions that rise smoothly from
zero to unity in the first small fraction (say 10 percent) of the data, then stay at
unity until the last small fraction (again say 10 percent) of the data, during which
the window function falls smoothly back to zero. These windows will squeeze a
little bit of extra narrowness out of the main lobe of the leakage function (never as

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

13.4 Power Spectrum Estimation Using the FFT 555

square window

amplitude

1 iti ota oti rit rit
0 50 100 150 200 250
bin number

Figure 13.4.1. Window functions commonly used in FFT power spectral estimation. The data segment,
here of length 256, is multiplied (bin by bin) by the window function before the FFT is computed. The
square window, which is equivalent to no windowing, is least recommended. The Welch and Bartlett
windows are good choices.

much as a factor of two, however), but trade this off by widening the leakage tail
by a significant factor (e.g., the reciprocal of 10 percent, a factor of ten). If we
distinguish between the width of a window (number of samples for which it is at
its maximum value) and its rise/fall time (number of samples during which it rises
and falls); and if we distinguish between the FWHM (full width to half maximum
value) of the leakage function’s main lobe and the leakage width (full width that
contains half of the spectral power that is not contained in the main lobe); then these
quantities are related roughly by

N
(window width)
N
(window rise/fall time)

(FWHM in bins) ~ (13.4.16)

(leakage width in bins) ~ (13.4.17)

For the windows given above in (13.4.13)-(13.4.15), the effective window
widths and the effective window rise/fall times are both of order 3N . Generally
speaking, we feel that the advantages of windows whose rise and fall times are
only small fractions of the data length are minor or nonexistent, and we avoid using
them. One sometimes hears it said that flat-topped windows “throw away less of
the data,” but we will now show you a better way of dealing with that problem by
use of overlapping data segments.

Let us now suppose that we have chosen a window function, and that we are
ready to segment the data into K segments of N = 2M points. Each segment will
be FFT’d, and the resulting Kk periodograms will be averaged together to obtain a

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

556 Chapter 13. Fourier and Spectral Applications

IE al
8 il
» L 4
Ed b 4
2 6b z
2 L 4
8 L 4
Zz b 4
Ss
2 o4t 4
a
£ L 4
a L 4
2e 4
0 }— 4
-8 -6 -4 -2 0 2 4 6 8
offset in units of frequency bins

Figure 13.4.2. Leakage functions for the window functions of Figure 13.4.1. A signal whose frequency
is actually located at zero offset “leaks” into neighboring bins with the amplitude shown. The purpose
of windowing is to reduce the leakage at large offsets, where square (no) windowing has large sidelobes.
Offset can have a fractional value, since the actual signal frequency can be located between two frequency
bins of the FFT.

PSD estimate at M + 1 frequency values from 0 to f.. We must now distinguish
between two possible situations. We might want to obtain the smallest variance
from a fixed amount of computation, without regard to the number of data points
used. This will generally be the goal when the data are being gathered in real time,
with the data-reduction being computer-limited. Alternatively, we might want to
obtain the smallest variance from a fixed number of available sampled data points.
This will generally be the goal in cases where the data are already recorded and
we are analyzing it after the fact.

In the first situation (smallest spectral variance per computer operation), it is
best to segment the data without any overlapping. The first 2/ data points constitute
segment number 1; the next 2/7 data points constitute segment number 2; and so on,
up to segment number Kx, for a total of 24M sampled points. The variance in this
case, relative to a single segment, is reduced by a factor Kr.

In the second situation (smallest spectral variance per data point), it turns out
to be optimal, or very nearly optimal, to overlap the segments by one half of their
length. The first and second sets of M points are segment number 1; the second
and third sets of 1 points are segment number 2; and so on, up to segment number
4, which is made of the Ath and Kk + 1st sets of M points. The total number of
sampled points is therefore (Ff + 1), just over half as many as with nonoverlapping
segments. The reduction in the variance is not a full factor of A’, since the segments
are not statistically independent. It can be shown that the variance is instead reduced
by a factor of about 9/4/11 (see the paper by Welch in [3]). This is, however,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

v
g
3.
a
8

3
Fy
Q
3
om
a
8
>
2
=
Ss
5

a
Pa
Fa
°
$s
2
é
5
2
Pa
8
2
3
3
3°
3
3
S
gs
=
ga
2
<
z
°
2
g
s
2
4
°
8
a
g
Zz
z
3
3
2
2
D
Fa
2.
3
3
&
3
=
fa
9
ce}
iw]
yD
fe)
=
#
=.
2
<
3
&
2
s

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

13.4 Power Spectrum Estimation Using the FFT 557

significantly better than the reduction of about ’/2 that would have resulted if the
same number of data points were segmented without overlapping.

We can now codify these ideas into a routine for spectral estimation. While
we generally avoid input/output coding, we make an exception here to show how
data are read sequentially in one pass through a data file (referenced through the
parameter FILE *fp). Only a small fraction of the data is in memory at any one time.
Note that spctrm returns the power at MM, not M + 1, frequencies, omitting the
component P(f-) at the Nyquist frequency. It would also be straightforward to
include that component.

#include <math.h>

#include <stdio.h>

#include "nrutil.h"

#define WINDOW(j,a,b) (1.0-fabs((((j)-1)-(a))*(b))) /* Bartlett */
/* #define WINDOW(j,a,b) 1.0 */ /* Square */

/* #define WINDOW(j,a,b) (1.0-SQR((((j)-1)-(a))*(b))) */ /* Welch */

void spctrm(FILE *fp, float p[], int m, int k, int ovrlap)
Reads data from input stream specified by file pointer fp and returns as p[j] the data’s power
(mean square amplitude) at frequency (j-1)/(2*m) cycles per gridpoint, for j=1,2,...,m,
based on (2*k+1)*m data points (if ovrlap is set true (1)) or 4*k*m data points (if ovrlap
is set false (0)). The number of segments of the data is 2*k in both cases: The routine calls
four1 k times, each call with 2 partitions each of 2*m real data points.
{

void fouri(float data[], unsigned long nn, int isign);

int mm,m44,m43,m4,kk, joffn, joff,j2,j;

float w,facp,facm,*w1,*w2,sumw=0.0,den=0.0;

mm=m+m ; Useful factors.
m43=(m4=mm+mm) +3;

m44=m43+1;

wi=vector(1,m4) ;

w2=vector(1,m);

facm=m;
facp=1.0/m;
for (j=1;j<=mm;j++) sumw += SQR(WINDOW(j,facm,facp));

Accumulate the squared sum of the weights.
for (j=1;j<=m;j++) p[j]=0.0; Initialize the spectrum to zero.
if (ovrlap) Initialize the “save” half-buffer.

for (j=1;j<=m;j++) fscanf(fp,"%f",aw20j]);
for (kk=1;kk<=k;kk++) {
Loop over data set segments in groups of two.

for (joff = -1;joff<=0;joff++) { Get two complete segments into workspace.

if (ovrlap) {
for (j=1;j<=m; j++) wiljoff+j+jl=w2[j];

for (j=1;j<=m;j++) fscanf(fp,"4f",ew2[j]);
joffn=joff+mm;
for (j=1;j<=m;jt+) wiljoffnt+j+j]=w2[j];

} else {
for (j=joff+2; j<=m4; j+=2)
fscanf(fp,"4f",&wilj]);

3
}
for (j=1;j<=mm;jt+) { Apply the window to the data.
j2=j+3i
w=WINDOW(j ,facm,facp) ;
wi[j2] *= w;
wi[j2-1] *= w;
}
fouri(wi1,mm, 1); Fourier transform the windowed data.

pli] += (SQR(w1[1])+SQR(w1[2])); Sum results into previous segments.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

558 Chapter 13. Fourier and Spectral Applications

for (j=2;j<=m;jt+) {
j2=j+5;
plj] += (SQR(w1[j2])+SQR(w1[j2-1])
+SQR (wi [m44-j2])+SQR(w1 [m43-j2]));

}

den += sumw;
3
den *= m4; Correct normalization.
for (j=1;j<=m; j++) p[j] /= den; Normalize the output.

free_vector(w2,1,m);
free_vector(wi,1,m4) ;

CITED REFERENCES AND FURTHER READING:

Oppenheim, A.V., and Schafer, R.W. 1989, Discrete-Time Signal Processing (Englewood Cliffs,
NJ: Prentice-Hall). [1]

Harris, F.J. 1978, Proceedings of the IEEE, vol. 66, pp. 51-83. [2]

Childers, D.G. (ed.) 1978, Modern Spectrum Analysis (New York: IEEE Press), paper by P.D.
Welch. [3]

Champeney, D.C. 1973, Fourier Transforms and Their Physical Applications (New York: Academic
Press).

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Applications (New York:
Academic Press).

Bloomfield, P. 1976, Fourier Analysis of Time Series — An Introduction (New York: Wiley).

Rabiner, L.R., and Gold, B. 1975, Theory and Application of Digital Signal Processing (Englewood
Cliffs, NJ: Prentice-Hall).

13.5 Digital Filtering in the Time Domain

Suppose that you have a signal that you want to filter digitally. For example, perhaps
you want to apply high-pass or low-pass filtering, to eliminate noise at low or high frequencies
respectively; or perhaps the interesting part of your signal lies only in a certain frequency
band, so that you need a bandpass filter. Or, if your measurements are contaminated by 60
Hz power-line interference, you may need a notch filter to remove only a narrow band around
that frequency. This section speaks particularly about the case in which you have chosen
to do such filtering in the time domain.

Before continuing, we hope you will reconsider this choice. Remember how convenient
it is to filter in the Fourier domain. You just take your whole data record, FFT it, multiply
the FFT output by a filter function 7/(f), and then do an inverse FFT to get back a filtered
data set in time domain. Here is some additional background on the Fourier technique that
you will want to take into account.

e Remember that you must define your filter function 11(f) for both positive and

negative frequencies, and that the magnitude of the frequency extremes is always
the Nyquist frequency 1/(2A), where A is the sampling interval. The magnitude
of the smallest nonzero frequencies in the FFT is +1/(NA), where N is the
number of (complex) points in the FFT. The positive and negative frequencies to
which this filter are applied are arranged in wrap-around order.

e Ifthe measured data are real, and you want the filtered output also to be real, then

your arbitrary filter function should obey H(—f) = H(f)*. You can arrange this
most easily by picking an 1 that is real and even in f.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.5 Digital Filtering in the Time Domain 559

e If your chosen H(f) has sharp vertical edges in it, then the impulse response of
your filter (the output arising from a short impulse as input) will have damped
“ringing” at frequencies corresponding to these edges. There is nothing wrong
with this, but if you don’t like it, then pick a smoother #1(f). To get a first-hand
look at the impulse response of your filter, just take the inverse FFT of your #((f).
If you smooth all edges of the filter function over some number k: of points, then
the impulse response function of your filter will have a span on the order of a
fraction 1/k of the whole data record.

e If your data set is too long to FFT all at once, then break it up into segments of
any convenient size, as long as they are much longer than the impulse response
function of the filter. Use zero-padding, if necessary.

e You should probably remove any trend from the data, by subtracting from it a
straight line through the first and last points (i.e., make the first and last points equal
to zero). If you are segmenting the data, then you can pick overlapping segments
and use only the middle section of each, comfortably distant from edge effects.

e A digital filter is said to be causal or physically realizable if its output for a
particular time-step depends only on inputs at that particular time-step or earlier.
It is said to be acausal if its output can depend on both earlier and later inputs.
Filtering in the Fourier domain is, in general, acausal, since the data are processed
“in a batch,” without regard to time ordering. Don’t let this bother you! Acausal
filters can generally give superior performance (e.g., less dispersion of phases,
sharper edges, less asymmetric impulse response functions). People use causal
filters not because they are better, but because some situations just don’t allow
access to out-of-time-order data. Time domain filters can, in principle, be either
causal or acausal, but they are most often used in applications where physical
realizability is a constraint. For this reason we will restrict ourselves to the causal
case in what follows.

If you are still favoring time-domain filtering after all we have said, it is probably because
you have a real-time application, for which you must process a continuous data stream and
wish to output filtered values at the same rate as you receive raw data. Otherwise, it may
be that the quantity of data to be processed is so large that you can afford only a very small
number of floating operations on each data point and cannot afford even a modest-sized FFT
(with a number of floating operations per data point several times the logarithm of the number
of points in the data set or segment).

Linear Filters

The most general linear filter takes a sequence x, of input points and produces a
sequence yn of output points by the formula

M N
yn = So ce tne + Y-dj n= (13.5.1)
k= j=l

Here the M + 1 coefficients cx, and the N coefficients dj are fixed and define the filter
response. The filter (13.5.1) produces each new output value from the current and M previous
input values, and from its own N previous output values. If N = 0, so that there is no
second sum in (13.5.1), then the filter is called nonrecursive or finite impulse response (FIR). If
N # 0, then it is called recursive or infinite impulse response (IIR). (The term “IIR” connotes
only that such filters are capable of having infinitely long impulse responses, not that their
impulse response is necessarily long in a particular application. Typically the response of an
IIR filter will drop off exponentially at late times, rapidly becoming negligible.)
The relation between the c;,’s and d;’s and the filter response function H(f) is

M
SP cye2riMEA)
H(f) = (13.5.2)
1 So dyen271(F4)

j=l

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

560 Chapter 13. Fourier and Spectral Applications

where A is, as usual, the sampling interval. The Nyquist interval corresponds to fA between
—1/2 and 1/2. For FIR filters the denominator of (13.5.2) is just unity.

Equation (13.5.2) tells how to determine 1{(f) from the c’s and d’s. To design a filter,
though, we need a way of doing the inverse, getting a suitable set of c’s and d’s — as small
a set as possible, to minimize the computational burden — from a desired H(f). Entire
books are devoted to this issue. Like many other “inverse problems,” it has no all-purpose
solution. One clearly has to make compromises, since H(f) is a full continuous function,
while the short list of c’s and d’s represents only a few adjustable parameters. The subject of
digital filter design concerns itself with the various ways of making these compromises. We
cannot hope to give any sort of complete treatment of the subject. We can, however, sketch
a couple of basic techniques to get you started. For further details, you will have to consult
some specialized books (see references).

FIR (Nonrecursive) Filters

When the denominator in (13.5.2) is unity, the right-hand side is just a discrete Fourier
transform. The transform is easily invertible, giving the desired small number of c; coefficients
in terms of the same small number of values of H(fi) at some discrete frequencies f;. This
fact, however, is not very useful. The reason is that, for values of c, computed in this way,
H(f) will tend to oscillate wildly in between the discrete frequencies where it is pinned
down to specific values.

A better strategy, and one which is the basis of several formal methods in the literature,
is this: Start by pretending that you are willing to have a relatively large number of filter
coefficients, that is, a relatively large value of M. Then H(f) can be fixed to desired values

on a relatively fine mesh, and the M coefficients ¢,, k = 0,..., 4 — 1 can be found by
an FFT. Next, truncate (set to zero) most of the c;,’s, leaving nonzero only the first, say,
K, (co,c1,...,eK-1) and last K — 1, (ew—K41,...,cm-—1). The last few c;’s are filter

coefficients at negative lag, because of the wrap-around property of the FFT. But we don’t
want coefficients at negative lag. Therefore we cyclically shift the array of c;’s, to bring
everything to positive lag. (This corresponds to introducing a time-delay into the filter.) Do
this by copying the c;’s into a new array of length M in the following order:

(em—K41,-++;€M-—1, Co, C1,---,CK-1, 0, 0,...,0) (13.5.3)

To see if your truncation is acceptable, take the FFT of the array (13.5.3), giving an
approximation to your original H(f). You will generally want to compare the modulus
|H(f)| to your original function, since the time-delay will have introduced complex phases
into the filter response.

If the new filter function is acceptable, then you are done and have a set of 2K — 1
filter coefficients. If it is not acceptable, then you can either (i) increase A and try again,
or (ii) do something fancier to improve the acceptability for the same A. An example of
something fancier is to modify the magnitudes (but not the phases) of the unacceptable H(f)
to bring it more in line with your ideal, and then to FFT to get new c;,’s. Once again set
to zero all but the first 2K — 1 values of these (no need to cyclically shift since you have
preserved the time-delaying phases), then inverse transform to get a new H(f), which will
often be more acceptable. You can iterate this procedure. Note, however, that the procedure
will not converge if your requirements for acceptability are more stringent than your 2K — 1
coefficients can handle.

The key idea, in other words, is to iterate between the space of coefficients and the space
of functions 711(f), until a Fourier conjugate pair that satisfies the imposed constraints in both
spaces is found. A more formal technique for this kind of iteration is the Remes Exchange
Algorithm which produces the best Chebyshev approximation to a given desired frequency
response with a fixed number of filter coefficients (cf. §5.13).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.5 Digital Filtering in the Time Domain 561

IIR (Recursive) Filters

Recursive filters, whose output at a given time depends both on the current and previous
inputs and on previous outputs, can generally have performance that is superior to nonrecursive
filters with the same total number of coefficients (or same number of floating operations per
input point). The reason is fairly clear by inspection of (13.5.2): A nonrecursive filter has a
frequency response that is a polynomial in the variable 1/z, where

mi(fA) (13.5.4)

Z=

By contrast, a recursive filter’s frequency response is a rational function in 1/z. The class of
rational functions is especially good at fitting functions with sharp edges or narrow features,
and most desired filter functions are in this category.

Nonrecursive filters are always stable. If you turn off the sequence of incoming 2;’s,
then after no more than M steps the sequence of y;’s produced by (13.5.1) will also turn off.
Recursive filters, feeding as they do on their own output, are not necessarily stable. If the
coefficients d; are badly chosen, a recursive filter can have exponentially growing, so-called
homogeneous, modes, which become huge even after the input sequence has been turned off.
This is not good. The problem of designing recursive filters, therefore, is not just an inverse
problem; it is an inverse problem with an additional stability constraint.

How do you tell if the filter (13.5.1) is stable for a given set of c, and dj; coefficients?
Stability depends only on the d;’s. The filter is stable if and only if all N complex roots
of the characteristic polynomial equation

(13.5.5)

are inside the unit circle, i.e., satisfy
jz) <1 (13.5.6)

The various methods for constructing stable recursive filters again form a subject area
for which you will need more specialized books. One very useful technique, however, is the
bilinear transformation method. For this topic we define a new variable w that reparametrizes
the frequency f,

(1-2 FA) _[1l-z
w = tan[m(fA)] i(iGan0s;) (+) (13.5.7)
Don’t be fooled by the 2’s in (13.5.7). This equation maps real frequencies f into real values of

w. In fact, it maps the Nyquist interval -4 <fAa< 3 onto the real w axis -00 < w < +00.
The inverse equation to (13.5.7) is

= risa) _ 1+ tw (13.5.8)

z -
1—iw

In reparametrizing f, w also reparametrizes z, of course. Therefore, the condition for
stability (13.5.5)-(13.5.6) can be rephrased in terms of w: If the filter response H(f) is
written as a function of w, then the filter is stable if and only if the poles of the filter function
(zeros of its denominator) are all in the upper half complex plane,

Im(w) > 0 (13.5.9)

The idea of the bilinear transformation method is that instead of specifying your desired
H(f), you specify only its desired modulus square, |H(f)|? = H(f)H(f)* = H(fYH(—f).
Pick this to be approximated by some rational function in w”. Then find all the poles of this
function in the w complex plane. Every pole in the lower half-plane will have a corresponding
pole in the upper half-plane, by symmetry. The idea is to form a product only of the factors
with good poles, ones in the upper half-plane. This product is your stably realizable 1/(f).
Now substitute equation (13.5.7) to write the function as a rational function in z, and compare
with equation (13.5.2) to read off the c’s and d’s.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

562 Chapter 13. Fourier and Spectral Applications

The procedure becomes clearer when we go through an example. Suppose we want to
design a simple bandpass filter, whose lower cutoff frequency corresponds to a value w = a,
and whose upper cutoff frequency corresponds to a value w = b, with a and b both positive
numbers. A simple rational function that accomplishes this is

(2 2
MNP = (ss) (aa) (135.10)

This function does not have a very sharp cutoff, but it is illustrative of the more general
case. To obtain sharper edges, one could take the function (13.5.10) to some positive integer
power, or, equivalently, run the data sequentially through some number of copies of the filter
that we will obtain from (13.5.10).

The poles of (13.5.10) are evidently at w = tia and w = +ib. Therefore the stably
realizable H(f) is

HU) (; “) (ss)

We put the i in the numerator of the second factor in order to end up with real-valued
coefficients. If we multiply out all the denominators, (13.5.11) can be rewritten in the form

(13.5.1)

b b —2
~aFaars + AFayarey*
i (A+a)(1~b)+G~a)(1+b) 51 , (l-a)(1—b) ,_9
(1¥a)(1+6) " (Fa) (+b)

H(f) (13.5.12)

from which one reads off the filter coefficients for equation (13.5.1),

_ b
OT +a +b)
a =0
_ b
= Tay +b)
a= (1+ a)(1—6) + (1-a)(1+6)

(1+ a)(1+6)

(1 —a)(1—b)
a= aE (13.5.13)

This completes the design of the bandpass filter.

Sometimes you can figure out how to construct directly a rational function in w for H(f),
rather than having to start with its modulus square. The function that you construct has to have
its poles only in the upper half-plane, for stability. It should also have the property of going into
its own complex conjugate if you substitute —w for w, so that the filter coefficients will be real.

For example, here is a function for a notch filter, designed to remove only a narrow
frequency band around some fiducial frequency w = wo, where wo is a positive number,

Hf) w—wo w+ wo
Ww — Wo — tEwo w+ wo — iewo

2

(13.5.14)

_ — we
~ (w= iewo)? — u2

In (13.5.14) the parameter € is a small positive number that is the desired width of the notch, as a

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.5 Digital Filtering in the Time Domain 563

(a)

(b)

Figure 13.5.1. (a) A “chirp.” or signal whose frequency increases continuously with time. (b) Same
signal after it has passed through the notch filter (13.5.15). The parameter € is here 0.2.

fraction of wo. Going through the arithmetic of substituting z for w gives the filter coefficients

l+w9
co = ———_ 2
(1 + ewo)? + we
ee
” (1 + ewo)? + w2
l+wo
C2 = 13.5.15
° (1+ ewo)? + w3 ( )
d=? 1— we — we
= (1 + ewo)? + we
bu- (1 — ewo)? + we
? (1+ ewo)? + we

Figure 13.5.1 shows the results of using a filter of the form (13.5.15) on a “chirp” input
signal, one that glides upwards in frequency, crossing the notch frequency along the way.

While the bilinear transformation may seem very general, its applications are limited
by some features of the resulting filters. The method is good at getting the general shape
of the desired filter, and good where “flatness” is a desired goal. However, the nonlinear
mapping between w and f makes it difficult to design to a desired shape for a cutoff, and
may move cutoff frequencies (defined by a certain number of dB) from their desired places.
Consequently, practitioners of the art of digital filter design reserve the bilinear transformation

“(eoueWY YON apis}no) Bio abpuquieo@ Masjsno}9auIp 0} jee PUaS JO ‘(AJUO BOVEWY YON) EZ7Z-Z28-008- I I/29 40 Woo" U'MMy/:d}Y

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

564 Chapter 13. Fourier and Spectral Applications

for specific situations, and arm themselves with a variety of other tricks. We suggest that
you do likewise, as your projects demand.

CITED REFERENCES AND FURTHER READING:

Hamming, R.W. 1983, Digital Filters, 2nd ed. (Englewood Cliffs, NJ: Prentice-Hall).
Antoniou, A. 1979, Digital Filters: Analysis and Design (New York: McGraw-Hill).
Parks, T.W., and Burrus, C.S. 1987, Digital Filter Design (New York: Wiley).

Oppenheim, A.V., and Schafer, R.W. 1989, Discrete-Time Signal Processing (Englewood Cliffs,
NJ: Prentice-Hall).

Rice, J.R. 1964, The Approximation of Functions (Reading, MA: Addison-Wesley); also 1969,
op. cit., Vol. 2.

Rabiner, L.R., and Gold, B. 1975, Theory and Application of Digital Signal Processing (Englewood
Cliffs, NJ: Prentice-Hall).

13.6 Linear Prediction and Linear Predictive
Coding

We begin with a very general formulation that will allow us to make connections
to various special cases. Let {y/,} be a set of measured values for some underlying
set of true values of a quantity y, denoted {ya}, related to these true values by
the addition of random noise,

Yn = Va + Na (13.6.1)

(compare equation 13.3.2, with a somewhat different notation). Our use of a Greek
subscript to index the members of the set is meant to indicate that the data points
are not necessarily equally spaced along a line, or even ordered: they might be
“random” points in three-dimensional space, for example. Now, suppose we want to
construct the “best” estimate of the true value of some particular point y, as a linear
combination of the known, noisy, values. Writing

Ys = >> deal + ts (13.6.2)

a

we want to find coefficients d,., that minimize, in some way, the discrepancy x,.
The coefficients d,.. have a “star” subscript to indicate that they depend on the choice
of point y,. Later, we might want to let y, be one of the existing y,’s. In that case,
our problem becomes one of optimal filtering or estimation, closely related to the
discussion in §13.3. On the other hand, we might want y, to be a completely new
point. In that case, our problem will be one of linear prediction.

A natural way to minimize the discrepancy x, is in the statistical mean square
sense. If angle brackets denote statistical averages, then we seek d,.’s that minimize

= (ays) + (nang))deadep — 2 > (Ysa) dea + (V2)
ap a

(13.6.3)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.6 Linear Prediction and Linear Predictive Coding 565

Here we have used the fact that noise is uncorrelated with signal, e.g., (ney~) =0.
The quantities (yay3) and (y.%q) describe the autocorrelation structure of the
underlying data. We have already seen an analogous expression, (13.2.2), for the
case of equally spaced data points on a line; we will meet correlation several times
again in its statistical sense in Chapters 14 and 15. The quantities (n,n) describe the
autocorrelation properties of the noise. Often, for point-to-point uncorrelated noise,
we have (nang) = (n2) dag. It is convenient to think of the various correlation
quantities as comprising matrices and vectors,

bop = Yoys) bra =(YeYa) Mags = (Mang) or (n2)dag (13.6.4)

Setting the derivative of equation (13.6.3) with respect to the d,.’s equal to zero,
one readily obtains the set of linear equations,

SO [eas + Nap] des = bea (13.6.5)
B

If we write the solution as a matrix inverse, then the estimation equation (13.6.2)
becomes, omitting the minimized discrepancy x,,

Ux © Ss Pree [oye + Two U3 (13.6.6)
as

From equations (13.6.3) and (13.6.5) one can also calculate the expected mean square

value of the discrepancy at its minimum, denoted (x 7?) °°

(2), = (ye) >> dygb.p = (U2) — >> bra ue + Tvkag ea (13.6.7)
ap

A final general result tells how much the mean square discrepancy (x?) is

increased if we use the estimation equation (13.6.2) not with the best values d ,g, but
with some other values d,. The above equations then imply

(x2) = ot Ya wa ~ deer) [603 + Nas] (deg — de) (13.6.8)
ap

Since the second term is a pure quadratic form, we see that the increase in the
discrepancy is only second order in any error made in estimating the d.,’s.

Connection to Optimal Filtering

If we change “star” to a Greek index, say 7, then the above formulas describe
optimal filtering, generalizing the discussion of §13.3. One sees, for example, that
if the noise amplitudes n. go to zero, so likewise do the noise autocorrelations
Nag, and, canceling a matrix times its inverse, equation (13.6.6) simply becomes
Yy = yy. Another special case occurs if the matrices ¢ag and Nag are diagonal.
In that case, equation (13.6.6) becomes

boy
WY 13.6.9
bay thy? ( )

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

566 Chapter 13. Fourier and Spectral Applications

which is readily recognizable as equation (13.3.6) with S? > ¢,,, N? > ny. What
is going on is this: For the case of equally spaced data points, and in the Fourier
domain, autocorrelations become simply squares of Fourier amplitudes (Wiener-
Khinchin theorem, equation 12.0.12), and the optimal filter can be constructed
algebraically, as equation (13.6.9), without inverting any matrix.

More generally, in the time domain, or any other domain, an optimal filter (one
that minimizes the square of the discrepancy from the underlying true value in the
presence of measurement noise) can be constructed by estimating the autocorrelation
matrices dag and nog, and applying equation (13.6.6) with x — 7. (Equation
13.6.8 is in fact the basis for the §13.3’s statement that even crude optimal filtering
can be quite effective.)

Linear Prediction

Classical linear prediction specializes to the case where the data points yg
are equally spaced along a line, y;, 7 = 1,2,...,N, and we want to use MZ
consecutive values of y; to predict an M + 1st. Stationarity is assumed. That is, the
autocorrelation (y;4;) is assumed to depend only on the difference |j — k|, and not
on j or k individually, so that the autocorrelation ¢ has only a single index,

N-j

Se vives; (13.6.10)

i=l

61 = (vibes) & 5

5 YiYi+g N-j
Here, the approximate equality shows one way to use the actual data set values to
estimate the autocorrelation components. (In fact, there is a better way to make these
estimates; see below.) In the situation described, the estimation equation (13.6.2) is

M

Yn = > djYn—j +n (13.6.1)
j=l

(compare equation 13.5.1) and equation (13.6.5) becomes the set of 1/7 equations for
the M unknown d,’s, now called the linear prediction (LP) coefficients,

M
So byw dj = Ge (= 1,.--,M) (13.6.12)
j=l

Notice that while noise is not explicitly included in the equations, it is properly
accounted for, if it is point-to-point uncorrelated: @o, as estimated by equation
(13.6.10) using measured values y/, actually estimates the diagonal part of daa +Maas
above. The mean square discrepancy (2?) is estimated by equation (13.6.7) as

(x?) = ¢0 — 1di — b2d2 —--»—-boudur (13.6.13)

To use linear prediction, we first compute the d;’s, using equations (13.6.10)
and (13.6.12). We then calculate equation (13.6.13) or, more concretely, apply
(13.6.11) to the known record to get an idea of how large are the discrepancies x ;.
If the discrepancies are small, then we can continue applying (13.6.11) right on into

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.6 Linear Prediction and Linear Predictive Coding 567

the future, imagining the unknown “future” discrepancies x; to be zero. In this
application, (13.6.11) is a kind of extrapolation formula. In many situations, this
extrapolation turns out to be vastly more powerful than any kind of simple polynomial
extrapolation. (By the way, you should not confuse the terms “linear prediction” and
“linear extrapolation”; the general functional form used by linear prediction is much
more complex than a straight line, or even a low-order polynomial!)

However, to achieve its full usefulness, linear prediction must be constrained in
one additional respect: One must take additional measures to guarantee its stability.
Equation (13.6.11) is a special case of the general linear filter (13.5.1). The condition
that (13.6.11) be stable as a linear predictor is precisely that given in equations
(13.5.5) and (13.5.6), namely that the characteristic polynomial

N
2N ~ So djzN =0 (13.6.14)

j=l
have all N of its roots inside the unit circle,
jz) <1 (13.6.15)

There is no guarantee that the coefficients produced by equation (13.6.12) will have
this property. If the data contain many oscillations without any particular trend
towards increasing or decreasing amplitude, then the complex roots of (13.6.14)
will generally all be rather close to the unit circle. The finite length of the data
set will cause some of these roots to be inside the unit circle, others outside. In
some applications, where the resulting instabilities are slowly growing and the linear
prediction is not pushed too far, it is best to use the “unmassaged” LP coefficients
that come directly out of (13.6.12). For example, one might be extrapolating to fill a
short gap in a data set; then one might extrapolate both forwards across the gap and
backwards from the data beyond the gap. If the two extrapolations agree tolerably
well, then instability is not a problem.
When instability is a problem, you have to “massage” the LP coefficients. You
do this by (i) solving (numerically) equation (13.6.14) for its N complex roots; (ii)
moving the roots to where you think they ought to be inside or on the unit circle; (iii)
reconstituting the now-modified LP coefficients. You may think that step (ii) sounds
a little vague. It is. There is no “best” procedure. If you think that your signal
is truly a sum of undamped sine and cosine waves (perhaps with incommensurate
periods), then you will want simply to move each root z; onto the unit circle,

a > z/ lzil (13.6.16)

In other circumstances it may seem appropriate to reflect a bad root across the
unit circle

zi > 1/zi* (13.6.17)
This alternative has the property that it preserves the amplitude of the output of

(13.6.11) when it is driven by a sinusoidal set of x;’s. It assumes that (13.6.12)
has correctly identified the spectral width of a resonance, but only slipped up on

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

568 Chapter 13. Fourier and Spectral Applications

identifying its time sense so that signals that should be damped as time proceeds end
up growing in amplitude. The choice between (13.6.16) and (13.6.17) sometimes
might as well be based on voodoo. We prefer (13.6.17).

Also magical is the choice of /, the number of LP coefficients to use. You
should choose MM to be as small as works for you, that is, you should choose it by
experimenting with your data. Try M = 5,10, 20,40. If you need larger M’s than
this, be aware that the procedure of “massaging” all those complex roots is quite
sensitive to roundoff error. Use double precision.

Linear prediction is especially successful at extrapolating signals that are smooth
and oscillatory, though not necessarily periodic. In such cases, linear prediction often
extrapolates accurately through many cycles of the signal. By contrast, polynomial
extrapolation in general becomes seriously inaccurate after at most a cycle or two.
A prototypical example of a signal that can successfully be linearly predicted is the
height of ocean tides, for which the fundamental 12-hour period is modulated in
phase and amplitude over the course of the month and year, and for which local
hydrodynamic effects may make even one cycle of the curve look rather different
in shape from a sine wave.

We already remarked that equation (13.6.10) is not necessarily the best way
to estimate the covariances ¢; from the data set. In fact, results obtained from
linear prediction are remarkably sensitive to exactly how the ¢;,’s are estimated.
One particularly good method is due to Burg [1], and involves a recursive procedure
for increasing the order M by one unit at a time, at each stage re-estimating the
coefficients dj, 7 = 1,...,.M so as to minimize the residual in equation (13.6.13).
Although further discussion of the Burg method is beyond our scope here, the method
is implemented in the following routine [1,2] for estimating the LP coefficients d;
of a data set.

#include <math.h>
#include "nrutil.h"

void memcof(float data[], int n, int m, float *xms, float d[])
Given a real vector of data[1..n], and given m, this routine returns m linear prediction coef-
ficients as d[1..m], and returns the mean square discrepancy as xms.
t
int k,j,i;
float p=0.0,*wk1,*wk2,*wkm;

wk1i=vector(1,n);

wk2=vector(1,n);

wkm=vector(1,m);

for (j=1;j<=n;jt++) p += SQR(datalj]);
*xms=p/n;

wk1[1]=data[1];

n-1;j++) {
atalj];
wk2[j-1]=data[j];

}
for (k=1;k<=m;k++) {

float m .0,denom=0.0;
for (j=1;j<=(-k); j++) {
num += wki[j]*wk2[j];
denom += SQR(wk1[j])+SQR(wk2[j]) ;
}
d[k]=2.0*num/denom;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.6 Linear Prediction and Linear Predictive Coding 569

axms *= (1.0-SQR(d[k]));

for (i=1;i<=(k-1) ;it+)
ai] =wkm [i] -d [k] *wkm [k-i] ;
The algorithm is recursive, building up the answer for larger and larger values of m
until the desired value is reached. At this point in the algorithm, one could return
the vector d and scalar xms for a set of LP coefficients with k (rather than m)
terms.

if (k == m) {
free_vector(wkm,1,m);
free_vector(wk2,1,n);
free_vector(wk1,1,n);
return;

}

for (i=1;i<=k;i++) wkm[i]=d[i];

for (j=1;j<=(n-k-1); j++) {
wk1(j] -= wkm[k]*wk2[j];
wk2([j]=wk2[j+1] -wkm [k] *wk1[j+1];

}

+

nrerror("never get here in memcof.");

Here are procedures for rendering the LP coefficients stable (if you choose to
do so), and for extrapolating a data set by linear prediction, using the original or
massaged LP coefficients. The routine zroots (§9.5) is used to find all complex
roots of a polynomial.

#include <math.h>

#include "complex.h"

#tdefine NMAX 100 Largest expected value of m.
#define ZERO Complex(0.0,0.0)

#define ONE Complex(1.0,0.0)

void fixrts(float d[], int m)
Given the LP coefficients d[1..m], this routine finds all roots of the characteristic polynomial
(13.6.14), reflects any roots that are outside the unit circle back inside, and then returns a
modified set of coefficients d[1..m].
{

void zroots(fcomplex a[], int m, fcomplex roots[], int polish);

int i,j,polish;

fcomplex a[NMAX] ,roots[NMAX] ;

a(m]=ONE;

for (j=m-1;j>=0;j--) Set up complex coefficients for polynomial root
a[j]=Complex(-d[m-j] ,0.0); finder.

polish=1;

zroots(a,m,roots,polish) ; Find all the roots.

for (j=1;j<=m; j++) Look for a...
if (Cabs(roots[j]) > 1.0) root outside the unit circle,

roots[j]=Cdiv(ONE,Conjg(roots[j])); and reflect it back inside.
a[l0]=Csub(ZERO,roots[1]); Now reconstruct the polynomial coefficients,

a(1]=ONE;

by looping over the roots

and synthetically multiplying.
ali-1]=Csub(a[i-2] ,Cmul (roots [j] ,ali-1]));
a[0]=Csub(ZERO,Cmul(roots[j],a[0]));
}
for (j= 1; j++) The polynomial coefficients are guaranteed to be
d[m-j] = -aljl.r; real, so we need only return the real part as
3 new LP coefficients.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

570 Chapter 13. Fourier and Spectral Applications

#include "nrutil.h"

void predic(float data[], int ndata, float d[], int m, float future[],

int nfut)
Given data[1..ndata], and given the data’s LP coefficients d[1..m], this routine ap-
plies equation (13.6.11) to predict the next nfut data points, which it returns in the array
future[1..nfut]. Note that the routine references only the last m values of data, as initial
values for the prediction.

{
int k,j;
float sum,discrp,*reg;
reg=vector(1,m);
j<=m;j++) reg[j]=data[ndata+1-j] ;
nfut;j++) {
discrp=0.0;
This is where you would put in a known discrepancy if you were reconstructing a
function by linear predictive coding rather than extrapolating a function by linear pre-
diction. See text.
sum=discrp;
for (k=1;k<=m;k++) sum += d[k]*reg[k] ;
for (k=m;k>=2;k--) reg[k]=reg[k-1]; [If you want to implement circular
future [j]=reg[1]=sum; arrays, you can avoid this shift-
} ing of coefficients.]
free_vector(reg,1,m);
3

Removing the Bias in Linear Prediction

You might expect that the sum of the d;’s in equation (13.6.11) (or, more
generally, in equation 13.6.2) should be 1, so that (e.g.) adding a constant to all the
data points y; yields a prediction that is increased by the same constant. However,
the d;’s do not sum to 1 but, in general, to a value slightly less than one. This fact
reveals a subtle point, that the estimator of classical linear prediction is not unbiased,
even though it does minimize the mean square discrepancy. At any place where the
measured autocorrelation does not imply a better estimate, the equations of linear
prediction tend to predict a value that tends towards zero.

Sometimes, that is just what you want. If the process that generates the y;’s
in fact has zero mean, then zero is the best guess absent other information. At
other times, however, this behavior is unwarranted. If you have data that show
only small variations around a positive value, you don’t want linear predictions
that droop towards zero.

Often it is a workable approximation to subtract the mean off your data set,
perform the linear prediction, and then add the mean back. This procedure contains
the germ of the correct solution; but the simple arithmetic mean is not quite the
correct constant to subtract. In fact, an unbiased estimator is obtained by subtracting
from every data point an autocorrelation-weighted mean defined by [3,4]

g= > [ou + Tywlag ue/ > [op + Twwlag (13.6.18)
B

a8

With this subtraction, the sum of the LP coefficients should be unity, up to roundoff
and differences in how the @;’s are estimated.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.6 Linear Prediction and Linear Predictive Coding 571

Linear Predictive Coding (LPC)

A different, though related, method to which the formalism above can be
applied is the “compression” of a sampled signal so that it can be stored more
compactly. The original form should be exactly recoverable from the compressed
version. Obviously, compression can be accomplished only if there is redundancy
in the signal. Equation (13.6.11) describes one kind of redundancy: It says that
the signal, except for a small discrepancy, is predictable from its previous values
and from a small number of LP coefficients. Compression of a signal by the use o
(13.6.11) is thus called linear predictive coding, or LPC.

The basic idea of LPC (in its simplest form) is to record as a compressed file (i)
the number of LP coefficients IM, (ii) their 17 values, e.g., as obtained by memcof,
(iii) the first 1/7 data points, and then (iv) for each subsequent data point only its
residual discrepancy x; (equation 13.6.1). When you are creating the compresse
file, you find the residual by applying (13.6.1) to the previous M/ points, subtracting
the sum from the actual value of the current point. When you are reconstructing the
original file, you add the residual back in, at the point indicated in the routine predic.
It may not be obvious why there is any compression at all in this scheme. After
all, we are storing one value of residual per data point! Why not just store the original
data point? The answer depends on the relative sizes of the numbers involved. The
residual is obtained by subtracting two very nearly equal numbers (the data and the
linear prediction). Therefore, the discrepancy typically has only a very small number
of nonzero bits. These can be stored in a compressed file. How do you do it in a
high-level language? Here is one way: Scale your data to have integer values, say
between + 1000000 and — 1000000 (supposing that you need six significant figures).
Modify equation (13.6.1) by enclosing the sum term in an “integer part of” operator.
The discrepancy will now, by definition, be an integer. Experiment with different
values of M, to find LP coefficients that make the range of the discrepancy as small
as you can. If you can get to within a range of +127 (and in our experience this is not
at all difficult) then you can write it to a file as a single byte. This is a compression
factor of 4, compared to 4-byte integer or floating formats.

Notice that the LP coefficients are computed using the quantized data, and that
the discrepancy is also quantized, i.e., quantization is done both outside and inside
the LPC loop. If you are careful in following this prescription, then, apart from the
initial quantization of the data, you will not introduce even a single bit of roundoff
error into the compression-reconstruction process: While the evaluation of the sum
in (13.6.11) may have roundoff errors, the residual that you store is the value which,
when added back to the sum, gives exactly the original (quantized) data value. Notice
also that you do not need to massage the LP coefficients for stability; by adding the
residual back in to each point, you never depart from the original data, so instabilities
cannot grow. There is therefore no need for fixrts, above.

Look at §20.4 to learn about Huffman coding, which will further compress the
residuals by taking advantage of the fact that smaller values of discrepancy will occur
more often than larger values. A very primitive version of Huffman coding would
be this: If most of the discrepancies are in the range +127, but an occasional one is
outside, then reserve the value 127 to mean “out of range,” and then record on the file
(immediately following the 127) a full-word value of the out-of-range discrepancy.
§20.4 explains how to do much better.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

572 Chapter 13. Fourier and Spectral Applications

There are many variant procedures that all fall under the rubric of LPC.

e If the spectral character of the data is time-variable, then it is best not
to use a single set of LP coefficients for the whole data set, but rather
to partition the data into segments, computing and storing different LP
coefficients for each segment.

e If the data are really well characterized by their LP coefficients, and you
can tolerate some small amount of error, then don’t bother storing all of the
residuals. Just do linear prediction until you are outside of tolerances, then
reinitialize (using | sequential stored residuals) and continue predicting.

e In some applications, most notably speech synthesis, one cares only about
the spectral content of the reconstructed signal, not the relative phases.
In this case, one need not store any starting values at all, but only the
LP coefficients for each segment of the data. The output is reconstructed
by driving these coefficients with initial conditions consisting of all zeros
except for one nonzero spike. A speech synthesizer chip may have of
order 10 LP coefficients, which change perhaps 20 to 50 times per second.

e Some people believe that it is interesting to analyze a signal by LPC, even
when the residuals x; are not small. The x,’s are then interpreted as the
underlying “input signal” which, when filtered through the all-poles filter
defined by the LP coefficients (see §13.7), produces the observed “output
signal.” LPC reveals simultaneously, it is said, the nature of the filter and
the particular input that is driving it. We are skeptical of these applications;
the literature, however, is full of extravagant claims.

CITED REFERENCES AND FURTHER READING:

Childers, D.G. (ed.) 1978, Modern Spectrum Analysis (New York: IEEE Press), especially the
paper by J. Makhoul (reprinted from Proceedings of the IEEE, vol. 63, p. 561, 1975).

Burg, J.P. 1968, reprinted in Childers, 1978. [1]

Anderson, N. 1974, reprinted in Childers, 1978. [2]

Cressie, N. 1991, in Spatial Statistics and Digital Image Analysis (Washington: National Academy
Press). [3]

Press, W.H., and Rybicki, G.B. 1992, Astrophysical Journal, vol. 398, pp. 169-176. [4]

13.7 Power Spectrum Estimation by the
Maximum Entropy (All Poles) Method

The FFT is not the only way to estimate the power spectrum of a process, nor is it
necessarily the best way for all purposes. To see how one might devise another method,
let us enlarge our view for a moment, so that it includes not only real frequencies in the
Nyquist interval —f. < f < fc, but also the entire complex frequency plane. From that
vantage point, let us transform the complex f-plane to a new plane, called the z-transform
plane or z-plane, by the relation

zeenta (13.7.1)

where A is, as usual, the sampling interval in the time domain. Notice that the Nyquist interval
on the real axis of the f-plane maps one-to-one onto the unit circle in the complex z-plane.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.7 Maximum Entropy (All Poles) Method 573

If we now compare (13.7.1) to equations (13.4.4) and (13.4.6), we see that the FFT
power spectrum estimate (13.4.5) for any real sampled function c, = c(t,) can be written,
except for normalization convention, as

N/2-1

P(f)=| So cuz* (13.7.2)

k=—N/2

Of course, (13.7.2) is not the true power spectrum of the underlying function c(t), but only an
estimate. We can see in two related ways why the estimate is not likely to be exact. First, in the
time domain, the estimate is based on only a finite range of the function c(t) which may, for all
we know, have continued from t = —oo to oo. Second, in the z-plane of equation (13.7.2), the
finite Laurent series offers, in general, only an approximation to a general analytic function of
z. In fact, a formal expression for representing “true” power spectra (up to normalization) is

2

P(f)= > cro" (13.7.3)
k=—oo

This is an infinite Laurent series which depends on an infinite number of values c,. Equation
(13.7.2) is just one kind of analytic approximation to the analytic function of z represented
by (13.7.3); the kind, in fact, that is implicit in the use of FFTs to estimate power spectra by
periodogram methods. It goes under several names, including direct method, all-zero model,
and moving average (MA) model. The term “all-zero” in particular refers to the fact that the
model spectrum can have zeros in the z-plane, but not poles.

If we look at the problem of approximating (13.7.3) more generally it seems clear that
we could do a better job with a rational function, one with a series of type (13.7.2) in both the
numerator and the denominator. Less obviously, it turns out that there are some advantages in
an approximation whose free parameters all lie in the denominator, namely,

P(f)® (13.7.4)

Here the second equality brings in a new set of coefficients a,,’s, which can be determined
from the b,’s using the fact that z lies on the unit circle. The b;,’s can be thought of as
being determined by the condition that power series expansion of (13.7.4) agree with the
first M + 1 terms of (13.7.3). In practice, as we shall see, one determines the b;,’s or
ax’s by another method.

The differences between the approximations (13.7.2) and (13.7.4) are not just cosmetic.
They are approximations with very different character. Most notable is the fact that (13.7.4)
can have poles, corresponding to infinite power spectral density, on the unit z-circle, i.e., at
real frequencies in the Nyquist interval. Such poles can provide an accurate representation
for underlying power spectra that have sharp, discrete “lines” or delta-functions. By contrast,
(13.7.2) can have only zeros, not poles, at real frequencies in the Nyquist interval, and must
thus attempt to fit sharp spectral features with, essentially, a polynomial. The approximation
(13.7.4) goes under several names: all-poles model, maximum entropy method (MEM),
autoregressive model (AR). We need only find out how to compute the coefficients ap and the
ax’s from a data set, so that we can actually use (13.7.4) to obtain spectral estimates.

A pleasant surprise is that we already know how! Look at equation (13.6.11) for linear
prediction. Compare it with linear filter equations (13.5.1) and (13.5.2), and you will see that,
viewed as a filter that takes input «:’s into output y’s, linear prediction has a filter function

Hf) = —— (13.7.5)

1- So djz
1

Thus, the power spectrum of the y’s should be equal to the power spectrum of the «:’s
multiplied by |H(f)|?. Now let us think about what the spectrum of the input x’s is, when

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

574 Chapter 13. Fourier and Spectral Applications

they are residual discrepancies from linear prediction. Although we will not prove it formally,
it is intuitively believable that the x’s are independently random and therefore have a flat
(white noise) spectrum. (Roughly speaking, any residual correlations left in the «’s would
have allowed a more accurate linear prediction, and would have been removed.) The overall
normalization of this flat spectrum is just the mean square amplitude of the x’s. But this is
exactly the quantity computed in equation (13.6.13) and returned by the routine memcof as
xms. Thus, the coefficients ap and a, in equation (13.7.4) are related to the LP coefficients
returned by memcof simply by

ao = xms ap =—d(k), k=1,...,M (13.7.6)

There is also another way to describe the relation between the a,’s and the autocorrelation
components @,. The Wiener-Khinchin theorem (12.0.12) says that the Fourier transform of
the autocorrelation is equal to the power spectrum. In z-transform language, this Fourier
transform is just a Laurent series in z. The equation that is to be satisfied by the coefficients
in equation (13.7.4) is thus

M
8 oye! (13.7.7)

M
[i+ > apz® j=-M
k=1

The approximately equal sign in (13.7.7) has a somewhat special interpretation. It means
that the series expansion of the left-hand side is supposed to agree with the right-hand side
term by term from z ™ to z™. Outside this range of terms, the right-hand side is obviously
zero, while the left-hand side will still have nonzero terms. Notice that 1/7, the number of
coefficients in the approximation on the left-hand side, can be any integer up to N, the total
number of autocorrelations available. (In practice, one often chooses M much smaller than
NN.) M is called the order or number of poles of the approximation.

Whatever the chosen value of IM, the series expansion of the left-hand side of (13.7.7)
defines a certain sort of extrapolation of the autocorrelation function to lags larger than /, in
fact even to lags larger than N, i.e., larger than the run of data can actually measure. It turns
out that this particular extrapolation can be shown to have, among all possible extrapolations,
the maximum entropy in a definable information-theoretic sense. Hence the name maximum
entropy method, or MEM. The maximum entropy property has caused MEM to acquire a
certain “cult” popularity; one sometimes hears that it gives an intrinsically “better” estimate
than is given by other methods. Don’t believe it. MEM has the very cute property of
being able to fit sharp spectral features, but there is nothing else magical about its power
spectrum estimates.

The operations count in memcof scales as the product of N (the number of data points)
and M (the desired order of the MEM approximation). If M4 were chosen to be as large as
N, then the method would be much slower than the N log N FFT methods of the previous
section. In practice, however, one usually wants to limit the order (or number of poles) of the
MEM approximation to a few times the number of sharp spectral features that one desires it
to fit. With this restricted number of poles, the method will smooth the spectrum somewhat,
but this is often a desirable property. While exact values depend on the application, one
might take 1 = 10 or 20 or 50 for N = 1000 or 10000. In that case MEM estimation is
not much slower than FFT estimation.

We feel obliged to warn you that memcof can be a bit quirky at times. If the number of
poles or number of data points is too large, roundoff error can be a problem, even in double
precision. With “peaky” data (i.e., data with extremely sharp spectral features), the algorithm
may suggest split peaks even at modest orders, and the peaks may shift with the phase of the
sine wave. Also, with noisy input functions, if you choose too high an order, you will find
spurious peaks galore! Some experts recommend the use of this algorithm in conjunction with
more conservative methods, like periodograms, to help choose the correct model order, and to
avoid getting too fooled by spurious spectral features. MEM can be finicky, but it can also do
remarkable things. We recommend that you try it out, cautiously, on your own problems. We
now turn to the evaluation of the MEM spectral estimate from its coefficients.

The MEM estimation (13.7.4) is a function of continuously varying frequency f. There
is no special significance to specific equally spaced frequencies as there was in the FFT case.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.8 Spectral Analysis of Unevenly Sampled Data 575

In fact, since the MEM estimate may have very sharp spectral features, one wants to be able to
evaluate it on a very fine mesh near to those features, but perhaps only more coarsely farther
away from them. Here is a function which, given the coefficients already computed, evaluates
(13.7.4) and returns the estimated power spectrum as a function of fA (the frequency times
the sampling interval). Of course, f A should lie in the Nyquist range between —1/2 and 1/2.

#include <math.h>

float evlmem(float fdt, float d[], int m, float xms)

Given d[1..m], m, xms as returned by memcof, this function returns the power spectrum
estimate P(f) as a function of fdt = fA.

{

int i;
float sumr=1.0,sumi=0.0;
double wr=1.0,wi=0.0,wpr,wpi,wtemp,theta; Trig. recurrences in double precision.

theta=6 .28318530717959*f dt ;

wpr=cos(theta) ; Set up for recurrence relations.
wpi=sin(theta) ;

for (i=1;i<=m;i++) {
‘temp=wr) *wpr-wi*wpi ;
*wprtwtemp*wpi ;

sumr -= d[i]*wr; These accumulate the denominator of (13.7.4).
sumi -= d[i]*wi;

Loop over the terms in the sum.

+

return xms/(sumr*sumr+sumi*sumi); Equation (13.7.4).

Be sure to evaluate P(f) on a fine enough grid to find any narrow features that may
be there! Such narrow features, if present, can contain virtually all of the power in the data.
You might also wish to know how the P(f) produced by the routines memcof and evlmem is
normalized with respect to the mean square value of the input data vector. The answer is

1/2 1/2
[ P(fA)d(fA) =2 P(fA)d(fA) = mean square value of data (13.7.8)
J-1/2 Jo

Sample spectra produced by the routines memcof and evlmem are shown in Figure 13.7.1.

CITED REFERENCES AND FURTHER READING:
Childers, D.G. (ed.) 1978, Modern Spectrum Analysis (New York: IEEE Press), Chapter Il.
Kay, S.M., and Marple, S.L. 1981, Proceedings of the IEEE, vol. 69, pp. 1380-1419.

13.8 Spectral Analysis of Unevenly Sampled
Data

Thus far, we have been dealing exclusively with evenly sampled data,

hn =h(nA)  n=...,—3,-2,-1,0,1,2,3,... (13.8.1)

where A is the sampling interval, whose reciprocal is the sampling rate. Recall also (§12.1)
the significance of the Nyquist critical frequency
1

fe=5x (13.8.2)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

576 Chapter 13. Fourier and Spectral Applications

1000

100

10

power spectral densitty

0.1

frequency f

Figure 13.7.1. Sample output of maximum entropy spectral estimation. The input signal consists of
512 samples of the sum of two sinusoids of very nearly the same frequency, plus white noise with about
equal power. Shown is an expanded portion of the full Nyquist frequency interval (which would extend
from zero to 0.5). The dashed spectral estimate uses 20 poles; the dotted, 40; the solid, 150. With the
larger number of poles, the method can resolve the distinct sinusoids; but the flat noise background is
beginning to show spurious peaks. (Note logarithmic scale.)

as codified by the sampling theorem: A sampled data set like equation (13.8.1) contains
complete information about all spectral components in a signal h(t) up to the Nyquist
frequency, and scrambled or aliased information about any signal components at frequencies
larger than the Nyquist frequency. The sampling theorem thus defines both the attractiveness,
and the limitation, of any analysis of an evenly spaced data set.

There are situations, however, where evenly spaced data cannot be obtained. A common
case is where instrumental drop-outs occur, so that data is obtained only on a (not consecutive
integer) subset of equation (13.8.1), the so-called missing data problem. Another case,
common in observational sciences like astronomy, is that the observer cannot completely
control the time of the observations, but must simply accept a certain dictated set of f;’s.

There are some obvious ways to get from unevenly spaced t;’s to evenly spaced ones, as
in equation (13.8.1). Interpolation is one way: lay down a grid of evenly spaced times on your
data and interpolate values onto that grid; then use FFT methods. In the missing data problem,
you only have to interpolate on missing data points. If a lot of consecutive points are missing,
you might as well just set them to zero, or perhaps “clamp” the value at the last measured point.
However, the experience of practitioners of such interpolation techniques is not reassuring.
Generally speaking, such techniques perform poorly. Long gaps in the data, for example,
often produce a spurious bulge of power at low frequencies (wavelengths comparable to gaps).

A completely different method of spectral analysis for unevenly sampled data, one that
mitigates these difficulties and has some other very desirable properties, was developed by
Lomb [1], based in part on earlier work by Barning [2] and Vaniéek [3], and additionally
elaborated by Scargle [4]. The Lomb method (as we will call it) evaluates data, and sines

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.8 Spectral Analysis of Unevenly Sampled Data 577

and cosines, only at times ¢; that are actually measured. Suppose that there are N data
points hi = h(ti), i = 1,...,.N. Then first find the mean and variance of the data by
the usual formulas,

~ 12
hay dm

Now, the Lomb normalized periodogram (spectral power as a function of angular
frequency w = 2nf > 0) is defined by

= wo hy (13.83)

1 [hy —h) cosw(t; — ny [x, (hj — h)sinw(t; — ny)

Py(w) =
ww) 20? cos? w(t; — 7) + yd; sin? w(t; — 7)
(13.8.4)
Here 7 is defined by the relation
, Sin Qwt.
tan(Qwr) = Xi ; (13.8.5)

Do cos wt;

The constant 7 is a kind of offset that makes Py (w) completely independent of shifting
all the ¢;’s by any constant. Lomb shows that this particular choice of offset has another,
deeper, effect: It makes equation (13.8.4) identical to the equation that one would obtain if one
estimated the harmonic content of a data set, at a given frequency w, by linear least-squares
fitting to the model

h(t) = Acoswt + Bsinwt (13.8.6)

This fact gives some insight into why the method can give results superior to FFT methods: It
weights the data on a “per point” basis instead of on a “per time interval” basis, when uneven
sampling can render the latter seriously in error.

A very common occurrence is that the measured data points h; are the sum of a periodic
signal and independent (white) Gaussian noise. If we are trying to determine the presence
or absence of such a periodic signal, we want to be able to give a quantitative answer to
the question, “How significant is a peak in the spectrum Py (w)?” In this question, the null
hypothesis is that the data values are independent Gaussian random values. A very nice
property of the Lomb normalized periodogram is that the viability of the null hypothesis can
be tested fairly rigorously, as we now discuss.

The word “normalized” refers to the factor 0” in the denominator of equation (13.8.4).
Scargle [4] shows that with this normalization, at any particular w and in the case of the null
hypothesis, Px (w) has an exponential probability distribution with unit mean. In other words,
the probability that Px (w) will be between some positive z and z + dz is exp(—z)dz. It
readily follows that, if we scan some M independent frequencies, the probability that none
give values larger than z is (1 — e*)”. So

P(>z)=1-(

(13.8.7)

is the false-alarm probability of the null hypothesis, that is, the significance level of any peak
in Py (w) that we do see. A small value for the false-alarm probability indicates a highly
significant periodic signal.

To evaluate this significance, we need to know M. After all, the more frequencies we
look at, the less significant is some one modest bump in the spectrum. (Look long enough,
find anything!) A typical procedure will be to plot Py(w) as a function of many closely
spaced frequencies in some large frequency range. How many of these are independent?

Before answering, let us first see how accurately we need to know M. The interesting
region is where the significance is a small (significant) number, < 1. There, equation (13.8.7)
can be series expanded to give

P(> 2) ® Me* (13.8.8)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

578 Chapter 13. Fourier and Spectral Applications

} ol T T T T T T j
2b 00 4
° ° q
ie) 6 oe P 00 ° o (? :|
2 IP fo) ° 0 OY oO
no} & O° fe) of fe)
f=} ° ° fo) fo)
= oF ° ° @ oO ° %
= 5 Oo 8 °° 8 2 © ° ° 4
Sob ° 5% 2000290 ° o 9° 4
2 50 0 ° ° q
° 20 6 fo) O64 o 4 F|
oe fo) ° ° ° q
biti L | L | L ! L ! |
0 10 20 30 40 50 60 70 80 90 100
time
14 T T T T Ty
12F-
10F -
3 8F
a
2 6b
4e 4
2b 4
ORL” in iti raul it petri iti tii id
0 l 2 3 A 5 6 7 8 9 1
frequency
Figure 13.8.1. _ Example of the Lomb algorithm in action. The 100 data points (upper figure) are at

random times between 0 and 100. Their sinusoidal component is readily uncovered (lower figure) by
the algorithm, at a significance level better than 0.001. If the 100 data points had been evenly spaced at
unit interval, the Nyquist critical frequency would have been 0.5. Note that, for these unevenly spaced
points, there is no visible aliasing into the Nyquist range.

We see that the significance scales linearly with 17. Practical significance levels are numbers
like 0.05, 0.01, 0.001, etc. An error of even +50% in the estimated significance is often
tolerable, since quoted significance levels are typically spaced apart by factors of 5 or 10. So
our estimate of M need not be very accurate.

Horne and Baliunas [5] give results from extensive Monte Carlo experiments for deter-
mining M in various cases. In general M depends on the number of frequencies sampled,
the number of data points NV, and their detailed spacing. It turns out that / is very nearly
equal to N when the data points are approximately equally spaced, and when the sampled
frequencies “fill” (oversample) the frequency range from 0 to the Nyquist frequency f-
(equation 13.8.2). Further, the value of M is not importantly different for random spacing of
the data points than for equal spacing. When a larger frequency range than the Nyquist range
is sampled, M increases proportionally. About the only case where M differs significantly
from the case of evenly spaced points is when the points are closely clumped, say into
groups of 3; then (as one would expect) the number of independent frequencies is reduced
by a factor of about 3.

The program period, below, calculates an effective value for MW based on the above
rough-and-ready rules and assumes that there is no important clumping. This will be adequate
for most purposes. In any particular case, if it really matters, it is not too difficult to compute
a better value of MM by simple Monte Carlo: Holding fixed the number of data points and their
locations t;, generate synthetic data sets of Gaussian (normal) deviates, find the largest values
of Py (w) for each such data set (using the accompanying program), and fit the resulting
distribution for M in equation (13.8.7).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.8 Spectral Analysis of Unevenly Sampled Data 579

Figure 13.8.1 shows the results of applying the method as discussed so far. In the
upper figure, the data points are plotted against time. Their number is N = 100, and their
distribution in ¢ is Poisson random. There is certainly no sinusoidal signal evident to the eye.
The lower figure plots Py (w) against frequency f = w/27. The Nyquist critical frequency
that would obtain if the points were evenly spaced is at f = f. = 0.5. Since we have searched
up to about twice that frequency, and oversampled the f’s to the point where successive values
of Py (w) vary smoothly, we take 1 = 2N. The horizontal dashed and dotted lines are
(respectively from bottom to top) significance levels 0.5, 0.1, 0.05, 0.01, 0.005, and 0.001.
One sees a highly significant peak at a frequency of 0.81. That is in fact the frequency of the
sine wave that is present in the data. (You will have to take our word for this!)

Note that two other peaks approach, but do not exceed the 50% significance level; that
is about what one might expect by chance. It is also worth commenting on the fact that the
significant peak was found (correctly) above the Nyquist frequency and without any significant
aliasing down into the Nyquist interval! That would not be possible for evenly spaced data. It
is possible here because the randomly spaced data has some points spaced much closer than
the “average” sampling rate, and these remove ambiguity from any aliasing.

Implementation of the normalized periodogram in code is straightforward, with, however,
a few points to be kept in mind. We are dealing with a slow algorithm. Typically, for N data
points, we may wish to examine on the order of 2N or 4N frequencies. Each combination
of frequency and data point has, in equations (13.8.4) and (13.8.5), not just a few adds or
multiplies, but four calls to trigonometric functions; the operations count can easily reach
several hundred times N?. It is highly desirable — in fact results in a factor 4 speedup —
to replace these trigonometric calls by recurrences. That is possible only if the sequence of
frequencies examined is a linear sequence. Since such a sequence is probably what most users
would want anyway, we have built this into the implementation.

At the end of this section we describe a way to evaluate equations (13.8.4) and (13.8.5)
— approximately, but to any desired degree of approximation — by a fast method [6] whose
operation count goes only as N log N. This faster method should be used for long data sets.

The lowest independent frequency f to be examined is the inverse of the span of the
input data, max;(t;) — min,(t;) = T. This is the frequency such that the data can include one
complete cycle. In subtracting off the data’s mean, equation (13.8.4) already assumed that you
are not interested in the data’s zero-frequency piece — which is just that mean value. In an
FFT method, higher independent frequencies would be integer multiples of 1/T’. Because we
are interested in the statistical significance of any peak that may occur, however, we had better
(over-) sample more finely than at interval 1/7’, so that sample points lie close to the top of
any peak. Thus, the accompanying program includes an oversampling parameter, called ofac;
a value ofac Z 4 might be typical in use. We also want to specify how high in frequency
to go, say fri. One guide to choosing f;,; is to compare it with the Nyquist frequency f.
which would obtain if the N data points were evenly spaced over the same span T’,, that is
fe = N/(2T). The accompanying program includes an input parameter hifac, defined as
fni/ fe. The number of different frequencies Np returned by the program is then given by

Np= ome (13.8.9)

(You have to remember to dimension the output arrays to at least this size.)

The code does the trigonometric recurrences in double precision and embodies a few

tricks with trigonometric identities, to decrease roundoff errors. If you are an aficionado of

such things you can puzzle it out. A final detail is that equation (13.8.7) will fail because of
roundoff error if z is too large; but equation (13.8.8) is fine in this regime.

#include <math.h>
#include "nrutil.h"
#define TWOPID 6.2831853071795865

void period(float x[], float y[], int n, float ofac, float hifac, float px[],
float py[], int np, int *nout, int *jmax, float *prob)

Given n data points with abscissas x[1..n] (which need not be equally spaced) and ordinates

y(1..n], and given a desired oversampling factor ofac (a typical value being 4 or larger),

this routine fills array px[1..np] with an increasing sequence of frequencies (not angular

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

580 Chapter 13. Fourier and Spectral Applications

frequencies) up to hifac times the “average” Nyquist frequency, and fills array py[1. .np]
with the values of the Lomb normalized periodogram at those frequencies. The arrays x and y
are not altered. np, the dimension of px and py, must be large enough to contain the output,
or an error results. The routine also returns jmax such that py[jmax] is the maximum element
in py, and prob, an estimate of the significance of that maximum against the hypothesis of
random noise. A small value of prob indicates that a significant periodic signal is present.
t

void avevar(float data[], unsigned long n, float *ave, float *var);

int i,j;

float ave,c,cc,cwtau,effm,expy,pnow,pymax,s,ss,sumc,sumcy,sums,sumsh,

sumsy ,Sswtau,var,wtau,xave,xdif ,xmax,xmin,yy;
double arg,wtemp,*wi,*wpi, *wpr, *wr;

wi=dvector(1,n);
wpi=dvector(1,n);
wpr=dvector(1,n);
wr=dvector(1,n);
*nout=0.5*ofac*hifac*n;
if (*nout > np) nrerror("output arrays too short in period");
avevar(y,n,kave,&var) ; Get mean and variance of the input data.
if (var 0.0) nrerror("zero variance in period");
xmax=xmin=x[1] ; Go through data to get the range of abscis-
for (j=1;j<=n;j++) { sas.
if (x[j] > xmax) xmax=x[j];
if (x[j] < xmin) xmin=x[j];

+
xdif=xmax-xmin;
xave=0.5*(xmax+xmin) ;

pyma: .0;

pnow=1.0/(xdif*ofac) ; Starting frequency.

for (j=1;j<=n;jt+) { Initialize values for the trigonometric recur-
arg=TWOPID*((x[j]-xave) *pnow) ; rences at each data point. The recur-
wpr[j] = -2.0*SQR(sin(0.5*arg)) ; rences are done in double precision.

wpilj]=sin(arg) ;
wr [jl=cos(arg) ;
wiljl=wpilj];

}
for (i=1;i<=(*nout);i++) { Main loop over the frequencies to be evalu-
px[i]=pnow; ated.
sumsh=sumce: First, loop over the data to get 7 and related
for (j=1;j<-n;j++) { quantities.
c=wr [jl];

s=wilj];
sumsh += s*c;
sumc += (c-s)*(c+ts);

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

}
wtau=0.5*atan2(2.0*sumsh,sumc) ;
swtau=sin(wtau) ;
cos (wtau) ;
umc=sumsy=sumcy=0.0; Then, loop over the data again to get the
for (j=1;j<=n;jt+) { periodogram value.
s=wilj];
c=wr [jl];

ss=s*cwtau-c*swtau;

cc=ckcwtauts*swtau;

sums += ss*ss;

sumc += cc*cc;

yy=y [jl-ave;

sumsy += yy*ss;

sumcy += yy*cc;

wr [j]=((wtemp=wr [j])*wpr (j]-wilj]*wpilj])+wrlj]; Update the trigono-
wilj]=(wi [j] *wpr [j]+wtemp*wpi [j])+wil[j]; metric recurrences.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

py [i] =0.5*(sumcy*sumcy/sumct+sumsy*sumsy/sums) /var ;

13.8 Spectral Analysis of Unevenly Sampled Data 581

if (pyli] >= pymax) pymax=py[(*jmax=i)] ;

pnow += 1.0/(ofac*xdif) ; The next frequency.
+
expy=exp (-pymax) ; Evaluate statistical significance of the max-
ef fm=2.0*(*nout) /ofac; imum.

*prob=effm*expy;

if (*prob > 0.01) *prob=1.0-pow(1.0-expy,effm) ;
free_dvector(wr,1,n);

free_dvector(wpr,1,n);

free_dvector(wpi,1,n);

free_dvector(wi,1,n);

Fast Computation of the Lomb Periodogram

We here show how equations (13.8.4) and (13.8.5) can be calculated — approximately,
but to any desired precision — with an operation count only of order Np log Np. The
method uses the FFT, but it is in no sense an FFT periodogram of the data. It is an actual
evaluation of equations (13.8.4) and (13.8.5), the Lomb normalized periodogram, with exactly
that method’s strengths and weaknesses. This fast algorithm, due to Press and Rybicki [6],
makes feasible the application of the Lomb method to data sets at least as large as 10° points;
it Jready faster than straightforward evaluation of equations (13.8.4) and (13.8.5) for data
set mall as 60 or 100 points

Notice that the trigonometric sums that occur in equations (13.8.5) and (13.8.4) can be
reduced to four simpler sums. If we define

N N
Sr = So(hj —h) sin(wt;) Ch = S0(hy — A) cos(wt) (13.8.10)
j=l j=1
and
N N
S2= > sin(2wt;) C2= > cos(2ut;) (13.8.11)
j=1 j=l
then

N
hy —h) cosw(tj — 7) = Ch cosw7 +S), sinwr
j=l

N
Vy —h) sinw(t; — 7) = Sp coswt — Ch sinwt
jh (13.8.12)

2

N
So cos? w(t; -—T)= ry + 502 cos(2wr) + 50 sin(2wT)
j=l

|Z

- 502 cos(2wr) — 50 sin(2wT)

N
Ye sin? w(t; -—T) =
J

Now notice that if the ¢;s were evenly spaced, then the four quantities S;,,C',,S2,and C2 could
be evaluated by two complex FFTs, and the results could then be substituted back through
equation (13.8.12) to evaluate equations (13.8.5) and (13.8.4). The problem is therefore only
to evaluate equations (13.8.10) and (13.8.11) for unevenly spaced data.

Interpolation, or rather reverse interpolation — we will here call it extirpolation —
provides the key. Interpolation, as classically understood, uses several function values on a
regular mesh to construct an accurate approximation at an arbitrary point. Extirpolation, just
the opposite, replaces a function value at an arbitrary point by several function values on a
regular mesh, doing this in such a way that sums over the mesh are an accurate approximation
to sums over the original arbitrary point.

It is not hard to see that the weight functions for extirpolation are identical to those for
interpolation. Suppose that the function h(t) to be extirpolated is known only at the discrete

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

582 Chapter 13. Fourier and Spectral Applications

(unevenly spaced) points h(t;) = hi, and that the function g(t) (which will be, e.g., cos wt)
can be evaluated anywhere. Let f;, be a sequence of evenly spaced points on a regular mesh.
Then Lagrange interpolation (§3.1) gives an approximation of the form

g(t) = So we(t)g(ée) (13.8.13)
k

where wz, (t) are interpolation weights. Now let us evaluate a sum of interest by the following
scheme:

So gg (ts) Yo ha | SF weltao (be) | = 32 SP rjwe(ts)] g(be) = SV he g(be)
j=l j=l k & Lj=l k

(13.8.14)
Here hy = y; hjwr(t;). Notice that equation (13.8.14) replaces the original sum by one
on the regular mesh. Notice also that the accuracy of equation (13.8.13) depends only on the
fineness of the mesh with respect to the function g and has nothing to do with the spacing of the
points ¢; or the function h; therefore the accuracy of equation (13.8.14) has this property.

The general outline of the fast evaluation method is therefore this: (i) Choose a mesh
size large enough to accommodate some desired oversampling factor, and large enough to
have several extirpolation points per half-wavelength of the highest frequency of interest. (ii)
Extirpolate the values h; onto the mesh and take the FFT; this gives S; and C), in equation
(13.8.10). (iii) Extirpolate the constant values 1 onto another mesh, and take its FFT; this,
with some manipulation, gives S2 and C2 in equation (13.8.11). (iv) Evaluate equations
(13.8.12), (13.8.5), and (13.8.4), in that order.

There are several other tricks involved in implementing this algorithm efficiently. You
can figure most out from the code, but we will mention the following points: (a) A nice way
to get transform values at frequencies 2w instead of w is to stretch the time-domain data by a
factor 2, and then wrap it to double-cover the original length. (This trick goes back to Tukey.)
In the program, this appears as a modulo function. (b) Trigonometric identities are used to
get from the left-hand side of equation (13.8.5) to the various needed trigonometric functions
of wr. C identifiers like (e.g.) cwt and hs2wt represent quantities like (e.g.) cosw7 and
3 sin(2wr). (c) The function spread does extirpolation onto the / most nearly centered
mesh points around an arbitrary point; its turgid code evaluates coefficients of the Lagrange
interpolating polynomials, in an efficient manner.

#include <math.h>
#include "nrutil.h"
#define MOD(a,b) while(a >= b) a -= b; Positive numbers only.
#define MACC 4 Number of interpolation points per 1/4
cycle of highest frequency.
void fasper(float x[], float y[], unsigned long n, float ofac, float hifac,
float wki[], float wk2[], unsigned long nwk, unsigned long *nout,
unsigned long *jmax, float *prob)
Given n data points with abscissas x[1..n] (which need not be equally spaced) and ordinates
y(1..n], and given a desired oversampling factor ofac (a typical value being 4 or larger), this
routine fills array Wk1[1..nwk] with a sequence of nout increasing frequencies (not angular
frequencies) up to hifac times the “average” Nyquist frequency, and fills array wk2[1. .nwk]
with the values of the Lomb normalized periodogram at those frequencies. The arrays X and
y are not altered. nwk, the dimension of wk1 and wk2, must be large enough for intermediate
work space, or an error results. The routine also returns jmax such that wk2[jmax] is the
maximum element in wk2, and prob, an estimate of the significance of that maximum against
the hypothesis of random noise. A small value of prob indicates that a significant periodic
signal is present.

void avevar(float data[], unsigned long n, float *ave, float *var);
void realft(float data[], unsigned long n, int isign);

void spread(float y, float yy[], unsigned long n, float x, int m);
unsigned long j,k,ndim,nfreq,nfreqt;

float ave,ck,ckk,cterm,cwt,den,df,effm,expy,fac,fndim,hc2wt;

float hs2wt ,hypo,pmax,sterm,swt ,var,xdif ,xmax,xmin;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.8 Spectral Analysis of Unevenly Sampled Data 583

*nout=0.5*ofac*hifac*n;
nfreqt=ofacthifactn*MACC; Size the FFT as next power of 2 above
nfreq=64; nfreqt.
while (nfreq < nfreqt) nfreq <<= 1;
ndim=nfreq << 1;
if (mdim > nwk) nrerror("workspaces too small in fasper");
avevar(y,n,kave,&var) ; Compute the mean, variance, and range
if (var == 0.0) nrerror("zero variance in fasper") ; of the data.
xmin=x [1];
xmax=xmin;
for (j=2;j<=n;j++) {
if (x[j] < xmin) xmin=x[j];
if (x[j] > xmax) xmax=x[j];

+
xdif=xmax-xmin;
for (j=1;j<=ndim; j++) wk1[j]=wk2[j]=0.0; Zero the workspaces.

fac=ndim/(xdif*ofac) ;
fndim=ndim;
for ( sjsen; j++) { Extirpolate the data into the workspaces.
ck=(x[j]-xmin) *fac;
MOD (ck, fndim)
ckk=2.0*(ck++) ;
MOD (ckk , fndim)
++ckk;
spread(y[j]-ave,wk1,ndim,ck,MACC) ;
spread(1.0,wk2,ndim,ckk,MACC) ;

+
realft(wki,ndim,1);
realft (wk2,ndim,1);
df=1.0/(xdif*ofac) ;
-1.0;

»j=1; j<=(*nout) ;j++,k+=2) { Compute the Lomb value for each fre-
hypo=sqrt (wk2[k] *wk2 [k] +wk2 [k+1] *wk2[k+1]) ; quency.

he2wt=0.5+*wk2[k] /hypo;

hs2ut=0.5*wk2[k+1] /hypo;

ewt=sqrt (0.5+hc2wt) ;

swt=SIGN(sqrt (0.5-hc2wt) ,hs2wt) ;

den=0.5*n+hc2wt*wk2[k] +hs2wt*wk2[k+1] ;

cterm=SQR (cwt*wk1 [k] +swt*wk1i[k+1])/den;

sterm=SQR (cwt*wk1 [k+1] -swt*wk1[k] )/(n-den) ;

wk1 [j]=j*df;

wk2[j]=(cterm+sterm) /(2.0*var) ;

if (wk2[j] > pmax) pmax=wk2[(*jmax=j)];

Take the Fast Fourier Transforms.

+

expy=exp (—pmax) ; Estimate significance of largest peak value.
ef fm=2.0*(*nout) /ofac;

*prob=ef fm*expy;

if (*prob > 0.01) *prob=1.0-pow(1.0-expy,effm) ;

#include "nrutil.h"

void spread(float y, float yy[], unsigned long n, float x, int m)

Given an array yy[1..n], extirpolate (spread) a value y into m actual array elements that best
approximate the “fictional” (i.e., possibly noninteger) array element number x. The weights
used are coefficients of the Lagrange interpolating polynomial.

{

int ihi,ilo,ix,j,nden;
static long nfac[11]={0,1,1,2,6,24,120,720,5040, 40320, 362880};
float fac;

if (m > 10) nrerror("factorial table too small in spread");

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

584 Chapter 13. Fourier and Spectral Applications

ix=(int)x;
if (x == (float)ix) yy[ix] += y;
else {
ilo=LMIN(LMAX ( (long) (x-0.5*m+1.0),1),n-m+1);
ihi=ilot+m-1;
nden=nfac [m] ;
fac=x-ilo;
for (j=ilot1;j<=ihi;j++) fac *= (x-j);
yy (ihi] y*fac/(nden*(x-ihi)) ;
for (j=ihi-1;j>=ilo;j--) {
nden=(nden/(j+1-ilo))*(j-ihi) ;
yy[j] += y*fac/(nden*(x-j));

CITED REFERENCES AND FURTHER READING:

Lomb, N.R. 1976, Astrophysics and Space Science, vol. 39, pp. 447-462. [1]

Barning, F.J.M. 1963, Bulletin of the Astronomical Institutes of the Netherlands, vol. 17, pp. 22—
28. [2]

Vaniéek, P. 1971, Astrophysics and Space Science, vol. 12, pp. 10-33. [3]

Scargle, J.D. 1982, Astrophysical Journal, vol. 263, pp. 835-853. [4]

Horne, J.H., and Baliunas, S.L. 1986, Astrophysical Journal, vol. 302, pp. 757—763. [5]

Press, W.H. and Rybicki, G.B. 1989, Astrophysical Journal, vol. 338, pp. 277-280. [6]

13.9 Computing Fourier Integrals Using the FFT

Not uncommonly, one wants to calculate accurate numerical values for integrals of
the form

b
I= [ e“*h(t)dt , (13.9.1)
or the equivalent real and imaginary parts
b b
Ie = [ cos(wt)h(t)dt I, = [ sin(wt)h(t)dt , (13.9.2)

and one wants to evaluate this integral for many different values of w. In cases of interest, h(t)
is often a smooth function, but it is not necessarily periodic in [a, b], nor does it necessarily
go to zero at a or b. While it seems intuitively obvious that the force majeure of the FFT
ought to be applicable to this problem, doing so turns out to be a surprisingly subtle matter,
as we will now see.

Let us first approach the problem naively, to see where the difficulty lies. Divide the
interval [a, b] into M subintervals, where M is a large integer, and define

tj =a+jA, hj =h(t;), j=0,...,M (13.9.3)

Notice that ho = h(a) and hay = h(b), and that there are M +1 values hj. We can
approximate the integral J by a sum,
M-1
TxA SY hyexp(iwty) (13.9.4)

j=0

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.9 Computing Fourier Integrals Using the FFT 585

which is at any rate first-order accurate. (If we centered the h;’s and the ¢;’s in the intervals,
we could be accurate to second order.) Now for certain values of w and M, the sum in
equation (13.9.4) can be made into a discrete Fourier transform, or DFT, and evaluated by
the fast Fourier transform (FFT) algorithm. In particular, we can choose M to be an integer
power of 2, and define a set of special w’s by
2am
nA = —_ 13.9.5
° M (13.95)
where m has the values m = 0,1,..., 1/2 — 1. Then equation (13.9.4) becomes
M-1
T (wm) & Al ® S hye?™9/M = el IDET (ho... hari) hm (13.9.6)

Equation (13.9.6), while simple and clear, is emphatically not recommended for use: It is
likely to give wrong answers!

The problem lies in the oscillatory nature of the integral (13.9.1). If h(t) is at all smooth,
and if w is large enough to imply several cycles in the interval [a, b] — in fact, w,,, in equation
(13.9.5) gives exactly m cycles — then the value of I is typically very small, so small that
it is easily swamped by first-order, or even (with centered values) second-order, truncation
error. Furthermore, the characteristic “small parameter” that occurs in the error term is not
A/(b— a) = 1/M,as it would be if the integrand were not oscillatory, but wA, which can be
as large as 7 for w’s within the Nyquist interval of the DFT (cf. equation 13.9.5). The result
is that equation (13.9.6) becomes systematically inaccurate as w increase

It is a sobering exercise to implement equation (13.9.6) for an integral that can be done
analytically, and to see just how bad it is. We recommend that you try it.

Let us therefore turn to a more sophisticated treatment. Given the sampled points h;, we
can approximate the function h(t) everywhere in the interval [a, b] by interpolation on nearby
hj;’s. The simplest case is linear interpolation, using the two nearest h;’s, one to the left and
one to the right. A higher-order interpolation, e.g., would be cubic interpolation, using two
points to the left and two to the right — except in the first and last subintervals, where we
must interpolate with three h;’s on one side, one on the other.

The formulas for such interpolation schemes are (piecewise) polynomial in the inde-
pendent variable ¢, but with coefficients that are of course linear in the function values
h;. Although one does not usually think of it in this way, interpolation can be viewed as
approximating a function by a sum of kernel functions (which depend only on the interpolation
scheme) times sample values (which depend only on the function). Let us write

h(t) © yn v (! =") + YS hye (: =") (13.9.7)

j=endpoints

Here ~)(s) is the kernel function of an interior point: It is zero for s sufficiently negative
or sufficiently positive, and becomes nonzero only when s is in the range where the
h; multiplying it is actually used in the interpolation. We always have (0) = 1 and
v(m) 0,m 1,+2,..., since interpolation right on a sample point should give the
sampled function value. For linear interpolation w(s) is piecewise linear, rises from 0 to 1
for s in (—1,0), and falls back to 0 for s in (0,1). For higher-order interpolation, 7)(s) is
made up piecewise of segments of Lagrange interpolation polynomials. It has discontinuous
derivatives at integer values of s, where the pieces join, because the set of points used in
the interpolation changes discretely.

As already remarked, the subintervals closest to a and b require different (noncentered)
interpolation formulas. This is reflected in equation (13.9.7) by the second sum, with the
special endpoint kernels p;(s). Actually, for reasons that will become clearer below, we have
included all the points in the first sum (with kernel y), so the y;’s are actually differences
between true endpoint kernels and the interior kernel yw. It is a tedious, but straightforward,
exercise to write down all the y;(s)’s for any particular order of interpolation, each one
consisting of differences of Lagrange interpolating polynomials spliced together piecewise.

Now apply the integral operator p dtexp(iwt) to both sides of equation (13.9.7),
interchange the sums and integral, and make the changes of variable s = (t — t;)/A in the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

586 Chapter 13. Fourier and Spectral Applications

first sum, s = (t — a)/A in the second sum. The result is

M
Tm Ae" |W(8) So hge’? + So hya(8) (13.9.8)
j=0 j=endpoints
Here 0 = wA, and the functions W(@) and a;(@) are defined by
W(d) = [ ds e'°*y(s) (13.9.9)
a;(6) = [ dse\*p;(s — j) (13.9.10)

The key point is that equations (13.9.9) and (13.9.10) can be evaluated, analytically,
once and for all, for any given interpolation scheme. Then equation (13.9.8) is an algorithm
for applying “endpoint corrections” to a sum which (as we will see) can be done using the
FFT, giving a result with high-order accuracy.

We will consider only interpolations that are left-right symmetric. Then symmetry
implies

_. _ 10M * 7p) _ _iw(b—a) *
pm—i(8) =9)(—8) — anrj(0) = e035 (0) =e" a5 (0) (1.9.11)
where * denotes complex conjugation. Also, &(s) = 7(—s) implies that W(6) is real.

Turn now to the first sum in equation (13.9.8), which we want to do by FFT methods.
To do so, choose some N that is an integer power of 2 with N > M+ 1. (Note that
M need not be a power of two, so M = N — 1 is allowed.) If N > M +1, define
hy =0,M+1<j7< N-—1,ie.,“zero pad” the array of h;’s so that 7 takes on the range
0 <j < N —1. Then the sum can be done as a DFT for the special values w = wn given by

2rn N
wrA= W =0 n=0,1,..., 3 1 (13.9.12)
For fixed M, the larger N is chosen, the finer the sampling in frequency space. The
value M, on the other hand, determines the highest frequency sampled, since A decreases
with increasing M (equation 13.9.3), and the largest value of wA is always just under 7
(equation 13.9.12). In general it is advantageous to oversample by at least a factor of 4, ie.,
N > 4M (see below). We can now rewrite equation (13.9.8) in its final form as

Twn) = etn {17 (@)[DFT( Ho oc hy-1)|n

+ ao(0)ho + a1(0)hi + a2(O)h2 +.03(0)h3 +...

+ ev (b-a) [a (8)haw t a1 (0)hu—1 t a2(0)hu—2 t a3(0)hu—s +.J}

(13.9.13)

For cubic (or lower) polynomial interpolation, at most the terms explicitly shown above

are nonzero; the ellipses (...) can therefore be ignored, and we need explicit forms only for
the functions W, ao, a1, a2, a3, calculated with equations (13.9.9) and (13.9.10). We have
worked these out for you, in the trapezoidal (second-order) and cubic (fourth-order) cases.
Here are the results, along with the first few terms of their power series expansions for small 0:

Trapezoidal order:

wey = 2b = 998) 2 Lge L ga 1 6
e? 12 360 20160

(1—cos@) | .(@—sin@)
|] +i

1 1 1 1 1 1 1 1
aa g2 — 94 4 ——_ 6% 4 19 (= — 02 4 6
2 + 24 720 + 40320 + 6 120 + 5040 362880

a1 = a2 =a3=0

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

13.9 Computing Fourier Integrals Using the FFT 587

Cubic order:

6+6? al 23
W(@) = 3—4cos6 20) 1-64 0
(#) ( 304 ) (3 — 4c0s 9+ cos 26) 720° * 15120
(—42 + 567) + (6 + 6?)(8cos@ —cos20) | _(—120 + 66%) + (6 + 6?) sin 20
a0 (9) = 7 +e y
604 604
2 1 103 169 2 2 8 86
wo ot 0° +10 oe — ot 0°
31 5° * ii20 26800" +” a5 * 05 2835" + 467775
14(3 — 6?) —7(6 +07) cos@ | 300 —5(6 + 6?) sind
6) =
a (9) 604 + 604
wi Tage, 3 pa r+ (Z- 1 pp Hoy 8 0°)
24 180 3456 259200 72 168 72576 5987520
—4(3 — 6?) +2(6+07)cosO  _—120 + 2(6 + 6?) sin®
an(0) = 304 +t ra
1 oi 5 1 7 1 1 13
was i ot 0° + 10 (- i ot 0°
6's cous” * Gasoo’ ** 90 * 210 90720° * 7484400
03(0) = 2(3 — 0?) — (6 + 6?) cos 4 60 — (6 + 6?) sind

604 : 604
141 : 1 7 1 ll 13

~ ty ot 0° +10 ( - 2 4 6
24 180 24192 259200 360 840 362880 29937600

The program dftcor, below, implements the endpoint corrections for the cubic case.
Given input values of w, A, a, b, and an array with the eight values ho , . .,hu,
it returns the real and imaginary parts of the endpoint corrections in equation (13.9.13), and the
factor W(@). The code is turgid, but only because the formulas above are complicated. The
formulas have cancellations to high powers of 0. It is therefore necessary to compute the right-
hand sides in double precision, even when the corrections are desired only to single precision.
It is also necessary to use the series expansion for small values of 6. The optimal cross-over
value of @ depends on your machine’s wordlength, but you can always find it experimentally
as the largest value where the two methods give identical results to machine precision.

»h3,hu—s

#include <math.h>

void dftcor(float w, float delta, float a, float b, float endpts[],

float *corre, float *corim, float *corfac)
For an integral approximated by a discrete Fourier transform, this routine computes the cor-
rection factor that multiplies the DFT and the endpoint correction to be added. Input is the
angular frequency w, stepsize delta, lower and upper limits of the integral a and b, while the
array endpts contains the first 4 and last 4 function values. The correction factor W(@) is
returned as corfac, while the real and imaginary parts of the endpoint correction are returned
as corre and corim.
{

void nrerror(char error_text[]);

float a0i,aOr,ali,air,a2i,a2r,a3i,a3r,arg,c,cl,cr,s,sl,sr,t;

float t2,t4,t6;

double cth,ctth,spth2,sth,sth4i,stth,th,th2,th4,tmth2,tth4i;

th=w*delta;
if (a >= b || th < 0.0e0 || th > 3.1416e0) nrerror("bad arguments to dftcor");
if (fabs(th) < 5.0e-2) { Use series.

t=th;

t2=txt;

t *t2;

t6=t4xt2;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

588 Chapter 13. Fourier and Spectral Applications

*corfac=1.0-(11.0/720.0) *t4+(23.0/15120.0) *t6;
a0r=(-2.0/3.0)+t2/45.0+(103.0/15120.0) *t4- (169 .0/226800.0)*t6;
alr=(7.0/24.0)-(7.0/180.0) *t2+(5.0/3456.0) *t4-(7.0/259200.0) *t6;
a2r=(-1.0/6.0)+t2/45.0-(5.0/6048.0) *t4+t6/64800.0;
a3r=(1.0/24.0)-t2/180.0+(5.0/24192.0) *t4-t6/259200.0;
aQ0i=t*(2.0/45.0+(2.0/105.0) *t2-(8.0/2835.0) *t4+(86.0/467775.0)*t6) ;
ali=t*(7.0/72.0-t2/168.0+(11.0/72576.0) *t4-(13.0/5987520.0) *t6) ;
a2i=t*(-7.0/90.0+t2/210.0-(11.0/90720.0)*t4+(13.0/7484400.0) *t6) ;
a3i=t*(7.0/360.0-t2/840.0+(11.0/362880 .0) *t4-(13.0/29937600.0)*t6) ;
} else { Use trigonometric formulas in double precision.
cth=cos(th) ;
sth=sin(th) ;
ctth=cth*cth-sth*sth;
stth=2.0e0*sth*cth;
th2=th*th;
th4=th2*th2;
tmth2=3.0e0-th2;
spth2=6 .0e0+th2;
sth4i=1.0/(6.0e0*th4) ;
tth4i=2.0e0*sth4i;
*corfac=tth4i*spth2*(3.0e0-4.0e0*cthtctth) ;
aOr=sth4i*(-42.0e0+5.0e0*th2+spth2*(8.0e0*cth-ctth)) ;
aQi=sth4i*(th*(-12.0e0+6.0e0*th2)+spth2*stth) ;
alr=sth4i*(14.0e0*tmth2-7.0e0*spth2*cth) ;
ali=sth4i*(30.0e0*th-5.0e0*spth2*sth) ;
a2r=tth4i*(-4.0e0*tmth2+2.0e0*spth2*cth) ;
a2i=tth4i*(-12.0e0*th+2.0e0*spth2*sth) ;
a3r=sth4i*(2.0e0*tmth2-spth2*cth) ;
a3i=sth4i*(6.0e0*th-spth2*sth) ;
}
cl=a0r*endpts [1] +a1r*endpts [2] +a2r*endpts [3] +a3r*endpts [4] ;
sl=a0ixendpts[1]+a1i*endpts [2] +a2i*endpts [3] +a3ixendpts [4] ;
cr=a0r*endpts [8] +alr*endpts [7] +a2r*endpts [6] +a3r*endpts [5] ;
sr = -a0ixendpts[8]-ali*endpts[7]-a2i*endpts[6]-a3ixendpts [5] ;
arg=w*(b-a) ;
c=cos (arg) ;
s=sin(arg) ;
*corre=cl+c*cr-s*sr;
*corim=sl+s*cr+c*sr;

Since the use of dftcor can be confusing, we also give an illustrative program dftint
which uses dftcor to compute equation (13.9.1) for general a,b, w, and h(t). Several points
within this program bear mentioning: The parameters M and NDFT correspond to M and N
in the above discussion. On successive calls, we recompute the Fourier transform only if
a or b or h(t) has changed.

Since dftint is designed to work for any value of w satisfying wA < 7, not just the
special values returned by the DFT (equation 13.9.12), we do polynomial interpolation of
degree MPOL on the DFT spectrum. You should be warned that a large factor of oversampling
(N > M) is required for this interpolation to be accurate. After interpolation, we add the
endpoint corrections from dftcor, which can be evaluated for any w.

While dftcor is good at what it does, dftint is illustrative only. It is not a general
purpose program, because it does not adapt its parameters M, NDFT, MPOL, or its interpolation
scheme, to any particular function h(t). You will have to experiment with your own application.

#include <math.h>

#include "nrutil.h"

#define M 64

#define NDFT 1024

#define MPOL 6

#define TWOPI (2.0*3.14159265)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.9 Computing Fourier Integrals Using the FFT 589

The values of M, NDFT, and MPOL are merely illustrative and should be optimized for your
particular application. M is the number of subintervals, NDFT is the length of the FFT (a power
of 2), and MPOL is the degree of polynomial interpolation used to obtain the desired frequency
from the FFT.

void dftint(float (*func) (float), float a, float b, float w, float *cosint,
float *sinint)
Example program illustrating how to use the routine dftcor. The user supplies an external
function func that returns the quantity h(t). The routine then returns J? cos(wt)h(t) dt as
cosint and rp sin(wt)h(t) dt as sinint.
{
void dftcor(float w, float delta, float a, float b, float endpts[],
float *corre, float *corim, float *corfac);
void polint(float xa[], float ya[], int n, float x, float *y, float *dy);
void realft(float data[], unsigned long n, int isign);
static int init=0;
int j,nn;
static float aold = -1.e30,bold = -1.e30,delta, (*funcold) (float) ;
static float data[NDFT+1] ,endpts [9];
float c,cdft,cerr,corfac,corim,corre,en,s;
float sdft,serr,*cpol,*spol,*xpol;

cpol=vector(1,MPOL) ;

spol=vector(1,MPOL) ;

xpol=vector(1,MPOL) ;

if (init != 1 || a != aold || b != bold || func != funcold) {
Do we need to initialize, or is only w changed?
init=1;

funcold=func;
delta=(b-a) /M;
Load the function values into the data array.
for (j=1;j<=M+1; j++)
data[j]=(*func) (a+(j-1) *delta) ;

for (j=M+2;j<=NDFT; j++) Zero pad the rest of the data array.
data[j]=0.0;
for (j=1;j<=4;j++) { Load the endpoints.

endpts[j]=data[j];
endpts[j+4]=data[M-3+j] ;
}
realft (data,NDFT,1);
realft returns the unused value corresponding to wy//2 in data[2]. We actually want
this element to contain the imaginary part corresponding to wo, which is zero.
data[2]=0.0;
}
Now interpolate on the DFT result for the desired frequency. If the frequency is an wn,
i.e., the quantity en is an integer, then cdft=data[2*en-1], sdft=data[2*en], and you
could omit the interpolation.
en=w*delta*NDFT/TWOPI+1.0;
nn=IMIN(IMAX( (int) (en-0.5+*MPOL+1.0),1) ,NDFT/2-MPOL+1); Leftmost point for the

for (j=1;j<=MPOL;j++,nn++) { interpolation.
cpol[j]=data[2*nn-1] ;
spol [j]=data[2*nn] ;

xpol[j]=nn;

polint (xpol,cpol,MPOL,en,&cdft ,&cerr) ;
polint (xpol,spol,MPOL,en,&sdft,&serr) ;

dftcor(w,delta,a,b,endpts,&corre,&corim,&corfac) ; Now get the endpoint cor-
cdft *= corfac; rection and the mul-
sdft *= corfac; tiplicative factor W (0).

cdft += corre;
sdft += corim;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

590 Chapter 13. Fourier and Spectral Applications

c=delta*cos (w*a) ; Finally multiply by A and exp(iwa).
s=delta*sin(w*a) ;

*cosint=c*cdft-s*sdft ;

*sinint=s*cdft+c*sdft ;

free_vector(cpol,1,MPOL) ;

free_vector(spol,1,MPOL) ;

free_vector(xpol,1,MPOL) ;

Sometimes one is interested only in the discrete frequencies w,, of equation (13.9.5),
the ones that have integral numbers of periods in the interval [a,b]. For smooth h(t), the
value of J tends to be much smaller in magnitude at these w’s than at values in between,
since the integral half-periods tend to cancel precisely. (That is why one must oversample for
interpolation to be accurate: J(w) is oscillatory with small magnitude near the w,,’s.) If you
want these Wms without messy (and possibly inaccurate) interpolation, you have to set N to
a multiple of MM (compare equations 13.9.5 and 13.9.12). In the method implemented above,
however, NV must be at least MZ + 1, so the smallest such multiple is 2M, resulting in a factor
~2 unnecessary computing. Alternatively, one can derive a formula like equation (13.9.13),
but with the last sample function hyy = h(b) omitted from the DFT, but included entirely in
the endpoint correction for hiv. Then one can set M = N (an integer power of 2) and get the
special frequencies of equation (13.9.5) with no additional overhead. The modified formula is

(wm) = Asctn {1 (@)[DFT( to -.har—1)\m
+ a0(A)ho + a1(A)hi + a2(A)h2 + a3(O)hs (13.9.14)
+609 [A@)har + 03()har—1 + 02(8)har—2 + 05(8)har—| }
where 0 = w,,A and A(@) is given by
A(8) = —a0(8) (13.9.15)

for the trapezoidal case, or

(—6 + 116?) + (6+ 6?) cos20

A(@) 7 iIm[ao(9)]
to e 4 (13.9.16)
wl = 2 _ 2 4 ss 6 _;
3+ ae? ~ ons? + ae? ~ ilo]

for the cubic case.

Factors like W (@) arise naturally whenever one calculates Fourier coefficients of smooth
functions, and they are sometimes called attenuation factors[1]. However, the endpoint
corrections are equally important in obtaining accurate values of integrals. Narasimhan
and Karthikeyan [2] have given a formula that is algebraically equivalent to our trapezoidal
formula. However, their formula requires the evaluation of two FFTs, which is unnecessary.
The basic idea used here goes back at least to Filon [3] in 1928 (before the FFT!). He used
Simpson’s rule (quadratic interpolation). Since this interpolation is not left-right symmetric,
two Fourier transforms are required. An alternative algorithm for equation (13.9.14) has been
given by Lyness in [4]; for related references, see [5]. To our knowledge, the cubic-order
formulas derived here have not previously appeared in the literature.

Calculating Fourier transforms when the range of integration is (—00, 00) can be tricky.
If the function falls off reasonably quickly at infinity, you can split the integral at a large
enough value of ¢. For example, the integration to + oo can be written

0° b oo
[ ena = f cenit [ e*h(t) dt

iwb , iwb
h(bje™” nie (13.9.17)
WwW

(iw)?

b
-[ e* h(t) dt —

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
22
ao
so
es
az
oe
$3
22
5
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
go
§5
an
gd
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

9
o
3
Ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms 591

The splitting point b must be chosen large enough that the remaining integral over (b, 00) is
small. Successive terms in its asymptotic expansion are found by integrating by parts. The
integral over (a,b) can be done using dftint. You keep as many terms in the asymptotic
expansion as you can easily compute. See[6] for some examples of this idea. More
powerful methods, which work well for long-tailed functions but which do not use the FFT,
are described in [7-9].

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
p. 88. [1]

Narasimhan, M.S. and Karthikeyan, M. 1984, IEEE Transactions on Antennas & Propagation,
vol. 32, pp. 404-408. [2]

Filon, L.N.G. 1928, Proceedings of the Royal Society of Edinburgh, vol. 49, pp. 38-47. [3]

Giunta, G. and Murli, A. 1987, ACM Transactions on Mathematical Software, vol. 13, pp. 97-107.

[4]
Lyness, J.N. 1987, in Numerical Integration, P. Keast and G. Fairweather, eds. (Dordrecht:
Reidel). [5]

Pantis, G. 1975, Journal of Computational Physics, vol. 17, pp. 229-233. [6]

Blakemore, M., Evans, G.A., and Hyslop, J. 1976, Journal of Computational Physics, vol. 22,
pp. 352-376. [7]

Lyness, J.N., and Kaper, T.J. 1987, SIAM Journal on Scientific and Statistical Computing, vol. 8,
pp. 1005-1011. [8]

Thakkar, A.J., and Smith, V.H. 1975, Computer Physics Communications, vol. 10, pp. 73-79. [9]

13.10 Wavelet Transforms

Like the fast Fourier transform (FFT), the discrete wavelet transform (DWT) is
a fast, linear operation that operates on a data vector whose length is an integer power
of two, transforming it into a numerically different vector of the same length. Also
like the FFT, the wavelet transform is invertible and in fact orthogonal — the inverse
transform, when viewed as a big matrix, is simply the transpose of the transform.
Both FFT and DWT, therefore, can be viewed as a rotation in function space, from
the input space (or time) domain, where the basis functions are the unit vectors e ;,
or Dirac delta functions in the continuum limit, to a different domain. For the FFT,
this new domain has basis functions that are the familiar sines and cosines. In the
wavelet domain, the basis functions are somewhat more complicated and have the
fanciful names “mother functions” and “wavelets.”

Of course there are an infinity of possible bases for function space, almost all of
them uninteresting! What makes the wavelet basis interesting is that, unlike sines and
cosines, individual wavelet functions are quite localized in space; simultaneously,
like sines and cosines, individual wavelet functions are quite localized in frequency
or (more precisely) characteristic scale. As we will see below, the particular kind
of dual localization achieved by wavelets renders large classes of functions and
operators sparse, or sparse to some high accuracy, when transformed into the wavelet
domain. Analogously with the Fourier domain, where a class of computations, like
convolutions, become computationally fast, there is a large class of computations

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

592 Chapter 13. Fourier and Spectral Applications

— those that can take advantage of sparsity — that become computationally fast
in the wavelet domain [1].

Unlike sines and cosines, which define a unique Fourier transform, there is
not one single unique set of wavelets; in fact, there are infinitely many possible
sets. Roughly, the different sets of wavelets make different trade-offs between
how compactly they are localized in space and how smooth they are. (There are
further fine distinctions.)

Daubechies Wavelet Filter Coefficients

A particular set of wavelets is specified by a particular set of numbers, called
wavelet filter coefficients. Here, we will largely restrict ourselves to wavelet filters
in a class discovered by Daubechies [2]. This class includes members ranging from
highly localized to highly smooth. The simplest (and most localized) member, often
called DAUB4, has only four coefficients, co, ..., cg. For the moment we specialize
to this case for ease of notation.

Consider the following transformation matrix acting on a column vector of
data to its right:

co Cc C2 C3

C3 C2 Cy Co
(13.10.1)
co C1 c2 C3
C3 C2 Cy eo
C2 C3 Co OL
C1 —Co C3. «C2

Here blank entries signify zeroes. Note the structure of this matrix. The first row
generates one component of the data convolved with the filter coefficients co ...,c3.
Likewise the third, fifth, and other odd rows. If the even rows followed this pattern,
offset by one, then the matrix would be a circulant, that is, an ordinary convolution
that could be done by FFT methods. (Note how the last two rows wrap around
like convolutions with periodic boundary conditions.) Instead of convolving with
Co,--+,€3, however, the even rows perform a different convolution, with coefficients
¢3, —C€2,C€1, —co. The action of the matrix, overall, is thus to perform two related
convolutions, then to decimate each of them by half (throw away half the values),
and interleave the remaining halves.

It is useful to think of the filter co, ..., cz as being a smoothing filter, call it H,
something like a moving average of four points. Then, because of the minus signs,
the filter c3, —c2,c1, —Co, call it G, is not a smoothing filter. (In signal processing
contexts, H and G are called quadrature mirror filters [3].) In fact, the c’s are chosen
so as to make G yield, insofar as possible, a zero response to a sufficiently smooth
data vector. This is done by requiring the sequence c3, —c2, c1, —co to have a certain
number of vanishing moments. When this is the case for p moments (starting with
the zeroth), a set of wavelets is said to satisfy an “approximation condition of order

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms 593

p.’ This results in the output of H, decimated by half, accurately representing the
data’s “smooth” information. The output of G, also decimated, is referred to as
the data’s “detail” information [4].

For such a characterization to be useful, it must be possible to reconstruct the
original data vector of length N from its N/2 smooth or s-components and its N/2
detail or d-components. That is effected by requiring the matrix (13.10.1) to be
orthogonal, so that its inverse is just the transposed matrix

Cc © oo C2 ey
cy —c2 oo c3. «Co
C2 C1 Co C3

c3. Co C1 C2

(13.10.2)
c Cy Co C3
C3 —Co Cy CQ
Cc C4, %& ©
c3  —Co C1 C2

One sees immediately that matrix (13.10.2) is inverse to matrix (13.10.1) if and
only if these two equations hold,

otat+agt+a=l (13.103)
13.10.

coco + 3C1 = 0

If additionally we require the approximation condition of order p = 2, then two
additional relations are required,

c3 — C2 +e, —Co = 0

(13.10.4)
0c3 — leg + 2c, — 3c9 = 0

Equations (13.10.3) and (13.10.4) are 4 equations for the 4 unknowns co,...,c3,
first recognized and solved by Daubechies. The unique solution (up to a left-right
reversal) is

co = (1+ V3)/4V2_ oc = (3 + V3) /4V2.
c2 = (3— V3)/4V2_ eg = (1 — V3) /4V2

In fact, DAUB4 is only the most compact of a sequence of wavelet sets: If we had
six coefficients instead of four, there would be three orthogonality requirements in
equation (13.10.3) (with offsets of zero, two, and four), and we could require the
vanishing of p = 3 moments in equation (13.10.4). In this case, DAUB6, the solution
coefficients can also be expressed in closed form,

(13.10.5)

co = (1+ V10 + V5 + 2V10)/16V2 cr = (5+ V10 + 35 + 2V10)/16V2
cq = (10 — 2V10 + 2/5 + 2/10) /16V2 cz = (10 — 2V/10 — 2/5 + 2/10) /16\/2
ca = (5+ V10—3V5+2V10)/16V2 cs = (1+ V10— V5 + 2V/10)/16V/2

(13.10.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

594 Chapter 13. Fourier and Spectral Applications

For higher p, up to 10, Daubechies [2] has tabulated the coefficients numerically. The
number of coefficients increases by two each time p is increased by one.

Discrete Wavelet Transform

We have not yet defined the discrete wavelet transform (DWT), but we are
almost there: The DWT consists of applying a wavelet coefficient matrix like
(13.10.1) hierarchically, first to the full data vector of length NV, then to the “smooth”
vector of length N/2, then to the “smooth-smooth” vector of length N/4, and
so on until only a trivial number of “smooth-. . .-smooth” components (usually 2)
remain. The procedure is sometimes called a pyramidal algorithm [4], for obvious

reasons. The output of the DWT consists of these remaining components and all
the “detail” components that were accumulated along the way. A diagram should
make the procedure clear:
rv rsi7 rsi7 r S17 r Si PS. 7
y2 dy 82 Di $2 | ote, | S2
y3 82 83 So S3 | —> | Di
ya da $4] 45 D2 te | S4 Do
YS 83 85 13.191 Sy | POMS | Di Di
Ye dz 86 D3 D2 D2
YT 84 87 S4 D3 Ds
ys dy te | 88 Da Da Da
yo | P4941 | 55 | Pomme | a dh di di
yo ds dz dg dz dg
yi 86 d3 d3 d3 ds
yi2 de d4 dq d4 d4
y13 87 ds ds ds ds
yi dz dg dg ds de
Y15 88 dz dz dz dz
Lye 4 Ldg 4 Ldg J Lds J L ds J Lds J
(13.10.7)
If the length of the data vector were a higher power of two, there would be
more stages of applying (13.10.1) (or any other wavelet coefficients) and permuting.
The endpoint will always be a vector with two S’s and a hierarchy of D’s, D’s,
d’s, etc. Notice that once d’s are generated, they simply propagate through to all
subsequent stages.
A value d; of any level is termed a “wavelet coefficient” of the original data
vector; the final values S;, Sz should strictly be called “mother-function coefficients,”

although the term “wavelet coefficients” is often used loosely for both d’s and final
S’s. Since the full procedure is a composition of orthogonal linear operations, the
whole DWT is itself an orthogonal linear operator.

To invert the DWT, one simply reverses the procedure, starting with the smallest
level of the hierarchy and working (in equation 13.10.7) from right to left. The
inverse matrix (13.10.2) is of course used instead of the matrix (13.10.1).

As already noted, the matrices (13.10.1) and (13.10.2) embody periodic (“wrap-
around”) boundary conditions on the data vector. One normally accepts this as a
minor inconvenience: the last few wavelet coefficients at each level of the hierarchy
are affected by data from both ends of the data vector. By circularly shifting the
matrix (13.10.1) N/2 columns to the left, one can symmetrize the wrap-around;
but this does not eliminate it. It is in fact possible to eliminate the wrap-around

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms 595

completely by altering the coefficients in the first and last N rows of (13.10.1),
giving an orthogonal matrix that is purely band-diagonal [5]. This variant, beyond
our scope here, is useful when, e.g., the data varies by many orders of magnitude
from one end of the data vector to the other.

Here is a routine, wt1, that performs the pyramidal algorithm (or its inverse
if isign is negative) on some data vector a[1..n]. Successive applications of
the wavelet filter, and accompanying permutations, are done by an assumed routine
wtstep, which must be provided. (We give examples of several different wtstep
routines just below.)

void wt1(float af], unsigned long n, int isign,

void (*wtstep) (float [], unsigned long, int))
One-dimensional discrete wavelet transform. This routine implements the pyramid algorithm,
replacing a[1..n] by its wavelet transform (for isign=1), or performing the inverse operation
(for isign=-1). Note that n MUST be an integer power of 2. The routine wtstep, whose
actual name must be supplied in calling this routine, is the underlying wavelet filter. Examples
of wtstep are daub4 and (preceded by pwtset) pwt.

{
unsigned long nn;
if (m < 4) return;
if (isign >= 0) { Wavelet transform.
for (nn=n;nn>=4;nn>>=1) (*wtstep)(a,nn,isign) ;
Start at largest hierarchy, and work towards smallest.
} else { Inverse wavelet transform.
for (nn=4;nn<=n;nn<<=1) (*wtstep)(a,nn,isign) ;
Start at smallest hierarchy, and work towards largest.
+
}

Here, as a specific instance of wtstep, is a routine for the DAUB4 wavelets:

#include "nrutil.h"

#define CO 0.4829629131445341
#define C1 0.8365163037378079
#define C2 0.2241438680420134
#define C3 -0.1294095225512604

void daub4(float a[], unsigned long n, int isign)
Applies the Daubechies 4-coefficient wavelet filter to data vector a[1..n] (for isign=1) or
applies its transpose (for isign=-1). Used hierarchically by routines wt1 and wtn.
t
float *wksp;
unsigned long nh,nh1,i,j;

if (n < 4) return;
wksp=vector(1,n);
nhi=(nh=n >> 1)+1;
if (isign >= 0) { Apply filter.
for (i=1,j=1;j<=n-3;j+=2,i++) {
wksp [i] =CO*a[j]+C1*a[j+1]+C2*a[j+2]+C3*a[j+3] ;
wksp[itnh] = C3*a[j]-C2*a[j+1]+C1*a[j+2]-Co*a[j+3] ;

3
wksp [i] =CO*a [n-1]+C1*a[n] +C2*a[1]+C3*a[2] ;
wksp[itnh] = C3+*a[n-1]-C2*a[n]+C1*a[1]-Co*a[2] ;
} else { Apply transpose filter.
wksp [1] =C2*a [nh] +C1*a[n] +CO*a[1]+C3*a[nh1] ;
wksp[2] = C3*a[nh]-CO*a[n]+C1*a[1]-C2*a[nh1] ;
for (i=1,j=3;i<nh;it+) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

596 Chapter 13. Fourier and Spectral Applications

wksp [j++] =C2*a[i]+C1*a[itnh] +CO*a[i+1]+C3*a[itnh1] ;
wksp[jt++] = C3*a[i]-CO*a[i+nh]+C1*a[i+1]-C2*a[litnh1] ;
}
}
for (i=1;i<=n;i++) ali]=wksp[i];
free_vector(wksp,1,n);

For larger sets of wavelet coefficients, the wrap-around of the last rows or
columns is a programming inconvenience. An efficient implementation would
handle the wrap-arounds as special cases, outside of the main loop. Here, we will
content ourselves with a more general scheme involving some extra arithmetic at
run time. The following routine sets up any particular wavelet coefficients whose
values you happen to know.

typedef struct {
int ncof,ioff, joff;
float *cc,*cr;

} wavefilt;

wavefilt wfilt; Defining declaration of a structure.

void pwtset(int n)
Initializing routine for pwt, here implementing the Daubechies wavelet filters with 4, 12, and
20 coefficients, as selected by the input value n. Further wavelet filters can be included in the
obvious manner. This routine must be called (once) before the first use of pwt. (For the case
n=4, the specific routine daub4 is considerably faster than pwt.)
{
void nrerror(char error_text[]);
int k;
float sig = -1.0;
static float c4[5]={0.0,0.4829629131445341 ,0.8365163037378079,
0. 2241438680420134,-0.1294095225512604} ;
static float c12[13]={0.0,0.111540743350, 0.494623890398, 0.751133908021,
0.315250351709 , -0. 226264693965 , -0. 129766867567,
0.097501605587, 0.027522865530 , -0.031582039318,
0.000553842201, 0.004777257511,-0.001077301085};
static float c20[21]={0.0,0.026670057901, 0.188176800078, 0.527201188932,
0.688459039454, 0.281172343661,-0.249846424327,
-0.195946274377, 0.127369340336, 0.093057364604,
-0.071394147166 ,-0.029457536822, 0.033212674059,
0.003606553567 ,-0.010733175483, 0.001395351747,
0.001992405295 , -0.000685856695 , -0.000116466855,
0.000093588670 , -0.000013264203} ;
static float c4r[5],c12r[13] ,c20r[21];

wfilt.ncof=n;

if (a 4ayf{
wfilt.cc=c4;
wfilt.cr=c4r;

+

else if (mn == 12) {
wfilt.cc=c12;
wfilt.cr=c12r;

+

else if (n == 20) {
wfilt.cc=c20;
wfilt.cr=c20r;

+

else nrerror("unimplemented value n in pwtset");

for (k=1;k<=n;k++) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms 597

wfilt.cr[wfilt.ncof+1-k]=sig*wfilt.cc[k];

sig = -sig;
+
wfilt.ioff = wfilt.joff = -(™ >> 1);
These values center the “support” of the wavelets at each level. Alternatively, the “peaks”
of the wavelets can be approximately centered by the choices ioff=-2 and joff=-n+2.
Note that daub4 and pwtset with n=4 use different default centerings.

Once pwtset has been called, the following routine can be used as a specific
instance of wtstep.

#include "nrutil.h"

typedef struct {
int ncof,ioff, joff;
float *cc,*cr;

} wavefilt;

extern wavefilt wfilt; Defined in pwtset.

void pwt(float af], unsigned long n, int isign)
Partial wavelet transform: applies an arbitrary wavelet filter to data vector a[1..n] (for isign =
1) or applies its transpose (for isign = —1). Used hierarchically by routines wt1 and wtn.
The actual filter is determined by a preceding (and required) call to pwtset, which initializes
the structure wfilt.
{

float ai,ail,*wksp;

unsigned long i,ii,j,jf,jr,k,n1,ni,nj,nh,nmod;

if (n < 4) return;
wksp=vector(1,n);

nmod=wfilt.ncof*n; A positive constant equal to zero mod n.
ni=n-1; Mask of all bits, since n a power of 2.
nh=n >> 1;

for (j=1;j<=n;j++) wksp[j]=0.0;

if (isign >= 0) { Apply filter.
for (ii=1,i=1;i<=nj;it=2,iit+) {

ni=i+nmod+wfilt.ioff; Pointer to be incremented and wrapped-around.

nj=itnmodt+wfilt.joff;

for (k=1;k<=wfilt.ncof;k++) {
j 1 & (nitk); We use bitwise and to wrap-around the point-
jr=ni & (nj+k); ers.

wksp[ii] += wfilt.cc[k]*a[jf+1];

wksp[iitnh] += wfilt.cr(k]*a[jr+i1];

Apply transpose filter.

See comments above.

nj=itnmod+wfilt.joff;
for (k=1;k<=wfilt .ncof

sk++) {

jf=(n1 & (nitk))+
jr=(n1 & (nj+k))+1;
wksp[jf] += wfilt.cc[k] *ai;
wksp[jr] += wfilt.cr[k]*ai1;

}
}
for (j=1;j<=n;j++) aljl=wksp[j]; Copy the results back from workspace.
free_vector(wksp,1,n);

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

598 Chapter 13. Fourier and Spectral Applications

0 100 200 300 400 500 600 700 800 900 1000

DAUB20 ey

1 1 L l L l L l L L
0 100 200 300 400 500 600 700 800 900 1000

functions from the wavelet families DAUB4
's of scalings and translations of either one
; DAUB20 would show similar behavior

Figure 13.10.1. | Wavelet functions, that is, single basi:
and DAUB20. A complete, orthonormal wavelet basis co:
of these functions. DAUB4 has an infinite number of cus}
in a higher derivative.

What Do Wavelets Look Like?

We are now in a position actually to see some wavelets. To do so, we simply
run unit vectors through any of the above discrete wavelet transforms, with isign
negative so that the inverse transform is performed. Figure 13.10.1 shows the
DAUB4 wavelet that is the inverse DWT of a unit vector in the 5th component of a
vector of length 1024, and also the DAUB20 wavelet that is the inverse of the 22nd
component. (One needs to go to a later hierarchical level for DAUB20, to avoid a
wavelet with a wrapped-around tail.) Other unit vectors would give wavelets with
the same shapes, but different positions and scales.

One sees that both DAUB4 and DAUB20 have wavelets that are continuous.
DAUB20 wavelets also have higher continuous derivatives. DAUB4 has the peculiar
property that its derivative exists only almost everywhere. Examples of where it fails
to exist are the points p/2”, where p and n are integers; at such points, DAUB4 is
left differentiable, but not right differentiable! This kind of discontinuity — at least
in some derivative — is a necessary feature of wavelets with compact support, like
the Daubechies series. For every increase in the number of wavelet coefficients by
two, the Daubechies wavelets gain about half a derivative of continuity. (But not
exactly half; the actual orders of regularity are irrational numbers!)

Note that the fact that wavelets are not smooth does not prevent their having
exact representations for some smooth functions ,as demanded by their approximation
order p. The continuity of a wavelet is not the same as the continuity of functions

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms

599

-2L — DAUB4 eyo + esg J
L ! L ! L L L ! ! L
0 100 200 300 400 500 600 = 700 800 900 1000
2 4
oF
-2- Lemarie e19 + e5g 4
L ! L ! L ! L ! ! Li
0 100 200 300 400 500 600 = 700 800 900 1000

Figure 13.10.2.

More wavelets, here generated from the sum of two unit vectors, e.9 + e5s, which

are in different hierarchical levels of scale, and also at different spatial positions. DAUB4 wavelets (a)
are defined by a filter in coordinate space (equation 13.10.5), while Lemarie wavelets (b) are defined by
a filter most easily written in Fourier space (equation 13.10.14).

that a set of wavelets can represent. For example, DAUB4 can represent (piecewise)
linear functions of arbitrary slope: in the correct linear combinations, the cusps all
cancel out. Every increase of two in the number of coefficients allows one higher
order of polynomial to be exactly represented.

Figure 13.10.2 shows the result of performing the inverse DWT on the input
vector €19 + e5g, again for the two different particular wavelets. Since 10 lies early
in the hierarchical range of 9 — 16, that wavelet lies on the left side of the picture.
Since 58 lies in a later (smaller-scale) hierarchy, it is a narrower wavelet; in the range
of 33-64 it is towards the end, so it lies on the right side of the picture. Note that
smaller-scale wavelets are taller, so as to have the same squared integral.

Wavelet Filters in the Fourier Domain

The Fourier transform of a set of filter coefficients c; is given by

Hw) = So cjetl# (13.10.8)

J

Here H is a function periodic in 27, and it has the same meaning as before: It is
the wavelet filter, now written in the Fourier domain. A very useful fact is that the
orthogonality conditions for the c’s (e.g., equation 13.10.3 above) collapse to two
simple relations in the Fourier domain,

S|H(O)? =1 (13.109)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

600 Chapter 13. Fourier and Spectral Applications

and
1 5 5
5 [|H(w)? +|Hw+n)P] =1 (13.10.10)
Likewise the approximation condition of order p (e.g., equation 13.10.4 above)
has a simple formulation, requiring that H(w) have a pth order zero at w = 7,
or (equivalently)

H(™ (nr) =0 m=0,1,...,p—1 (13.10.11)

It is thus relatively straightforward to invent wavelet sets in the Fourier domain.
You simply invent a function H(w) satisfying equations (13.10.9)-(13.10.11). To
find the actual c;’s applicable to a data (or s-component) vector of length N, and
with periodic wrap-around as in matrices (13.10.1) and (13.10.2), you invert equation
(13.10.8) by the discrete Fourier transform

N-1
op = Yo (ee 2 miakIN (13.10.12)
, N k=0 N : _

The quadrature mirror filter G (reversed c;’s with alternating signs), incidentally,
has the Fourier representation

G(w) =e" (w +7) (13.10.13)
where asterisk denotes complex conjugation.

In general the above procedure will not produce wavelet filters with compact
support. In other words, all N of the cj’s, 7 = 0,..., N — 1 will in general be
nonzero (though they may be rapidly decreasing in magnitude). The Daubechies
wavelets, or other wavelets with compact support, are specially chosen so that H (w)
is a trigonometric polynomial with only a small number of Fourier components,
guaranteeing that there will be only a small number of nonzero c ;’s.

On the other hand, there is sometimes no particular reason to demand compact
support. Giving it up in fact allows the ready construction of relatively smoother
wavelets (higher values of p). Even without compact support, the convolutions
implicit in the matrix (13.10.1) can be done efficiently by FFT methods.

Lemarie’s wavelet (see [4]) has p = 4, does not have compact support, and is
defined by the choice of H(w),

4315 — 420u + 126u? — 4u3 a

A = |2(1- 13.10.14
(#) = 20 a)" S15 — 4200-4 12602 — 403 ( )
where

u=sin? > v =sin?w (13.10.15)

It is beyond our scope to explain where equation (13.10.14) comes from. An
informal description is that the quadrature mirror filter G(w) deriving from equation
(13.10.14) has the property that it gives identically zero when applied to any function
whose odd-numbered samples are equal to the cubic spline interpolation of its
even-numbered samples. Since this class of functions includes many very smooth
members, it follows that H(w) does a good job of truly selecting a function’s smooth
information content. Sample Lemarie wavelets are shown in Figure 13.10.2.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

13.10 Wavelet Transforms 60

0 100 200 300 400 500 600 700 800 900 =: 1000

wavelet amplitude

PL | L ! L L L L |
100 200 300 400 500 600 700 800 900 1000
wavelet number

Figure 13.10.3. (a) Arbitrary test function, with cusp, sampled on a vector of length 1024. (b)
Absolute value of the 1024 wavelet coefficients produced by the discrete wavelet transform of (a). Note
log scale. The dotted curve plots the same amplitudes when sorted by decreasing size. One sees that
only 130 out of 1024 coefficients are larger than 10~4 (or larger than about 10~5 times the largest
coefficient, whose value is ~ 10).

Truncated Wavelet Approximations

Most of the usefulness of wavelets rests on the fact that wavelet transforms
can usefully be severely truncated, that is, turned into sparse expansions. The case
of Fourier transforms is different: FFTs are ordinarily used without truncation,
to compute fast convolutions, for example. This works because the convolution
operator is particularly simple in the Fourier basis. There are not, however, any
standard mathematical operations that are especially simple in the wavelet basis.

To see how truncation works, consider the simple example shown in Figure
13.10.3. The upper panel shows an arbitrarily chosen test function, smooth except
for a square-root cusp, sampled onto a vector of length 2!°. The bottom panel (solid
curve) shows, on a log scale, the absolute value of the vector’s components after it has
been run through the DAUB4 discrete wavelet transform. One notes, from right to
left, the different levels of hierarchy, 513-1024, 257-512, 129-256, etc. Within each
level, the wavelet coefficients are non-negligible only very near the location of the
cusp, or very near the left and right boundaries of the hierarchical range (edge effects).

The dotted curve in the lower panel of Figure 13.10.3 plots the same amplitudes
as the solid curve, but sorted into decreasing order of size. One can read off, for
example, that the 130th largest wavelet coefficient has an amplitude less than 10 ~>
of the largest coefficient, whose magnitude is ~ 10 (power or square integral ratio
less than 10~!°). Thus, the example function can be represented quite accurately
by only 130, rather than 1024, coefficients — the remaining ones being set to
zero. Note that this kind of truncation makes the vector sparse, but not shorter
than 1024. It is very important that vectors in wavelet space be truncated according
to the amplitude of the components, not their position in the vector. Keeping the
first 256 components of the vector (all levels of the hierarchy except the last two)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

602 Chapter 13. Fourier and Spectral Applications

would give an extremely poor, and jagged, approximation to the function. When
you compress a function with wavelets, you have to record both the values and the
positions of the nonzero coefficients.

Generally, compact (and therefore unsmooth) wavelets are better for lower
accuracy approximation and for functions with discontinuities (like edges), while
smooth (and therefore noncompact) wavelets are better for achieving high numerical
accuracy. This makes compact wavelets a good choice for image compression, for
example, while it makes smooth wavelets best for fast solution of integral equations.

Wavelet Transform in Multidimensions

A wavelet transform of a d-dimensional array is most easily obtained by
transforming the array sequentially on its first index (for all values of its other indices),
then on its second, and so on. Each transformation corresponds to multiplication
by an orthogonal matrix. By matrix associativity, the result is independent of the
order in which the indices were transformed. The situation is exactly like that for
multidimensional FFTs. A routine for effecting the multidimensional DWT can thus
be modeled on a multidimensional FFT routine like fourn:

#include "nrutil.h"

void wtn(float a[], unsigned long nn[], int ndim, int isign,

void (*wtstep) (float [], unsigned long, int))
Replaces a by its ndim-dimensional discrete wavelet transform, if isign is input as 1. Here
nn[1..ndim] is an integer array containing the lengths of each dimension (number of real
values), which MUST all be powers of 2. a is a real array of length equal to the product of
these lengths, in which the data are stored as in a multidimensional real array. If isign is input
as —1, a is replaced by its inverse wavelet transform. The routine wtstep, whose actual name
must be supplied in calling this routine, is the underlying wavelet filter. Examples of wtstep
are daub4 and (preceded by pwtset) pwt.
{

unsigned long i1,i2,i3,k,n,nnew,nprev=1,nt ,ntot=1;

int idim;

float *wksp;

for (idim=1;idim<=ndim;idim++) ntot *= nn[idim];
wksp=vector(1,ntot) ;
for (idim=1;idim<=ndim;idim++) { Main loop over the dimensions.
n=nn[idim] ;
nnew=n*nprev;
if (n> 4) {
for (i2=0;i2<ntot;i2t=nnew) {
for (i1=1;il<=nprev;ilt+) {
for (i3=i1+i2,k=1;k<=n;k++,i3+=nprev) wksp[k]=a[i3] ;
Copy the relevant row or column or etc. into workspace.
if (isign >= 0) { Do one-dimensional wavelet transform.
for(nt=n;nt>=4;nt >>= 1)
(*«wtstep) (wksp,nt,isign) ;
} else { Or inverse transform.
for (nt=4;nt<=n;nt <<= 1)
(*«wtstep) (wksp,nt,isign) ;
+

for (i3=i1+i2,k=1;k<=n;k++,i3+=nprev) a[i3]=wksp[k] ;
Copy back from workspace.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.10 Wavelet Transforms 603

nprev=nnew;
}

free_vector(wksp,1,ntot) ;

Here, as before, wtstep is an individual wavelet step, either daub4 or pwt.
Compression of Images

An immediate application of the multidimensional transform wtn is to image
compression. The overall procedure is to take the wavelet transform of a digitized
image, and then to “allocate bits” among the wavelet coefficients in some highly
nonuniform, optimized, manner. In general, large wavelet coefficients get quantized
accurately, while small coefficients are quantized coarsely with only a bit or two
— or else are truncated completely. If the resulting quantization levels are still
statistically nonuniform, they may then be further compressed by a technique like
Huffman coding (§20.4).

While a more detailed description of the “back end” of this process, namely the
quantization and coding of the image, is beyond our scope, it is quite straightforward
to demonstrate the “front-end” wavelet encoding with a simple truncation: We keep
(with full accuracy) all wavelet coefficients larger than some threshold, and we delete
(set to zero) all smaller wavelet coefficients. We can then adjust the threshold to
vary the fraction of preserved coefficients.

Figure 13.10.4 shows a sequence of images that differ in the number of wavelet
coefficients that have been kept. The original picture (a), which is an official IEEE
test image, has 256 by 256 pixels with an 8-bit grayscale. The two reproductions
following are reconstructed with 23% (b) and 5.5% (c) of the 65536 wavelet
coefficients. The latter image illustrates the kind of compromises made by the
truncated wavelet representation. High-contrast edges (the model’s right cheek and
hair highlights ,e.g.) are maintained at a relatively high resolution, while low-contrast
areas (the model’s left eye and cheek, e.g.) are washed out into what amounts to
large constant pixels. Figure 13.10.4 (d) is the result of performing the identical
procedure with Fourier, instead of wavelet, transforms: The figure is reconstructed
from the 5.5% of 65536 real Fourier components having the largest magnitudes.
One sees that, since sines and cosines are nonlocal, the resolution is uniformly poor
across the picture; also, the deletion of any components produces a mottled “ringing”
everywhere. (Practical Fourier image compression schemes therefore break up an
image into small blocks of pixels, 16 x 16, say, and do rather elaborate smoothing
across block boundaries when the image is reconstructed.)

Fast Solution of Linear Systems

One of the most interesting, and promising, wavelet applications is linear
algebra. The basic idea [1] is to think of an integral operator (that is, a large matrix) as
a digital image. Suppose that the operator compresses well under a two-dimensional
wavelet transform, i.e., that a large fraction of its wavelet coefficients are so small
as to be negligible. Then any linear system involving the operator becomes a sparse
system in the wavelet basis. In other words, to solve

A-x=b (13.10.16)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

604 Chapter 13. Fourier and Spectral Applications

Figure 13.10.4. (a) IEEE test image, 256 x 256 pixels with 8-bit grayscale. (b) The image is transformed
into the wavelet basis; 77% of its wavelet components are set to zero (those of smallest magnitude); it
is then reconstructed from the remaining 23%. (c) Same as (b), but 94.5% of the wavelet components
are deleted. (d) Same as (c), but the Fourier transform is used instead of the wavelet transform. Wavelet
coefficients are better than the Fourier coefficients at preserving relevant details.

we first wavelet-transform the operator A and the right-hand side b by
A=W-A-W!, b=W-b (13.10.17)

where W represents the one-dimensional wavelet transform, then solve

‘xX=b (13.10.18)

and finally transform to the answer by the inverse wavelet transform

x=W’-x (13.10.19)

(Note that the routine wtn does the complete transformation of A into A)

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

13.10 Wavelet Transforms 605

Figure 13.10.5. Wavelet transform of a 256 x 256 matrix, represented graphically. The original matrix
has a discontinuous cusp along its diagonal, decaying smoothly away on both sides of the diagonal. In
wavelet basis, the matrix becomes sparse: Components larger than 10~* are shown as black, components
larger than 10~© as gray, and smaller-magnitude components are white. The matrix indices i and j
number from the lower left.

A typical integral operator that compresses well into wavelets has arbitrary (or
even nearly singular) elements near to its main diagonal, but becomes smooth away
from the diagonal. An example might be

-1 ifi= 7
Aig = { li — j[12 otherwise (13.10.20)

Figure 13.10.5 shows a graphical representation of the wavelet transform of this
matrix, where 7 and j range over 1... 256, using the DAUB12 wavelets. Elements
larger in magnitude than 10~* times the maximum element are shown as black
pixels, while elements between 10~? and 10~° are shown in gray. White pixels are
< 10~°. The indices i and j each number from the lower left.

In the figure, one sees the hierarchical decomposition into power-of-two sized
blocks. At the edges or corners of the various blocks, one sees edge effects caused
by the wrap-around wavelet boundary conditions. Apart from edge effects, within
each block, the nonnegligible elements are concentrated along the block diagonals.
This is a statement that, for this type of linear operator, a wavelet is coupled mainly
to near neighbors in its own hierarchy (square blocks along the main diagonal) and
near neighbors in other hierarchies (rectangular blocks off the diagonal).

The number of nonnegligible elements in a matrix like that in Figure 13.10.5
scales only as NV, the linear size of the matrix; as a rough rule of thumb it is about
10N log,9(1/e), where ¢ is the truncation level, e.g., 10~°. For a 2000 by 2000
matrix, then, the matrix is sparse by a factor on the order of 30.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

606 Chapter 13. Fourier and Spectral Applications

Various numerical schemes can be used to solve sparse linear systems of this
“hierarchically band diagonal” form. Beylkin, Coifman, and Rokhlin [1] make
the interesting observations that (1) the product of two such matrices is itself
hierarchically band diagonal (truncating, of course, newly generated elements that
are smaller than the predetermined threshold €); and moreover that (2) the product
can be formed in order N operations.

Fast matrix multiplication makes it possible to find the matrix inverse by
Schultz’s (or Hotelling’s) method, see §2.5.

Other schemes are also possible for fast solution of hierarchically band diagonal
forms. For example, one can use the conjugate gradient method, implemented in
§2.7 as linbcg.

CITED REFERENCES AND FURTHER READING:

Daubechies, |. 1992, Wavelets (Philadelphia: S.1.A.M.).

Strang, G. 1989, SIAM Review, vol. 31, pp. 614-627.

Beylkin, G., Coifman, R., and Rokhlin, V. 1991, Communications on Pure and Applied Mathe-
matics, vol. 44, pp. 141-183. [1]

Daubechies, |. 1988, Communications on Pure and Applied Mathematics, vol. 41, pp. 909-996.
[2]

Vaidyanathan, P.P. 1990, Proceedings of the IEEE, vol. 78, pp. 56-93. [3]

Mallat, S.G. 1989, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11,
pp. 674-693. [4]

Freedman, M.H., and Press, W.H. 1992, preprint. [5]

13.11 Numerical Use of the Sampling Theorem

In §6.10 we implemented an approximating formula for Dawson’s integral due to
Rybicki. Now that we have become Fourier sophisticates, we can learn that the formula
derives from numerical application of the sampling theorem (§12.1), normally considered to
be a purely analytic tool. Our discussion is identical to Rybicki [1].

For present purposes, the sampling theorem is most conveniently stated as follows:
Consider an arbitrary function g(t) and the grid of sampling points t, = a + nh, where n
ranges over the integers and a is a constant that allows an arbitrary shift of the sampling
grid. We then write

oo

t= >> g(tn) sinc F(t ~ tn) + e(t) (13.11.1)

n=—oo

where sinc = sinaz/x. The summation over the sampling points is called the sampling
representation of g(t), and e(¢) is its error term. The sampling theorem asserts that the
sampling representation is exact, that is, e(¢) = 0, if the Fourier transform of g(t),

G(w) = [- g(te! dt (13.11.2)

vanishes identically for |w| > m/h.

When can sampling representations be used to advantage for the approximate numerical
computation of functions? In order that the error term be small, the Fourier transform G(w)
must be sufficiently small for |w| > 7/h. On the other hand, in order for the summation
in (13.11.1) to be approximated by a reasonably small number of terms, the function g(t)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

13.11 Numerical Use of the Sampling Theorem 607

itself should be very small outside of a fairly limited range of values of t. Thus we are
led to two conditions to be satisfied in order that (13.11.1) be useful numerically: Both the
function g(t) and its Fourier transform G(w) must rapidly approach zero for large values
of their respective arguments.

Unfortunately, these two conditions are mutually antagonistic — the Uncertainty Princi-
ple in quantum mechanics. There exist strict limits on how rapidly the simultaneous approach
to zero can be in both arguments. According to a theorem of Hardy [2], if g(t) = Ole”)
as |t| — oo and G(w) = O(e~*”/) as |w| — oo, then g(t) = Ce”, where C is a
constant. This can be interpreted as saying that of all functions the Gaussian is the most
rapidly decaying in both ¢ and w, and in this sense is the “best” function to be expressed
numerically as a sampling representation.

Let us then write for the Gaussian g(t) = e,
ee -ye" sine F(t — tn) + e(t) (13.11.3)

The error e(¢) depends on the parameters h and a as well as on ¢, but it is sufficient for
the present purposes to state the bound,

le(t)| < eo 2” (13.114)

which can be understood simply as the order of magnitude of the Fourier transform of the
Gaussian at the point where it “spills over” into the region |w| > 7/h.

When the summation in (13.11.3) is approximated by one with finite limits, say from
No — N to No + N, where No is the integer nearest to —a/h, there is a further truncation
error. However, if N is chosen so that N > 7/(2h?), the truncation error in the summation
is less than the bound given by (13.11.4), and, since this bound is an overestimate, we
shall continue to use it for (13.11.3) as well. The truncated summation gives a remarkably
accurate representation for the Gaussian even for moderate values of N. For example,
le(t)| <5 x 10-° for h = 1/2 and N = 7; |e(t)| < 2x 107'° for h = 1/3 and N = 15;
and |e(t)| < 7 x 107! for h = 1/4 and N = 25.

One may ask, what is the point of such a numerical representation for the Gaussian,
which can be computed so easily and quickly as an exponential? The answer is that many
transcendental functions can be expressed as an integral involving the Gaussian, and by
substituting (13.11.3) one can often find excellent approximations to the integrals as a sum
over elementary functions.

Let us consider as an example the function w(z) of the complex variable z = x + iy,
related to the complex error function by

w(z) = e* erfc(—iz) (13.11.5)
having the integral representation
2
w(z) = =| ev dt (13.11.6)
Tito t-—z

where the contour C' extends from —oo to oo, passing below z (see, e.g.,[3]). Many methods
exist for the evaluation of this function (e.g.,[4]). Substituting the sampling representation
(13.11.3) into (13.11.6) and performing the resulting elementary contour integrals, we obtain

i 20 “el- (-1)"e~Tla-2/h
we) = So he a ae (13.11.7)
n=—00
where we now omit the error term. One should note that there is no singularity as z — tn
for some n = m, but a special treatment of the mth term will be required in this case (for
example, by power series expansion).
An alternative form of equation (13.11.7) can be found by expressing the complex expo-
nential in (13.11.7) in terms of trigonometric functions and using the sampling representation

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

608 Chapter 13. Fourier and Spectral Applications

(13.11.3) with z replacing t. This yields

~21—(-1)"cosa(a— z)/h

13.118
a ( )

22 1 <
ze * = he
w(z)ee 7 + a y e€

n=—00

This form is particularly useful in obtaining Re w(z) when |y| < 1. Note that in evaluating
(13.11.7) the exponential inside the summation is a constant and needs to be evaluated only
once; a similar comment holds for the cosine in (13.11.8).

There are a variety of formulas that can now be derived from either equation (13.11.7)
or (13.11.8) by choosing particular values of a. Eight interesting choices are: a = 0, 2, iy,
or z, plus the values obtained by adding h/2 to each of these. Since the error bound (13.11.3)
assumed a real value of a, the choices involving a complex a are useful only if the imaginary
part of z is not too large. This is not the place to catalog all sixteen possible formulas, and we
give only two particular cases that show some of the important features.

First of all let a = 0 in equation (13.11.8), which yields,

2 1 ~(nn)2 1 — (—1)" cos(mz/h)
xe = he (mh) = NA) SOMES
w(z) xe + mi y e

n=—00

13.11.9
nh — z ( )

This approximation is good over the entire z-plane. As stated previously, one has to treat the
case where one denominator becomes small by expansion in a power series. Formulas for
the case a = 0 were discussed briefly in [5]. They are similar, but not identical, to formulas
derived by Chiarella and Reichel [6], using the method of Goodwin [7].

Next, let a = z in (13.11.7), which yields

_p 2 ed
w(z) te 5 >> = (13.11.10)
the sum being over all odd integers (positive and negative). Note that we have made the
substitution n — —n in the summation. This formula is simpler than (13.11.9) and contains
half the number of terms, but its error is worse if y is large. Equation (13.11.10) is the source
of the approximation formula (6.10.3) for Dawson’s integral, used in §6.10.

CITED REFERENCES AND FURTHER READING:

Rybicki, G.B. 1989, Computers in Physics, vol. 3, no. 2, pp. 85-87. [1]

Hardy, G.H. 1933, Journal of the London Mathematical Society, vol. 8, pp. 227-231. [2]

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York). [3]

Gautschi, W. 1970, SIAM Journal on Numerical Analysis, vol. 7, pp. 187-198. [4]

Armstrong, B.H., and Nicholls, R.W. 1972, Emission, Absorption and Transfer of Radiation in
Heated Atmospheres (New York: Pergamon). [5]

Chiarella, C., and Reichel, A. 1968, Mathematics of Computation, vol. 22, pp. 137-143. [6]

Goodwin, E.T. 1949, Proceedings of the Cambridge Philosophical Society, vol. 45, pp. 241-245.
7]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 14. Statistical Description
of Data

14.0 Introduction

In this chapter and the next, the concept of data enters the discussion more
prominently than before.

Data consist of numbers, of course. But these numbers are fed into the computer,
not produced by it. These are numbers to be treated with considerable respect, neither
to be tampered with, nor subjected to a numerical process whose character you do
not completely understand. You are well advised to acquire a reverence for data that
is rather different from the “sporty” attitude that is sometimes allowable, or even
commendable, in other numerical tasks.

The analysis of data inevitably involves some trafficking with the field of
statistics, that gray area which is not quite a branch of mathematics — and just as
surely not quite a branch of science. In the following sections, you will repeatedly
encounter the following paradigm:

e apply some formula to the data to compute “a statistic”

e compute where the value of that statistic falls in a probability distribution

that is computed on the basis of some “null hypothesis”

e if it falls in a very unlikely spot, way out on a tail of the distribution,

conclude that the null hypothesis is false for your data set

If a statistic falls in a reasonable part of the distribution, you must not make
the mistake of concluding that the null hypothesis is “verified” or “proved.” That is
the curse of statistics, that it can never prove things, only disprove them! At best,
you can substantiate a hypothesis by ruling out, statistically, a whole long list of
competing hypotheses, every one that has ever been proposed. After a while your
adversaries and competitors will give up trying to think of alternative hypotheses,
or else they will grow old and die, and then your hypothesis will become accepted.
Sounds crazy, we know, but that’s how science works!

In this book we make a somewhat arbitrary distinction between data analysis
procedures that are model-independent and those that are model-dependent. In the
former category, we include so-called descriptive statistics that characterize a data
set in general terms: its mean, variance, and so on. We also include statistical tests
that seek to establish the “sameness” or “differentness” of two or more data sets, or
that seek to establish and measure a degree of correlation between two data sets.
These subjects are discussed in this chapter.

609

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

610 Chapter 14. Statistical Description of Data

In the other category, model-dependent statistics, we lump the whole subject of
fitting data to a theory, parameter estimation, least-squares fits, and so on. Those
subjects are introduced in Chapter 15.

Section 14.1 deals with so-called measures of central tendency, the moments of
a distribution, the median and mode. In §14.2 we learn to test whether different data
sets are drawn from distributions with different values of these measures of central
tendency. This leads naturally, in §14.3, to the more general question of whether two
distributions can be shown to be (significantly) different.

In §14.4-8§14.7, we deal with measures of association for two distributions.
We want to determine whether two variables are “correlated” or “dependent” on
one another. If they are, we want to characterize the degree of correlation in
some simple ways. The distinction between parametric and nonparametric (rank)
methods is emphasized.

Section 14.8 introduces the concept of data smoothing, and discusses the
particular case of Savitzky-Golay smoothing filters.

This chapter draws mathematically on the material on special functions that
was presented in Chapter 6, especially §6.1-§6.4. You may wish, at this point, to
review those sections.

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill).

Stuart, A., and Ord, J.K. 1987, Kendall’s Advanced Theory of Statistics, 5th ed. (London: Griffin
and Co.) [previous eds. published as Kendall, M., and Stuart, A., The Advanced Theory
of Statistics].

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

Dunn, O.J., and Clark, V.A. 1974, Applied Statistics: Analysis of Variance and Regression (New
York: Wiley).

14.1 Moments of a Distribution: Mean,
Variance, Skewness, and So Forth

When a set of values has a sufficiently strong central tendency, that is, a tendency
to cluster around some particular value, then it may be useful to characterize the
set by a few numbers that are related to its moments, the sums of integer powers
of the values.

Best known is the mean of the values 71,...,7y,

(14.1.1)

which estimates the value around which central clustering occurs. Note the use of
an overbar to denote the mean; angle brackets are an equally common notation, e.g.,
(x). You should be aware that the mean is not the only available estimator of this

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.1 Moments of a Distribution: Mean, Variance, Skewness 611

quantity, nor is it necessarily the best one. For values drawn from a probability
distribution with very broad “tails,” the mean may converge poorly, or not at all, as
the number of sampled points is increased. Alternative estimators, the median and
the mode, are mentioned at the end of this section.

Having characterized a distribution’s central value, one conventionally next
characterizes its “width” or “variability” around that value. Here again, more than
one measure is available. Most common is the variance,

N
Var(ay ...2y) = wo he _zp (14.1.2)
j=l

or its square root, the standard deviation,

= V Var(r1... tN) (14.1.3)

Equation (14.1.2) estimates the mean squared deviation of x from its mean value.
There is a long story about why the denominator of (14.1.2) is N — 1 instead of
N. If you have never heard that story, you may consult any good statistics text.
Here we will be content to note that the N — 1 should be changed to N if you
are ever in the situation of measuring the variance of a distribution whose mean
= is known a priori rather than being estimated from the data. (We might also
comment that if the difference between N and N — 1 ever matters to you, then you
are probably up to no good anyway — e.g., trying to substantiate a questionable
hypothesis with marginal data.)

As the mean depends on the first moment of the data, so do the variance and
standard deviation depend on the second moment. It is not uncommon, in real
life, to be dealing with a distribution whose second moment does not exist (i.e., is
infinite). In this case, the variance or standard deviation is useless as a measure
of the data’s width around its central value: The values obtained from equations
(14.1.2) or (14.1.3) will not converge with increased numbers of points, nor show
any consistency from data set to data set drawn from the same distribution. This can
occur even when the width of the peak looks, by eye, perfectly finite. A more robust
estimator of the width is the average deviation or mean absolute deviation, defined by

1
ADev(x1...%N) = yD! —2| (14.1.4)
j=1

One often substitutes the sample median xy,eq for ¥ in equation (14.1.4). For any
fixed sample, the median in fact minimizes the mean absolute deviation.

Statisticians have historically sniffed at the use of (14.1.4) instead of (14.1.2),
since the absolute value brackets in (14.1.4) are “nonanalytic” and make theorem-
proving difficult. In recent years, however, the fashion has changed, and the subject
of robust estimation (meaning, estimation for broad distributions with significant
numbers of “outlier” points) has become a popular and important one. Higher
moments, or statistics involving higher powers of the input data, are almost always
less robust than lower moments or statistics that involve only linear sums or (the
lowest moment of all) counting.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

612 Chapter 14. Statistical Description of Data

Skewness Kurtosis
L positive
. Jeptokurtic
negative (leptokurtic)

(platykurtic) -
ca

--7 N

U

(a) (b)

Figure 14.1.1. Distributions whose third and fourth moments are significantly different from a normal
(Gaussian) distribution. (a) Skewness or third moment. (b) Kurtosis or fourth moment.

That being the case, the skewness or third moment, and the kurtosis or fourth
moment should be used with caution or, better yet, not at all.

The skewness characterizes the degree of asymmetry of a distribution around its
mean. While the mean, standard deviation, and average deviation are dimensional
quantities, that is, have the same units as the measured quantities x ;, the skewness
is conventionally defined in such a way as to make it nondimensional. It is a pure
number that characterizes only the shape of the distribution. The usual definition is

N —73
Skew(a1...2y) = ix E =] (14.1.5)
j=l

oO

where o = o(21 ...2y) is the distribution’s standard deviation (14.1.3). A positive
value of skewness signifies a distribution with an asymmetric tail extending out
towards more positive x; a negative value signifies a distribution whose tail extends
out towards more negative x (see Figure 14.1.1).

Of course, any set of N measured values is likely to give a nonzero value for
(14.1.5), even if the underlying distribution is in fact symmetrical (has zero skewness).
For (14.1.5) to be meaningful, we need to have some idea of its standard deviation
as an estimator of the skewness of the underlying distribution. Unfortunately, that
depends on the shape of the underlying distribution, and rather critically on its tails!
For the idealized case of a normal (Gaussian) distribution, the standard deviation of
(14.1.5) is approximately \/15/N when Z is the true mean, and ,/6/N when it is
estimated by the sample mean, (14.1.1). In real life it is good practice to believe in
skewnesses only when they are several or many times as large as this.

The kurtosis is also a nondimensional quantity. It measures the relative
peakedness or flatness of a distribution. Relative to what? A normal distribution,
what else! A distribution with positive kurtosis is termed leptokurtic; the outline
of the Matterhorn is an example. A distribution with negative kurtosis is termed
platykurtic; the outline of a loaf of bread is an example. (See Figure 14.1.1.) And,
as you no doubt expect, an in-between distribution is termed mesokurtic.

The conventional definition of the kurtosis is

(14.1.6)

j=l
where the —3 term makes the value zero for a normal distribution.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.1 Moments of a Distribution: Mean, Variance, Skewness 613

The standard deviation of (14.1.6) as an estimator of the kurtosis of an underlying
normal distribution is J 96/N when a is the true standard deviation, and J 24/N
when it is the sample estimate (14.1.3). However, the kurtosis depends on such
a high moment that there are many real-life distributions for which the standard
deviation of (14.1.6) as an estimator is effectively infinite.

Calculation of the quantities defined in this section is perfectly straightforward.
Many textbooks use the binomial theorem to expand out the definitions into sums
of various powers of the data, e.g., the familiar

14.1.7
Wol (14.1.7)

N
Var(v1...0N) = — Soe — Nx
j=l
but this can magnify the roundoff error by a large factor and is generally unjustifiable
in terms of computing speed. A clever way to minimize roundoff error, especially
for large samples, is to use the corrected two-pass algorithm [1]: First calculate 7,
then calculate Var(z1...ay) by

(14.1.8)

The second sum would be zero if % were exact, but otherwise it does a good job of
correcting the roundoff error in the first term.

#include <math.h>

void moment(float data[], int n, float *ave, float *adev, float *sdev,
float *var, float *skew, float *curt)
Given an array of data[1..n], this routine returns its mean ave, average deviation adev,
standard deviation sdev, variance var, skewness skew, and kurtosis curt.
{
void nrerror(char error_text[]);
int j;
float ep=0.0,s,p;

if (n <= 1) nrerror("n must be at least 2 in moment");
s=0.0; First pass to get the mean.

for (j=1;j<=n;j++) s += dataljl];
*ave=s/n;
*adev=(*var)=(*skew)=(*curt)=0.0; Second pass to get the first (absolute), sec-
for (j=1;j<=n;j++) { ond, third, and fourth moments of the
*adev fabs(s=data[j]-(*ave)); deviation from the mean.
ep += s;

‘var += (p=sts);

askew += (p *= s);

curt += (p *= s);
+
xadev /= n;
xvar=(*var-ep*ep/n) /(n-1) ; Corrected two-pass formula.
¥sdev=sqrt (*var) ; Put the pieces together according to the con-
if (#var) { ventional definitions.

‘skew /= (n*(*var)*(*sdev)) 5

*curt=(*curt) /(n*(*var) *(*var))-3.0;
} else nrerror("No skew/kurtosis when variance = 0 (in moment)");

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

614 Chapter 14. Statistical Description of Data

Semi-Invariants

The mean and variance of independent random variables are additive: If 2 and y are
drawn independently from two, possibly different, probability distributions, then

(c+y)="+ 9 ~~ Var(x + y) = Var(x) + Var(x) (14.1.9)

Higher moments are not, in general, additive. However, certain combinations of them,
called semi-invariants, are in fact additive. If the centered moments of a distribution are
denoted M;,,

M. = (Ci -z)*) (14.1.10)

so that, e.g., Mz = Var(x), then the first few semi-invariants, denoted J, are given by
Iz=Mz  I3=M3 In = Ma —3M3
9 3 (14.1.11)
Is = Ms — 10M2 M3 Ig = Me — 15M2 M4 — 10M3 + 30M

Notice that the skewness and kurtosis, equations (14.1.5) and (14.1.6) are simple powers
of the semi-invariants,
Skew(x) = 13/13’? Kurt(x) = La/13 (14.1.12)

A Gaussian distribution has all its semi-invariants higher than Jz equal to zero. A Poisson

distribution has all of its semi-invariants equal to its mean. For more details, see [2].

Median and Mode

The median of a probability distribution function p(x) is the value 2 eq for
which larger and smaller values of x are equally probable:

[~ pla) de = : _ [ p(x) de (14.1.13)

med

The median of a distribution is estimated from a sample of values 71,...,
xy by finding that value x; which has equal numbers of values above it and below
it. Of course, this is not possible when N is even. In that case it is conventional
to estimate the median as the mean of the unique two central values. If the values
a j=l,..., N are sorted into ascending (or, for that matter, descending) order,

then the formula for the median is

zu y/o. N odd
tot = { wee (14.1.14)

$(@N/2 + 2(N/2)41), N even

If a distribution has a strong central tendency, so that most of its area is under
a single peak, then the median is an estimator of the central value. It is a more
robust estimator than the mean is: The median fails as an estimator only if the area
in the tails is large, while the mean fails if the first moment of the tails is large;
it is easy to construct examples where the first moment of the tails is large even
though their area is negligible.

To find the median of a set of values, one can proceed by sorting the set and
then applying (14.1.14). This is a process of order N log N. You might rightly think

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.2 Do Two Distributions Have the Same Means or Variances? 615

that this is wasteful, since it yields much more information than just the median
(e.g., the upper and lower quartile points, the deciles, etc.). In fact, we saw in
§8.5 that the element x 41)/2 can be located in of order N operations. Consult
that section for routines.

The mode of a probability distribution function p(x) is the value of x where it
takes on a maximum value. The mode is useful primarily when there is a single, sharp
maximum, in which case it estimates the central value. Occasionally, a distribution
will be bimodal, with two relative maxima; then one may wish to know the two
modes individually. Note that, in such cases, the mean and median are not very
useful, since they will give only a “compromise” value between the two peaks.

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill), Chapter 2.

Stuart, A., and Ord, J.K. 1987, Kendall’s Advanced Theory of Statistics, 5th ed. (London: Griffin
and Co.) [previous eds. published as Kendall, M., and Stuart, A., The Advanced Theory
of Statistics], vol. 1, 810.15

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

Chan, T.F., Golub, G.H., and LeVeque, R.J. 1983, American Statistician, vol. 37, pp. 242-247. [1]

Cramér, H. 1946, Mathematical Methods of Statistics (Princeton: Princeton University Press),
§15.10. [2]

14.2 Do Two Distributions Have the Same
Means or Variances?

Not uncommonly we want to know whether two distributions have the same
mean. For example, a first set of measured values may have been gathered before
some event, a second set after it. We want to know whether the event, a “treatment”
or a “change in a control parameter,” made a difference.

Our first thought is to ask “how many standard deviations” one sample mean is
from the other. That number may in fact be a useful thing to know. It does relate to
the strength or “importance” of a difference of means if that difference is genuine.
However, by itself, it says nothing about whether the difference is genuine, that is,
statistically significant. A difference of means can be very small compared to the
standard deviation, and yet very significant, if the number of data points is large.
Conversely, a difference may be moderately large but not significant, if the data
are sparse. We will be meeting these distinct concepts of strength and significance
several times in the next few sections.

A quantity that measures the significance of a difference of means is not the
number of standard deviations that they are apart, but the number of so-called
standard errors that they are apart. The standard error of a set of values measures
the accuracy with which the sample mean estimates the population (or “true”) mean.
Typically the standard error is equal to the sample’s standard deviation divided by
the square root of the number of points in the sample.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

616 Chapter 14. Statistical Description of Data

Student’s t-test for Significantly Different Means

Applying the concept of standard error, the conventional statistic for measuring
the significance of a difference of means is termed Student’s t. When the two
distributions are thought to have the same variance, but possibly different means,
then Student’s t is computed as follows: First, estimate the standard error of the
difference of the means, sp, from the “pooled variance” by the formula

v4, — 2A)? ; xv; —@pB)2
8p [zat wat Rel 5) (+ ~) (14.2.1)

Na Neg

where each sum is over the points in one sample, the first or second, each mean
likewise refers to one sample or the other, and N 4 and Vz are the numbers of points
in the first and second samples, respectively. Second, compute t by

p= “A *B (14.2.2)

Third, evaluate the significance of this value of t for Student’s distribution with
Na+ Ng — 2 degrees of freedom, by equations (6.4.7) and (6.4.9), and by the
routine betai (incomplete beta function) of §6.4.

The significance is a number between zero and one, and is the probability that
|t| could be this large or larger just by chance, for distributions with equal means.
Therefore, a small numerical value of the significance (0.05 or 0.01) means that the
observed difference is “very significant.” The function A(t|v) in equation (6.4.7)
is one minus the significance.

As a routine, we have

#include <math.h>

void ttest(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *t, float *prob)
Given the arrays datai[1..n1] and data2[1..n2], this routine returns Student's ¢ as t,
and its significance as prob, small values of prob indicating that the arrays have significantly
different means. The data arrays are assumed to be drawn from populations with the same
true variance.
{
void avevar(float data[], unsigned long n, float *ave, float *var);
float betai(float a, float b, float x);
float vari,var2,svar,df,avel,ave2;

avevar (datai,ni,&avel,&var1);

avevar (data2,n2,éave2,&var2) ;

df=ni+n2-2; Degrees of freedom.
svar=((ni-1)*vari+(n2-1) *var2) /df; Pooled variance.
*t=(avel-ave2) /sqrt (svar*(1.0/ni+1.0/n2)) ;

*prob=betai (0.5+df ,0.5,d£/(df+(*t) *(#t))); See equation (6.4.9).

which makes use of the following routine for computing the mean and variance
of a set of numbers,

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.2 Do Two Distributions Have the Same Means or Variances? 617

void avevar(float data[], unsigned long n, float *ave, float *var)
Given array data[1..n], returns its mean as ave and its variance as var.
{

unsigned long j;

float s,ep;

for (*ave=0.0,j=1;j<=n;jt+) *ave += data[j];
*ave /= n;
*var=ep=0.
for (j sj+4) {
s=data[j]-(*ave) ;
ep += s;
‘var += s*s;

+

xvar=(*var-ep*ep/n) /(n-1); Corrected two-pass formula (14.1.8).

The next case to consider is where the two distributions have significantly
different variances, but we nevertheless want to know if their means are the same or
different. (A treatment for baldness has caused some patients to lose all their hair
and turned others into werewolves, but we want to know if it helps cure baldness on
the average!) Be suspicious of the unequal-variance t-test: If two distributions have
very different variances, then they may also be substantially different in shape; in
that case, the difference of the means may not be a particularly useful thing to know.

To find out whether the two data sets have variances that are significantly
different, you use the F-test, described later on in this section.

The relevant statistic for the unequal variance t-test is

TA-TB
[Var(ca)/Na + Var(rp)/Npp72

(14.2.3)

This statistic is distributed approximately as Student’s ¢ with a number of degrees
of freedom equal to

Var(x 4) 4 Var(2 p)
Na Np (14.2.4)
[Var(ra)/Na]* , [Var(xp)/Nol* ~
Na-l1 / Nge-1l

Expression (14.2.4) is in general not an integer, but equation (6.4.7) doesn’t care.
The routine is

#include <math.h>
#include "nrutil.h"

void tutest(float datai[], unsigned long ni, float data2[], unsigned long n2,

float *t, float *prob)
Given the arrays datai[1..n1] and data2[1..n2], this routine returns Student's t as t, and
its significance as prob, small values of prob indicating that the arrays have significantly differ-
ent means. The data arrays are allowed to be drawn from populations with unequal variances.
t

void avevar(float data[], unsigned long n, float *ave, float *var);

float betai(float a, float b, float x);

float vari,var2,df,avel,ave2;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

618 Chapter 14. Statistical Description of Data

avevar (datai,ni,&avel,&var1);

avevar (data2,n2,éave2,&var2) ;

*t=(avel-ave2)/sqrt (vari/nit+var2/n2) ;

df=SQR(vari/ni+var2/n2) /(SQR(vari/n1) /(n1-1)+SQR(var2/n2) /(n2-1)) ;
*prob=betai (0.5*df,0.5,df/(df£+SQR(*t)));

Our final example of a Student’s t test is the case of paired samples. Here
we imagine that much of the variance in both samples is due to effects that are
point-by-point identical in the two samples. For example, we might have two job
candidates who have each been rated by the same ten members of a hiring committee.
We want to know if the means of the ten scores differ significantly. We first try
ttest above, and obtain a value of prob that is not especially significant (e.g.,
> 0.05). But perhaps the significance is being washed out by the tendency of some
committee members always to give high scores, others always to give low scores,
which increases the apparent variance and thus decreases the significance of any
difference in the means. We thus try the paired-sample formulas,

N
Cov(r4, 2B Seas —Ta)(tpi -F (14.2.5)
i=l
Var(2 4) + Var(a gz) — 2Cov(24, rp) 172
8D (14.2.6)
N
,— TASTE (14.2.7)

where N is the number in each sample (number of pairs). Notice that it is important
that a particular value of i label the corresponding points in each sample, that is,
the ones that are paired. The significance of the ¢ statistic in (14.2.7) is evaluated
for N — 1 degrees of freedom.

The routine is

#include <math.h>

void tptest(float datai[], float data2[], unsigned long n, float *t,

float *prob)
Given the paired arrays datai1[1..n] and data2[1..n], this routine returns Student's ¢ for
paired data as t, and its significance as prob, small values of prob indicating a significant
difference of means.
{

void avevar(float data[], unsigned long n, float *ave, float *var);

float betai(float a, float b, float x);

unsigned long j;

float vari,var2,avel,ave2,sd,df,cov=0.0;

avevar (datai,n,éavel,&var1) ;
avevar (data2,n,%ave2,&var2) ;
for (j=1;j<=n; j++)

cov += (datai[j]-ave1)*(data2[j]-ave2) ;
cov /= df=n-1;
sd=sqrt ((vari+var2-2.0*cov) /n) ;
*t=(avel-ave2)/sd;
*xprob=betai(0.5*df,0.5,d£/(df+(#t) *(#t)));

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.2 Do Two Distributions Have the Same Means or Variances? 619

F-Test for Significantly Different Variances

The F-test tests the hypothesis that two samples have different variances by
trying to reject the null hypothesis that their variances are actually consistent. The
statistic F' is the ratio of one variance to the other, so values either >> 1 or < 1
will indicate very significant differences. The distribution of F’ in the null case is
given in equation (6.4.11), which is evaluated using the routine betai. In the most
common case, we are willing to disprove the null hypothesis (of equal variances) by
either very large or very small values of F’, so the correct significance is two-tailed,
the sum of two incomplete beta functions. It turns out, by equation (6.4.3), that the
two tails are always equal; we need compute only one, and double it. Occasionally,
when the null hypothesis is strongly viable, the identity of the two tails can become
confused, giving an indicated probability greater than one. Changing the probability
to two minus itself correctly exchanges the tails. These considerations and equation
(6.4.3) give the routine

void ftest(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *f, float *prob)

Given the arrays datai[1..n1] and data2[1..n2], this routine returns the value of f, and

its significance as prob. Small values of prob indicate that the two arrays have significantly

different variances.

{
void avevar(float data[], unsigned long n, float *ave, float *var);
float betai(float a, float b, float x);
float vari,var2,avel,ave2,df1,df2;
avevar (datai,ni,&avel,&var1);
avevar (data2,n2,éave2,&var2) ;
if (var1 > var2) { Make F the ratio of the larger variance to the smaller
*f=varl/var2; one.
dfi=n1-1;
df2=n2-1;
} else {
*f=var2/vari1;
df1=n2-1;
df2=n1-1;
+
*prob = 2.0*betai(0.5*df2,0.5*df1,df£2/(df2+df1*(*£)));
if (*prob > 1.0) *prob=2.0-*prob;
}

CITED REFERENCES AND FURTHER READING:

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), Chapter IX(B).

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

620 Chapter 14. Statistical Description of Data

14.3 Are Two Distributions Different?

Given two sets of data, we can generalize the questions asked in the previous
section and ask the single question: Are the two sets drawn from the same distribution
function, or from different distribution functions? Equivalently, in proper statistical
language, “Can we disprove, to a certain required level of significance, the null
hypothesis that two data sets are drawn from the same population distribution
function?” Disproving the null hypothesis in effect proves that the data sets are from
different distributions. Failing to disprove the null hypothesis, on the other hand,
only shows that the data sets can be consistent with a single distribution function.
One can never prove that two data sets come from a single distribution, since (e.g.)
no practical amount of data can distinguish between two distributions which differ
only by one part in 102°.

Proving that two distributions are different, or showing that they are consistent,
is a task that comes up all the time in many areas of research: Are the visible stars
distributed uniformly in the sky? (That is, is the distribution of stars as a function
of declination — position in the sky — the same as the distribution of sky area as
a function of declination?) Are educational patterns the same in Brooklyn as in the
Bronx? (That is, are the distributions of people as a function of last-grade-attended
the same?) Do two brands of fluorescent lights have the same distribution of
burn-out times? Is the incidence of chicken pox the same for first-born, second-born,
third-born children, etc.?

These four examples illustrate the four combinations arising from two different
dichotomies: (1) The data are either continuous or binned. (2) Either we wish to
compare one data set to a known distribution, or we wish to compare two equally
unknown data sets. The data sets on fluorescent lights and on stars are continuous,
since we can be given lists of individual burnout times or of stellar positions. The
data sets on chicken pox and educational level are binned, since we are given
tables of numbers of events in discrete categories: first-born, second-born, etc.; or
6th Grade, 7th Grade, etc. Stars and chicken pox, on the other hand, share the
property that the null hypothesis is a known distribution (distribution of area in the
sky, or incidence of chicken pox in the general population). Fluorescent lights and
educational level involve the comparison of two equally unknown data sets (the two
brands, or Brooklyn and the Bronx).

One can always turn continuous data into binned data, by grouping the events
into specified ranges of the continuous variable(s): declinations between 0 and 10
degrees, 10 and 20, 20 and 30, etc. Binning involves a loss of information, however.
Also, there is often considerable arbitrariness as to how the bins should be chosen.
Along with many other investigators, we prefer to avoid unnecessary binning of data.

The accepted test for differences between binned distributions is the chi-square
test. For continuous data as a function of a single variable, the most generally
accepted test is the Kolmogorov-Smirnov test. We consider each in turn.

Chi-Square Test

Suppose that NV; is the number of events observed in the ith bin, and that n; is
the number expected according to some known distribution. Note that the N ;’s are

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.3 Are Two Distributions Different? 621

integers, while the n;’s may not be. Then the chi-square statistic is

(14.3.1)

where the sum is over all bins. A large value of x? indicates that the null hypothesis
(that the NV;’s are drawn from the population represented by the n ,’s) is rather unlikely.

Any term j in (14.3.1) with 0 = n; = Nj should be omitted from the sum. A
term with n; = 0, N; 4 0 gives an infinite x, as it should, since in this case the
NN;,’s cannot possibly be drawn from the n,’s!

The chi-square probability function Q(x?|v) is an incomplete gamma function,
and was already discussed in §6.2 (see equation 6.2.18). Strictly speaking Q(x ?|v)
is the probability that the sum of the squares of v random normal variables of unit
variance (and zero mean) will be greater than y?. The terms in the sum (14.3.1)
are not individually normal. However, if either the number of bins is large (> 1),
or the number of events in each bin is large (>> 1), then the chi-square probability
function is a good approximation to the distribution of (14.3.1) in the case of the null
hypothesis. Its use to estimate the significance of the chi-square test is standard.

The appropriate value of v, the number of degrees of freedom, bears some
additional discussion. If the data are collected with the model nj;’s fixed — that
is, not later renormalized to fit the total observed number of events NN; — then v
equals the number of bins Ng. (Note that this is not the total number of events!)
Much more commonly, the n;’s are normalized after the fact so that their sum equals
the sum of the N;’s. In this case the correct value for v is Ng — 1, and the model
is said to have one constraint (knstrn=1 in the program below). If the model that
gives the n,’s has additional free parameters that were adjusted after the fact to agree
with the data, then each of these additional “fitted” parameters decreases v (and
increases knstrn) by one additional unit.

We have, then, the following program:

void chsone(float bins[], float ebins[], int nbins, int knstrn, float *df,

float *chsq, float *prob)
Given the array bins[1..nbins] containing the observed numbers of events, and an array
ebins[1..nbins] containing the expected numbers of events, and given the number of con-
straints knstrn (normally one), this routine returns (trivially) the number of degrees of freedom
df, and (nontrivially) the chi-square chsq and the significance prob. A small value of prob
indicates a significant difference between the distributions bins and ebins. Note that bins
and ebins are both float arrays, although bins will normally contain integer values.
{

float gammq(float a, float x);

void nrerror(char error_text[]);

int j;

float temp;

*df=nbins-knstrn;

*chsq=0.0;

for (j=1;j<=nbins;j++) {
if (ebins[j] <= 0.0) nrerror("Bad expected number in chsone");
temp=bins[j]-ebins[j];
*chsq += temp*temp/ebins[j] ;

+

*prob=gammq(0.5*(*df) ,0.5*(*chsq)) ; Chi-square probability function. See §6.2.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

622 Chapter 14. Statistical Description of Data

Next we consider the case of comparing two binned data sets. Let R; be the
number of events in bin i for the first data set, S; the number of events in the same
bin 7 for the second data set. Then the chi-square statistic is

2 _ yn (Ri Si)?
x => Ee (14.3.2)
Comparing (14.3.2) to (14.3.1), you should note that the denominator of (14.3.2) is
not just the average of R; and S; (which would be an estimator of n,; in 14.3.1).
Rather, it is twice the average, the sum. The reason is that each term in a chi-square
sum is supposed to approximate the square of a normally distributed quantity with
unit variance. The variance of the difference of two normal quantities is the sum
of their individual variances, not the average.

If the data were collected in such a way that the sum of the R,’s is necessarily
equal to the sum of S;;’s, then the number of degrees of freedom is equal to one
less than the number of bins, Ng — 1 (that is, knstrn = 1), the usual case. If
this requirement were absent, then the number of degrees of freedom would be N gz.
Example: A birdwatcher wants to know whether the distribution of sighted birds
as a function of species is the same this year as last. Each bin corresponds to one
species. If the birdwatcher takes his data to be the first 1000 birds that he saw in
each year, then the number of degrees of freedom is Ng — 1. If he takes his data to
be all the birds he saw on a random sample of days, the same days in each year, then
the number of degrees of freedom is N g (knstrn = 0). In this latter case, note that
he is also testing whether the birds were more numerous overall in one year or the
other: That is the extra degree of freedom. Of course, any additional constraints on
the data set lower the number of degrees of freedom (i.e., increase knstrn to more
positive values) in accordance with their number.

The program is

void chstwo(float binsi[], float bins2[], int nbins, int knstrn, float *df,

float *chsq, float *prob)
Given the arrays binsi[1..nbins] and bins2[1..nbins], containing two sets of binned
data, and given the number of constraints knstrn (normally 1 or 0), this routine returns the
number of degrees of freedom df, the chi-square chsq, and the significance prob. A small value
of prob indicates a significant difference between the distributions bins1 and bins2. Note that
bins1 and bins2 are both float arrays, although they will normally contain integer values.
{

float gammq(float a, float x);

int j;

float temp;

*df=nbins-knstrn;
.0;
}j<=nbins; j++)
if (binsi[j] == 0.0 && bins2[j] == 0.0)
--(#df) ; No data means one less degree of free-
else { dom.
temp=bins1[j]-bins2[j];
*chsq += temp*temp/(bins1[j]+bins2[j]);

}
*prob=gammq(0.5*(*df) ,0.5*(*chsq)) ;

Chi-square probability function. See §6.2.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.3 Are Two Distributions Different? 623

Equation (14.3.2) and the routine chstwo both apply to the case where the total
number of data points is the same in the two binned sets. For unequal numbers of
data points, the formula analogous to (14.3.2) is

v= (/S/RR; ~ VR/SSi)” (14.3.3)

R,+ 8S;

a

where
R=yR S=NS; (14.3.4)

are the respective numbers of data points. It is straightforward to make the
corresponding change in chstwo.

Kolmogorov-Smirnov Test

The Kolmogorov-Smirnov (or K-S) test is applicable to unbinned distributions
that are functions of a single independent variable, that is, to data sets where each
data point can be associated with a single number (lifetime of each lightbulb when
it burns out, or declination of each star). In such cases, the list of data points can
be easily converted to an unbiased estimator Sy (x) of the cumulative distribution
function of the probability distribution from which it was drawn: If the N events are
located at values x;, i = 1,...,N, then S,y(2) is the function giving the fraction
of data points to the left of a given value x. This function is obviously constant
between consecutive (i.e., sorted into ascending order) x ;’s, and jumps by the same
constant 1/N at each x;. (See Figure 14.3.1.)

Different distribution functions, or sets of data, give different cumulative
distribution function estimates by the above procedure. However, all cumulative
distribution functions agree at the smallest allowable value of x (where they are
zero), and at the largest allowable value of x (where they are unity). (The smallest
and largest values might of course be -Eoo.) So it is the behavior between the largest
and smallest values that distinguishes distributions.

One can think of any number of statistics to measure the overall difference
between two cumulative distribution functions: the absolute value of the area between
them, for example. Or their integrated mean square difference. The Kolmogorov-
Smirnov D is a particularly simple measure: It is defined as the maximum value
of the absolute difference between two cumulative distribution functions. Thus,
for comparing one data set’s Sy (x) to a known cumulative distribution function
P(x), the K-S statistic is

—00<u"<0o

while for comparing two different cumulative distribution functions Sy, (x) and
S,(a), the K-S statistic is

D= max |Sy,(x)— Sy,(2)| (14.3.6)

—00<u"<o00

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

624 Chapter 14. Statistical Description of Data

cumulative probability distribution

x

Figure 14.3.1. | Kolmogorov-Smirnov statistic D. A measured distribution of values in a (shown
as N dots on the lower absci is to be compared with a theoretical distribution whose cumulative
probability distribution is plotted as P(a). A step-function cumulative probability distribution Sy (a) is
constructed, one that rises an equal amount at each measured point. D is the greatest distance between
the two cumulative distributions.

What makes the K-S statistic useful is that its distribution in the case of the null
hypothesis (data sets drawn from the same distribution) can be calculated, at least to
useful approximation, thus giving the significance of any observed nonzero value of
D. A central feature of the K—S test is that it is invariant under reparametrization
of x; in other words, you can locally slide or stretch the a axis in Figure 14.3.1,
and the maximum distance D remains unchanged. For example, you will get the
same significance using x as using log x.

The function that enters into the calculation of the significance can be written
as the following sum:

Qxs() =2 0-11 (14.3.7)
j=l

which is a monotonic function with the limiting values
Qx«s(0)=1 Qxks(o) =0 (14.3.8)

In terms of this function, the significance level of an observed value of D (as
a disproof of the null hypothesis that the distributions are the same) is given
approximately [1] by the formula

Probability (D > observed ) = Qs ( [VN +012+0.11 /VN.| D)
(14.3.9)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.3 Are Two Distributions Different? 625

where N, is the effective number of data points, N. = N for the case (14.3.5)
of one distribution, and

N\N2
Ne NE Ns (14.3.10)
for the case (14.3.6) of two distributions, where Nj, is the number of data points in
the first distribution, No the number in the second.

The nature of the approximation involved in (14.3.9) is that it becomes asymp-
totically accurate as the NV. becomes large, but is already quite good for NV. > 4, as
small a number as one might ever actually use. (See [1].)

So, we have the following routines for the cases of one and two distributions:

#include <math.h>
#include "nrutil.h"

void ksone(float data[], unsigned long n, float (*func) (float), float *d,
float *prob)

Given an array data[1..n], and given a user-supplied function of a single variable func which
is a cumulative distribution function ranging from 0 (for smallest values of its argument) to 1
(for largest values of its argument), this routine returns the K-S statistic d, and the significance
level prob. Small values of prob show that the cumulative distribution function of data is
significantly different from func. The array data is modified by being sorted into ascending
order.

{
float probks(float alam);
void sort(unsigned long n, float arr[]);
unsigned long j;
float dt,en,ff,fn,fo=0.0;
sort (n,data) ; If the data are already sorted into as-
en=n; cending order, then this call can be
¥*d=0.0; omitted.
for ( sjsen; j++) { Loop over the sorted data points.
fn=j/en; Data's c.d.f. after this step.
ff=(*func) (data[j]); Compare to the user-supplied function.
dt=FMAX (fabs (fo-ff) , fabs (fn-ff)) ; Maximum distance.
if (dt > *d) *d=dt;
fo=fn;
+
en=sqrt (en) ;
*prob=probks ((en+0.12+0.11/en)*(*d)) ; Compute significance.
3

#include <math.h>

void kstwo(float datai[], unsigned long ni, float data2[], unsigned long n2,

float *d, float *prob)
Given an array datai[1..ni], and an array data2[1..n2], this routine returns the K-
S statistic d, and the significance level prob for the null hypothesis that the data sets are
drawn from the same distribution. Small values of prob show that the cumulative distribution
function of datat is significantly different from that of data2. The arrays datal and data2
are modified by being sorted into ascending order.
{

float probks(float alam);

void sort(unsigned long n, float arr[]);

unsigned long ji=1,j2=1;

float di,d2,dt,en1,en2,en,fni=0.0,fn2=0.0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

626 Chapter 14. Statistical Description of Data

sort(ni,datat) ;
sort (n2,data2) ;

eni=n1;

en2=n2;

*d:

while (ji <= nl && j2 <= n2) { If we are not done...
if ((disdatai[ji]) <= (d2=data2[j2])) fni=ji++/en1; Next step is in data1.
if (d2 <= d1) fn2=j2++/en2; Next step is in data2.
if ((dt=fabs(fn2-fn1)) > *d) *d=dt;

+

en=sqrt (en1*en2/(eni+en2));

*prob=probks ((en+0.12+0.11/en) *(*d)) ; Compute significance.

Both of the above routines use the following routine for calculating the function

Qxs:

#include <math.h>
#define EPS1 0.001
#define EPS2 1.0e-8

float probks(float alam)
Kolmogorov-Smirnov probability function.

{
int j;
float a2,fac=2.0,sum=0.0,term,termbf=0.0;
a2 = -2.0*alam*alam;
for ( <=100;j++) {
term=fac*exp(a2*j*j);
sum += term;
if (fabs(term) <= EPSi*termbf || fabs(term) <= EPS2*sum) return sum;
fac = -fac; Alternating signs in sum.
termbf=fabs (term) ;
+
return 1.0; Get here only by failing to converge.
3

Variants on the K-S Test

The sensitivity of the K—S test to deviations from a cumulative distribution function
P(z) is not independent of x. In fact, the K-S test tends to be most sensitive around the
median value, where P(x) = 0.5, and less sensitive at the extreme ends of the distribution,
where P(r) is near 0 or 1. The reason is that the difference |Siy (2) — P(x)| does not, in the
null hypothesis, have a probability distribution that is independent of x. Rather, its variance is
proportional to P(«)[{1 — P(«)], which is largest at P = 0.5. Since the K-S statistic (14.3.5)
is the maximum difference over all x of two cumulative distribution functions, a deviation that
might be statistically significant at its own value of x gets compared to the expected chance
deviation at P = 0.5, and is thus discounted. A result is that, while the K-S test is good at
finding shifts in a probability distribution, especially changes in the median value, it is not
always so good at finding spreads, which more affect the tails of the probability distribution,
and which may leave the median unchanged.

One way of increasing the power of the K-S statistic out on the tails is to replace
D (equation 14.3.5) by a so-called stabilized or weighted statistic [2-4], for example the
Anderson-Darling statistic,

pt= max x)= P@)| (14.3.1)
-oc<a<ce \/P(x)[l — P(2)]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.3 Are Two Distributions Different? 627

Unfortunately, there is no simple formula analogous to equations (14.3.7) and (14.3.9) for this
statistic, although Noé [5] gives a computational method using a recursion relation and provides
a graph of numerical results. There are many other possible similar statistics, for example

sek ' [Sw (x) — P(a)? x
D** = I. P(a)il = Pla)] dP(x) (14.3.12)

which is also discussed by Anderson and Darling (see [3]).

Another approach, which we prefer as simpler and more direct, is due to Kuiper [6,7].
We already mentioned that the standard K-S test is invariant under reparametrizations of the
variable a. An even more general symmetry, which guarantees equal sensitivities at all values
of x, is to wrap the « axis around into a circle (identifying the points at +00), and to look for
a statistic that is now invariant under all shifts and parametrizations on the circle. This allows,
for example, a probability distribution to be “cut” at some central value of x, and the left and
right halves to be interchanged, without altering the statistic or its significance.

Kuiper’s statistic, defined as

V=D,+D-= max [Sn(x)—P(x)]}+ max [P(x)—Swn(x)] (143.13)
<x<oo —co<@<oo

oo

is the sum of the maximum distance of Sy (x) above and below P(x). You should be able
to convince yourself that this statistic has the desired invariance on the circle: Sketch the
indefinite integral of two probability distributions defined on the circle as a function of angle
around the circle, as the angle goes through several times 360°. If you change the starting
point of the integration, D, and D_ change individually, but their sum is constant.

Furthermore, there is a simple formula for the asymptotic distribution of the statistic V,
directly analogous to equations (14.3.7)-(14.3.10). Let

Qed) = 25 (42x? = 102 (143.14)

j=l
which is monotonic and satisfies
Qxp(0)=1 Qxp(cc) =0 (14.3.15)

In terms of this function the significance level is [1]
Probability (V > observed ) = Qxp ( [VNe + 0.155 + 0.24/V/ N.| v) (14.3.16)

Here N. is N in the one-sample case, or is given by equation (14.3.10) in the case of
two samples.

Of course, Kuiper’s test is ideal for any problem originally defined on a circle, for
example, to test whether the distribution in longitude of something agrees with some theory,
or whether two somethings have different distributions in longitude. (See also [8].)

We will leave to you the coding of routines analogous to ksone, kstwo, and probks,
above. (For \ < 0.4, don’t try to do the sum 14.3.14. Its value is 1, to 7 figures, but the series
can require many terms to converge, and loses accuracy to roundoff.)

Two final cautionary notes: First, we should mention that all varieties of K—S test lack
the ability to discriminate some kinds of distributions. A simple example is a probability
distribution with a narrow “notch” within which the probability falls to zero. Such a
distribution is of course ruled out by the existence of even one data point within the notch,
but, because of its cumulative nature, a K-S test would require many data points in the notch
before signaling a discrepancy.

Second, we should note that, if you estimate any parameters from a data set (e.g.,a mean
and variance), then the distribution of the K—S statistic D for a cumulative distribution function
P(z) that uses the estimated parameters is no longer given by equation (14.3.9). In general,
you will have to determine the new distribution yourself, e.g., by Monte Carlo methods.

CITED REFERENCES AND FURTHER READING:

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), Chapters IX(C) and IX(E).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

628 Chapter 14. Statistical Description of Data

Stephens, M.A. 1970, Journal of the Royal Statistical Society, ser. B, vol. 32, pp. 115-122. [1]

Anderson, T.W., and Darling, D.A. 1952, Annals of Mathematical Statistics, vol. 23, pp. 193-212.
[2]

Darling, D.A. 1957, Annals of Mathematical Statistics, vol. 28, pp. 823-838. [3]

Michael, J.R. 1983, Biometrika, vol. 70, no. 1, pp. 11-17. [4]

Noé, M. 1972, Annals of Mathematical Statistics, vol. 43, pp. 58-64. [5]

Kuiper, N.H. 1962, Proceedings of the Koninklijke Nederlandse Akademie van Wetenschappen,
ser. A., vol. 63, pp. 38-47. [6]

Stephens, M.A. 1965, Biometrika, vol. 52, pp. 309-321. [7]

Fisher, N.I., Lewis, T., and Embleton, B.J.J. 1987, Statistical Analysis of Spherical Data (New
York: Cambridge University Press). [8]

14.4 Contingency Table Analysis of Two
Distributions

In this section, and the next two sections, we deal with measures of association
for two distributions. The situation is this: Each data point has two or more different
quantities associated with it, and we want to know whether knowledge of one quantity
gives us any demonstrable advantage in predicting the value of another quantity. In
many cases, one variable will be an “independent” or “control” variable, and another
will be a “dependent” or “measured” variable. Then, we want to know if the latter
variable is in fact dependent on or associated with the former variable. If it is, we
want to have some quantitative measure of the strength of the association. One often
hears this loosely stated as the question of whether two variables are correlated or
uncorrelated, but we will reserve those terms for a particular kind of association
(linear, or at least monotonic), as discussed in $14.5 and §14.6.

Notice that, as in previous sections, the different concepts of significance and
strength appear: The association between two distributions may be very significant
even if that association is weak — if the quantity of data is large enough.

It is useful to distinguish among some different kinds of variables, with different
categories forming a loose hierarchy.

e A variable is called nominal if its values are the members of some unordered

set. For example, “state of residence” is a nominal variable that (in the
U.S.) takes on one of 50 values; in astrophysics, “type of galaxy” is a
nominal variable with the three values “spiral,” “elliptical,” and “irregular.”

e A variable is termed ordinal if its values are the members of a discrete, but
ordered, set. Examples are: grade in school, planetary order from the Sun
(Mercury = 1, Venus = 2, ...), number of offspring. There need not be
any concept of “equal metric distance” between the values of an ordinal
variable, only that they be intrinsically ordered.

e We will call a variable continuous if its values are real numbers, as
are times, distances, temperatures, etc. (Social scientists sometimes
distinguish between interval and ratio continuous variables, but we do not
find that distinction very compelling.)

A continuous variable can always be made into an ordinal one by binning it

into ranges. If we choose to ignore the ordering of the bins, then we can turn it into

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.4 Contingency Table Analysis of Two Distributions 629

| # of # of

1. male red males green males
Niu Ni
# of # of

2. female red females green females

Noi Noo

# of red # of green total #
Ny Ny N

Figure 14.4.1. Example of a contingency table for two nominal variables, here sex and color. The row
and column marginals (totals) are shown. The variables are “nominal,” i.e., the order in which their values
are listed is arbitrary and does not affect the result of the contingency table analysis. If the ordering
of values has some intrinsic meaning, then the variables are “ordinal” or “continuous,” and correlation
techniques (§14.5-§14.6) can be utilized.

a nominal variable. Nominal variables constitute the lowest type of the hierarchy,
and therefore the most general. For example, a set of several continuous or ordinal
variables can be turned, if crudely, into a single nominal variable, by coarsely
binning each variable and then taking each distinct combination of bin assignments
as a single nominal value. When multidimensional data are sparse, this is often
the only sensible way to proceed.

The remainder of this section will deal with measures of association between
nominal variables. For any pair of nominal variables, the data can be displayed as
a contingency table, a table whose rows are labeled by the values of one nominal
variable, whose columns are labeled by the values of the other nominal variable,
and whose entries are nonnegative integers giving the number of observed events
for each combination of row and column (see Figure 14.4.1). The analysis of
association between nominal variables is thus called contingency table analysis or
crosstabulation analysis.

We will introduce two different approaches. The first approach, based on the
chi-square statistic, does a good job of characterizing the significance of association,
but is only so-so as a measure of the strength (principally because its numerical
values have no very direct interpretations). The second approach, based on the
information-theoretic concept of entropy, says nothing at all about the significance of
association (use chi-square for that!), but is capable of very elegantly characterizing
the strength of an association already known to be significant.

Measures of Association Based on Chi-Square

Some notation first: Let Nj; denote the number of events that occur with the

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

630 Chapter 14. Statistical Description of Data

first variable x taking on its ith value, and the second variable y taking on its jth
value. Let N denote the total number of events, the sum of all the Nj;’s. Let Nj.
denote the number of events for which the first variable x takes on its ith value
regardless of the value of y; N.; is the number of events with the jth value of y
regardless of x. So we have

Ni. =SONy NG =SONG
j i
N=SON.=30N;
i j

(14.4.1)

N.; and Nj. are sometimes called the row and column totals or marginals, but we
will use these terms cautiously since we can never keep straight which are the rows
and which are the columns!

The null hypothesis is that the two variables x and y have no association. In this
case, the probability of a particular value of x given a particular value of y should
be the same as the probability of that value of x regardless of y. Therefore, in the
null hypothesis, the expected number for any N;;, which we will denote n;;, can be
calculated from only the row and column totals,

iy Ni. La: Ni.N.;
x = which implies nig = Tr

(14.4.2)

Notice that if a column or row total is zero, then the expected number for all the
entries in that column or row is also zero; in that case, the never-occurring bin of x
or y should simply be removed from the analysis.

The chi-square statistic is now given by equation (14.3.1), which, in the present
case, is summed over all entries in the table,

ay Mua mii)” (14.4.3)

Nig
ing J

The number of degrees of freedom is equal to the number of entries in the table
(product of its row size and column size) minus the number of constraints that have
arisen from our use of the data themselves to determine the n ;;. Each row total and
column total is a constraint, except that this overcounts by one, since the total of the
column totals and the total of the row totals both equal N, the total number of data
points. Therefore, if the table is of size J by J, the number of degrees of freedom is
IJ —I— J+ 1. Equation (14.4.3), along with the chi-square probability function
(86.2), now give the significance of an association between the variables x and y.

Suppose there is a significant association. How do we quantify its strength, so
that (e.g.) we can compare the strength of one association with another? The idea
here is to find some reparametrization of y? which maps it into some convenient
interval, like 0 to 1, where the result is not dependent on the quantity of data that we
happen to sample, but rather depends only on the underlying population from which
the data were drawn. There are several different ways of doing this. Two of the more
common are called Cramer’s V and the contingency coefficient C.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.4 Contingency Table Analysis of Two Distributions 631

The formula for Cramer’s V is

x?

N min(J — 1, J —1)

(14.44)

where I and J are again the numbers of rows and columns, and N is the total
number of events. Cramer’s V has the pleasant property that it lies between zero
and one inclusive, equals zero when there is no association, and equals one only
when the association is perfect: All the events in any row lie in one unique column,
and vice versa. (In chess parlance, no two rooks, placed on a nonzero table entry,
can capture each other.)

In the case of J = J = 2, Cramer’s V is also referred to as the phi statistic.

The contingency coefficient C' is defined as

x?

C= Ve+N

(14.4.5)

It also lies between zero and one, but (as is apparent from the formula) it can never
achieve the upper limit. While it can be used to compare the strength of association
of two tables with the same J and J, its upper limit depends on J and J. Therefore
it can never be used to compare tables of different sizes.

The trouble with both Cramer’s V and the contingency coefficient C'is that, when
they take on values in between their extremes, there is no very direct interpretation
of what that value means. For example, you are in Las Vegas, and a friend tells you
that there is a small, but significant, association between the color of a croupier’s
eyes and the occurrence of red and black on his roulette wheel. Cramer’s V is about
0.028, your friend tells you. You know what the usual odds against you are (because
of the green zero and double zero on the wheel). Is this association sufficient for
you to make money? Don’t ask us!

#include <math.h>
#include "nrutil.h"
#define TINY 1.0e-30 A small number.

void cntabl(int **nn, int ni, int nj, float *chisq, float *df, float *prob,
float *cramrv, float *ccc)
Given a two-dimensional contingency table in the form of an integer array nn[1..ni] [1..nj],
this routine returns the chi-square chisq, the number of degrees of freedom df, the significance
level prob (small values indicating a significant association), and two measures of association,
Cramer's V (cramrv) and the contingency coefficient C (ccc).
{
float gammq(float a, float x);
int nnj,nni,j,i,minij;
float sum=0.0,expctd,*sumi,*sumj,temp;

sumi=vector(1,ni);
sumj=vector(1,nj);

nni=ni; Number of rows
nnj=nj; and columns.
for (i= Get the row totals.
sumi [i
for (j=1;j<=

sumi[i] += nn[il] [j];
sum += nn[i] [j];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

632 Chapter 14. Statistical Description of Data

}

aif (sumil[i] == 0.0) --nni; Eliminate any zero rows by reducing the num-
} ber.
for (j=1;j<=njij++) { Get the column totals.

sumj[j]=0.0;

for (i ij;it+) sumj[j] += nn([il] [jl];

if (sumj[j] == 0.0) --nnj; Eliminate any zero columns.
+
*df=nni*nnj-nni-nnj+1; Corrected number of degrees of freedom.
pissni;it+) { Do the chi-square sum.
for (j=1;j<=nj;jt+) {
expctd=sumj[j]*sumi [i] /sum;
temp=nn [i] [j]-expctd;
*chisq += temp*temp/(expctd+TINY) ; Here TINY guarantees that any
} eliminated row or column will
+ not contribute to the sum.
*prob=gammq(0.5*(*df) ,0.5*(*chisq)) ; Chi-square probability function.
minij = nni < nnj ? nni-1 : nnj-1;
*cramrv=sqrt (*chisq/(sum*minij));
*ccc=sqrt (*chisq/(*chisq+sum) ) ;
free_vector(sumj,1,nj);
free_vector(sumi,1,ni);

Measures of Association Based on Entropy

Consider the game of “twenty questions,” where by repeated yes/no questions
you try to eliminate all except one correct possibility for an unknown object. Better
yet, consider a generalization of the game, where you are allowed to ask multiple
choice questions as well as binary (yes/no) ones. The categories in your multiple
choice questions are supposed to be mutually exclusive and exhaustive (as are “yes”
and “no”).

The value to you of an answer increases with the number of possibilities that
it eliminates. More specifically, an answer that eliminates all except a fraction p of
the remaining possibilities can be assigned a value — In p (a positive number, since
p <1). The purpose of the logarithm is to make the value additive, since (e.g.) one
question that eliminates all but 1/6 of the possibilities is considered as good as two
questions that, in sequence, reduce the number by factors 1/2 and 1/3.

So that is the value of an answer; but what is the value of a question? If there
are I possible answers to the question (i = 1,..., 7) and the fraction of possibilities
consistent with the ith answer is p; (with the sum of the p;’s equal to one), then the
value of the question is the expectation value of the value of the answer, denoted H,

I
H=- J pilnpi (14.4.6)
i=1

In evaluating (14.4.6), note that

lim plnp =0 (14.4.7)
p—0

The value H lies between 0 and In J. It is zero only when one of the p;’s is one, all
the others zero: In this case, the question is valueless, since its answer is preordained.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.4 Contingency Table Analysis of Two Distributions 633

Hf takes on its maximum value when all the p,;’s are equal, in which case the question
is sure to eliminate all but a fraction 1/I of the remaining possibilities.

The value H is conventionally termed the entropy of the distribution given by
the p;’s, a terminology borrowed from statistical physics.

So far we have said nothing about the association of two variables; but suppose
we are deciding what question to ask next in the game and have to choose between
two candidates, or possibly want to ask both in one order or another. Suppose that
one question, z, has I possible answers, labeled by 7, and that the other question,
y, as J possible answers, labeled by j. Then the possible outcomes of asking both
questions form a contingency table whose entries N;,;, when normalized by dividing
by the total number of remaining possibilities NV, give all the information about the
p’s. In particular, we can make contact with the notation (14.4.1) by identifying

Nij
Pij = NV

Ni. .
Pc = > (outcomes of question x alone) (14.4.8)
pj = y (outcomes of question y alone)

The entropies of the questions x and y are, respectively,

A(x) =—- Spi. In pj. A(y) =—- Yop; Inp.; (14.4.9)
i j

The entropy of the two questions together is

H(x,y) = =~ pis Inpiy (14.4.10)
id

Now what is the entropy of the question y given x (that is, if x is asked first)?
It is the expectation value over the answers to x of the entropy of the restricted
y distribution that lies in a single column of the contingency table (corresponding
to the x answer):

Pig) Pig
A(y|x Pi- In = 144.11
(os) = — Sm. Fn (144.11)
Correspondingly, the entropy of x given y is
(14.4.12)

A (aly) =— Sop; eS

Fi Py ij

We can readily prove that the entropy of y given x is never more than the
entropy of y alone, i.e., that asking x first can only reduce the usefulness of asking

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

634 Chapter 14. Statistical Description of Data

y (in which case the two variables are associated!):

pig [Dic
A(y|x) — H(y) = = pj n Pale
inj J

py in
Av

Pig
< Spi (Le: _ 1) (14.4.13)
‘i Pig
= Lops
ig i,j
=1-1=0

where the inequality follows from the fact
Inw<w-1 (14.4.14)

We now have everything we need to define a measure of the “dependency” of y
on x, that is to say a measure of association. This measure is sometimes called the
uncertainty coefficient of y. We will denote it as U(y|zx),

— Aly) = Ale)

Vile) =~" FG)
This measure lies between zero and one, with the value 0 indicating that x and y
have no association, the value 1 indicating that knowledge of x completely predicts
y. For in-between values, U(y|2) gives the fraction of y’s entropy H(y) that is
lost if x is already known (i.e., that is redundant with the information in x). In our
game of “twenty questions,” U(y|x) is the fractional loss in the utility of question
y if question x is to be asked first.

If we wish to view x as the dependent variable, y as the independent one, then
interchanging x and y we can of course define the dependency of x on y,

H(z) — H(aly)

If we want to treat x and y symmetrically, then the useful combination turns
out to be

(14.4.15)

A(y) + H(«) — H(«,y)
Hx) + Ay)

If the two variables are completely independent, then H(x,y) = H(x) + H(y), so
(14.4.17) vanishes. If the two variables are completely dependent, then H(x) =
H(y) = H(x,y), so (14.4.16) equals unity. In fact, you can use the identities (easily
proved from equations 14.4.9-14.4.12)

(x,y) = H(z) + H(y|x) = H(y) + H(cly) (14.4.18)
to show that

U(x, y) =2

(14.4.17)

H(x)U (ely) + H(y)U(ylz)
A(x) + H(y)
ie., that the symmetrical measure is just a weighted average of the two asymmetrical
measures (14.4.15) and (14.4.16), weighted by the entropy of each variable separately.
Here is a program for computing all the quantities discussed, H(x), H(y),
H (aly), H(ylx), H(w,y), U(aly), U(yle), and U(w, y):

U(a,y) (14.4.19)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

14.4 Contingency Table Analysis of Two Distributions 635

#include <math.h>
#include "nrutil.h"
#define TINY 1.0e-30 A small number.

void cntab2(int **nn, int ni, int nj, float *h, float *hx, float *hy,

float *hygx, float *hxgy, float *uygx, float *uxgy, float *uxy)
Given a two-dimensional contingency table in the form of an integer array nn[i] [j], where i
labels the x variable and ranges from 1 to ni, j labels the y variable and ranges from 1 to nj,
this routine returns the entropy h of the whole table, the entropy hx of the « distribution, the
entropy hy of the y distribution, the entropy hygx of y given x, the entropy hxgy of « given y,
the dependency uygx of y on a (eq. 14.4.15), the dependency uxgy of on y (eq. 14.4.16),
and the symmetrical dependency uxy (eq. 14.4.17).
{

int i,j;

float sum=0.0,p,*sumi,*sumj;

sumi=vector(1,ni);
sumj=vector(1,nj);
for (i=1;i<=ni;it+) { Get the row totals.
sumi [i. .03
for (j=1;j<=nj;jt+) {
sumi[i] += nn[i] (jl;
sum += nn[i] [j];

+
+
for (j=1;j<-njijt++) { Get the column totals.
sumj[j]=0.0;
for (i=1;i<=ni;i++)
sumj(j] += nn[i] (j];

Entropy of the a distribution,

3 i; i++)

if (sumi[i]) {
p=sumi [i] /sum;
*hx -= p*log(p);

}
*hy=0.0; and of the y distribution.
for (j=1;j<=nj;j++)

if (sumj[j]) {

p=sumj[j]/sum;
*hy -= p:

}
*h=0.0;
for (i=1;i<=ni;i++) Total entropy: loop over both «

for (j=1;j<=nj;jt+) and y.

if (mn(i][j]) {
p=nn [i] [j]/sum;
*h -= p*log(p);

3
*hygx=(*h)-(*hx) ; Uses equation (14.4.18),
*hxgy= (+h) - (#hy) ; as does this.
*uygx= (*hy-*hygx) /(+hy+TINY) ; Equation (14.4.15).
xuxgy= (+hx-*hxgy) / (*hx+TINY) ; Equation (14.4.16).
*uxy=2.0* (xhxt+*hy-*h) / (*hx+*hy+TINY) ; Equation (14.4.17).
free_vector(sumj,1,nj);
free_vector(sumi,1,ni);

CITED REFERENCES AND FURTHER READING:

Dunn, O.J., and Clark, V.A. 1974, Applied Statistics: Analysis of Variance and Regression (New
York: Wiley).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

636 Chapter 14. Statistical Description of Data

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

Fano, R.M. 1961, Transmission of Information (New York: Wiley and MIT Press), Chapter 2.

14.5 Linear Correlation

We next turn to measures of association between variables that are ordinal
or continuous, rather than nominal. Most widely used is the linear correlation
coefficient. For pairs of quantities (x;,y;:), 7 = 1,...,N, the linear correlation
coefficient r (also called the product-moment correlation coefficient, or Pearson’s
r) is given by the formula

U(ei — Z)(yi — 9)

(14.5.1)

where, as usual, © is the mean of the x;’s, y is the mean of the y;’s.

The value of r lies between —1 and 1, inclusive. It takes on a value of 1, termed
“complete positive correlation,” when the data points lie on a perfect straight line
with positive slope, with x and y increasing together. The value 1 holds independent
of the magnitude of the slope. If the data points lie on a perfect straight line with
negative slope, y decreasing as x increases, then r has the value —1; this is called
“complete negative correlation.” A value of r near zero indicates that the variables
x and y are uncorrelated.

When a correlation is known to be significant, r is one conventional way of
summarizing its strength. In fact, the value of r can be translated into a statement
about what residuals (root mean square deviations) are to be expected if the data are
fitted to a straight line by the least-squares method (see §15.2, especially equations
15.2.13 — 15.2.14). Unfortunately, r is a rather poor statistic for deciding whether
an observed correlation is statistically significant, and/or whether one observed
correlation is significantly stronger than another. The reason is that r is ignorant of
the individual distributions of x and y, so there is no universal way to compute its
distribution in the case of the null hypothesis.

About the only general statement that can be made is this: If the null hypothesis
is that x and y are uncorrelated, and if the distributions for x and y each have
enough convergent moments (“‘tails” die off sufficiently rapidly), and if N is large
(typically > 500), then r is distributed approximately normally, with a mean of zero
and a standard deviation of 1/ VN. In that case, the (double-sided) significance of
the correlation, that is, the probability that |r| should be larger than its observed
value in the null hypothesis, is

Ir VN
erfe ( a ) (14.5.2)

where erfc(x) is the complementary error function, equation (6.2.8), computed by
the routines erffc or erfcc of 86.2. A small value of (14.5.2) indicates that the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.5 Linear Correlation 637

two distributions are significantly correlated. (See expression 14.5.9 below for a
more accurate test.)

Most statistics books try to go beyond (14.5.2) and give additional statistical
tests that can be made using r. In almost all cases, however, these tests are valid
only for a very special class of hypotheses, namely that the distributions of x and y
jointly form a binormal or two-dimensional Gaussian distribution around their mean
values, with joint probability density

1 F
p(x, y) dxdy = const. x exp 5 (ane? — Qaioxry + a22y”) dxdy (14.5.3)

where a11, @12, and a2 are arbitrary constants. For this distribution r has the value

a2
r mis (14.5.4)
There are occasions when (14.5.3) may be known to be a good model of the
data. There may be other occasions when we are willing to take (14.5.3) as at least
a rough and ready guess, since many two-dimensional distributions do resemble a
binormal distribution, at least not too far out on their tails. In either situation, we can
use (14.5.3) to go beyond (14.5.2) in any of several directions:
First, we can allow for the possibility that the number NV of data points is not
large. Here, it turns out that the statistic

N-2

t=r
1l-r?

(14.5.5)

is distributed in the null case (of no correlation) like Student’s ¢-distribution with
v = N — 2 degrees of freedom, whose two-sided significance level is given by
1 — A(t|v) (equation 6.4.7). As N becomes large, this significance and (14.5.2)
become asymptotically the same, so that one never does worse by using (14.5.5),
even if the binormal assumption is not well substantiated.

Second, when N is only moderately large (> 10), we can compare whether
the difference of two significantly nonzero r’s, e.g., from different experiments, is
itself significant. In other words, we can quantify whether a change in some control
variable significantly alters an existing correlation between two other variables. This
is done by using Fisher’s z-transformation to associate each measured r with a

corresponding z,
1 l+r
z=-l o.
z sm(7*) (14.5.6)

Then, each z is approximately normally distributed with a mean value

= 1 1+ true Ttrue
=={1 + 14.5.7
* 5 (in ( N-1 (14.5.7)
where rtrue is the actual or population value of the correlation coefficient, and with
a standard deviation

o(z) > —— (14.5.8)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

638 Chapter 14. Statistical Description of Data

Equations (14.5.7) and (14.5.8), when they are valid, give several useful
statistical tests. For example, the significance level at which a measured value of r
differs from some hypothesized value rire is given by

nfo (s=3 Ve=3)

v2

where z and Z are given by (14.5.6) and (14.5.7), with small values of (14.5.9)
indicating a significant difference. (Setting Z = 0 makes expression 14.5.9 a more
accurate replacement for expression 14.5.2 above.) Similarly, the significance of a
difference between two measured correlation coefficients r; and rz is

(14.5.9)

|z1 — 22

(14.5.10)

where z; and zz are obtained from r; and rz using (14.5.6), and where N; and N»2
are, respectively, the number of data points in the measurement of r; and ro.

All of the significances above are two-sided. If you wish to disprove the null
hypothesis in favor of a one-sided hypothesis, such as that r1 > r2 (where the sense
of the inequality was decided a priori), then (i) if your measured r; and rz have
the wrong sense, you have failed to demonstrate your one-sided hypothesis, but (ii)
if they have the right ordering, you can multiply the significances given above by
0.5, which makes them more significant.

But keep in mind: These interpretations of the r statistic can be completely
meaningless if the joint probability distribution of your variables x and y is too
different from a binormal distribution.

#include <math.h>
#define TINY 1.0e-20 Will regularize the unusual case of complete correlation.

void pearsn(float x[], float y[], unsigned long n, float *r, float *prob,

float *z)
Given two arrays x[1..n] and y[1..n], this routine computes their correlation coefficient
r (returned as r), the significance level at which the null hypothesis of zero correlation is
disproved (prob whose small value indicates a significant correlation), and Fisher's z (returned
as z), whose value can be used in further statistical tests as described above.
{

float betai(float a, float b, float x);

float erfcc(float x);

unsigned long j;

float yt,xt,t,df;

float syy=0.0,sxy=0.0,sxx=0.0,ay=0.0,ax=0.0;

for (j=1;j<=n;j++) { Find the means.
ax x(jl;
ay += yljl;
+
ax /= n;
sjsen;j++) { Compute the correlation coefficient.
xt=x[j]-ax;
yt=ylj]-ay;

sxx += xt*xt;
syy += yt*yt;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.6 Nonparametric or Rank Correlation 639

sxy += xt*yt;

+

xr=sxy/ (sqrt (sxx*syy)+TINY) ;

*2=0.5*log((1.0+(#r)+TINY) /(1.0-(#r)+TINY)) ; Fisher's z transformation.
df=n-2;

t=(ar) #sqrt (df/((1.0-(#r)+TINY) *(1.0+(#r)+TINY))) 5 Equation (14.5.5).
*prob=betai(0.5*df,0.5,df£/(df+t*t)); Student's t probability.

/* *prob=erfcc (fabs ((*z)*sqrt (n-1.0))/1.4142136) */
For large n, this easier computation of prob, using the short routine erfcc, would give approx-
imately the same value.

}

CITED REFERENCES AND FURTHER READING:

Dunn, O.J., and Clark, V.A. 1974, Applied Statistics: Analysis of Variance and Regression (New
York: Wiley).

Hoel, P.G. 1971, Introduction to Mathematical Statistics, 4th ed. (New York: Wiley), Chapter 7.

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), Chapters IX(A) and IX(B).

Korn, G.A., and Korn, T.M. 1968, Mathematical Handbook for Scientists and Engineers, 2nd ed.
(New York: McGraw-Hill), §19.7.

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

14.6 Nonparametric or Rank Correlation

It is precisely the uncertainty in interpreting the significance of the linear
correlation coefficient r that leads us to the important concepts of nonparametric or
rank correlation. As before, we are given N pairs of measurements (x ;, y;). Before,
difficulties arose because we did not necessarily know the probability distribution
function from which the x;’s or y;’s were drawn.

The key concept of nonparametric correlation is this: If we replace the value
of each x; by the value of its rank among all the other z;’s in the sample, that
is, 1,2,3,...,.N, then the resulting list of numbers will be drawn from a perfectly
known distribution function, namely uniformly from the integers between 1 and NV,
inclusive. Better than uniformly, in fact, since if the x;’s are all distinct, then each
integer will occur precisely once. If some of the x;’s have identical values, it is
conventional to assign to all these “ties” the mean of the ranks that they would have
had if their values had been slightly different. This midrank will sometimes be an
integer, sometimes a half-integer. In all cases the sum of all assigned ranks will be
the same as the sum of the integers from 1 to NV, namely 3N(N +1).

Of course we do exactly the same procedure for the y;’s, replacing each value
by its rank among the other y;’s in the sample.

Now we are free to invent statistics for detecting correlation between uniform
sets of integers between 1 and N, keeping in mind the possibility of ties in the ranks.
There is, of course, some loss of information in replacing the original numbers by
ranks. We could construct some rather artificial examples where a correlation could
be detected parametrically (e.g., in the linear correlation coefficient 7), but could not

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

640 Chapter 14. Statistical Description of Data

be detected nonparametrically. Such examples are very rare in real life, however,
and the slight loss of information in ranking is a small price to pay for a very major
advantage: When a correlation is demonstrated to be present nonparametrically,
then it is really there! (That is, to a certainty level that depends on the significance
chosen.) Nonparametric correlation is more robust than linear correlation, more
resistant to unplanned defects in the data, in the same sort of sense that the median
is more robust than the mean. For more on the concept of robustness, see §15.7.

As always in statistics, some particular choices of a statistic have already been
invented for us and consecrated, if not beatified, by popular use. We will discuss
two, the Spearman rank-order correlation coefficient (r ,), and Kendall’s tau (7).

Spearman Rank-Order Correlation Coefficient

Let R; be the rank of x; among the other x’s, S; be the rank of y; among the
other y’s, ties being assigned the appropriate midrank as described above. Then the
rank-order correlation coefficient is defined to be the linear correlation coefficient
of the ranks, namely,

. E.(Ri ~ R)(Si ~ 3)
(a(R — BP VSS: — 3

The significance of a nonzero value of r, is tested by computing

(14.6.1)

(14.6.2)

which is distributed approximately as Student’s distribution with N — 2 degrees of
freedom. A key point is that this approximation does not depend on the original
distribution of the x’s and y’s; it is always the same approximation, and always
pretty good.

It turns out that r, is closely related to another conventional measure of
nonparametric correlation, the so-called sum squared difference of ranks, defined as

N
D= (Ri - Si)? (14.6.3)
i=1

(This D is sometimes denoted D**, where the asterisks are used to indicate that
ties are treated by midranking.)

When there are no ties in the data, then the exact relation between D and r, is
6D
N3—-N
When there are ties, then the exact relation is slightly more complicated: Let f, be
the number of ties in the /th group of ties among the R;’s, and let g;, be the number
of ties in the mth group of ties among the S;’s. Then it turns out that

6 ;
Le sy [D + BEA fe) + BL (2 - 9m)]

f _ Eee sa] " : — Lnlgis = a) e

(14.6.4)

rg =1-

(14.6.5)

Ts

N?—N N°-—N

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.6 Nonparametric or Rank Correlation 641

holds exactly. Notice that if all the f;,’s and all the g,,,’s are equal to one, meaning
that there are no ties, then equation (14.6.5) reduces to equation (14.6.4).

In (14.6.2) we gave a t-statistic that tests the significance of a nonzero r ,. It is
also possible to test the significance of D directly. The expectation value of D in
the null hypothesis of uncorrelated data sets is

1 1

D= 59 —N) 5 CUB - fe) - 75 UG — 9) (14.6.6)

k m
its variance is
(N —1)N?(N +1)?
36
ve fy Safi] fy Enl08, = gm)
N?-N N3-—N

Var(D) =
(14.6.7)

and it is approximately normally distributed, so that the significance level is a
complementary error function (cf. equation 14.5.2). Of course, (14.6.2) and (14.6.7)
are not independent tests, but simply variants of the same test. In the program that
follows, we calculate both the significance level obtained by using (14.6.2) and the
significance level obtained by using (14.6.7); their discrepancy will give you an idea
of how good the approximations are. You will also notice that we break off the task
of assigning ranks (including tied midranks) into a separate function, crank.

#include <math.h>
#include "nrutil.h"

void spear(float datai[], float data2[], unsigned long n, float *d, float *zd,

float *probd, float *rs, float *probrs)
Given two data arrays, datai[1..n] and data2[1..n], this routine returns their sum-squared
difference of ranks as D, the number of standard deviations by which D deviates from its null-
hypothesis expected value as zd, the two-sided significance level of this deviation as probd,
Spearman's rank correlation rs as rs, and the two-sided significance level of its deviation from
zero as probrs. The external routines crank (below) and sort2 (§8.2) are used. A small value
of either probd or probrs indicates a significant correlation (rs positive) or anticorrelation
(xs _ negative).
t

float betai(float a, float b, float x);

void crank(unsigned long n, float w[], float *s);

float erfcc(float x);

void sort2(unsigned long n, float arr[], float brr[]);

unsigned long j;

float vard,t,sg,sf,fac,en3n,en,df,aved,*wksp1,*wksp2;

wkspi=vector(1,n);

wksp2=vector(1,n);

for (j=1;j<=n;j++) {
wksp1[j]=datai[j];
wksp2[j]=data2[j];

+

sort2(n,wksp1,wksp2) ; Sort each of the data arrays, and convert the entries to
crank(n,wksp1,&sf) ; ranks. The values sf and sg return the sums )-(f? — fi)
sort2(n,wksp2,wksp1) ; and 3\(92, — gm), respectively.

crank(n,wksp2,&sg) ;

¥*d=0.0;

for (j= =n; j++) Sum the squared difference of ranks.

4d += SQR(wksp1[j]-wksp2[j]);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

642 Chapter 14. Statistical Description of Data

en=n;
en3n=en*en*en-en;

aved=en3n/6 .0-(sft+sg)/12.0; Expectation value of D,
fac=(1.0-sf/en3n)*(1.0-sg/en3n) ;

vard=((en-1.0)*en*en*SQR(en+1.0)/36.0)*fac; and variance of D give
*zd=(*d-aved) /sqrt (vard) ; number of standard devia-
*probd=erf cc (fabs (*zd)/1.4142136) ; tions and significance.

#rs=(1.0-(6.0/en3n) *(*d+(sft+sg) /12.0))/sqrt (fac) ; Rank correlation coefficient,
fac=(*rst1.0)*(1.0-(*rs)) 5
if (fac > 0.0) {

t=(#rs) *sqrt ((en-2.0)/fac) ; and its t value,

df=en-2.0;

*probrs=betai(0.5*df,0.5,df/(df+t*t)) ; give its significance.
} else

*probrs=0.0;

free_vector (wksp2,1,n);
free_vector(wksp1,1,n);

void crank(unsigned long n, float w[], float *s)
Given a sorted array w[1. .n], replaces the elements by their rank, including midranking of ties,
and returns as s the sum of f* — f, where f is the number of elements in each tie.
t
unsigned long j=1,ji,jt;
float t,rank;

*s=0.0;
while (j <n) {
if (w[j+1] != wlj]l) { Not a tie.
wlj]=j;
+435
} else { A tie:
for (jt=jt1;jt<=n && wljt]==wlj];jt++); How far does it go?
rank=0.5*(j+jt-1); This is the mean rank of the tie,
for (ji=j;ji<=(jt-1);jit+) wljil=rank; so enter it into all the tied
t=jt-j; entries,
*Ss += tetFt-t; and update s.
j-jts
}
+
if (j == n) win)=n; If the last element was not tied, this is its rank.

Kendall’s Tau

Kendall’s 7 is even more nonparametric than Spearman’s r, or D. Instead of
using the numerical difference of ranks, it uses only the relative ordering of ranks:
higher in rank, lower in rank, or the same in rank. But in that case we don’t even
have to rank the data! Ranks will be higher, lower, or the same if and only if
the values are larger, smaller, or equal, respectively. On balance, we prefer r, as
being the more straightforward nonparametric test, but both statistics are in general
use. In fact, 7 and rs are very strongly correlated and, in most applications, are
effectively the same test.

To define 7, we start with the N data points (x;,y;). Now consider all
3N (N — 1) pairs of data points, where a data point cannot be paired with itself,
and where the points in either order count as one pair. We call a pair concordant

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.6 Nonparametric or Rank Correlation 643

if the relative ordering of the ranks of the two x’s (or for that matter the two ’s
themselves) is the same as the relative ordering of the ranks of the two y’s (or for
that matter the two y’s themselves). We call a pair discordant if the relative ordering
of the ranks of the two x’s is opposite from the relative ordering of the ranks of the
two y’s. If there is a tie in either the ranks of the two ’s or the ranks of the two
y’s, then we don’t call the pair either concordant or discordant. If the tie is in the
x’s, we will call the pair an “extra y pair.” If the tie is in the y’s, we will call the
pair an “extra x pair.” If the tie is in both the x’s and the y’s, we don’t call the pair
anything at all. Are you still with us?
Kendall’s 7 is now the following simple combination of these various counts:

concordant — discordant

,/concordant + discordant + extra-y \/concordant + discordant + extra-x
(14.6.8)

You can easily convince yourself that this must lie between 1 and —1, and that it
takes on the extreme values only for complete rank agreement or complete rank
reversal, respectively.

More important, Kendall has worked out, from the combinatorics, the approx-
imate distribution of 7 in the null hypothesis of no association between x and y.
In this case 7 is approximately normally distributed, with zero expectation value
and a variance of

4N +10

Var(7) = SNTN— 1

(14.6.9)

The following program proceeds according to the above description, and
therefore loops over all pairs of data points. Beware: This is an O(N) algorithm,
unlike the algorithm for r,, whose dominant sort operations are of order N log NV. If
you are routinely computing Kendall’s 7 for data sets of more than a few thousand
points, you may be in for some serious computing. If, however, you are willing to
bin your data into a moderate number of bins, then read on.

#include <math.h>

void kendli(float datai[], float data2[], unsigned long n, float *tau,

float *z, float *prob)
Given data arrays data1[1..n] and data2[1..n], this program returns Kendall's 7 as tau,
its number of standard deviations from zero as Z, and its two-sided significance level as prob.
Small values of prob indicate a significant correlation (tau positive) or anticorrelation (tau
negative).

float erfcc(float x);
unsigned long n2=0,n1=0,k,j;
long is=0;

float svar,aa,a2,a1;

for (j=1;j<n;j++) { Loop over first member of pair,
for (k=(j+1);k<=n;kt+) { and second member.
ai=datai[j]-data1 [k];
a2=data2[j]-data2[k] ;
aa=al*a2;
if (aa) { Neither array has a tie.
+tn1;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

644 Chapter 14. Statistical Description of Data

+4+n2;
aa > 0.0 ? t+tis : --is;

} else { One or both arrays have ties.
if (a1) ++n1; An “extra 2” event.
if (a2) ++n2; An “extra y” event.

}

}
+
*tau=is/(sqrt((double) ni)*sqrt((double) n2)); Equation (14.6.8).
svar=(4.0+*n+10.0)/(9.0*n*(n-1.0)) 5 Equation (14.6.9).
*z=(*tau)/sqrt (svar) ;
*prob=erfcc (fabs (*z) /1.4142136) ; Significance.

Sometimes it happens that there are only a few possible values each for x and
y. In that case, the data can be recorded as a contingency table (see §14.4) that gives
the number of data points for each contingency of x and y.

Spearman’s rank-order correlation coefficient is not a very natural statistic
under these circumstances, since it assigns to each x and y bin a not-very-meaningful
midrank value and then totals up vast numbers of identical rank differences. Kendall’s
tau, on the other hand, with its simple counting, remains quite natural. Furthermore,
its O(N) algorithm is no longer a problem, since we can arrange for it to loop over
pairs of contingency table entries (each containing many data points) instead of over
pairs of data points. This is implemented in the program that follows.

Note that Kendall’s tau can be applied only to contingency tables where both
variables are ordinal, i.e., well-ordered, and that it looks specifically for monotonic
correlations, not for arbitrary associations. These two properties make it less general
than the methods of §14.4, which applied to nominal, i.e., unordered, variables and
arbitrary associations.

Comparing kend11 above with kend12 below, you will see that we have
“floated” a number of variables. This is because the number of events in a
contingency table might be sufficiently large as to cause overflows in some of the
integer arithmetic, while the number of individual data points in a list could not
possibly be that large [for an O(N”) routine!].

#include <math.h>

void kendl2(float **tab, int i, int j, float *tau, float *z, float *prob)
Given a two-dimensional table tab[1..i] [1..j], such that tab[k] [1] contains the number
of events falling in bin k of one variable and bin 1 of another, this program returns Kendall's 7
as tau, its number of standard deviations from zero as Z, and its two-sided significance level as
prob. Small values of prob indicate a significant correlation (tau positive) or anticorrelation
(tau negative) between the two variables. Although tab is a float array, it will normally
contain integral values.
{

float erfcc(float x);

long nn,mm,m2,m1,1j,1i,1,kj,ki,k;

float svar,s=0.0,points,pairs,en2=0.0,en1=0.0;

nn=i*j; Total number of entries in contingency table.
points=tab[i] [j];
for (k=0;k<=nn-2;k++) { Loop over entries in table,

ki=(k/j); decoding a row,

kj=k-j*ki; and a column.

points += tab[ki+1] [kj+1]; Increment the total count of events.

for (l=k+1;1<=nn-1;1++) { Loop over other member of the pair,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.7 Do Two-Dimensional Distributions Differ? 645

decoding its row
and column.

mm=(m1=1i-ki) *(m2=1j-kj) ;
pairs=tab[ki+1] [kj+1] *tab[1i+1] [1j+1];
if (mm) { Not a tie.
eni += pairs;
en2 += pairs;
s += (mm > 0 ? pairs : -pairs); Concordant, or discordant.
} else {
if (m1) eni += pairs;
if (m2) en2 += pairs;

}
+
*tau=s/sqrt (en1*en2) ;
svar=(4.0*points+10.0)/(9.0*points*(points-1.0));
*z=(*tau)/sqrt (svar) ;
*prob=erfcc (fabs (*z) /1.4142136) ;

CITED REFERENCES AND FURTHER READING:

Lehmann, E.L. 1975, Nonparametrics: Statistical Methods Based on Ranks (San Francisco:
Holden-Day).

Downie, N.M., and Heath, R.W. 1965, Basic Statistical Methods, 2nd ed. (New York: Harper &
Row), pp. 206-209.

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and 1985, SPSS-
X Advanced Statistics Guide (New York: McGraw-Hill).

14.7 Do Two-Dimensional Distributions Differ?

We here di

ss a useful generalization of the K-S test (§14.3) to two-dimensional
distributions. T] generalization is due to Fasano and Franceschini [1], a variant on an earlier
idea due to Peacock [2].

In a two-dimensional distribution, each data point is characterized by an (a, y) pair of
values. An example near to our hearts is that each of the 19 neutrinos that were detected
from Supernova 1987A is characterized by a time t; and by an energy E; (see [3]). We
might wish to know whether these measured pairs (ti, Ei), i = 1... 19 are consistent with a
theoretical model that predicts neutrino flux as a function of both time and energy — that is,
a two-dimensional probability distribution in the (a, y) [here, (t, £)] plane. That would be
a one-sample test. Or, given two sets of neutrino detections, from two comparable detectors,
we might want to know whether they are compatible with each other, a two-sample test.

In the spirit of the tried-and-true, one-dimensional K-S test, we want to range over
the (x,y) plane in search of some kind of maximum cumulative difference between two
two-dimensional distributions. Unfortunately, cumulative probability distribution is not
well-defined in more than one dimension! Peacock’s insight was that a good surrogate is
the integrated probability in each of four natural quadrants around a given point (2, yi),
namely the total probabilities (or fraction of data) in (x > x,y > yi), (@ < i,y > Yi),
(a < @i,y < yi), (u@ > wi,y < yi). The two-dimensional K-S statistic D is now taken
to be the maximum difference (ranging both over data points and over quadrants) of the
corresponding integrated probabilities. When comparing two data sets, the value of D may
depend on which data set is ranged over. In that case, define an effective D as the average

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

646 Chapter 14. Statistical Description of Data

-3 -2 -1 0 1 2 3

Figure 14.7.1. Two-dimensional distributions of 65 triangles and 35 squares. The two-dimensional K-S
test finds that point one of whose quadrants (shown by dotted lines) maximizes the difference between
fraction of triangles and fraction of squares. Then, equation (14.7.1) indicates whether the difference is
statistically significant, i.e., whether the triangles and squares must have different underlying distributions.

of the two values obtained. If you are confused at this point about the exact definition of D,
don’t fret; the accompanying computer routines amount to a precise algorithmic definition.

Figure 14.7.1 gives a feeling for what is going on. The 65 triangles and 35 squares seem
to have somewhat different distributions in the plane. The dotted lines are centered on the
triangle that maximizes the D statistic; the maximum occurs in the upper-left quadrant. That
quadrant contains only 0.12 of all the triangles, but it contains 0.56 of all the squares. The
value of D is thus 0.44. Is this statistically significant?

Even for fixed sample sizes, it is unfortunately not rigorously true that the distribution of
Din the null hypothesis is independent of the shape of the two-dimensional distribution. In this
respect the two-dimensional K-S test is not as natural as its one-dimensional parent. However,
extensive Monte Carlo integrations have shown that the distribution of the two-dimensional
D is very nearly identical for even quite different distributions, as long as they have the same
coefficient of correlation r, defined in the usual way by equation (14.5.1). In their paper,
Fasano and Franceschini tabulate Monte Carlo results for (what amounts to) the distribution of
D as a function of (of course) D, sample size N, and coefficient of correlation r. Analyzing
their results, one finds that the significance levels for the two-dimensional K-S test can be
summarized by the simple, though approximate, formulas,

xs ( YNP
BS \ 74 V1 —13(0.25 — 0.75/VN)

Probability (D > observed )

) (14.7.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.7 Do Two-Dimensional Distributions Differ? 647

for the one-sample case, and the same for the two-sample case, but with

NiN2

= ——. 14.7.2
Ni + No ( )

The above formulas are accurate enough when N 2 20, and when the indicated
probability (significance level) is less than (more significant than) 0.20 or so. When the
indicated probability is > 0.20, its value may not be accurate, but the implication that the
data and model (or two data sets) are not significantly different is certainly correct. Notice
that in the limit of r — 1 (perfect correlation), equations (14.7.1) and (14.7.2) reduce to
equations (14.3.9) and (14.3.10): The two-dimensional data lie on a perfect straight line, and
the two-dimensional K-S test becomes a one-dimensional K-S test.

The significance level for the data in Figure 14.7.1, by the way, is about 0.001. This
establishes to a near-certainty that the triangles and squares were drawn from different
distributions. (As in fact they were.)

Of course, if you do not want to rely on the Monte Carlo experiments embodied in
equation (14.7.1), you can do your own: Generate a lot of synthetic data sets from your
model, each one with the same number of points as the real data set. Compute D for each
synthetic data set, using the accompanying computer routines (but ignoring their calculated
probabilities), and count what fraction of the time these synthetic D’s exceed the D from the
real data. That fraction is your significance.

One disadvantage of the two-dimensional tests, by comparison with their one-dimensional
progenitors, is that the two-dimensional tests require of order N? operations: Two nested
loops of order N take the place of an N log N sort. For small computers, this restricts the
usefulness of the tests to N less than several thousand.

We now give computer implementations. The one-sample case is embodied in the
routine ks2d1s (that is, 2-dimensions, 1-sample). This routine calls a straightforward utility
routine quadct to count points in the four quadrants, and it calls a user-supplied routine
quadv1 that must be capable of returning the integrated probability of an analytic model in
each of four quadrants around an arbitrary (a, y) point. A trivial sample quadv1 is shown;
realistic quadvls can be quite complicated, often incorporating numerical quadratures over
analytic two-dimensional distributions.

#include <math.h>
#include "nrutil.h"

void ks2dis(float x1[], float yi[], unsigned long ni,
void (*quadvl) (float, float, float *, float +, float *, float *),
float *d1, float *prob)
Two-dimensional Kolmogorov-Smirnov test of one sample against a model. Given the x and y
coordinates of ni data points in arrays x1[1..n1] and y1[1..n1], and given a user-supplied
function quadvl that exemplifies the model, this routine returns the two-dimensional K-S
statistic as di, and its significance level as prob. Small values of prob show that the sample
is significantly different from the model. Note that the test is slightly distribution-dependent,
so prob is only an estimate.
{
void pearsn(float x[], float y[], unsigned long n, float *r, float *prob,
float *z);
float probks(float alam);
void quadct(float x, float y, float xx[], float yy[], unsigned long nn,
float *fa, float *fb, float *fc, float *fd);
unsigned long j;
float dum,dumm,fa,fb,fc,fd,ga,gb,gc,gd,ri,rr,sqen;

*d1=0.0;
for (j=1;j<en1;j++) { Loop over the data points.
quadct (x1 [j] ,yilj],x1,y1,n1,&fa,&fb,&fc,&fd);
(*quadv1) (x1[j],y1[j],&ga,&gb, &gc, &gd) ;
*d1=FMAX(*d1, fabs (fa-ga) ) ;
*d1=FMAX(*d1, fabs (fb-gb) ) ;
*d1=FMAX(*d1, fabs (fc-gc)) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

648 Chapter 14.

Statistical Description of Data

¥d1=FMAX (*d1, fabs (fd-gd) ) ;

For both the sample and the model, the distribution is integrated in each of four
quadrants, and the maximum difference is saved.

+

pearsn(x1,y1,n1,&r1,&dum, &dumm) ;

sqen=sqrt ((double)n1) ;
rr=sqrt(1.0-ri*r1);

Get the linear correlation coefficient r1.

Estimate the probability using the K-S probability function probks.
*prob=probks (*d1*sqen/(1.0+rr*(0.25-0.75/sqen) )) ;

void quadct(float x, float y, float xx[], float yy[], unsigned long nn,

float *fa, float *fb, float *fc, float *fd)
Given an origin (x, y), and an array of nn points with coordinates xx[1..nn] and yy[1..nn],
count how many of them are in each quadrant around the origin, and return the normalized
fractions. Quadrants are labeled alphabetically, counterclockwise from the upper right. Used

by ks2dis and ks2d2s.

{
unsigned long k,na,nb,nc,nd;
float ff;
na=nb=nc=nd=0;
for (k=1;k<=nn;k++) {
if (yyik] >y) {
xx[k] > x ? ++na : ++nb;
} else {
xx[k] > x ? ++nd : ++nc;
}
}
ff=1.0/nn;
*fc=ff*nc;
*fd=ff*nd;
3

#include "nrutil.h"

void quadvl(float x, float y, float *fa, float *fb, float *fc, float *fd)

This is a sample of a user-supplied routine to be used with ks2d1s. In this case, the model
distribution is uniform inside the square —1 < a < 1, —1 < y < 1. In general this routine
should return, for any point (x,y), the fraction of the total distribution in each of the four
quadrants around that point. The fractions, fa, fb, fc, and fd, must add up to 1. Quadrants
are alphabetical, counterclockwise from the upper right.

{
float qa,qb,qc,qd;

qa=FMIN(2.0,FMAX(0.
qb=FMIN(2.0,FMAX(0.
qc=FMIN(2.0,FMAX(0.
qdg=FMIN(2.0,FMAX(0.
*fa=0.25*qa*qb;
*f£b=0 .25*qb*qc ;
*£c=0.25*qc*qd;
*f£d=0.25*qd*qa;

»1.0-x));
»1.0-y));
»xt1.0));
syt1.0));

ooo°o

The routine ks2d2s is the two-sample case of the two-dimensional K-S test. It also calls
quadct, pearsn, and probks. Being a two-sample test, it does not need an analytic model.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.7 Do Two-Dimensional Distributions Differ? 649

#include <math.h>
#include "nrutil.h"

void ks2d2s(float x1[], float yi[], unsigned long ni, float x2[], float y2[],
unsigned long n2, float *d, float *prob)

Two-dimensional Kolmogorov-Smirnov test on two samples. Given the x and y coordinates of
the first sample as n1 values in arrays x1[1..n1] and y1[1..n1], and likewise for the second
sample, n2 values in arrays x2 and y2, this routine returns the two-dimensional, two-sample
K-S statistic as d, and its significance level as prob. Small values of prob show that the
two samples are significantly different. Note that the test is slightly distribution-dependent, so
prob is only an estimate.

{

void pearsn(float x[], float y[], unsigned long n, float *r, float *prob,
float *z);

float probks(float alam);

void quadct(float x, float y, float xx[], float yy[], unsigned long nn,
float *fa, float *fb, float *fc, float *fd);

unsigned long j;

float di,d2,dum,dumm,fa,fb,fc,fd,ga,gb,gc,gd,ri,r2,rr,sqen;

d1=0.0;

for (j=1;j<=n1;j++) { First, use points in the first sample as ori-
quadct (x1 [j],yilj],x1,y1,n1,&fa,&fb,&fc,&fd); gins.
quadct (x1[j],yi[j],x2,y2,n2,&ga,&gb,&gc,&gd) ;
d1=FMAX (d1,fabs(fa-ga)) ;
d1=FMAX(d1,fabs(fb-gb)) ;
d1=FMAX(d1,fabs(fc-gce)) ;
d1=FMAX (d1, fabs (fd-gd)) ;

3

d2=0.0;

for (j=1;j<=n2;j++) { Then, use points in the second sample as
quadct (x2[j] ,y2[j],x1,y1,n1,&fa,&fb,&fc,&fd); origins.
quadct (x2[j] ,y2[j],x2,y2,n2,&ga,&gb,&gc,&gd) ;
d2=FMAX (d2, fabs (fa-ga)) ;
d2=FMAX (d2,fabs(fb-gb) ) ;
d2=FMAX (d2,fabs(fc-gc)) ;
d2=FMAX (d2, fabs (fd-gd)) ;

+

¥d=0.5*(d1+d2) ; Average the K-S statistics.

sqen=sqrt (n1*n2/(double) (n1+n2));

pearsn(x1,y1,n1,&r1,&dum, &dumm) ; Get the linear correlation coefficient for each

pearsn(x2,y2,n2,&r2,&dum, &dumm) ; sample.

rr=sqrt (1.0-0.5*(ri*ri+r2+*r2) ) ;

Estimate the probability using the K-S probability function probks.

*prob=probks (*d*sqen/(1.0+rr*(0.25-0.75/sqen) ));

}

CITED REFERENCES AND FURTHER READING:

Fasano, G. and Franceschini, A. 1987, Monthly Notices of the Royal Astronomical Society,
vol. 225, pp. 155-170. [1]

Peacock, J.A. 1983, Monthly Notices of the Royal Astronomical Society, vol. 202, pp. 615-627. [2]

Spergel, D.N., Piran, T., Loeb, A., Goodman, J., and Bahcall, J.N. 1987, Science, vol. 237,
pp. 1471-1473. [3]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

650 Chapter 14. Statistical Description of Data

14.8 Savitzky-Golay Smoothing Filters

In §13.5 we learned something about the construction and application of digital filters,
but little guidance was given on which particular filter to use. That, of course, depends
on what you want to accomplish by filtering. One obvious use for low-pass filters is to
smooth noisy data.

The premise of data smoothing is that one is measuring a variable that is both slowly
varying and also corrupted by random noise. Then it can sometimes be useful to replace
each data point by some kind of local average of surrounding data points. Since nearby
points measure very nearly the same underlying value, averaging can reduce the level of noise
without (much) biasing the value obtained.

We must comment editorially that the smoothing of data lies in a murky area, beyond
the fringe of some better posed, and therefore more highly recommended, techniques that are
discussed elsewhere in this book. If you are fitting data to a parametric model, for example
(see Chapter 15), it is almost always better to use raw data than to use data that has been
pre-processed by a smoothing procedure. Another alternative to blind smoothing is so-called
“optimal” or Wiener filtering, as discussed in §13.3 and more generally in §13.6. Data
smoothing is probably most justified when it is used simply as a graphical technique, to guide
the eye through a forest of data points all with large error bars; or as a means of making initial
rough estimates of simple parameters from a graph.

In this section we discuss a particular type of low-pass filter, well-adapted for data
smoothing, and termed variously Savitzky-Golay[1], least-squares [2], or DISPO (Digital
Smoothing Polynomial) [3] filters. Rather than having their properties defined in the Fourier
domain, and then translated to the time domain, Savitzky-Golay filters derive directly from
a particular formulation of the data smoothing problem in the time domain, as we will now
see. Savitzky-Golay filters were initially (and are still often) used to render visible the relative
widths and heights of spectral lines in noisy spectrometric data.

Recall that a digital filter is applied to a series of equally spaced data values f; = f (ti),
where t; = to + iA for some constant sample spacing A andi = .
We have seen (§13.5) that the simplest type of digital filter (the nonrecursive or finite impulse
response filter) replaces each data value f; by a linear combination g; of itself and some
number of nearby neighbors,

nR

g= Yo enfitn (14.8.1)

n=—np

Here nz is the number of points used “to the left” of a data point 4, i.e., earlier than it, while
mp is the number used to the right, i.e., later. A so-called causal filter would have nr = 0.

As a starting point for understanding Savitzky-Golay filters, consider the simplest
possible averaging procedure: For some fixed nx = nr, compute each gj as the average of
the data points from fin, to firn,- This is sometimes called moving window averaging
and corresponds to equation (14.8.1) with constant c, = 1/(nz +n + 1). If the underlying
function is constant, or is changing linearly with time (increasing or decreasing), then no
bias is introduced into the result. Higher points at one end of the averaging interval are on
the average balanced by lower points at the other end. A bias is introduced, however, if
the underlying function has a nonzero second derivative. At a local maximum, for example,
moving window averaging always reduces the function value. In the spectrometric application,
a narrow spectral line has its height reduced and its width increased. Since these parameters
are themselves of physical interest, the bias introduced is distinctly undesirable.

Note, however, that moving window averaging does preserve the area under a spectral
line, which is its zeroth moment, and also (if the window mmetric with ny, = np) its
mean position in time, which is its first moment. What is violated is the second moment,
equivalent to the line width.

The idea of Savitzky-Golay filtering is to find filter coefficients c, that preserve higher
moments. Equivalently, the idea is to approximate the underlying function within the moving
window not by a constant (whose estimate is the average), but by a polynomial of higher
order, typically quadratic or quartic: For each point f;, we least-squares fit a polynomial to all

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.8 Savitzky-Golay Smoothing Filters 651

S
S
s

6

nR Sample Savitzky-Golay Coefficients

2 —0.086 0.343 |0.486/0.343 —0.086

—0.143 0.171 0.343)0.371 | 0.257
0.086 —0.143 —0.086 0.257|0.886
—0.084 0.021 0.103 0.161 0.196]0.207|0.196 0.161 0.103 0.021 —0.084

0.035 —0.128 0.070 0.315/0.417}0.315 0.070 —0.128 0.035

eB Ble yw ww
Ww BlA B ww
wn plA oO

0.042 —0.105 —0.023 0.140 0.280]0.333|0.280 0.140 —0.023 —0.105 0.042

nyt +nr + 1 points in the moving window, and then set g; to be the value of that polynomial
at position 7. (If you are not familiar with least-squares fitting, you might want to look ahead
to Chapter 15.) We make no use of the value of the polynomial at any other point. When we
move on to the next point fi1, we do a whole new least-squares fit using a shifted window.

All these least-squares fits would be laborious if done as described. Luckily, since the
process of least-squares fitting involves only a linear matrix inversion, the coefficients of a
fitted polynomial are themselves linear in the values of the data. That means that we can do
all the fitting in advance, for fictitious data consisting of all zeros except for a single 1, and
then do the fits on the real data just by taking linear combinations. This is the key point, then:
There are particular sets of filter coefficients c, for which equation (14.8.1) “automatically”
accomplishes the process of polynomial least-squares fitting inside a moving window.

To derive such coefficients, consider how go might be obtained: We want to fit a
polynomial of degree M ini, namely ao + ai + +++ + ami™ to the values fenps---s dng:
Then go will be the value of that polynomial at i = 0, namely ao. The design matrix for
this problem (§15.4) is

Ay =i? i=—mz,...,nR, j=0,...,M (14.8.2)
and the normal equations for the vector of a;’s in terms of the vector of f,’s is in matrix notation
(AT-A)-a=A™-f or a=(A™-A)!-(A™-f) (14.8.3)
We also have the specific forms.
rR nR
{ar . a} = Yo AwAn= D> Re (14.8.4)
S pony k=—ny
and
1R 1R
{ar -t} = Yo Aghk= YS hh (14.8.5)
J
k=—ny k=—ny

Since the coefficient c, is the component ao when f is replaced by the unit vector en,
—nt <n < nr, we have

cn = {ca A). (AT -en)} = > {av ay} n™ (14.8.6)

Om
m=0

Note that equation (14.8.6) says that we need only one row of the inverse matrix. (Numerically
we can get this by LU decomposition with only a single backsubstitution.)

The function savgol, below, implements equation (14.8.6). As input, it takes the
parameters nl = n,, nr = nz, and m = M (the desired order). Also input is np, the
physical length of the output array c, and a parameter 1d which for data fitting should be
zero. In fact, 1d specifies which coefficient among the ai’s should be returned, and we are
here interested in a9. For another purpose, namely the computation of numerical derivatives
(already mentioned in §5.7) the useful choice is 1d > 1. With 1d = 1, for example, the
filtered first derivative is the convolution (14.8.1) divided by the stepsize A. For 1d = k > 1,
the array c must be multiplied by k! to give derivative coefficients. For derivatives, one
usually wants m = 4 or larger.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

652 Chapter 14. Statistical Description of Data

#include <math.h>
#include "nrutil.h"

void savgol(float c[], int np, int nl, int nr, int 1d, int m)

Returns in c[1..np], in wrap-around order (N.B.!) consistent with the argument respns in
routine convlv, a set of Savitzky-Golay filter coefficients. nl is the number of leftward (past)
data points used, while nr is the number of rightward (future) data points, making the total
number of data points used nl + nr +1. 1d is the order of the derivative desired (e.g., 1d = 0
for smoothed function). m is the order of the smoothing polynomial, also equal to the highest
conserved moment; usual values are m = 2 orm = 4.

t

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

int imj,ipj,j,k,kk,mm,*indx;

float d,fac,sum,**a,*b;

if (mp < nltnrti || nl <0 || nr <0 || 1d > m || nl+nr < m)

nrerror("bad args in savgol");

indx=ivector(1,m+1);

a=matrix(1,mt1,1,m+1);

b=vector(1,mt+1);

for (ipj=0;ipj<=(m << 1);ipjt++) { Set up the normal equations of the desired
sum=(ipj ? 0.0 : 1.0); least-squares fit.
for (k=1;k<=nr;k++) sum += pow((double)k, (double) ipj) ;
for (k=1;k<=nl;k++) sum += pow((double)-k, (double) ipj) ;
mm=IMIN(ipj,2*m-ipj);
for (imj = -mm;imj<=mm;imj+=2) al[1+(ipj+imj)/2] [1+(ipj-imj)/2]=sum;

+

ludemp(a,m+1,indx,&d) ; Solve them: LU decomposition.

for (j= =m+1;j++) b[j]=0.0;

b[1d+1]=1.0;

Right-hand side vector is unit vector, depending on which derivative we want.

lubksb(a,m+1,indx,b) ; Get one row of the inverse matrix.

for (kk=1;kk<=np;kk++) c[kk]=0.0; Zero the output array (it may be bigger than

for (k = -nl;k<=nr;k++) { number of coefficients).
sum=b[1] ; Each Savitzky-Golay coefficient is the dot
fac=1.0; product of powers of an integer with the
for (mm=1;mm<=m;mm++) sum += b[mmt+ti]*(fac *= k); inverse matrix row.
kk=((np-k) % np)+1; Store in wrap-around order.
c(kk]=sum;

+

free_vector(b,1,m+1);

free_matrix(a,1,m+1,1,m+1);

free_ivector(indx,1,m+1);

}

As output, savgol returns the coefficients c,, for —n, <n < ng. These are stored in
c in “wrap-around order”; that is, co is in c[1] , c-1 is in c[2], and so on for further negative
indices. The value c; is stored in c [np] , c2 in c[mp-1], and so on for positive indices. This
order may seem arcane, but it is the natural one where causal filters have nonzero coefficients
in low array elements of c. It is also the order required by the function convlv in §13.1,
which can be used to apply the digital filter to a data set.

The accompanying table shows some typical output from savgol. For orders 2 and
4, the coefficients of Savitzky-Golay filters with several choices of nz and nr are shown.
The central column is the coefficient applied to the data f; in obtaining the smoothed gj.
Coefficients to the left are applied to earlier data; to the right, to later. The coefficients
always add (within roundoff error) to unity. One sees that, as befits a smoothing operator,
the coefficients always have a central positive lobe, but with smaller, outlying corrections of
both positive and negative sign. In practice, the Savitzky-Golay filters are most useful for
much larger values of nz and nr, since these few-point formulas can accomplish only a
relatively small amount of smoothing.

Figure 14.8.1 shows a numerical experiment using a 33 point smoothing filter, that is,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

14.8 Savitzky-Golay Smoothing Filters 653

before

oN FD

0 100 200 300 400 500 600 700 800 900

after square (16,16,0)

So

100 200 300 400 500 600 700 800 900

after S—G (16,164)

0 100 200 300 400 500 600 700 800 900

Figure 14.8.1. Top: Synthetic noisy data consisting of a sequence of progressively narrower bumps,
and additive Gaussian white noise. Center: Result of smoothing the data by a simple moving window
average. The window extends 16 points leftward and rightward, for a total of 33 points. Note that narrow
features are broadened and suffer corresponding loss of amplitude. The dotted curve is the underlying
function used to generate the synthetic data. Bottom: Result of smoothing the data by a Savitzky-Golay
smoothing filter (of degree 4) using the same 33 points. While there is less smoothing of the broadest
feature, narrower features have their heights and widths preserved.

nyt = Nr = 16. The upper panel shows a test function, constructed to have six “bumps” of
varying widths, all of height 8 units. To this function Gaussian white noise of unit variance
has been added. (The test function without noise is shown as the dotted curves in the center
and lower panels.) The widths of the bumps (full width at half of maximum, or FWHM) are
140, 43, 24, 17, 13, and 10, respectively.

The middle panel of Figure 14.8.1 shows the result of smoothing by a moving window
average. One sees that the window of width 33 does quite a nice job of smoothing the broadest
bump, but that the narrower bumps suffer considerable loss of height and increase of width.
The underlying signal (dotted) is very badly represented.

The lower panel shows the result of smoothing with a Savitzky-Golay filter of the
identical width, and degree M = 4. One sees that the heights and widths of the bumps are
quite extraordinarily preserved. A trade-off is that the broadest bump is less smoothed. That
is because the central positive lobe of the Savitzky-Golay filter coefficients fills only a fraction
of the full 33 point width. As a rough guideline, best results are obtained when the full width
of the degree 4 Savitzky-Golay filter is between 1 and 2 times the FWHM of desired features
in the data. (References [3] and [4] give additional practical hints.)

Figure 14.8.2 shows the result of smoothing the same noisy “data” with broader
Savitzky-Golay filters of 3 different orders. Here we have np = ng = 32 (65 point filter)
and M = 2,4,6. One sees that, when the bumps are too narrow with respect to the filter
size, then even the Savitzky-Golay filter must at some point give out. The higher order filter
manages to track narrower features, but at the cost of less smoothing on broad features.

To summarize: Within limits, Savitzky-Golay filtering does manage to provide smoothing

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

654 Chapter 14. Statistical Description of Data

6E after S~G (32,32,2)

4eF

2b

ORS

0 100 = 200-—-s300-'s« 400.—“ié«C00s«i000

6 after S—G (32,324)

4F

2

ORO

0 100 =©=6. 200 «300s 400—“‘i‘C00sCis—“‘izr~SsC«i0s—Ct«C00
6E after S—G (32,32,6)

4F

2E

OF TT

0 100 =©=6 200 «300s 400—“‘iéi‘C00'si«is—“‘irSs«é00S—t—«é00

Figure 14.8.2. Result of applying wider 65 point Savitzky-Golay filters to the same data set as in Figure
14.8.1. Top: degree 2. Center: degree 4. Bottom: degree 6. All of these filters are inoptimally broad
for the resolution of the narrow features. Higher-order filters do best at preserving feature heights and
widths, but do less smoothing on broader features.

without loss of resolution. It does this by assuming that relatively distant data points have
some significant redundancy that can be used to reduce the level of noise. The specific nature
of the assumed redundancy is that the underlying function should be locally well-fitted by a
polynomial. When this is true, as it is for smooth line profiles not too much narrower than
the filter width, then the performance of Savitzky-Golay filters can be spectacular. When it
is not true, then these filters have no compelling advantage over other classes of smoothing
filter coefficients.

A last remark concerns irregularly sampled data, where the values f; are not uniformly
spaced in time. The obvious generalization of Savitzky-Golay filtering would be to do a
least-squares fit within a moving window around each data point, one containing a fixed
number of data points to the left (nz) and right (nz). Because of the irregular spacing,
however, there is no way to obtain universal filter coefficients applicable to more than one
data point. One must instead do the actual least-squares fits for each data point. This becomes
computationally burdensome for larger nz, np, and M.

As a cheap alternative, one can simply pretend that the data points are equally spaced.
This amounts to virtually shifting, within each moving window, the data points to equally
spaced positions. Such a shift introduces the equivalent of an additional source of noise
into the function values. In those cases where smoothing is useful, this noise will often be
much smaller than the noise already present. Specifically, if the location of the points is
approximately random within the window, then a rough criterion is this: If the change in f
across the full width of the N = nz + np +1 point window is less than \/N/2 times the
measurement noise on a single point, then the cheap method can be used.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

655

14.8 Savitzky-Golay Smoothing Filters

CITED REFERENCES AND FURTHER READING:

Savitzky A., and Golay, M.J.E. 1964, Analytical Chemistry, vol. 36, pp. 1627-1639. [1]

Hamming, R.W. 1983, Digital Filters, 2nd ed. (Englewood Cliffs, NJ: Prentice-Hall). [2]

Ziegler, H. 1981, Applied Spectroscopy, vol. 35, pp. 88-92. [3]

Bromba, M.U.A., and Ziegler, H. 1981, Analytical Chemistry, vol. 53, pp. 1583-1586. [4]

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).


Chapter 15. Modeling of Data

15.0 Introduction

Given a set of observations, one often wants to condense and summarize the
data by fitting it to a “model” that depends on adjustable parameters. Sometimes the
model is simply a convenient class of functions, such as polynomials or Gaussians,
and the fit supplies the appropriate coefficients. Other times, the model’s parameters
come from some underlying theory that the data are supposed to satisfy; examples
are coefficients of rate equations in a complex network of chemical reactions, or
orbital elements of a binary star. Modeling can also be used as a kind of constrained
interpolation, where you want to extend a few data points into a continuous function,
but with some underlying idea of what that function should look like.

The basic approach in all cases is usually the same: You choose or design a
figure-of-merit function (“merit function,” for short) that measures the agreement
between the data and the model with a particular choice of parameters. The merit
function is conventionally arranged so that small values represent close agreement.
The parameters of the model are then adjusted to achieve a minimum in the merit
function, yielding best-fit parameters. The adjustment process is thus a problem in
minimization in many dimensions. This optimization was the subject of Chapter 10;
however, there exist special, more efficient, methods that are specific to modeling,
and we will discuss these in this chapter.

There are important issues that go beyond the mere finding of best-fit parameters.
Data are generally not exact. They are subject to measurement errors (called noise
in the context of signal-processing). Thus, typical data never exactly fit the model
that is being used, even when that model is correct. We need the means to assess
whether or not the model is appropriate, that is, we need to test the goodness-of-fit
against some useful statistical standard.

We usually also need to know the accuracy with which parameters are determined
by the data set. In other words, we need to know the likely errors of the best-fit
parameters.

Finally, it is not uncommon in fitting data to discover that the merit function
is not unimodal, with a single minimum. In some cases, we may be interested in
global rather than local questions. Not, “how good is this fit?” but rather, “how sure
am I that there is not a very much better fit in some corner of parameter space?”
As we have seen in Chapter 10, especially §10.9, this kind of problem is generally
quite difficult to solve.

The important message we want to deliver is that fitting of parameters is not
the end-all of parameter estimation. To be genuinely useful, a fitting procedure

656

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

15.1 Least Squares as a Maximum Likelihood Estimator 657

should provide (i) parameters, (ii) error estimates on the parameters, and (iii) a
statistical measure of goodness-of-fit. When the third item suggests that the model
is an unlikely match to the data, then items (i) and (ii) are probably worthless.
Unfortunately, many practitioners of parameter estimation never proceed beyond
item (i). They deem a fit acceptable if a graph of data and model “looks good.” This
approach is known as chi-by-eye. Luckily, its practitioners get what they deserve.

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill).

Brownlee, K.A. 1965, Statistical Theory and Methodology, 2nd ed. (New York: Wiley).
Martin, B.R. 1971, Statistics for Physicists (New York: Academic Press).

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), Chapter X.

Korn, G.A., and Korn, T.M. 1968, Mathematical Handbook for Scientists and Engineers, 2nd ed.
(New York: McGraw-Hill), Chapters 18-19.

15.1 Least Squares as a Maximum Likelihood

Estimator
Suppose that we are fitting N data points (7;,y;) i = 1,..., N, to a model that
has M adjustable parameters aj, j = 1,...,. The model predicts a functional

relationship between the measured independent and dependent variables,

y(x) = y(a3a1...an) (15.1.1)

where the dependence on the parameters is indicated explicitly on the right-hand side.
What, exactly, do we want to minimize to get fitted values for the a;’s? The
first thing that comes to mind is the familiar least-squares fit,

N
minimize overa,...ayy : Yi = y(wi;a1...an)|° (15.1.2)
i=l

But where does this come from? What general principles is it based on? The answer
to these questions takes us into the subject of maximum likelihood estimators.
Given a particular data set of x;’s and y;’s, we have the intuitive feeling that
some parameter sets a,...a,y are very unlikely — those for which the model
function y(x) looks nothing like the data — while others may be very likely — those
that closely resemble the data. How can we quantify this intuitive feeling? How can
we select fitted parameters that are “most likely” to be correct? It is not meaningful
to ask the question, “What is the probability that a particular set of fitted parameters
..@yg is correct?” The reason is that there is no statistical universe of models
from which the parameters are drawn. There is just one model, the correct one, and
a statistical universe of data sets that are drawn from it!

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

658 Chapter 15. Modeling of Data

That being the case, we can, however, turn the question around, and ask, “Given
a particular set of parameters, what is the probability that this data set could have
occurred?” If the y;’s take on continuous values, the probability will always be
zero unless we add the phrase, “...plus or minus some fixed Ay on each data point.”
So let’s always take this phrase as understood. If the probability of obtaining the
data set is infinitesimally small, then we can conclude that the parameters under
consideration are “unlikely” to be right. Conversely, our intuition tells us that the
data set should not be too improbable for the correct choice of parameters.

In other words, we identify the probability of the data given the parameters
(which is a mathematically computable number), as the likelihood of the parameters
given the data. This identification is entirely based on intuition. It has no formal
mathematical basis in and of itself; as we already remarked, statistics is not a
branch of mathematics!

Once we make this intuitive identification, however, it is only a small further
step to decide to fit for the parameters a, ...a,¢ precisely by finding those values
that maximize the likelihood defined in the above way. This form of parameter
estimation is maximum likelihood estimation.

We are now ready to make the connection to (15.1.2). Suppose that each data
point y; has a measurement error that is independently random and distributed as a
normal (Gaussian) distribution around the “true” model y(x). And suppose that the
standard deviations o of these normal distributions are the same for all points. Then
the probability of the data set is the product of the probabilities of each point,

N L/y, )\?
Px] exp |—5 a“) Ay (15.1.3)
i=1

Notice that there is a factor Ay in each term in the product. Maximizing (15.1.3) is
equivalent to maximizing its logarithm, or minimizing the negative of its logarithm,
namely,

yi — yl)?

502 — Nlog Ay (15.1.4)

Since N, o, and Ay are all constants, minimizing this equation is equivalent to
minimizing (15.1.2).

What we see is that least-squares fitting is a maximum likelihood estimation
of the fitted parameters if the measurement errors are independent and normally
distributed with constant standard deviation. Notice that we made no assumption
about the linearity or nonlinearity of the model y(z;a1...) in its parameters

1..-a@y. Just below, we will relax our assumption of constant standard deviations
and obtain the very similar formulas for what is called “chi-square fitting” or
“weighted least-squares fitting.’ First, however, let us discuss further our very
stringent assumption of a normal distribution.

For a hundred years or so, mathematical statisticians have been in love with
the fact that the probability distribution of the sum of a very large number of very
small random deviations almost always converges to a normal distribution. (For
precise statements of this central limit theorem, consult [1] or other standard works

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.1 Least Squares as a Maximum Likelihood Estimator 659

on mathematical statistics.) This infatuation tended to focus interest away from the
fact that, for real data, the normal distribution is often rather poorly realized, if it is
realized at all. We are often taught, rather casually, that, on average, measurements
will fall within --o of the true value 68 percent of the time, within +20 95 percent
of the time, and within +30 99.7 percent of the time. Extending this, one would
expect a measurement to be off by +20¢ only one time out of 2 x 108°. We all
know that “glitches” are much more likely than that!

In some instances, the deviations from a normal distribution are easy to
understand and quantify. For example, in measurements obtained by counting
events, the measurement errors are usually distributed as a Poisson distribution,
whose cumulative probability function was already discussed in 86.2. When the
number of counts going into one data point is large, the Poisson distribution converges
towards a Gaussian. However, the convergence is not uniform when measured in
fractional accuracy. The more standard deviations out on the tail of the distribution,
the larger the number of counts must be before a value close to the Gaussian is
realized. The sign of the effect is always the same: The Gaussian predicts that “tail”
events are much less likely than they actually (by Poisson) are. This causes such
events, when they occur, to skew a least-squares fit much more than they ought.

Other times, the deviations from a normal distribution are not so easy to
understand in detail. Experimental points are occasionally just way off. Perhaps
the power flickered during a point’s measurement, or someone kicked the apparatus,
or someone wrote down a wrong number. Points like this are called outliers.
They can easily turn a least-squares fit on otherwise adequate data into nonsense.
Their probability of occurrence in the assumed Gaussian model is so small that the
maximum likelihood estimator is willing to distort the whole curve to try to bring
them, mistakenly, into line.

The subject of robust statistics deals with cases where the normal or Gaussian
model is a bad approximation, or cases where outliers are important. We will discuss
robust methods briefly in §15.7. All the sections between this one and that one
assume, one way or the other, a Gaussian model for the measurement errors in the
data. It it quite important that you keep the limitations of that model in mind, even
as you use the very useful methods that follow from assuming it.

Finally, note that our discussion of measurement errors has been limited to
statistical errors, the kind that will average away if we only take enough data.
Measurements are also susceptible to systematic errors that will not go away with
any amount of averaging. For example, the calibration of a metal meter stick might
depend on its temperature. If we take all our measurements at the same wrong
temperature, then no amount of averaging or numerical processing will correct for
this unrecognized systematic error.

Chi-Square Fitting

We considered the chi-square statistic once before, in §14.3. Here it arises
in a slightly different context.

If each data point (x;, y;) has its own, known standard deviation o;, then
equation (15.1.3) is modified only by putting a subscript 7 on the symbol a. That
subscript also propagates docilely into (15.1.4), so that the maximum likelihood

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

660 Chapter 15. Modeling of Data

estimate of the model parameters is obtained by minimizing the quantity

N 2
vs) (aa) (15.1.5)

O;
i=1 '

called the “chi-square.”

To whatever extent the measurement errors actually are normally distributed, the
quantity y? is correspondingly a sum of N squares of normally distributed quantities,
each normalized to unit variance. Once we have adjusted the a; ...aj to minimize
the value of \, the terms in the sum are not all statistically independent. For models
that are linear in the a’s, however, it turns out that the probability distribution for
different values of x? at its minimum can nevertheless be derived analytically, and
is the chi-square distribution for N — M degrees of freedom. We learned how to
compute this probability function using the incomplete gamma function gammq in
§6.2. In particular, equation (6.2.18) gives the probability Q that the chi-square
should exceed a particular value y? by chance, where v = N — M is the number of
degrees of freedom. The quantity Q, or its complement P = 1 — Q, is frequently
tabulated in appendices to statistics books, but we generally find it easier to use
gammq and compute our own values: Q = gammq (0.5v, 0.5x). It is quite common,
and usually not too wrong, to assume that the chi-square distribution holds even for
models that are not strictly linear in the a’s.

This computed probability gives a quantitative measure for the goodness-of-fit
of the model. If Q is a very small probability for some particular data set, then the
apparent discrepancies are unlikely to be chance fluctuations. Much more probably
either (i) the model is wrong — can be statistically rejected, or (ii) someone has lied to
you about the size of the measurement errors 0; — they are really larger than stated.

It is an important point that the chi-square probability Q does not directly
measure the credibility of the assumption that the measurement errors are normally
distributed. It assumes they are. In most, but not all, cases, however, the effect of
nonnormal errors is to create an abundance of outlier points. These decrease the
probability Q, so that we can add another possible, though less definitive, conclusion
to the above list: (iii) the measurement errors may not be normally distributed.

Possibility (iii) is fairly common, and also fairly benign. It is for this reason
that reasonable experimenters are often rather tolerant of low probabilities Q. It is
not uncommon to deem acceptable on equal terms any models with, say,Q > 0.001.
This is not as sloppy as it sounds: Truly wrong models will often be rejected with
vastly smaller values of Q, 10~18, say. However, if day-in and day-out you find
yourself accepting models with Q ~ 10~%, you really should track down the cause.

If you happen to know the actual distribution law of your measurement errors,
then you might wish to Monte Carlo simulate some data sets drawn from a particular
model, cf. §7.2-§7.3. You can then subject these synthetic data sets to your actual
fitting procedure, so as to determine both the probability distribution of the x?
statistic, and also the accuracy with which your model parameters are reproduced
by the fit. We discuss this further in §15.6. The technique is very general, but it
can also be very expensive.

At the opposite extreme, it sometimes happens that the probability Q is too large,
too near to 1, literally too good to be true! Nonnormal measurement errors cannot
in general produce this disease, since the normal distribution is about as “compact”

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.2 Fitting Data to a Straight Line 661

as a distribution can be. Almost always, the cause of too good a chi-square fit
is that the experimenter, in a “fit” of conservativism, has overestimated his or her
measurement errors. Very rarely, too good a chi-square signals actual fraud, data
that has been “fudged” to fit the model.

A rule of thumb is that a “typical” value of .? for a “moderately” good fit is
y? & v. More precise is the statement that the x? statistic has a mean v anda standard
deviation \/2v, and, asymptotically for large v, becomes normally distributed.

In some cases the uncertainties associated with a set of measurements are not
known in advance, and considerations related to x? fitting are used to derive a value
for a. If we assume that all measurements have the same standard deviation, 0 ; = 0,
and that the model does fit well, then we can proceed by first assigning an arbitrary
constant ¢ to all points, next fitting for the model parameters by minimizing 7,
and finally recomputing

N

o? = yi — yles)2/(N — M) (15.1.6)

i=l

Obviously, this approach prohibits an independent assessment of goodness-of-fit, a
fact occasionally missed by its adherents. When, however, the measurement error
is not known, this approach at least allows some kind of error bar to be assigned
to the points.

If we take the derivative of equation (15.1.5) with respect to the parameters a x,
we obtain equations that must hold at the chi-square minimum,

0= S (4 <n) (a = 2) k=1,...,M (15.1.7)

i=l i

Equation (15.1.7) is, in general, a set of M/ nonlinear equations for the 7 unknown
a,. Various of the procedures described subsequently in this chapter derive from
(15.1.7) and its specializations.

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill), Chapters 1-4.

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York: Academic
Press), §VI.C. [1]

15.2 Fitting Data to a Straight Line

A concrete example will make the considerations of the previous section more
meaningful. We consider the problem of fitting a set of N data points (x ;, yi) to
a straight-line model

y(«) = y(x;a,b) = at ba (15.2.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

662 Chapter 15. Modeling of Data

This problem is often called linear regression, a terminology that originated, long
ago, in the social sciences. We assume that the uncertainty 0; associated with
each measurement y; is known, and that the 2:;’s (values of the dependent variable)
are known exactly.

To measure how well the model agrees with the data, we use the chi-square
merit function (15.1.5), which in this case is

e@)=->> c=

(15.2.2)
i=l %
If the measurement errors are normally distributed, then this merit function will give
maximum likelihood parameter estimations of a and 0; if the errors are not normally
distributed, then the estimations are not maximum likelihood, but may still be useful
in a practical sense. In §15.7, we will treat the case where outlier points are so
numerous as to render the x? merit function useless.
Equation (15.2.2) is minimized to determine a and b.
derivatives of y?(a,b) with respect to a,b vanish.

At its minimum,

Ox?
0 Oa

7 (15.2.3)
o= &

Ob

These conditions can be rewritten in a convenient form if we define the following
sums:

WD
Ill
iM=
Qe

mt (15.2.4)
N42
With these definitions (15.2.3) becomes
aS + bS; = Sy
15.2.5
aSz + Sire = Sey ( )
The solution of these two equations in two unknowns is calculated as
A= SS ex — (Sx)?
SraSy — SxS,
a= meaty eet (15.2.6)
b= SSiry — SxSy
7 A

Equation (15.2.6) gives the solution for the best-fit model parameters a and b.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.2 Fitting Data to a Straight Line 663

We are not done, however. We must estimate the probable uncertainties in
the estimates of a and b, since obviously the measurement errors in the data must
introduce some uncertainty in the determination of those parameters. If the data
are independent, then each contributes its own bit of uncertainty to the parameters.
Consideration of propagation of errors shows that the variance oF in the value of
any function will be

N 2 \ 2

For the straight line, the derivatives of a and b with respect to y; can be directly
evaluated from the solution:

Oa Sax — SpXi

dy oA
‘ 15.2.8
ob _ St, ~ 5, 528)
Oyi ~ o7A
Summing over the points as in (15.2.7), we get
02 = Syn /A
: 15.2.9
of = S/A ( )

which are the variances in the estimates of a and b, respectively. We will see in
§15.6 that an additional number is also needed to characterize properly the probable
uncertainty of the parameter estimation. That number is the covariance of a and b,
and (as we will see below) is given by

Cov(a,b) = —S;,/A (15.2.10)
The coefficient of correlation between the uncertainty in a and the uncertainty

in b, which is a number between —1 and 1, follows from (15.2.10) (compare
equation 14.5.1),

(15.2.11)

A positive value of rq» indicates that the errors in a and b are likely to have the
same sign, while a negative value indicates the errors are anticorrelated, likely to
have opposite signs.

We are still not done. We must estimate the goodness-of-fit of the data to the
model. Absent this estimate, we have not the slightest indication that the parameters
a and b in the model have any meaning at all! The probability Q that a value of
chi-square as poor as the value (15.2.2) should occur by chance is

N-2 y?

Q = ganna (“*, ) (15.2.12)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

664 Chapter 15. Modeling of Data

Here gammq is our routine for the incomplete gamma function Q(a,x), §6.2. If
Q is larger than, say, 0.1, then the goodness-of-fit is believable. If it is larger
than, say, 0.001, then the fit may be acceptable if the errors are nonnormal or have
been moderately underestimated. If Q is less than 0.001 then the model and/or
estimation procedure can rightly be called into question. In this latter case, turn
to 815.7 to proceed further.

If you do not know the individual measurement errors of the points o ;, and are
proceeding (dangerously) to use equation (15.1.6) for estimating these errors, then
here is the procedure for estimating the probable uncertainties of the parameters a
and b: Set o; = 1 in all equations through (15.2.6), and multiply 7, and op, as
obtained from equation (15.2.9), by the additional factor \/2/(N — 2), where y?
is computed by (15.2.2) using the fitted parameters a and b. As discussed above,
this procedure is equivalent to assuming a good fit, so you get no independent
goodness-of-fit probability Q.

In $14.5 we promised a relation between the linear correlation coefficient
r (equation 14.5.1) and a goodness-of-fit measure, y? (equation 15.2.2). For

unweighted data (all o; = 1), that relation is
x? = (L—1?)NVar (yi... yw) (15.2.13)
where
N
NVar (1 ..-yv) = Sy — 97)? (15.2.14)
i=1

For data with varying weights 0;, the above equations remain valid if the sums in
equation (14.5.1) are weighted by 1/c?.

The following function, fit, carries out exactly the operations that we have
discussed. When the weights o are known in advance, the calculations exactly
correspond to the formulas above. However, when weights o are unavailable,
the routine assumes equal values of o for each point and assumes a good fit, as
discussed in §15.1.

The formulas (15.2.6) are susceptible to roundoff error. Accordingly, we
rewrite them as follows: Define

_1 Ss ;
=> (= =). i=1,2,...,N (15.2.15)
and
N
Su= S08 (15.2.16)
i=1

Then, as you can verify by direct substitution,

(15.2.17)

(15.2.18)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

15.2 Fitting Data to a Straight Line 665

(15.2.19)
(15.2.20)
(15.2.21)

(15.2.22)

#include <math.h>
#include "nrutil.h"

void fit(float x[], float y[], int ndata, float sig[], int mwt, float *a,

float *b, float *siga, float *sigb, float *chi2, float *q)
Given a set of data points x[1..ndata],y[1..ndata] with individual standard deviations
sig[1..ndata], fit them to a straight line y = a+ ba by minimizing x?. Returned are
a,b and their respective probable uncertainties siga and sigh, the chi-square chi2, and the
goodness-of-fit probability q (that the fit would have x? this large or larger). If mwt=0 on
input, then the standard deviations are assumed to be unavailable: q is returned as 1.0 and
the normalization of chi2 is to unit standard deviation on all points.
{

float gammq(float a, float x);

int i;

float wt,t,sxoss,sx=0.0,sy=0.0,st2=0.0,ss,sigdat;

*b=0.0;
if (owt) { Accumulate sums ...
ss=0.0;
for (i=1;i<=ndata;i++) { .-With weights
wt=1.0/SQR(sig[il);
ss += wt;
sx += x[i]*wt;
sy += yli]*wt;
}
} else {
for (i=1;i<=ndata;i++) { ..or without weights.
sx += x[i];
sy += y(il;
}
ss=ndata;
+
sxoss=sx/ss;
if (mwt) {

for (i=1;i<=ndata;i++) {
t=(x[i]-sxoss)/sig([il];
st2 += t*t;
¥b += t*yli]/siglil];

}

} else {

for (i=1;i<=ndata;i++) {

t=x[i]-sxoss;

st2 += t*t;
*b += t*y[il;
}
+
*b /= st2; Solve for a, b, ga, and ap.

*a=(sy-sx*(*b))/ss;
=sqrt ((1.0+sx*sx/(ss*st2))/ss) ;
=sqrt (1.0/st2) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

666 Chapter 15. Modeling of Data

*chi2=0.0; Calculate x?.
¥q=1.0;
if (awt 2
for ( jis<=sndata; i++)
*chi2 += SQR(y[i]-(*a)-(*b)*x([i]) ;
sigdat=sqrt ((*chi2)/(ndata-2)) ; For unweighted data evaluate typ-
*siga sigdat; ical sig using chi2, and ad-
*sigb *= sigdat; just the standard deviations.
} else {
for (i=1;i<=ndata; i++)
*chi2 += SQR((y [i] -(*a)-(*b) *x[i])/sigli]);
if (ndata>2) *q=gammq(0.5*(ndata-2) ,0.5*(*chi2)); Equation (15.2.12).

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill), Chapter 6.

15.3 Straight-Line Data with Errors in Both
Coordinates

If experimental data are subject to measurement error not only in the yi’s, but also in
the x;’s, then the task of fitting a straight-line model

y(x) = a+ br (15.3.1)

is considerably harder. It is straightforward to write down the x? merit function for this case,

(yi — a — bai)?
-y¥ “oi, +02, (15.3.2)

where oxi and oy; are, respectively, the x and y standard deviations for the ith point. The
weighted sum of variances in the denominator of equation (15.3.2) can be understood both
as the variance in the direction of the smallest y? between each data point and the line with
slope b, and also as the variance of the linear combination yi — a — bai of two random
variables x; and yj,

Var(yi — a — ba) = Var(yi) + b?Var(ai) = 02; + oz; = 1/wi (15.3.3)

yi

The sum of the square of N random variables, each normalized by its variance, is thus
x?-distributed.

We want to minimize equation (15.3.2) with respect to a and b. Unfortunately, the
occurrence of b in the denominator of equation (15.3.2) makes the resulting equation for
the slope 0x?/2b = 0 nonlinear. However, the corresponding condition for the intercept,
Ox?/Aa = 0, is still linear and yields

a= b wilyi — 7) [>= (15.3.4)

where the w,’s are defined by equation (15.3.3). A reasonable strategy, now, is to use the
machinery of Chapter 10 (e.g., the routine brent) for minimizing a general one-dimensional
function to minimize with respect to b, while using equation (15.3.4) at each stage to ensure
that the minimum with respect to b is also minimized with respect to a.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.3 Straight-Line Data with Errors in Both Coordinates 667

Figure 15.3.1. Standard errors for the parameters a and b. The point B can be found by varying the
slope b while simultaneously minimizing the intercept a. This gives the standard error q,, and also the
value s. The standard error oq can then be found by the geometric relation o2 = s? + r?.

Because of the finite error bars on the 2;’s, the minimum x? as a function of b will
be finite, though usually large, when b equals infinity (line of infinite slope). The angle
6 = arctan b is thus more suitable as a parametrization of slope than b itself. The value of y?
will then be periodic in 6 with period 7 (not 27!). If any data points have very small oy ’s
but moderate or large oz ’s, then it is also possible to have a maximum in x? near zero slope,
6 = 0. In that case, there can sometimes be two x? minima, one at positive slope and the
other at negative. Only one of these is the correct global minimum. It is therefore important
to have a good starting guess for b (or @). Our strategy, implemented below, is to scale the
yi’ SO as to have variance equal to the x;’s, then to do a conventional (as in §15.2) linear fit
with weights derived from the (scaled) sum o ; +02. This yields a good starting guess for
b if the data are even plausibly related to a straight-line model.

Finding the standard errors 0, and 0 on the parameters a and 6 is more complicated.
We will see in §15.6 that, in appropriate circumstances, the standard errors in a and b are the
respective projections onto the a and b axes of the “confidence region boundary” where y?
takes on a value one greater than its minimum, Ax? = 1. In the linear case of §15.2, these
projections follow from the Taylor series expansion

21 a? Fate Fate
2 | da? Ob? Oadb

Because of the present nonlinearity in b, however, analytic formulas for the second derivatives
are quite unwieldy; more important, the lowest-order term frequently gives a poor approxima-
tion to Ay”. Our strategy is therefore to find the roots of Ay? = 1 numerically, by adjusting
the value of the slope b away from the minimum. In the program below the general root finder
zbrent is used. It may occur that there are no roots at all — for example, if all error bars are
so large that all the data points are compatible with each other. It is important, therefore, to
make some effort at bracketing a putative root before refining it (cf. §9.1).

Because a is minimized at each stage of varying b, successful numerical root-finding
leads to a value of Aa that minimizes y” for the value of Ab that gives Ax? = 1. This (see
Figure 15.3.1) directly gives the tangent projection of the confidence region onto the b axis,
and thus oy. It does not, however, give the tangent projection of the confidence region onto
the a axis. In the figure, we have found the point labeled B; to find a. we need to find the

Ax (Aa)? 4 (Ab)? | 4 AaAb (15.3.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

668 Chapter 15. Modeling of Data

point A. Geometry to the rescue: To the extent that the confidence region is approximated
by an ellipse, then you can prove (see figure) that 02 = r? + s”. The value of s is known
from having found the point B. The value of r follows from equations (15.3.2) and (15.3.3)
applied at the x? minimum (point O in the figure), giving

r= [SD (15.3.6)

Actually, since b can go through infinity, this whole procedure makes more sense in
(a,@) space than in (a,b) space. That is in fact how the following program works. Since
it is conventional, however, to return standard errors for a and 6, not a and @, we finally
use the relation

oy = 09/ cos’ 0 (15.3.7)

We caution that if b and its standard error are both large, so that the confidence region actually
includes infinite slope, then the standard error 0} is not very meaningful. The function chixy
is normally called only by the routine f£itexy. However, if you want, you can yourself explore
the confidence region by making repeated calls to chixy (whose argument is an angle @, not
a slope b), after a single initializing call to fitexy.

A final caution, repeated from §15.0, is that if the goodness-of-fit is not acceptable
(returned probability is too small), the standard errors o and o>» are surely not believable. In
dire circumstances, you might try scaling all your x and y error bars by a constant factor until
the probability is acceptable (0.5, say), to get more plausible values for gq and op.

#include <math.h>
#include "nrutil.h"
#define POTN 1.571000
#define BIG 1.0e30
#define PI 3.14159265
#define ACC 1.0e-3

int nn; Global variables communicate with
float *xx,*yy,*sx,*sy,*ww,aa,offs; chixy.

void fitexy(float x[], float y[], int ndat, float sigx[], float sigy[],
float *a, float *b, float *siga, float *sigb, float *chi2, float *q)
Straight-line fit to input data x[1..ndat] and y[1..ndat] with errors in both x and y, the re-
spective standard deviations being the input quantities sigx[1..ndat] and sigy[1..ndat].
Output quantities are a and b such that y = a + bx minimizes x?, whose value is returned
as chi2. The x? probability is returned as q, a small value indicating a poor fit (sometimes
indicating underestimated errors). Standard errors on a and b are returned as siga and sigb.
These are not meaningful if either (i) the fit is poor, or (ii) b is so large that the data are
consistent with a vertical (infinite b) line. If siga and sigh are returned as BIG, then the data
are consistent with all values of b.
{
void avevar(float data[], unsigned long n, float *ave, float *var);
float brent(float ax, float bx, float cx,
float (*f) (float), float tol, float *xmin);
float chixy(float bang);
void fit(float x[], float y[], int ndata, float sig[], int mut,
float *a, float *b, float *siga, float *sigb, float *chi2, float *q);
float gammq(float a, float x);
void mnbrak(float *ax, float *bx, float *cx, float *fa, float *fb,
float *fc, float (*func) (float));
float zbrent(float (*func) (float), float x1, float x2, float tol);
int j;
float swap,amx,amn,varx,vary,ang[7] ,ch[7] ,scale,bmn,bmx,d1,d2,r2,
dumi ,dum2,dum3 , dum4, dum5 ;

xx=vector(1,ndat) ;
yy=vector(1,ndat) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.3 Straight-Line Data with Errors in Both Coordinates 669

sx=vector(1,ndat) ;
sy=vector(1,ndat) ;
ww=vector(1,ndat) ;

avevar (x,ndat ,&dum1,&varx) ;
avevar (y ,ndat ,&dum1,&vary) ;
scale=sqrt (varx/vary) ;
nn=ndat ;

xx(jl=x[jl;

yy (jl=y[j]*scale;
sx[jl=sigx[j];

sy (j]=sigy[j]*scale;

ww[j]=sqrt (SQR(sx[j])+SQR(sy[j]));

+

fit(xx,yy,nn,ww,1,&dumi,b, &dum2, &dum3, &dum4, &dum5) ;

offs=ang[1]=0.0;
ang([2]=atan(*b) ;
ang[4]=0.0;

ang [5]=ang[2] ;

33 j++) ch[j]=chixy(anglj]);

Find the « and y variances, and scale
the data into the global variables
for communication with the func-
tion chixy.

Use both x and y weights in first
trial fit.

Trial fit for b.

Construct several angles for refer-
ence points, and make b an an-
gle.

mnbrak (&ang[1] ,&ang[2] ,zang[3] ,&ch[1] ,&ch[2] ,&ch[3] , chixy) ;
Bracket the x? minimum and then locate it with brent.
*chi2=brent (ang[1] ,ang[2] ,ang[3] , chixy ,ACC,b) ;

*chi2=chixy(*b) ;
*a=aa;
¥q=gammq(0.5* (nn-2) , *chi2*0.5) ;

for (r2=0.0,j=1;j<=nn;j++) r2 += wwIj];

r2=1.0/r2;
bmx=BIG;
bmn=BIG;

offs=(*chi2)+1.0;
for (j=1;j<=6;j++) {
if (ch[j] > offs) {
di=fabs (ang[j]-(*b));
while (di >= PI) di -= PI;
d2=PI-d1;
if (ang[j] < *b) f
swap=d1;
di=d2;
d2=swap;
3
if (di < bmx) bmx=d1;
if (d2 < bmn) bmn=d2;

}
+
if (bmx < BIG) {

bmx=zbrent (chixy, *b, *b+bmx , ACC) -(*b) ;

amx=aa-(*a) ;

bmn=zbrent (chixy, *b, *b-bmn, ACC) -(*b) ;

amn=aa-(*a) ;

Compute y? probability.

Save the inverse sum of weights at
the minimum.

Now, find standard errors for b as
points where Ay? = 1.

Go through saved values to bracket

the desired roots. Note period-
icity in slope angles.

Call zbrent to find the roots.

*sigb=sqrt (0.5* (bmx*bmx+bmn*bmn) ) / (scale*SQR(cos(*b))) ;

*siga=sqrt (0.5*(amx*amx+amn*amn)+r2)/scale;

} else (*sigb)=(*siga)=BIG;
*a /= scale;
*b=tan(*b)/scale;
free_vector(ww,1,ndat) ;
free_vector(sy,1,ndat) ;
free_vector(sx,1,ndat) ;
free_vector(yy,1,ndat) ;
free_vector(xx,1,ndat) ;

Error in a has additional piece
r2.
Unscale the answers.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

670 Chapter 15. Modeling of Data

#include <math.h>
#include "nrutil.h"
#define BIG 1.0e30

extern int nn;
extern float *xx,*yy,*sx,*sy,*ww,aa,offs;

float chixy(float bang)
Captive function of fitexy, returns the value of (vy? — offs) for the slope b=tan(bang) .
Scaled data and offs are communicated via the global variables.
t
int j;
float ans,avex=0.0,avey=0.0,sumw=0.0,b;

b=tan (bang) ;
for (j=1;j<=nn; j++) {
ww[j] = SQR(b*sx[j])+SQR(sy[j]);
sumw += (ww[j] = (ww[j] < 1.0/BIG ? BIG : 1.0/ww[j]));
avex += ww[j]*xx[j];
avey += ww[jl*yy[jl;
+
avex /= sumw;
avey /= sumw;
aa=avey-b*avex;
for (ans = -offs,j=1;j<=nn; j++)
ans += ww[j]*SQR(yy[j]-aa-b*xx[j]);
return ans;

Be aware that the literature on the seemingly straightforward subject of this secti
is generally confusing and sometimes plain wrong. Deming’s[1] early treatment
but its reliance on Taylor expansions gives inaccurate error estimates. References [2-4] are
reliable, more recent, general treatments with critiques of earlier work. York [5] and Reed [6]
usefully discuss the simple case of a straight line as treated here, but the latter paper has
some errors, corrected in[7]. All this commotion has attracted the Bayesians [8-10], who
have still different points of view.

CITED REFERENCES AND FURTHER READING:

Deming, W.E. 1943, Statistical Adjustment of Data (New York: Wiley), reprinted 1964 (New York:
Dover). [1]

Jefferys, W.H. 1980, Astronomical Journal, vol. 85, pp. 177-181; see also vol. 95, p. 1299
(1988). [2]

Jefferys, W.H. 1981, Astronomical Journal, vol. 86, pp. 149-155; see also vol. 95, p. 1300
(1988). [3]

Lybanon, M. 1984, American Journal of Physics, vol. 52, pp. 22-26. [4]

York, D. 1966, Canadian Journal of Physics, vol. 44, pp. 1079-1086. [5]

Reed, B.C. 1989, American Journal of Physics, vol. 57, pp. 642-646; see also vol. 58, p. 189,
and vol. 58, p. 1209. [6]

Reed, B.C. 1992, American Journal of Physics, vol. 60, pp. 59-62. [7]

Zellner, A. 1971, An Introduction to Bayesian Inference in Econometrics (New York: Wiley);
reprinted 1987 (Malabar, FL: R. E. Krieger Pub. Co.). [8]

Gull, S.F. 1989, in Maximum Entropy and Bayesian Methods, J. Skilling, ed. (Boston: Kluwer). [9]

Jaynes, E.T. 1991, in Maximum-Entropy and Bayesian Methods, Proc. 10th Int. Workshop,
W.T. Grandy, Jr., and L.H. Schick, eds. (Boston: Kluwer). [10]

Macdonald, J.R., and Thompson, W.J. 1992, American Journal of Physics, vol. 60, pp. 66-73.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.4 General Linear Least Squares 671

15.4 General Linear Least Squares

An immediate generalization of §15.2 is to fit a set of data points (x;, y;) toa
model that is not just a linear combination of 1 and x (namely a + bx), but rather a
linear combination of any M specified functions of x. For example, the functions
could be 1, x, 2?,...,2~!, in which case their general linear combination,

y(a) = ay + aga + aga? +--»+ayaM-! (15.4.1)

is a polynomial of degree / — 1. Or, the functions could be sines and cosines, in
which case their general linear combination is a harmonic series.
The general form of this kind of model is

M
k=1
where X1(x),...,Xs(z) are arbitrary fixed functions of x, called the basis

functions.
Note that the functions X,(2) can be wildly nonlinear functions of x. In this
discussion “linear” refers only to the model’s dependence on its parameters a,.
For these linear models we generalize the discussion of the previous section
by defining a merit function
2 S yi — Dk Xe (ci) (15.43)

OF

x

i=1

As before, a; is the measurement error (standard deviation) of the ith data point,

presumed to be known. If the measurement errors are not known, they may all (as
discussed at the end of §15.1) be set to the constant value o = 1.

Once again, we will pick as best parameters those that minimize y?. There are
several different techniques available for finding this minimum. Two are particularly
useful, and we will discuss both in this section. To introduce them and elucidate
their relationship, we need some notation.

Let A be a matrix whose NV x M components are constructed from the MW
basis functions evaluated at the N abscissas x;, and from the N measurement errors
oi, by the prescription

Ay =e (15.4.4)

The matrix A is called the design matrix of the fitting problem. Notice that in general
A has more rows than columns, N >, since there must be more data points than
model parameters to be solved for. (You can fit a straight line to two points, but not a
very meaningful quintic!) The design matrix is shown schematically in Figure 15.4.1.
Also define a vector b of length N by
Yi
bj = a (15.4.5)
and denote the M/Z vector whose components are the parameters to be fitted,
a1,.-.,am, by a.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

672 Chapter 15. Modeling of Data

<——— basis functions ———>

XC) XC) +++ XC )
~ xX Xi) HO) Xu)
Ol Or or
XQ Xi(X)_— XX) XX)
02 02 G2
2
Es
g
sg
XN Xi(xy)  -Xo(xy) | Xu(Xn)
v ON ON ON

Figure 15.4.1. Design matrix for the least-squares fit of a linear combination of M basis functions to N
data points. The matrix elements involve the basis functions evaluated at the values of the independent
variable at which measurements are made, and the standard deviations of the measured dependent variable.
The measured values of the dependent variable do not enter the design matrix.

Solution by Use of the Normal Equations

The minimum of (15.4.3) occurs where the derivative of y? with respect to all
M parameters a; vanishes. Specializing equation (15.1.7) to the case of the model
(15.4.2), this condition yields the MZ equations

N M
1
o=)> 3 yi-> aj; Xj(xi)} Xe(ai) k=1,...,M (15.4.6)
j=l

i=l t

Interchanging the order of summations, we can write (15.4.6) as the matrix equation

M
Yo anja = Br (15.4.7)
j=l

where

Xj (wi) Xx (wi)
Qnj = > cee or equivalently =AT-A (15.4.8)

an M x M matrix, and

N
i Xp (x; .
Be = > wn fe) or equivalently [A] =AT-b (15.4.9)
i=l i

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.4 General Linear Least Squares 673

a vector of length M.

The equations (15.4.6) or (15.4.7) are called the normal equations of the least-
squares problem. They can be solved for the vector of parameters a by the standard
methods of Chapter 2, notably LU decomposition and backsubstitution, Choleksy
decomposition, or Gauss-Jordan elimination. In matrix form, the normal equations
can be written as either

[a]-a= [6] — oras (AT-A)-a=A™-b (15.4.10)

The inverse matrix Cj, = olin is closely related to the probable (or, more
precisely, standard) uncertainties of the estimated parameters a. To estimate these
uncertainties, consider that

nae (15.4.1)

and that the variance associated with the estimate a; can be found as in (15.2.7) from

N \ 2
o7(a;) = S007 (52) (15.4.12)

i=1

Note that aj, is independent of y;, so that

M
Oa; 2

= Xp(ai)/02 15.4.13
=D CuXulo a (154.13)

Consequently, we find that

M M N
07(a;) = S> S> Cir Cyu Is Zoot) (15.4.14)

k=11=1

The final term in brackets is just the matrix [a]. Since this is the matrix inverse of
[C], (15.4.14) reduces immediately to

2

= C4; (15.4.15)

In other words, the diagonal elements of [C] are the variances (squared
uncertainties) of the fitted parameters a. It should not surprise you to learn that the
off-diagonal elements Cj, are the covariances between aj and ay, (cf. 15.2.10); but
we shall defer discussion of these to §15.6.

We will now give a routine that implements the above formulas for the general
linear least-squares problem, by the method of normal equations. Since we wish to
compute not only the solution vector a but also the covariance matrix [C], it is most
convenient to use Gauss-Jordan elimination (routine gaussj of §2.1) to perform the
linear algebra. The operation count, in this application, is no larger than that for LU
decomposition. If you have no need for the covariance matrix, however, you can
save a factor of 3 on the linear algebra by switching to LU decomposition, without

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

674 Chapter 15. Modeling of Data

computation of the matrix inverse. In theory, since A7 - A is positive definite,
Cholesky decomposition is the most efficient way to solve the normal equations.
However, in practice most of the computing time is spent in looping over the data
to form the equations, and Gauss-Jordan is quite adequate.

We need to warn you that the solution of a least-squares problem directly from
the normal equations is rather susceptible to roundoff error. An alternative, and
preferred, technique involves QR decomposition (§2.10, §11.3, and §11.6) of the
design matrix A. This is essentially what we did at the end of §15.2 for fitting data to
a straight line, but without invoking all the machinery of QR to derive the necessary
formulas. Later in this section, we will discuss other difficulties in the least-squares
problem, for which the cure is singular value decomposition (SVD), of which we give
an implementation. It turns out that SVD also fixes the roundoff problem, so it is our
recommended technique for all but “easy” least-squares problems. It is for these easy
problems that the following routine, which solves the normal equations, is intended.

The routine below introduces one bookkeeping trick that is quite useful in
practical work. Frequently it is a matter of “art” to decide which parameters a,
in a model should be fit from the data set, and which should be held constant at
fixed values, for example values predicted by a theory or measured in a previous
experiment. One wants, therefore, to have a convenient means for “freezing”
and “unfreezing” the parameters a;,. In the following routine the total number of
parameters aj, is denoted ma (called M above). As input to the routine, you supply
an array ia[1..ma], whose components are either zero or nonzero (e.g., 1). Zeros
indicate that you want the corresponding elements of the parameter vector a[1. .ma]
to be held fixed at their input values. Nonzeros indicate parameters that should be
fitted for. On output, any frozen parameters will have their variances, and all their
covariances, set to zero in the covariance matrix.

#include "nrutil.h"

void lfit(float x[], float y[], float sig[], int ndat, float a[], int ia[],

int ma, float **covar, float *chisq, void (*funcs) (float, float [], int))
Given a set of data points x[1..ndat], y[1..ndat] with individual standard deviations
sig[1..ndat], use x? minimization to fit for some or all of the coefficients a[1..ma] of
a function that depends linearly on a, y = )>, a; x afunc;(a). The input array ia[1..ma]
indicates by nonzero entries those components of a that should be fitted for, and by zero entries
those components that should be held fixed at their input values. The program returns values
for a{1..ma], y? = chisgq, and the covariance matrix covar[1..ma] [1..ma]. (Parameters
held fixed will return zero covariances.) The user supplies a routine funcs(x,afunc,ma) that
returns the ma basis functions evaluated at x = x in the array afunc[1..ma].
{

void covsrt(float **covar, int ma, int ia[], int mfit);

void gaussj(float **a, int n, float **b, int m);

int i,j,k,1,m,mfit=0;

float ym,wt,sum,sig2i,**beta,*afunc;

beta=matrix(1,ma,1,1);

afunc=vector(1,ma) ;

for (j=1;j<=ma; j++)
if (ialj]) mfit++;

if (mfit == 0) nrerror("lfit: no parameters to be fitted");

for (j=1 fit; j++) { Initialize the (symmetric) matrix.
for (k=1;k<=mfit;k++) covar[j] [k]=0.0;
beta[j] [1]=0.0;

}
for (i=1;i<=ndat;i++) { Loop over data to accumulate coefficients of

the normal equations.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.4 General Linear Least Squares 675

(*funcs) (x [i] ,afunc,ma) ;

ym=y [i];
if (mfit < ma) { Subtract off dependences on known pieces
for (j=1;j<=ma; j++) of the fitting function.

if (!ialj]) ym -= alj]*afunc[j];
+
sig2i=1.0/SQR(sig[i]);
for (j=0,1=1;1<=ma;1++) {

if Giall]) { gaege
wt=afunc[1]*sig2i; > Ey 3253
for (j++,k=0,m=1;m<=1;m++) Zo2ss
if (ialm]) covar[j][++k] += wt*afunc[m] ; = = 358
betalj] [1] += yme*wt; Beane
} 3388$
> gages
} 8258S
for (j=2;j<=mfit; j++) Fill in above the diagonal from symmetry. Ssi00
for (k=1;k<j;k++) ee e583
covar [k] [j]=covar[j] [kl]; %2a8 z
gaussj(covar,mfit,beta,1); Matrix solution. No © g Fs
for (j=0,1=1;1<=ma;1++) REEF ae)
if (ia[1]) a[1]=beta[++j] [1]; Partition solution to appropriate coefficients NS S c mn
*chisq=0.0; a. 383 22
for (i=1;i<=ndat;it++) { Evaluate x? of the fit. SeRxoz
A : 59880
(*funcs) (x [i] ,afunc,ma) ; pegs)
for (sum=0.0,j=1;j<=ma;j++) sum += a[j]*afunc[j]; 323 0z
; *chisq += SQR((y[i]-sum)/sigli]); e388 $ >
ao¢
a8o4
covsrt(covar,ma,ia,mfit) ; Sort covariance matrix to true order of fitting 8 A 8 3 fo}
free_vector(afunc,1,ma) ; coefficients. > ae $ o
free_matrix(beta,1,ma,1,1); SE530
3 S3so8
ages s
Fost
g2320
That last call to a function covsrt is only for the purpose of spreading the SIBS 8
covariances back into the full ma x ma covariance matrix, in the proper rows and 20825
: : . . : P@asagc
columns and with zero variances and covariances set for variables which were 22a 2e3
8sc8
held frozen. a 28 so
The function covsrt is as follows. S22eG
ORs=2
Bales
#define SWAP(a,b) {swap=(a) ; (a)=(b) ; (b)=swap;} e833
agsan
aoaes
void covsrt(float **covar, int ma, int ia[], int mfit) $ os pa
Expand in storage the covariance matrix Covar, so as to take into account parameters that are a 3 iy 8. 3
being held fixed. (For the latter, return zero covariances.) Poe i
£°°o8
Lown
int i,j,k; e923
float swap; -o8 5
sees
for (i=mfitt+1;i<=ma;it++) <6
for (j=1;j<=i;j++) covar [i] [j]=covar[j] [i]=0.0; 32S
ke=mfit; 383

for (j=ma;j>=1;j--) {
if Gialj]) ¢

for (i ma;it++) SWAP(covar [i] [k] ,covar [i] [j])
for ( ma;it++) SWAP(covar[k] [i] ,covar[j] [i])
k--;

676 Chapter 15. Modeling of Data

Solution by Use of Singular Value Decomposition

In some applications, the normal equations are perfectly adequate for linear
least-squares problems. However, in many cases the normal equations are very close
to singular. A zero pivot element may be encountered during the solution of the
linear equations (e.g., in gaussj), in which case you get no solution at all. Ora
very small pivot may occur, in which case you typically get fitted parameters a;
with very large magnitudes that are delicately (and unstably) balanced to cancel out
almost precisely when the fitted function is evaluated.

Why does this commonly occur? The reason is that, more often than experi-
menters would like to admit, data do not clearly distinguish between two or more of
the basis functions provided. If two such functions, or two different combinations
of functions, happen to fit the data about equally well — or equally badly — then
the matrix [a], unable to distinguish between them, neatly folds up its tent and
becomes singular. There is a certain mathematical irony in the fact that least-squares
problems are both overdetermined (number of data points greater than number of
parameters) and underdetermined (ambiguous combinations of parameters exist);
but that is how it frequently is. The ambiguities can be extremely hard to notice
a priori in complicated problems.

Enter singular value decomposition (SVD). This would be a good time for you
to review the material in §2.6, which we will not repeat here. In the case of an
overdetermined system, SVD produces a solution that is the best approximation in
the least-squares sense, cf. equation (2.6.10). That is exactly what we want. In the
case of an underdetermined system, SVD produces a solution whose values (for us,
the a;,’s) are smallest in the least-squares sense, cf. equation (2.6.8). That is also
what we want: When some combination of basis functions is irrelevant to the fit, that
combination will be driven down to a small, innocuous, value, rather than pushed
up to delicately canceling infinities.

In terms of the design matrix A (equation 15.4.4) and the vector b (equation
15.4.5), minimization of y? in (15.4.3) can be written as

find a thatminimizes y?=|A-a—b|° (15.4.16)

Comparing to equation (2.6.9), we see that this is precisely the problem that routines
svdcmp and svbksb are designed to solve. The solution, which is given by equation
(2.6.12), can be rewritten as follows: If U and V enter the SVD decomposition
of A according to equation (2.6.1), as computed by svdcmp, then let the vectors
Ui) i = 1,..., M denote the columns of U (each one a vector of length NV); and
let the vectors V(;);i = 1,...,M/ denote the columns of V (each one a vector
of length 17). Then the solution (2.6.12) of the least-squares problem (15.4.16)
can be written as

M
Ua +b

= —— } Vy; 15.417

a 3 ( wi (i) ( )

where the w; are, as in $2.6, the singular values calculated by svdcmp.
Equation (15.4.17) says that the fitted parameters a are linear combinations of
the columns of V, with coefficients obtained by forming dot products of the columns

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.4 General Linear Least Squares 677

of U with the weighted data vector (15.4.5). Though it is beyond our scope to prove
here, it turns out that the standard (loosely, “‘probable”) errors in the fitted parameters
are also linear combinations of the columns of V. In fact, equation (15.4.17) can
be written in a form displaying these errors as

[ M
Up) -b 1 1
i)
a > ( Wi ) vi = wy a oy VO (15.4.18)
Li=1 t U

Here each + is followed by a standard deviation. The amazing fact is that,
decomposed in this fashion, the standard deviations are all mutually independent
(uncorrelated). Therefore they can be added together in root-mean-square fashion.
What is going on is that the vectors V(;) are the principal axes of the error ellipsoid
of the fitted parameters a (see §15.6).

It follows that the variance in the estimate of a parameter a; is given by

M 1 M V; 2
2a) = Vols > (2) (15.4.19)
i=1 7 i=1 ‘

whose result should be identical with (15.4.14). As before, you should not be
surprised at the formula for the covariances, here given without proof,

M
Cov(aj,ax) = S> (2) (15.4.20)
i=l v

We introduced this subsection by noting that the normal equations can fail
by encountering a zero pivot. We have not yet, however, mentioned how SVD
overcomes this problem. The answer is: If any singular value w, is zero, its
reciprocal in equation (15.4.18) should be set to zero, not infinity. (Compare the
discussion preceding equation 2.6.7.) This corresponds to adding to the fitted
parameters a a zero multiple, rather than some random large multiple, of any linear
combination of basis functions that are degenerate in the fit. It is a good thing to do!

Moreover, if a singular value w,; is nonzero but very small, you should also
define its reciprocal to be zero, since its apparent value is probably an artifact of
roundoff error, not a meaningful number. A plausible answer to the question “how
small is small?” is to edit in this fashion all singular values whose ratio to the
largest singular value is less than N times the machine precision €. (You might
argue for VN, or a constant, instead of N as the multiple; that starts getting into
hardware-dependent questions.)

There is another reason for editing even additional singular values, ones large
enough that roundoff error is not a question. Singular value decomposition allows
you to identify linear combinations of variables that just happen not to contribute
much to reducing the y? of your data set. Editing these can sometimes reduce the
probable error on your coefficients quite significantly, while increasing the minimum
x? only negligibly. We will learn more about identifying and treating such cases
in $15.6. In the following routine, the point at which this kind of editing would
occur is indicated.

Generally speaking, we recommend that you always use SVD techniques instead
of using the normal equations. SVD’s only significant disadvantage is that it requires

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

678 Chapter 15. Modeling of Data

an extra array of size N x M to store the whole design matrix. This storage
is overwritten by the matrix U. Storage is also required for the M x M matrix
V, but this is instead of the same-sized matrix for the coefficients of the normal
equations. SVD can be significantly slower than solving the normal equations;
however, its great advantage, that it (theoretically) cannot fail, more than makes up
for the speed disadvantage.

In the routine that follows, the matrices u,v and the vector w are input as
working space. The logical dimensions of the problem are ndata data points by ma
basis functions (and fitted parameters). If you care only about the values a of the
fitted parameters, then u,v,w contain no useful information on output. If you want
probable errors for the fitted parameters, read on.

#include "nrutil.h"
#define TOL 1.0e-5 Default value for single precision and vari-
ables scaled to order unity.
void svdfit(float x[], float y[], float sig[], int ndata, float a[], int ma,
float **u, float **v, float w[], float *chisq,
void (*funcs) (float, float [], int))
Given a set of data points x[1..ndata],y[1..ndata] with individual standard deviations
sig[1..ndata], use y? minimization to determine the coefficients a[1..ma] of the fit-
ting function y = >>; a; x afunc;(x). Here we solve the fitting equations using singular
value decomposition of the ndata by ma matrix, as in §2.6. Arrays u[1..ndata] [1..ma],
v(1..ma][1..ma], and w[1..ma] provide workspace on input; on output they define the
singular value decomposition, and can be used to obtain the covariance matrix. The pro-
gram returns values for the ma fit parameters a, and x?, chisq. The user supplies a routine
funcs(x,afunc,ma) that returns the ma basis functions evaluated at « = x in the array
afunc[1..ma].

t
void svbksb(float **u, float w[], float **v, int m, int n, float b[],
float x{[]);
void svdemp(float **a, int m, int n, float wl], float **v);
int j,i;

float wmax,tmp,thresh,sum,*b,*afunc;

b=vector(1,ndata) ;
afunc=vector(1,ma) ;
for (i=1;i<=ndata;it+) { Accumulate coefficients of the fitting ma-
(*funcs) (x[i] ,afunc,ma) ; trix.
tmp=1.0/sig[il] ;
for (j=1;j<=ma; j++) uli] [j]=afunc[j]*tmp;
bli]=y [i] *tmp;

+
svdcmp(u,ndata,ma,w,v) ; Singular value decomposition.
wmax=0.0; Edit the singular values, given TOL from the
for (j=1;j<=ma; j++) #define statement, between here ...
if (w[j] > wmax) wmax=w[j];
thresh=TOL*wmax;
for (j=1;j<=ma; j++)
if (w[j] < thresh) w[j]=0.0; ..and here.
svbksb(u,w,v,ndata,ma,b,a);
*chisq=0.0; Evaluate chi-square.
for (i=1;i<=ndata;it+) {

(*funcs) (x [i] ,afunc,ma) ;
for (sum=0.0,j=1;j<=ma;j++) sum += a[j]*afunc[j];
*chisq += (tmp=(y[i]-sum)/sig[i] ,tmp*tmp) ;

}

free_vector(afunc,1,ma) ;

free_vector(b,1,ndata) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.4 General Linear Least Squares 679

Feeding the matrix v and vector w output by the above program into the
following short routine, you easily obtain variances and covariances of the fitted
parameters a. The square roots of the variances are the standard deviations of
the fitted parameters. The routine straightforwardly implements equation (15.4.20)
above, with the convention that singular values equal to zero are recognized as
having been edited out of the fit.

#include "nrutil.h"

void svdvar(float **v, int ma, float wf], float **cvm)

To evaluate the covariance matrix cvm[1..ma] [1..ma] of the fit for ma parameters obtained
by svdfit, call this routine with matrices v[1..ma][1..ma], w[1..ma] as returned from
svdfit.

{
int k,j,i;
float sum, *wti;
wti=vector(1,ma) ;
for (i=1;i<=ma;i++) {
wtili]=0.0;
if (wli]) wtili]=1.0/(wli]*wli]);
ma;i++) { Sum contributions to covariance matrix (15.4.20).
sj<sisjt) {
.0,k=1;k<=ma;k++) sum += v(i] (k] *v[j] (k] *wtilkl ;
cvm[j] [i]=cvm(i] (j]=sum;
}
}
free_vector(wti,1,ma) ;
3
Examples

Be aware that some apparently nonlinear problems can be expressed so that
they are linear. For example, an exponential model with two parameters a and b,

y(«) = aexp(—bz) (15.4.21)
can be rewritten as
logly(x)| = ¢ — bx (15.4.22)

which is linear in its parameters c and b. (Of course you must be aware that such
transformations do not exactly take Gaussian errors into Gaussian errors.)
Also watch out for “non-parameters,” as in

y(«) = aexp(—bx + d) (15.4.23)

Here the parameters a and d are, in fact, indistinguishable. This is a good example of
where the normal equations will be exactly singular, and where SVD will find a zero
singular value. SVD will then make a “least-squares” choice for setting a balance
between a and d (or, rather, their equivalents in the linear model derived by taking the
logarithms). However — and this is true whenever SVD gives back a zero singular
value — you are better advised to figure out analytically where the degeneracy is
among your basis functions, and then make appropriate deletions in the basis set.

Here are two examples for user-supplied routines funcs. The first one is trivial
and fits a general polynomial to a set of data:

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

680 Chapter 15. Modeling of Data

void fpoly(float x, float p[], int np)

Fitting routine for a polynomial of degree np-1, with coefficients in the array p[1..np].

{
int j;
pl1]=1.0;
for (j=2;j<=np;j++) pljl=p[j-1] *x;

The second example is slightly less trivial. It is used to fit Legendre polynomials
up to some order nl-1 through a data set.

void fleg(float x, float pl[], int nl)
Fitting routine for an expansion with nl Legendre polynomials p1, evaluated using the recurrence
relation as in §5.5.
{
int j;
float twox,f2,f1,d;

pl[1]=1.0;
pi[2]=x;

if (nl > 2) {
twox=2.0*x;

d=1.0
for (j=3;j<=nl;jt+) {
fil=d++;
£2 += twox;
pl[j]=(£2*pl (j-1]-£1*p1[j-2])/d;

Multidimensional Fits

If you are measuring a single variable y as a function of more than one variable
— say, a vector of variables x, then your basis functions will be functions of a vector,

Xi(x),...,Xa(x). The x? merit function is now
N M 2
2 Yi Vet aX ke (Xi)
15.4.24
vad * (15.4.24)

i=1

All of the preceding discussion goes through unchanged, with x replaced by x. In
fact, if you are willing to tolerate a bit of programming hack, you can use the above
programs without any modification: In both 1fit and svdfit, the only use made
of the array elements x [i] is that each element is in turn passed to the user-supplied
routine funcs, which duly gives back the values of the basis functions at that point.
If you set x [i] =i before calling 1fit or svdfit, and independently provide funcs
with the true vector values of your data points (e.g., in global variables), then funcs
can translate from the fictitious x [i]’s to the actual data points before doing its work.

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:

McGraw-Hill), Chapters 8-9.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.5 Nonlinear Models 681

Lawson, C.L., and Hanson, R. 1974, Solving Least Squares Problems (Englewood Cliffs, NJ:
Prentice-Hall).

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall), Chapter 9.

15.5 Nonlinear Models

We now consider fitting when the model depends nonlinearly on the set of MZ
unknown parameters a;,,k = 1,2,..., 1. We use the same approach as in previous
sections, namely to define a y? merit function and determine best-fit parameters
by its minimization. With nonlinear dependences, however, the minimization must
proceed iteratively. Given trial values for the parameters, we develop a procedure
that improves the trial solution. The procedure is then repeated until y? stops (or
effectively stops) decreasing.

How is this problem different from the general nonlinear function minimization
problem already dealt with in Chapter 10? Superficially, not at all: Sufficiently
close to the minimum, we expect the y” function to be well approximated by a
quadratic form, which we can write as

VQ) 27 deat SarDea (15.5.1)

where d is an M-vector and D is an M x M matrix. (Compare equation 10.6.1.)
If the approximation is a good one, we know how to jump from the current trial
parameters ay, to the minimizing ones a;pin in a single leap, namely

amin = Acur + D7! - [—Vx?(acur)] (15.5.2)

(Compare equation 10.7.4.)

On the other hand, (15.5.1) might be a poor local approximation to the shape
of the function that we are trying to minimize at a.y;. In that case, about all we
can do is take a step down the gradient, as in the steepest descent method (§10.6).
In other words,

Anext = Acur — Constant x Vx? (acur) (15.5.3)

where the constant is small enough not to exhaust the downhill direction.

To use (15.5.2) or (15.5.3), we must be able to compute the gradient of the y ”
function at any set of parameters a. To use (15.5.2) we also need the matrix D, which
is the second derivative matrix (Hessian matrix) of the x? merit function, at any a.

Now, this is the crucial difference from Chapter 10: There, we had no way of
directly evaluating the Hessian matrix. We were given only the ability to evaluate
the function to be minimized and (in some cases) its gradient. Therefore, we had
to resort to iterative methods not just because our function was nonlinear, but also
in order to build up information about the Hessian matrix. Sections 10.7 and 10.6
concerned themselves with two different techniques for building up this information.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

682 Chapter 15. Modeling of Data

Here, life is much simpler. We know exactly the form of y?, since it is based
on a model function that we ourselves have specified. Therefore the Hessian matrix
is known to us. Thus we are free to use (15.5.2) whenever we care to do so. The
only reason to use (15.5.3) will be failure of (15.5.2) to improve the fit, signaling
failure of (15.5.1) as a good local approximation.

Calculation of the Gradient and Hessian

The model to be fitted is

y = y(a3a) (15.5.4)
and the \? merit function is
fui — ylwisa) |?
2a)= So) 4 15.5.5
e@)=y [| (1555)

i=1

The gradient of y? with respect to the parameters a, which will be zero at the ?
minimum, has components

ax? [yi — y(wisa)] Oy(wisa)
Dar > 7 Tar k=1,2,...,M (15.5.6)
Taking an additional partial derivative gives
07x? NL [dy(a;3a) Oy(xi;a) 0? y(xi3a)
2 - - - — = [yi — y(xi3.a)| ——— 15.5.7
Oa,0a; > o? Oar Oa, [yi — y(risa)] Oa/Oax ( )

It is conventional to remove the factors of 2 by defining

1 0y? 1 07x?
Bb, == — 15.5.8
Ps 2 Oar Ont 2 Oaz0a, ( )
making [a] = 3D in equation (15.5.2), in terms of which that equation can be
rewritten as the set of linear equations
M
> Ag Oa, = Be (15.5.9)
l=1

This set is solved for the increments da; that, added to the current approximation,
give the next approximation. In the context of least-squares, the matrix [a], equal to
one-half times the Hessian matrix, is usually called the curvature matrix.

Equation (15.5.3), the steepest descent formula, translates to

da, = constant x (3; (15.5.10)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

15.5 Nonlinear Models 683

Note that the components a; of the Hessian matrix (15.5.7) depend both on the
first derivatives and on the second derivatives of the basis functions with respect to
their parameters. Some treatments proceed to ignore the second derivative without
comment. We will ignore it also, but only after a few comments.

Second derivatives occur because the gradient (15.5.6) already has a dependence
on Oy/0ax, so the next derivative simply must contain terms involving 0?y/aj0ax.
The second derivative term can be dismissed when it is zero (as in the linear case
of equation 15.4.8), or small enough to be negligible when compared to the term
involving the first derivative. It also has an additional possibility of being ignorably
small in practice: The term multiplying the second derivative in equation (15.5.7)
is [y; — y(xi;a)]. For a successful model, this term should just be the random
measurement error of each point. This error can have either sign, and should in
general be uncorrelated with the model. Therefore, the second derivative terms tend
to cancel out when summed over i.

Inclusion of the second-derivative term can in fact be destabilizing if the model
fits badly or is contaminated by outlier points that are unlikely to be offset by
compensating points of opposite sign. From this point on, we will always use as
the definition of aj, the formula

(15.5.11)

N
1 [Oy(ai;a) Oy(wis a)
Ont > o? Oar Oa;

This expression more closely resembles its linear cousin (15.4.8). You should
understand that minor (or even major) fiddling with [a] has no effect at all on
what final set of parameters a is reached, but affects only the iterative route that is
taken in getting there. The condition at the y? minimum, that 3, = 0 for all k,
is independent of how [a] is defined.

Levenberg-Marquardt Method

Marquardt [1] has put forth an elegant method, related to an earlier suggestion of
Levenberg, for varying smoothly between the extremes of the inverse-Hessian method
(15.5.9) and the steepest descent method (15.5.10). The latter method is used far from
the minimum, switching continuously to the former as the minimum is approached.
This Levenberg-Marquardt method (also called Marquardt method) works very well
in practice and has become the standard of nonlinear least-squares routines.

The method is based on two elementary, but important, insights. Consider the
“constant” in equation (15.5.10). What should it be, even in order of magnitude?
What sets its scale? There is no information about the answer in the gradient.
That tells only the slope, not how far that slope extends. Marquardt’s first insight
is that the components of the Hessian matrix, even if they are not usable in any
precise fashion, give some information about the order-of-magnitude scale of the
problem.

The quantity x? is nondimensional, i.c., is a pure number; this is evident from
its definition (15.5.5). On the other hand, 3; has the dimensions of 1/a;, which
may well be dimensional, i.e., have units like cm | or kilowatt-hours, or whatever.
(In fact, each component of (;, can have different dimensions!) The constant of
proportionality between 3, and da, must therefore have the dimensions of a?.. Scan

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

684 Chapter 15. Modeling of Data

the components of [a] and you see that there is only one obvious quantity with these
dimensions, and that is 1/a,,, the reciprocal of the diagonal element. So that must
set the scale of the constant. But that scale might itself be too big. So let’s divide
the constant by some (nondimensional) fudge factor \, with the possibility of setting
A > 1 to cut down the step. In other words, replace equation (15.5.10) by

1
da, = —— fy or Aa ba, = 3 (15.5.12)
dat

It is necessary that aj, be positive, but this is guaranteed by definition (15.5.11) —
another reason for adopting that equation.

Marquardt’s second insight is that equations (15.5.12) and (15.5.9) can be
combined if we define a new matrix a’ by the following prescription

ay, = ayj(1+ A)

. (15.5.13)
jn = ayn (J #k)
and then replace both (15.5.12) and (15.5.9) by
M
SP oy Sar = Br (15.5.14)
l=1

When 4 is very large, the matrix a’ is forced into being diagonally dominant, so
equation (15.5.14) goes over to be identical to (15.5.12). On the other hand, as 4
approaches zero, equation (15.5.14) goes over to (15.5.9).

Given an initial guess for the set of fitted parameters a, the recommended
Marquardt recipe is as follows:

© Compute x(a).

e Pick a modest value for A, say A = 0.001.

© (;) Solve the linear equations (15.5.14) for da and evaluate x(a + da).

e If y7(a + 6a) >yx(a), increase \ by a factor of 10 (or any other

substantial factor) and go back to ({).

e If y?(a + da) < x?(a), decrease \ by a factor of 10, update the trial

solution a — a + da, and go back to (f).

Also necessary is a condition for stopping. Iterating to convergence (to machine
accuracy or to the roundoff limit) is generally wasteful and unnecessary since the
minimum is at best only a statistical estimate of the parameters a. As we will see
in §15.6, a change in the parameters that changes x? by an amount < 1 is never
statistically meaningful.

Furthermore, it is not uncommon to find the parameters wandering
around near the minimum in a flat valley of complicated topography. The rea-
son is that Marquardt’s method generalizes the method of normal equations (§15.4),
hence has the same problem as that method with regard to near-degeneracy of the
minimum. Outright failure by a zero pivot is possible, but unlikely. More often,
a small pivot will generate a large correction which is then rejected, the value of
A being then increased. For sufficiently large \ the matrix [a’] is positive definite
and can have no small pivots. Thus the method does tend to stay away from zero

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

15.5 Nonlinear Models 685

pivots, but at the cost of a tendency to wander around doing steepest descent in
very un-steep degenerate valleys.

These considerations suggest that, in practice, one might as well stop iterating
on the first or second occasion that x? decreases by a negligible amount, say either
less than 0.01 absolutely or (in case roundoff prevents that being reached) some
fractional amount like 10~*. Don’t stop after a step where x? increases: That only
shows that \ has not yet adjusted itself optimally.

Once the acceptable minimum has been found, one wants to set \ = 0 and
compute the matrix

(C] = {a}! (15.5.15)

which, as before, is the estimated covariance matrix of the standard errors in the
fitted parameters a (see next section).

The following pair of functions encodes Marquardt’s method for nonlinear
parameter estimation. Much of the organization matches that used in 1fit of §15.4.
In particular the array ia[1..ma] must be input with components one or zero
corresponding to whether the respective parameter values a[1..ma] are to be fitted
for or held fixed at their input values, respectively.

The routine mrqmin performs one iteration of Marquardt’s method. It is first
called (once) with alamda < 0, which signals the routine to initialize. alamda is set
on the first and all subsequent calls to the suggested value of for the next iteration;
a and chisq are always given back as the best parameters found so far and their
y?. When convergence is deemed satisfactory, set alamda to zero before a final call.
The matrices alpha and covar (which were used as workspace in all previous calls)
will then be set to the curvature and covariance matrices for the converged parameter
values. The arguments alpha, a, and chisq must not be modified between calls,
nor should alamda be, except to set it to zero for the final call. When an uphill
step is taken, chisq and a are given back with their input (best) values, but alamda
is set to an increased value.

The routine mrqmin calls the routine mrqcof for the computation of the matrix
[a] (equation 15.5.11) and vector 3 (equations 15.5.6 and 15.5.8). In turn mrqcof
calls the user-supplied routine funcs(x,a,y,dyda), which for input values x = 2 ;
and a = a calculates the model function y = y(x,;a) and the vector of derivatives
dyda = Oy/Oax.

#include "nrutil.h"

void mrqmin(float x[], float y[], float sig[], int ndata, float a[], int ial],

int ma, float **covar, float **alpha, float *chisq,

void (*funcs) (float, float [], float *, float [], int), float *alamda)
Levenberg-Marquardt method, attempting to reduce the value x? of a fit between a set of data
points x[1..ndata], y[1..ndata] with individual standard deviations sig[1..ndata],
and a nonlinear function dependent on ma coefficients a[1..ma]. The input array ia[1..ma]
indicates by nonzero entries those components of a that should be fitted for, and by zero
entries those components that should be held fixed at their input values. The program re-
turns current best-fit values for the parameters a[1..ma], and y? = chisq. The arrays
covar[1..ma][1..ma], alpha[1..ma][1..ma] are used as working space during most
iterations. Supply a routine funcs(x,a,yfit,dyda,ma) that evaluates the fitting function
yfit, and its derivatives dyda[1..ma] with respect to the fitting parameters a at x. On
the first call provide an initial guess for the parameters a, and set alamda<0O for initialization
(which then sets alamda=.001). If a step succeeds chisq becomes smaller and alamda de-
creases by a factor of 10. If a step fails alamda grows by a factor of 10. You must call this

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

686 Chapter 15. Modeling of Data

routine repeatedly until convergence is achieved. Then, make one final call with alamda=0, so
that covar[1..ma] [1..ma] returns the covariance matrix, and alpha the curvature matrix.
(Parameters held fixed will return zero covariances.)

{
void covsrt(float **covar, int ma, int ia[], int mfit);
void gaussj(float **a, int n, float **b, int m);
void mrqcof(float x[], float y[], float sig[], int ndata, float a[],
int ia[], int ma, float **alpha, float beta[], float *chisq, zg 000
void (*funcs) (float, float [], float *, float [], int)); Baggs
int j,k,1; S83 3
static int mfit; Zo2ss
static float ochisq,*atry,*beta, *da, **oneda; = F258
gaqus
if (*alamda < 0.0) { Initialization. gs as gs
atry=vector(1,ma) ; ebsts
beta=vector(1,ma) ; 8 2 Fs 8 s
da=vector(1,ma) ; IS > . oi
for (mfit=0, j=1;j<=ma; j++) ee B55
if Gialj]) mfit+; S3ase
oneda=matrix(1,mfit,1,1); Nog ea
*alamda=0.001; NE 280
mrqcof (x,y,sig,ndata,a,ia,ma,alpha, beta, chisq,funcs) ; NS S c mn
ochisq=(*chisq) ; 3332 Q
for (j=1;j<+ma;j++) atry[j]=alj]; SoRce
} 38025
for (j=1;j<=mfit;j++) { Alter linearized fitting matrix, by augmenting di- 333 DL
for (k=1;k<=mfit;k++) covar[j] [k]=alpha[j] [k]; agonal elements. 253 $ z
covar[j] [j]=alpha[j] [j]*(1.0+(*alamda)) ; Sogn
oneda[j] [1]=beta[j]; 2a 850
} Ss5en
. . . . oss 2on
gaussj (covar,mfit,oneda,1); Matrix solution. at s 2 le}
for (j=1;j<=mfit;j++) dalj]=oneda[j] [1]; 83so08
if (*alamda == 0.0) { Once converged, evaluate covariance matrix. ases =
covsrt (covar,ma,ia,mfit) ; 3 e2eo
covsrt (alpha,ma,ia,mfit) ; Spread out alpha to its full size too. = ps Ewe)
free_matrix(oneda,1,mfit,1,1); S386
free_vector(da,1,ma) ; 3 38 oe
free_vector(beta,1,ma) ; §e2R5
free_vector(atry,1,ma) ; a Z& 3 6
return; S25 RE
} agscs
for (j=0,1=1;1<=ma;1++) Did the trial succeed? 3 tcl Zo
if Giaf1]) atry(1]=a[1]+dal++j]; 223 3 a
mrqcof (x,y,sig,ndata,atry,ia,ma,covar,da,chisq,funcs) ; 2953 = re
if (*chisq < ochisq) { Success, accept the new solution. 3 oe Po
*alamda *= 0.1; 88228
ochisq=(*chisq) ; Bardo
for (j=1;j<=mfit;j++) { £298
fit;k++) alpha(j] (k]=covar[j] (kl; a9eg
betalj]=daljl; 2638 A
t 2233
for (1=1;1<=ma;1++) a[l]=atry([1]; Se 2
} else { Failure, increase alamda and return. 2 Bo
*alamda *= 10.0; ee3
*chisq=ochisq; £38
+
3

Notice the use of the routine covsrt from §15.4. This is merely for rearranging
the covariance matrix covar into the order of all ma parameters. The above routine
also makes use of

15.5 Nonlinear Models 687

#include "nrutil.h"

void mrqcof(float x[], float y[], float sig[], int ndata, float a[], int ial],
int ma, float **alpha, float beta[], float *chisq,
void (*funcs)(float, float [], float *, float [], int))
Used by mrqmin to evaluate the linearized fitting matrix alpha, and vector beta as in (15.5.8),
and calculate y?.

{
int i,j,k,1,m,mfit=0;
float ymod,wt,sig2i,dy,*dyda;
dyda=vector(1,ma) ;
for (j=1;j<=ma; j++)
if (ialj]) mfit++;
for (j=1;j<=mfit;jt++) { Initialize (symmetric) alpha, beta.
for (k=1;k<=j;k++) alpha[j] [k]=0.0;
beta[j]=0.0;
ji<=ndata;it+) { Summation loop over all data.
(*funcs) (x[i] ,a,&ymod,dyda,ma) ;
sig2i=1.0/(siglil*sig[i]);
dy=y [i]-ymod;
for (j=0,1=1;1<=ma;1++) {
if (ia(l]) {
wt=dyda [1] *sig2i;
for (j++,k=0,m=1;m<=1;m++)
if (ialm]) alpha(j] [++k] += wt*dyda[m] ;
betalj] += dy*wt;
3
}
xchisq += dy*dy*sig2i; And find x?.
}
for (j=2;j<=mfit; j++) Fill in the symmetric side.
for (k=1;k<j;k++) alpha[k] [j]=alpha[j] [k];
free_vector (dyda,1,ma) ;
}
Example

The following function fgauss is an example of a user-supplied function
funcs. Used with the above routine mrqmin (in turn using mrqcof, covsrt, and
gaussj), it fits for the model

K 2—-E 2
y(x) = > By exp |- (—) (15.5.16)
k=1

which is a sum of /v Gaussians, each having a variable position, amplitude, and
width. We store the parameters in the order By, F),G1, Bo, E2,G2,...,Br,
Ex,Gr.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

688 Chapter 15. Modeling of Data

#include <math.h>

void fgauss(float x, float a[], float *y, float dyda[], int na)
y(a;a) is the sum of na/3 Gaussians (15.5.16). The amplitude, center, and width of the
Gaussians are stored in consecutive locations of a: afi] = B,, alit+1] = E,, alit2] = Gy,
k = 1,...,.na/3. The dimensions of the arrays are a[1..na], dyda[1..na].
{

int i;

float fac,ex,arg;

for (i=1;i<=na-1;i+=3) {
arg=(x-a[i+1])/a[i+2];
ex=exp(-arg*arg) ;
fac=a[i] *ex*2.0*arg;
ay += ali]*ex;
dyda[i]=ex;
dyda[it+1]=fac/a[i+2];
dyda [i+2]=fac*arg/a[it2] ;

More Advanced Methods for Nonlinear Least Squares

The Levenberg-Marquardt algorithm can be implemented as a model-trust
region method for minimization (see §9.7 and ref. [2]) applied to the special case
of a least squares function. A code of this kind due to Moré [3] can be found in
MINPACK [4]. Another algorithm for nonlinear least-squares keeps the second-
derivative term we dropped in the Levenberg-Marquardt method whenever it would
be better to do so. These methods are called “full Newton-type” methods and
are reputed to be more robust than Levenberg-Marquardt, but more complex. One
implementation is the code NL2SOL [5].

CITED REFERENCES AND FURTHER READING:

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences (New York:
McGraw-Hill), Chapter 11.

Marquardt, D.W. 1963, Journal of the Society for Industrial and Applied Mathematics, vol. 11,
pp. 431-441. [1]

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London: Academic Press),
Chapter III.2 (by J.E. Dennis).

Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Englewood Cliffs, NJ: Prentice-Hall). [2]

Moré, J.J. 1977, in Numerical Analysis, Lecture Notes in Mathematics, vol. 630, G.A. Watson,
ed. (Berlin: Springer-Verlag), pp. 105-116. [3]

Moré, J.J., Garbow, B.S., and Hillstrom, K.E. 1980, User Guide for MINPACK-1, Argonne National
Laboratory Report ANL-80-74. [4]

Dennis, J.E., Gay, D.M, and Welsch, R.E. 1981, ACM Transactions on Mathematical Software,
vol. 7, pp. 348-368; op. cit., pp. 369-383. [5].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.6 Confidence Limits on Estimated Model Parameters 689

15.6 Confidence Limits on Estimated Model
Parameters

Several times already in this chapter we have made statements about the standard
errors, or uncertainties, in a set of M estimated parameters a. We have given some
formulas for computing standard deviations or variances of individual parameters
(equations 15.2.9, 15.4.15, 15.4.19), as well as some formulas for covariances
between pairs of parameters (equation 15.2.10; remark following equation 15.4.15;
equation 15.4.20; equation 15.5.15).

In this section, we want to be more explicit regarding the precise meaning
of these quantitative uncertainties, and to give further information about how
quantitative confidence limits on fitted parameters can be estimated. The subject
can get somewhat technical, and even somewhat confusing, so we will try to make
precise statements, even when they must be offered without proof.

Figure 15.6.1 shows the conceptual scheme of an experiment that “measures”
a set of parameters. There is some underlying true set of parameters a true that are
known to Mother Nature but hidden from the experimenter. These true parameters
are statistically realized, along with random measurement errors, as a measured data
set, which we will symbolize as Dg). The data set D(q) is known to the experimenter.
He or she fits the data to a model by x? minimization or some other technique, and
obtains measured, i.e., fitted, values for the parameters, which we here denote a (0):

Because measurement errors have a random component, D (q) is not a unique
realization of the true parameters atrue. Rather, there are infinitely many other
realizations of the true parameters as “hypothetical data sets” each of which could
have been the one measured, but happened not to be. Let us symbolize these
by Day, DPia),.... Each one, had it been realized, would have given a slightly
different set of fitted parameters, a(1),a(2),-.., respectively. These parameter sets
ai) therefore occur with some probability distribution in the /-dimensional space
of all possible parameter sets a. The actual measured set a (q) is one member drawn
from this distribution.

Even more interesting than the probability distribution of a (;) would be the
distribution of the difference (i) — Atrue- This distribution differs from the former
one by a translation that puts Mother Nature’s true value at the origin. If we knew this
distribution, we would know everything that there is to know about the quantitative
uncertainties in our experimental measurement a (9).

So the name of the game is to find some way of estimating or approximating
the probability distribution of a(;) — atrue without knowing atrue and without having
available to us an infinite universe of hypothetical data sets.

Monte Carlo Simulation of Synthetic Data Sets

Although the measured parameter set aq) is not the true one, let us consider
a fictitious world in which it was the true one. Since we hope that our measured
parameters are not too wrong, we hope that that fictitious world is not too different
from the actual world with parameters at;ue. In particular, let us hope — no, let us
assume — that the shape of the probability distribution a (;) — a(q) in the fictitious
world is the same, or very nearly the same, as the shape of the probability distribution

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

690 Chapter 15. Modeling of Data

fitted
actual data set - parameters
ao
hypothetical a
ata set '
true parameters
Atrue
hypothetical a
2
jata set
hypothetical a3
jata set

Figure 15.6.1. A statistical universe of data sets from an underlying model. True parameters atrue are
realized in a data set, from which fitted (observed) parameters ag are obtained. If the experiment were
repeated many times, new data sets and new values of the fitted parameters would be obtained.

a(i) — Atrue in the real world. Notice that we are not assuming that a (0) and atrue are
equal; they are certainly not. We are only assuming that the way in which random
errors enter the experiment and data analysis does not vary rapidly as a function of
Atrue, SO that ao) can serve as a reasonable surrogate.

Now, often, the distribution of a(;) — ao) in the fictitious world is within our
power to calculate (see Figure 15.6.2). If we know something about the process
that generated our data, given an assumed set of parameters ao), then we can
usually figure out how to simulate our own sets of “synthetic” realizations of these
parameters as “synthetic data sets.” The procedure is to draw random numbers from
appropriate distributions (cf. §7.2-§7.3) so as to mimic our best understanding of
the underlying process and measurement errors in our apparatus. With such random
draws, we construct data sets with exactly the same numbers of measured points, and
precisely the same values of all control (independent) variables, as our actual data set
Do). Let us call these simulated data sets Day ; Dy, .... By construction these are
supposed to have exactly the same statistical relationship to ao) as the Di,)’s have
tO Atrue. (For the case where you don’t know enough about what you are measuring
to do a credible job of simulating it, see below.)

Next, for each De). perform exactly the same procedure for estimation of

parameters, e.g., y” minimization, as was performed on the actual data to get
the parameters ag), giving simulated measured parameters ay aQy geen Each

simulated measured parameter set yields a point avy — ao). Simulate enough data
sets and enough derived simulated measured parameters, and you map out the desired
probability distribution in M dimensions.

In fact, the ability to do Monte Carlo simulations in this fashion has revo-

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.6 Confidence Limits on Estimated Model Parameters 691

Monte Carlo

parameters
()
ay

actual
data set

parameters
ao

)
a3

(s)

synthetic a§

lata set 4

Figure 15.6.2. Monte Carlo simulation of an experiment. The fitted parameters from an actual experiment
are used as surrogates for the true parameters. Computer-generated random numbers are used to simulate
many synthetic data sets. Each of these is analyzed to obtain its fitted parameters. The distribution of
these fitted parameters around the (known) surrogate true parameters is thus studied.

lutionized many fields of modern experimental science. Not only is one able to
characterize the errors of parameter estimation in a very precise way; one can also
try out on the computer different methods of parameter estimation, or different data
reduction techniques, and seek to minimize the uncertainty of the result according
to any desired criteria. Offered the choice between mastery of a five-foot shelf of
analytical statistics books and middling ability at performing statistical Monte Carlo
simulations, we would surely choose to have the latter skill.

Quick-and-Dirty Monte Carlo: The Bootstrap Method

Here is a powerful technique that can often be used when you don’t know
enough about the underlying process, or the nature of your measurement errors,
to do a credible Monte Carlo simulation. Suppose that your data set consists of
N independent and identically distributed (or iid) “data points.” Each data point
probably consists of several numbers, e.g., one or more control variables (uniformly
distributed, say, in the range that you have decided to measure) and one or more
associated measured values (each distributed however Mother Nature chooses). “lid”
means that the sequential order of the data points is not of consequence to the process
that you are using to get the fitted parameters a. For example, a y7 sum like
(15.5.5) does not care in what order the points are added. Even simpler examples
are the mean value of a measured quantity, or the mean of some function of the
measured quantities.

The bootstrap method [1] uses the actual data set Diy , with its NV data points, to

generate any number of synthetic data sets Diy: Diy ;--+, also with NV data points.
The procedure is simply to draw N data points at a time with replacement from the

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

692 Chapter 15. Modeling of Data

set Di . Because of the replacement, you do not simply get back your original
data set each time. You get sets in which a random fraction of the original points,
typically ~ 1/e ~ 37%, are replaced by duplicated original points. Now, exactly
as in the previous discussion, you subject these data sets to the same estimation
procedure as was performed on the actual data, giving a set of simulated measured
parameters an 5 as, »++++ These will be distributed around a (q) in close to the same
way that ao) is distributed around atruc-

Sounds like getting something for nothing, doesn’t it? In fact, it has taken more
than a decade for the bootstrap method to become accepted by statisticians. By now,
however, enough theorems have been proved to render the bootstrap reputable (see [2]
for references). The basic idea behind the bootstrap is that the actual data set, viewed
as a probability distribution consisting of delta functions at the measured values, is
in most cases the best — or only — available estimator of the underlying probability
distribution. It takes courage, but one can often simply use that distribution as the
basis for Monte Carlo simulations.

Watch out for cases where the bootstrap’s “iid” assumption is violated. For
example, if you have made measurements at evenly spaced intervals of some control
variable, then you can usually get away with pretending that these are “iid,” uniformly
distributed over the measured range. However, some estimators of a (e.g., ones
involving Fourier methods) might be particularly sensitive to all the points on a grid
being present. In that case, the bootstrap is going to give a wrong distribution. Also
watch out for estimators that look at anything like small-scale clumpiness within the
N data points, or estimators that sort the data and look at sequential differences.
Obviously the bootstrap will fail on these, too. (The theorems justifying the method
are still true, but some of their technical assumptions are violated by these examples.)

For a large class of problems, however, the bootstrap does yield easy, very
quick, Monte Carlo estimates of the errors in an estimated parameter set.

Confidence Limits

Rather than present all details of the probability distribution of errors in
parameter estimation, it is common practice to summarize the distribution in the
form of confidence limits. The full probability distribution is a function defined
on the /-dimensional space of parameters a. A confidence region (or confidence
interval) is just a region of that 1/-dimensional space (hopefully a small region) that
contains a certain (hopefully large) percentage of the total probability distribution.
You point to a confidence region and say, e.g., “there is a 99 percent chance that the
true parameter values fall within this region around the measured value.”

It is worth emphasizing that you, the experimenter, get to pick both the
confidence level (99 percent in the above example), and the shape of the confidence
region. The only requirement is that your region does include the stated percentage
of probability. Certain percentages are, however, customary in scientific usage: 68.3
percent (the lowest confidence worthy of quoting), 90 percent, 95.4 percent, 99
percent, and 99.73 percent. Higher confidence levels are conventionally “ninety-nine
point nine ... nine.’ As for shape, obviously you want a region that is compact
and reasonably centered on your measurement a (q), since the whole purpose of a
confidence limit is to inspire confidence in that measured value. In one dimension,
the convention is to use a line segment centered on the measured value; in higher
dimensions, ellipses or ellipsoids are most frequently used.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.6 Confidence Limits on Estimated Model Parameters 693

(s)
@i2 7 Aoy2

68% confidence.

i ate it 68% confidence region
interval on a = .
| ee a on a, and a jointly

68% confidence interval on a

(s)
aii — 41

Figure 15.6.3. Confidence intervals in 1 and 2 dimensions. The same fraction of measured points (here
68%) lies (i) between the two vertical lines, (ii) between the two horizontal lines, (iii) within the ellipse.

You might suspect, correctly, that the numbers 68.3 percent, 95.4 percent,
and 99.73 percent, and the use of ellipsoids, have some connection with a normal
distribution. That is true historically, but not always relevant nowadays. In general,
the probability distribution of the parameters will not be normal, and the above
numbers, used as levels of confidence, are purely matters of convention.

Figure 15.6.3 sketches a possible probability distribution for the case M = 2.
Shown are three different confidence regions which might usefully be given, all at
the same confidence level. The two vertical lines enclose a band (horizontal interval)
which represents the 68 percent confidence interval for the variable a ; without regard
to the value of a2. Similarly the horizontal lines enclose a 68 percent confidence
interval for a2. The ellipse shows a 68 percent confidence interval for a; and a2
jointly. Notice that to enclose the same probability as the two bands, the ellipse must
necessarily extend outside of both of them (a point we will return to below).

Constant Chi-Square Boundaries as Confidence Limits

When the method used to estimate the parameters a (q) is chi-square minimiza-
tion, as in the previous sections of this chapter, then there is a natural choice for the
shape of confidence intervals, whose use is almost universal. For the observed data
set Dig), the value of x? is a minimum at aq). Call this minimum value xXrAin Lf

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

694 Chapter 15. Modeling of Data

Figure 15.6.4. Confidence region ellipses corresponding to values of chi-square larger than the fitted
minimum. The solid curves, with Ax? = 1.00, 2.71, 6.63 project onto one-dimensional intervals AA’,
BB',CC’. These intervals — not the ellipses themselves — contain 68.3%, 90%, and 99% of normally
distributed data. The ellipse that contains 68.3% of normally distributed data is shown dashed, and has
Ax? = 2.30. For additional numerical values, see accompanying table.

the vector a of parameter values is perturbed away from a (o), then x? increases. The
region within which ? increases by no more than a set amount Ay? defines some
M-dimensional confidence region around a (o). If Ay? is set to be a large number,
this will be a big region; if it is small, it will be small. Somewhere in between there
will be choices of Ay? that cause the region to contain, variously, 68 percent, 90
percent, etc. of probability distribution for a’s, as defined above. These regions are
taken as the confidence regions for the parameters a (9).

Very frequently one is interested not in the full 1/-dimensional confidence
region, but in individual confidence regions for some smaller number v of parameters.
For example, one might be interested in the confidence interval of each parameter
taken separately (the bands in Figure 15.6.3), in which case v = 1. In that case,
the natural confidence regions in the v-dimensional subspace of the (/-dimensional
parameter space are the projections of the /-dimensional regions defined by fixed
Ax? into the v-dimensional spaces of interest. In Figure 15.6.4, for the case M = 2,
we show regions corresponding to several values of Ay?. The one-dimensional
confidence interval in a2 corresponding to the region bounded by Ay? = 1 lies
between the lines A and A’.

Notice that the projection of the higher-dimensional region on the lower-
dimension space is used, not the intersection. The intersection would be the band
between Z and Z’. It is never used. It is shown in the figure only for the purpose of
making this cautionary point, that it should not be confused with the projection.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.6 Confidence Limits on Estimated Model Parameters 695

Probability Distribution of Parameters in the Normal Case

You may be wondering why we have, in this section up to now, made no
connection at all with the error estimates that come out of the x? fitting procedure,
most notably the covariance matrix C;;. The reason is this: y? minimization
is a useful means for estimating parameters even if the measurement errors are
not normally distributed. While normally distributed errors are required if the x?
parameter estimate is to be a maximum likelihood estimator (§15.1), one is often
willing to give up that property in return for the relative convenience of the x?
procedure. Only in extreme cases, measurement error distributions with very large
“tails?” is y? minimization abandoned in favor of more robust techniques, as will
be discussed in §15.7.

However, the formal covariance matrix that comes out of a y? minimization has
aclear quantitative interpretation only if (or to the extent that) the measurement errors
actually are normally distributed. In the case of nonnormal errors, you are “allowed”

e to fit for parameters by minimizing 1?

 touseacontour of constant Ax? as the boundary of your confidence region

e to use Monte Carlo simulation or detailed analytic calculation in deter-

mining which contour Ax? is the correct one for your desired confidence
level

e to give the covariance matrix C; as the “formal covariance matrix of

the fit.”
You are not allowed

e to use formulas that we now give for the case of normal errors, which

establish quantitative relationships among Ay”, C;;, and the confidence
level.

Here are the key theorems that hold when (i) the measurement errors are
normally distributed, and either (ii) the model is linear in its parameters or (iii) the
sample size is large enough that the uncertainties in the fitted parameters a do not
extend outside a region in which the model could be replaced by a suitable linearized
model. [Note that condition (iii) does not preclude your use of a nonlinear routine
like mqrfit to find the fitted parameters.]

Theorem A. y2,;,, iS distributed as a chi-square distribution with N — M
degrees of freedom, where N is the number of data points and 1 is the number of
fitted parameters. This is the basic theorem that lets you evaluate the goodness-of-fit
of the model, as discussed above in §15.1. We list it first to remind you that unless
the goodness-of-fit is credible, the whole estimation of parameters is suspect.

Theorem B. If ay is drawn from the universe of simulated data sets with
actual parameters ao), then the probability distribution of da = avy — a) is the
multivariate normal distribution

1
P(6a) day ...dayy = const. x exp (-36 - [a] 6a) da, ...dayg

where [a] is the curvature matrix defined in equation (15.5.8).

Theorem C. If ab is drawn from the universe of simulated data sets with
actual parameters ao), then the quantity Ay? = y(a(j)) — x?(a(o)) is distributed as
a chi-square distribution with M degrees of freedom. Here the y7’s are all evaluated

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

696 Chapter 15. Modeling of Data

using the fixed (actual) data set Dg). This theorem makes the connection between
particular values of Ay? and the fraction of the probability distribution that they
enclose as an /-dimensional region, i.e., the confidence level of the 1/-dimensional
confidence region.

Theorem D. Suppose that ae) is drawn from the universe of simulated data
sets (as above), that its first v components a;,...,a, are held fixed, and that its
remaining M — v components are varied so as to minimize x7. Call this minimum
value x2. Then Ax? = x? — y2\in is distributed as a chi-square distribution with
vy degrees of freedom. If you consult Figure 15.6.4, you will see that this theorem
connects the projected Ay? region with a confidence level. In the figure, a point that
is held fixed in az and allowed to vary in a, minimizing y? will seek out the ellipse
whose top or bottom edge is tangent to the line of constant a2, and is therefore the
line that projects it onto the smaller-dimensional space.

As a first example, let us consider the case v = 1, where we want to find
the confidence interval of a single parameter, say a;. Notice that the chi-square
distribution with v = 1 degree of freedom is the same distribution as that of the square
of a single normally distributed quantity. Thus Ay? < 1 occurs 68.3 percent of the
time (1-o for the normal distribution), Ay? < 4 occurs 95.4 percent of the time (2-0
for the normal distribution), Ay? < 9 occurs 99.73 percent of the time (3-c for the
normal distribution), etc. In this manner you find the Ax? that corresponds to your
desired confidence level. (Additional values are given in the accompanying table.)

Let da be a change in the parameters whose first component is arbitrary, da1,
but the rest of whose components are chosen to minimize the Ay?. Then Theorem
D applies. The value of Ay? is given in general by

Ax? = da: [a] «da (15.6.1)

which follows from equation (15.5.8) applied at y2,;,, where 3;, = 0. Since da by
hypothesis minimizes ? in all but its first component, the second through Mth
components of the normal equations (15.5.9) continue to hold. Therefore, the
solution of (15.5.9) is

da = [a]! - : =(C]- : (15.6.2)
0 0

where c is one arbitrary constant that we get to adjust to make (15.6.1) give the
desired left-hand value. Plugging (15.6.2) into (15.6.1) and using the fact that [C]
and [a] are inverse matrices of one another, we get

c= 6a1/Cy, and Ax? = (6a1)?/Ci (15.6.3)
or

day = +VAx2 VC (15.6.4)

At last! A relation between the confidence interval +da, and the formal
standard error 01 = C11. Not unreasonably, we find that the 68 percent confidence
interval is +01, the 95 percent confidence interval is +201, etc.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.6 Confidence Limits on Estimated Model Parameters 697

Ay? as a Function of Confidence Level and Degrees of Freedom

Vv

D I 2 3 4 5 6
68.3% | 1.00 2.30 353. 4.72 589 7.04
90% 271 461 6.25 7.18 9.24 10.6
95.4% | 400 6.17 8.02 9.70 113 128
99% 6.63 921 113 13.3 15.1 16.8

99.73% | 9.00 11.8 14.2 16.3 18.2 20.1
99.99% | 15.1 18.4 21.1 23.5 25.7 27.8

These considerations hold not just for the individual parameters a;, but also
for any linear combination of them: If
M
b= So eai=c-a (15.6.5)
k=1

then the 68 percent confidence interval on b is

6b= +c: [C]-c (15.6.6)

However, these simple, normal-sounding numerical relationships do not hold in
the case vy > 1 [3]. In particular, Ay? = 1 is not the boundary, nor does it project
onto the boundary, of a 68.3 percent confidence region when v > 1. If you want
to calculate not confidence intervals in one parameter, but confidence ellipses in
two parameters jointly, or ellipsoids in three, or higher, then you must follow the
following prescription for implementing Theorems C and D above:

e Let v be the number of fitted parameters whose joint confidence region you
wish to display, v <M. Call these parameters the “parameters of interest.”

e Let p be the confidence limit desired, e.g., p = 0.68 or p = 0.95.

e Find A (i.e., Ay?) such that the probability of a chi-square variable with
vy degrees of freedom being less than A is p. For some useful values of p
and v, A is given in the table. For other values, you can use the routine
gammq and a simple root-finding routine (e.g., bisection) to find A such
that gammq(v/2, A/2) = 1— p.

e Take the M x M covariance matrix [C] = [a]~! of the chi-square fit.
Copy the intersection of the v rows and columns corresponding to the
parameters of interest into a v x v matrix denoted [Cproj].

e Invert the matrix [C),0j]. (In the one-dimensional case this was just taking
the reciprocal of the element Cy.)

e The equation for the elliptical boundary of your desired confidence region
in the v-dimensional subspace of interest is

A = dal: [Cproj] + - da’ (15.6.7)

-1

where da’ is the v-dimensional vector of parameters of interest.

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

698 Chapter 15. Modeling of Data

ag

1
length Wa

ay

Figure 15.6.5. Relation of the confidence region ellipse Ax? = 1 to quantities computed by singular

value decomposition. The vectors V(;) are unit vectors along the principal axes of the confidence region.

The semi-axes have lengths equal to the reciprocal of the singular values uw. If the axes are all scaled
by some constant factor a, Ay? is scaled by the factor a2.

If you are confused at this point, you may find it helpful to compare Figure
15.6.4 and the accompanying table, considering the case MM = 2 with y = 1 and
v = 2. You should be able to verify the following statements: (i) The horizontal
band between C’ and C’ contains 99 percent of the probability distribution, so it
is a confidence limit on a2 alone at this level of confidence. (ii) Ditto the band
between B and B’ at the 90 percent confidence level. (iii) The dashed ellipse,
labeled by Ax? = 2.30, contains 68.3 percent of the probability distribution, so it is
a confidence region for a; and ag jointly, at this level of confidence.

Confidence Limits from Singular Value Decomposition

When you have obtained your y? fit by singular value decomposition (§15.4), the
information about the fit’s formal errors comes packaged in a somewhat different, but
generally more convenient, form. The columns of the matrix V are an orthonormal
set of M vectors that are the principal axes of the Ay? = constant ellipsoids.
We denote the columns as V1)... Var). The lengths of those axes are inversely
proportional to the corresponding singular values w  ... waz; see Figure 15.6.5. The
boundaries of the ellipsoids are thus given by

Ay? = wi (Vay: 6a)? +++ + wie(Vcun « 6a)? (15.6.8)
which is the justification for writing equation (15.4.18) above. Keep in mind that
it is much easier to plot an ellipsoid given a list of its vector principal axes, than
given its matrix quadratic form!

The formula for the covariance matrix [C] in terms of the columns V (,) is

My
=> a @Vw (15.6.9)
i=1 ¢

or, in components,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.7 Robust Estimation 699

M
1
Cin = ye Vai Vai (15.6.10)
i=1

CITED REFERENCES AND FURTHER READING:

Efron, B. 1982, The Jackknife, the Bootstrap, and Other Resampling Plans (Philadelphia: S.1.A.M.).
(1)

Efron, B., and Tibshirani, R. 1986, Statistical Science vol. 1, pp. 54-77. [2]

Avni, Y. 1976, Astrophysical Journal, vol. 210, pp. 642-646. [3]

Lampton, M., Margon, M., and Bowyer, S. 1976, Astrophysical Journal, vol. 208, pp. 177-190.

Brownlee, K.A. 1965, Statistical Theory and Methodology, 2nd ed. (New York: Wiley).

Martin, B.R. 1971, Statistics for Physicists (New York: Academic Press).

15.7 Robust Estimation

The concept of robustness has been mentioned in passing several times already.
In §14.1 we noted that the median was a more robust estimator of central value than
the mean; in §14.6 it was mentioned that rank correlation is more robust than linear
correlation. The concept of outlier points as exceptions to a Gaussian model for
experimental error was discussed in §15.1.

The term “robust” was coined in statistics by G.E.P. Box in 1953. Various
definitions of greater or lesser mathematical rigor are possible for the term, but in
general, referring to a statistical estimator, it means “insensitive to small departures
from the idealized assumptions for which the estimator is optimized.” [1,2] The word
“small” can have two different interpretations, both important: either fractionally
small departures for all data points, or else fractionally large departures for a small
number of data points. It is the latter interpretation, leading to the notion of outlier
points, that is generally the most stressful for statistical procedures.

Statisticians have developed various sorts of robust statistical estimators. Many,
if not most, can be grouped in one of three categories.

M-estimates follow from maximum-likelihood arguments very much as equa-
tions (15.1.5) and (15.1.7) followed from equation (15.1.3). M-estimates are usually
the most relevant class for model-fitting, that is, estimation of parameters. We
therefore consider these estimates in some detail below.

L-estimates are “linear combinations of order statistics.” These are most
applicable to estimations of central value and central tendency, though they can
occasionally be applied to some problems in estimation of parameters. Two
“typical” L-estimates will give you the general idea. They are (i) the median, and
(ii) Tukey’s trimean, defined as the weighted average of the first, second, and third
quartile points in a distribution, with weights 1/4, 1/2, and 1/4, respectively.

R-estimates are estimates based on rank tests. For example, the equality or
inequality of two distributions can be estimated by the Wilcoxon test of computing
the mean rank of one distribution in a combined sample of both distributions.
The Kolmogorov-Smirnov statistic (equation 14.3.6) and the Spearman rank-order

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

700 Chapter 15. Modeling of Data

narrow

central peak

doa,

tail of

LO outliers

Buipnjoul) say ajqepeos

2 Uv n
EEEE
epee
20288
23023
esq u>
8a
eaatz
es20E
Beses
) Bone 2
(a 8339s
elaat
. a)
B2 695
SaRS2
593 g2
38.0
Z3a07
BERET
B89 Es
gaer
. E2883
e258e
least squares fit e832
Oo8-25M
assess
$ZoST
23265
Basag
NL ot teat se S3BSGO
robust straight-line fit aggls
gae8s
s22e5
2582
() BE go
3 Re
Figure 15.7.1. Examples where robust statistical methods are desirable: (a) A one-dimensional gese g
distribution with a tail of outlie: tatistical fluctuations in these outliers can prevent accurate determination 8 4 Z
of the position of the central peak. (b) A distribution in two dimensions fitted to a straight line; non-robust 30 3 § Ry
techniques such as least-squares fitting can have undesired sensitivity to outlying points. So8 2 z
s 8 8 Po
correlation coefficient (14.6.1) are R-estimates in essence, if not always by formal gsze z 8
en eeoo
definition. eeege
Some other kinds of robust techniques, coming from the fields of optimal control £Q8 2
. ~ . a . S928
and filtering rather than from the field of mathematical statistics, are mentioned at 283 zg
the end of this section. Some examples where robust statistical methods are desirable 3 aS ®
are shown in Figure 15.7.1. 35S

Estimation of Parameters by Local M-Estimates

Suppose we know that our measurement errors are not normally distributed.
Then, in deriving a maximum-likelihood formula for the estimated parameters a in a
model y(x;a), we would write instead of equation (15.1.3)

N
P=] fexp[-e(ui,y {2i:a})] Ay} (15.7.1)
i=1

15.7 Robust Estimation 701

where the function p is the negative logarithm of the probability density. Taking the
logarithm of (15.7.1) analogously with (15.1.4), we find that we want to minimize
the expression

N
SS elvis y {ais a}) (15.7.2)
i=1

Very often, it is the case that the function p depends not independently on its
two arguments, measured y; and predicted y(a;), but only on their difference, at least
if scaled by some weight factors a; which we are able to assign to each point. In this
case the M-estimate is said to be local, and we can replace (15.7.2) by the prescription

N
minimize over a Ss p (oe) (15.7.3)

0,
i=l ‘

where the function p(z) is a function of a single variable z = [y; — y(xi)|/o;.
If we now define the derivative of p(z) to be a function 7(z),

iv) — del)
VW(2b= dz (15.7.4)
then the generalization of (15.1.7) to the case of a general M-estimate is
“1 (yi —ylai)\ ( Oy(eisa)
> on ( on ) ( Oar ( )

If you compare (15.7.3) to (15.1.3), and (15.7.5) to (15.1.7), you see at once
that the specialization for normally distributed errors is

1
= ,?

5 Wz) =2 (normal) (15.7.6)

p(z)
If the errors are distributed as a double or two-sided exponential, namely

Prob {yi — y(xi)} ~ exp (- vi — v() ) (15.7.7)

then, by contrast,
p(x) = |z| w(z) = sgn(z) (double exponential) (15.7.8)

Comparing to equation (15.7.3), we see that in this case the maximum likelihood
estimator is obtained by minimizing the mean absolute deviation, rather than the
mean square deviation. Here the tails of the distribution, although exponentially
decreasing, are asymptotically much larger than any corresponding Gaussian.

A distribution with even more extensive — therefore sometimes even more
realistic — tails is the Cauchy or Lorentzian distribution,

Prob {yi — y(ai)} ~ (15.7.9)

1
1+

1 (yi —y(ai)\”
oi

2

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

702 Chapter 15. Modeling of Data

This implies

p(z) = log (1 + >) (Zz) = t4i2 (Lorentzian) (15.7.10)

Notice that the ~ function occurs as a weighting function in the generalized
normal equations (15.7.5). For normally distributed errors, equation (15.7.6) says
that the more deviant the points, the greater the weight. By contrast, when tails are
somewhat more prominent, as in (15.7.7), then (15.7.8) says that all deviant points
get the same relative weight, with only the sign information used. Finally, when
the tails are even larger, (15.7.10) says the w increases with deviation, then starts
decreasing, so that very deviant points — the true outliers — are not counted at all
in the estimation of the parameters.

This general idea, that the weight given individual points should first increase
with deviation, then decrease, motivates some additional prescriptions for ~ which
do not especially correspond to standard, textbook probability distributions. Two
examples are

Andrew’s sine

wea {Glo ser (15.7.1)

If the measurement errors happen to be normal after all, with standard deviations a ;,
then it can be shown that the optimal value for the constant c is c = 2.1.
Tukey’s biweight

2(1 — 22/e2)2 zdl<e
w= { 1-2/2) ESS (15.7.12)

where the optimal value of c for normal errors is c = 6.0.
Numerical Calculation of M-Estimates

To fit a model by means of an M-estimate, you first decide which M-estimate
you want, that is, which matching pair p, 7 you want to use. We rather like
(15.7.8) or (15.7.10).

You then have to make an unpleasant choice between two fairly difficult
problems. Either find the solution of the nonlinear set of 17 equations (15.7.5), or
else minimize the single function in M variables (15.7.3).

Notice that the function (15.7.8) has a discontinuous w, and a discontinuous
derivative for p. Such discontinuities frequently wreak havoc on both general
nonlinear equation solvers and general function minimizing routines. You might
now think of rejecting (15.7.8) in favor of (15.7.10), which is smoother. However,
you will find that the latter choice is also bad news for many general equation solving
or minimization routines: small changes in the fitted parameters can drive ~(z)
off its peak into one or the other of its asymptotically small regimes. Therefore,
different terms in the equation spring into or out of action (almost as bad as analytic
discontinuities).

Don’t despair. If your computer budget (or, for personal computers, patience)
is up to it, this is an excellent application for the downhill simplex minimization

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.7 Robust Estimation 703

algorithm exemplified in amoeba $10.4 or amebsa in $10.9. Those algorithms make
no assumptions about continuity; they just ooze downhill and will work for virtually
any sane choice of the function p.

It is very much to your (financial) advantage to find good starting values,
however. Often this is done by first fitting the model by the standard y ? (nonrobust)
techniques, e.g., as described in §15.4 or §15.5. The fitted parameters thus obtained
are then used as starting values in amoeba, now using the robust choice of p and
minimizing the expression (15.7.3).

Fitting a Line by Minimizing Absolute Deviation

Occasionally there is a special case that happens to be much easier than is
suggested by the general strategy outlined above. The case of equations (15.7.7)—
(15.7.8), when the model is a simple straight line

y(a;a,b) =a+ br (15.7.13)

and where the weights o; are all equal, happens to be such a case. The problem is
precisely the robust version of the problem posed in equation (15.2.1) above, namely
fit a straight line through a set of data points. The merit function to be minimized is

So yi — a = ba| (15.7.14)
i=1

rather than the .? given by equation (15.2.2).
The key simplification is based on the following fact: The median c y, of a set
of numbers c; is also that value which minimizes the sum of the absolute deviations

> lc; — cm |
7

(Proof: Differentiate the above expression with respect to c jy and set it to zero.)
It follows that, for fixed b, the value of a that minimizes (15.7.14) is

a = median {y; — bx; } (15.7.15)
Equation (15.7.5) for the parameter b is
N
O= 5 x; sgn(y; — a — ba) (15.7.16)

i=1

(where sgn(0) is to be interpreted as zero). If we replace a in this equation by the
implied function a(b) of (15.7.15), then we are left with an equation in a single
variable which can be solved by bracketing and bisection, as described in §9.1.
(in fact, it is dangerous to use any fancier method of root-finding, because of the
discontinuities in equation 15.7.16.)

Here is a routine that does all this. It calls select (88.5) to find the median.
The bracketing and bisection are built in to the routine, as is the y” solution that
generates the initial guesses for a and b. Notice that the evaluation of the right-hand
side of (15.7.16) occurs in the function rofunc, with communication via global
(top-level) variables.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

704 Chapter 15. Modeling of Data

#include <math.h>
#include "nrutil.h"

int ndatat;

float *xt,*yt,aa,abdevt;

void medfit(float x[], float y[], int ndata, float +a, float *b, float *abdev)
Fits y = a+ bx by the criterion of least absolute deviations. The arrays x[1..ndata] and
y(1..ndata] are the input experimental points. The fitted parameters a and b are output,
along with abdev, which is the mean absolute deviation (in y) of the experimental points from
the fitted line. This routine uses the routine rofunc, with communication via global variables.
{

float rofunc(float b);

int j;

float bb,b1,b2,del,f,f1,f2,sigb,temp;

float sx=0.0,sy=0.0,sxy=0.0,sxx=0.0,chisq=0.0;

ndatat=ndata;

xt=x;

yt=y;

for (j= =ndata;jtt+) { As a first guess for a and b, we will find the
sx += x[j]; least-squares fitting line.
sy t= yj];

sxy t= x[jl*y[jl];
sxx t= x[j]*x[j];

3
del=ndata*sxx-sx*sx};
aa=(sxx*sy-sx*sxy) /del; Least-squares solutions.

bb=(ndata*sxy-sx*sy) /del;
for (j=1;j<=ndata; j++)
chisq += (temp=y[j]-(aatbb*x[j]) ,temp*temp) ;
sigb=sqrt (chisq/del) ; The standard deviation will give some idea of
bi=bb; how big an iteration step to take.
f1=rofunc(b1) ;
if (sigb > 0.0) {
b2=bb+SIGN(3.0*sigb,f1) ;
Guess bracket as 3-o away, in the downhill direction known from f1.
£2=rofunc(b2) ;
if (b2 == b1) {
*azaa;
*b=bb;
*abdev=abdevt/ndata;
return;
}
while (f1*f2 > 0.0) { Bracketing.
bb=b2+1.6* (b2-b1) ;

b2=bb;
f£2=rofunc(b2) ;

}
sigb=0.01*sigb; Refine until error a negligible number of standard
while (fabs(b2-b1) > sigb) { deviations.
bb=b1+0.5* (b2-b1) ; Bisection.
if (bb == bi || bb == b2) break;
f=rofunc (bb) ;
if (f*f1 >= 0.0) {
fi=f;
bi=bb;
} else {
f2=f;
b2=bb;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

15.7 Robust Estimation 705

*azaa;
*b=bb ;
*abdev=abdevt/ndata;

#include <math.h>
#include "nrutil.h"
#define EPS 1.0e-7

extern int ndatat; Defined in medfit.
extern float *xt,*yt,aa,abdevt;

float rofunc(float b)
Evaluates the right-hand side of equation (15.7.16) for a given value of b. Communication with
the routine medfit is through global variables.
{
float select(unsigned long k, unsigned long n, float arr[]);
int j;
float *arr,d,sum=0.0;

arr=vector(1,ndatat) ;
for (j=1;j<=ndatat; j++) arr[jl=yt([j]-b*xt[j];
if (ndatat & 1) {
aa=select ((ndatatt+1)>>1,ndatat,arr) ;
+
else {
jandatat >> 1;
aa=0.5* (select (j,ndatat ,arr)+select(j+1,ndatat,arr));

=ndatat;jt+) {

d=yt [j]-(b*xt [j]+aa) ;

abdevt += fabs(d);

if (yt{j] != 0.0) d /= fabs(yt[j]);

if (fabs(d) > EPS) sum += (d >= 0.0 ? xt[j] : -xt[j]);

free_vector(arr,1,ndatat) ;
return sum;

Other Robust Techniques

Sometimes you may have a priori knowledge about the probable values and
probable uncertainties of some parameters that you are trying to estimate from a data
set. In such cases you may want to perform a fit that takes this advance information
properly into account, neither completely freezing a parameter at a predetermined
value (as in 1fit $15.4) nor completely leaving it to be determined by the data set.
The formalism for doing this is called “use of a priori covariances.”

A related problem occurs in signal processing and control theory, where it is
sometimes desired to “track” (i.e., maintain an estimate of) a time-varying signal in
the presence of noise. If the signal is known to be characterized by some number
of parameters that vary only slowly, then the formalism of Kalman filtering tells
how the incoming, raw measurements of the signal should be processed to produce
best parameter estimates as a function of time. For example, if the signal is a
frequency-modulated sine wave, then the slowly varying parameter might be the
instantaneous frequency. The Kalman filter for this case is called a phase-locked
loop and is implemented in the circuitry of good radio receivers [3,4].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Modeling of Data

Chapter 15.

706

CITED REFERENCES AND FURTHER READING:

Huber, P.J. 1981, Robust Statistics (New York: Wiley). [1]

Launer, R.L., and Wilkinson, G.N. (eds.) 1979, Robustness in Statistics (New York: Academic

Press). [2]
Bryson, A. E., and Ho, Y.C. 1969, Applied Optimal Control (Waltham, MA: Ginn). [3]

Jazwinski, A. H. 1970, Stochastic Processes and Filtering Theory (New York: Academic

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permissi is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

Press). [4]

Chapter 16. Integration of Ordinary
Differential Equations

16.0 Introduction

Problems involving ordinary differential equations (ODEs) can always be
reduced to the study of sets of first-order differential equations. For example the
second-order equation

z

can be rewritten as two first-order equations

dy
= r(x) ~ a(a)2(2)

where z is a new variable. This exemplifies the procedure for an arbitrary ODE. The
usual choice for the new variables is to let them be just derivatives of each other (and
of the original variable). Occasionally, it is useful to incorporate into their definition
some other factors in the equation, or some powers of the independent variable,
for the purpose of mitigating singular behavior that could result in overflows or
increased roundoff error. Let common sense be your guide: If you find that the
original variables are smooth in a solution, while your auxiliary variables are doing
crazy things, then figure out why and choose different auxiliary variables.

The generic problem in ordinary differential equations is thus reduced to the
study of a set of N coupled first-order differential equations for the functions
yi, t = 1,2,...,.N, having the general form

dyi(x)
dx

= filt,y1,---, YN), i=1,...,N (16.0.3)

where the functions f; on the right-hand side are known.

A problem involving ODEs is not completely specified by its equations. Even
more crucial in determining how to attack the problem numerically is the nature of
the problem’s boundary conditions. Boundary conditions are algebraic conditions
on the values of the functions y; in (16.0.3). In general they can be satisfied at

707

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees
-auiyoeul jo BuiAdoo Aue Jo ‘uononpoidas ayjin4 ‘asn jeuosiad UMO JIAaU} 10} Adoo saded auo ayeW 0} Suasn JEUJa\U! 10} Pa}UeJGH SI UOISSIULOg

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUOWNN Aq Z661-8B6l (D) IYBUAdOD swesBOlY “‘ssalg AysIaAIUN EBpLqueD Aq 2661-8861 (D) jyBUAdOD

708 Chapter 16. Integration of Ordinary Differential Equations

discrete specified points, but do not hold between those points, i.e., are not preserved
automatically by the differential equations. Boundary conditions can be as simple as
requiring that certain variables have certain numerical values, or as complicated as
a set of nonlinear algebraic equations among the variables.

Usually, it is the nature of the boundary conditions that determines which
numerical methods will be feasible. Boundary conditions divide into two broad
categories.

e In initial value problems all the y; are given at some starting value x,, and
it is desired to find the y;’s at some final point x +, or at some discrete list
of points (for example, at tabulated intervals).

e In two-point boundary value problems, on the other hand, boundary
conditions are specified at more than one x. Typically, some of the
conditions will be specified at 2; and the remainder at x.

This chapter will consider exclusively the initial value problem, deferring two-
point boundary value problems, which are generally more difficult, to Chapter 17.
The underlying idea of any routine for solving the initial value problem is always
this: Rewrite the dy’s and dx’s in (16.0.3) as finite steps Ay and Az, and multiply the
equations by Ax. This gives algebraic formulas for the change in the functions when
the independent variable x is “stepped” by one “stepsize” Az. In the limit of making
the stepsize very small, a good approximation to the underlying differential equation
is achieved. Literal implementation of this procedure results in Euler’s method
(16.1.1, below), which is, however, not recommended for any practical use. Euler’s
method is conceptually important, however; one way or another, practical methods all
come down to this same idea: Add small increments to your functions corresponding
to derivatives (right-hand sides of the equations) multiplied by stepsizes.

In this chapter we consider three major types of practical numerical methods
for solving initial value problems for ODEs:

e Runge-Kutta methods

e Richardson extrapolation and its particular implementation as the Bulirsch-
Stoer method

e predictor-corrector methods.

A brief description of each of these types follows.

1. Runge-Kutta methods propagate a solution over an interval by combining
the information from several Euler-style steps (each involving one evaluation of the
right-hand f’s), and then using the information obtained to match a Taylor series
expansion up to some higher order.

2. Richardson extrapolation uses the powerful idea of extrapolating a computed
result to the value that would have been obtained if the stepsize had been very
much smaller than it actually was. In particular, extrapolation to zero stepsize is
the desired goal. The first practical ODE integrator that implemented this idea was
developed by Bulirsch and Stoer, and so extrapolation methods are often called
Bulirsch-Stoer methods.

3. Predictor-corrector methods store the solution along the way, and use
those results to extrapolate the solution one step advanced; they then correct the
extrapolation using derivative information at the new point. These are best for
very smooth functions.

Runge-Kutta is what you use when (i) you don’t know any better, or (ii) you
have an intransigent problem where Bulirsch-Stoer is failing, or (iii) you have a trivial

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.0 Introduction 709

problem where computational efficiency is of no concern. Runge-Kutta succeeds
virtually always; but it is not usually fastest, except when evaluating f; is cheap and
moderate accuracy (S 10~>) is required. Predictor-corrector methods, since they
use past information, are somewhat more difficult to start up, but, for many smooth
problems, they are computationally more efficient than Runge-Kutta. In recent years
Bulirsch-Stoer has been replacing predictor-corrector in many applications, but it
is too soon to say that predictor-corrector is dominated in all cases. However, it
appears that only rather sophisticated predictor-corrector routines are competitive.
Accordingly, we have chosen not to give an implementation of predictor-corrector
in this book. We discuss predictor-corrector further in $16.7, so that you can use
a canned routine should you encounter a suitable problem. In our experience, the
relatively simple Runge-Kutta and Bulirsch-Stoer routines we give are adequate
for most problems.

Each of the three types of methods can be organized to monitor internal
consistency. This allows numerical errors which are inevitably introduced into
the solution to be controlled by automatic, (adaptive) changing of the fundamental
stepsize. We always recommend that adaptive stepsize control be implemented,
and we will do so below.

In general, all three types of methods can be applied to any initial value
problem. Each comes with its own set of debits and credits that must be understood
before it is used.

We have organized the routines in this chapter into three nested levels. The
lowest or “nitty-gritty” level is the piece we call the algorithm routine. This
implements the basic formulas of the method, starts with dependent variables y ; at
x, and calculates new values of the dependent variables at the value x + h. The
algorithm routine also yields up some information about the quality of the solution
after the step. The routine is dumb, however, and it is unable to make any adaptive
decision about whether the solution is of acceptable quality or not.

That quality-control decision we encode in a stepper routine. The stepper
routine calls the algorithm routine. It may reject the result, set a smaller stepsize, and
call the algorithm routine again, until compatibility with a predetermined accuracy
criterion has been achieved. The stepper’s fundamental task is to take the largest
stepsize consistent with specified performance. Only when this is accomplished does
the true power of an algorithm come to light.

Above the stepper is the driver routine, which starts and stops the integration,
stores intermediate results, and generally acts as an interface with the user. There is
nothing at all canonical about our driver routines. You should consider them to be
examples, and you can customize them for your particular application.

Of the routines that follow, rk4, rkck, mmid, stoerm, and simpr are algorithm
routines; rkqs, bsstep, stiff, and stifbs are steppers; rkdumb and odeint
are drivers.

Section 16.6 of this chapter treats the subject of stiff equations, relevant both to
ordinary differential equations and also to partial differential equations (Chapter 19).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

710 Chapter 16. Integration of Ordinary Differential Equations

CITED REFERENCES AND FURTHER READING:

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall).

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 5.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 7.

Lambert, J. 1973, Computational Methods in Ordinary Differential Equations (New York: Wiley).

Lapidus, L., and Seinfeld, J. 1971, Numerical Solution of Ordinary Differential Equations (New
York: Academic Press).

16.1 Runge-Kutta Method

The formula for the Euler method is

Yn41 = Yn + hf (an, Yn) (16.1.1)

which advances a solution from v, to p41 = Lp +h. The formula is unsymmetrical:
It advances the solution through an interval h, but uses derivative information only
at the beginning of that interval (see Figure 16.1.1). That means (and you can verify
by expansion in power series) that the step’s error is only one power of h smaller
than the correction, ic O(h?) added to (16.1.1).

There are several reasons that Euler’s method is not recommended for practical
use, among them, (i) the method is not very accurate when compared to other,
fancier, methods run at the equivalent stepsize, and (ii) neither is it very stable
(see §16.6 below).

Consider, however, the use of a step like (16.1.1) to take a “trial” step to the
midpoint of the interval. Then use the value of both x and y at that midpoint
to compute the “real” step across the whole interval. Figure 16.1.2 illustrates the
idea. In equations,

ky =hf (tn, yn)
ka = hf (an + gh, yn + 5k) (16.1.2)
Yn+1 = Yn + ke + O(N?)

As indicated in the error term, this symmetrization cancels out the first-order error
term, making the method second order. [A method is conventionally called nth
order if its error term is O(h"*!).] In fact, (16.1.2) is called the second-order
Runge-Kutta or midpoint method.

We needn’t stop there. There are many ways to evaluate the right-hand side
f(x, y) that all agree to first order, but that have different coefficients of higher-order
error terms. Adding up the right combination of these, we can eliminate the error
terms order by order. That is the basic idea of the Runge-Kutta method. Abramowitz
and Stegun [1], and Gear [2], give various specific formulas that derive from this basic

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.1 Runge-Kutta Method 711

y) rad
Oo
+ t t
XL XQ X3 x

Figure 16.1.1. Euler’s method. In this simplest (and least accurate) method for integrating an ODE,
the derivative at the starting point of each interval is extrapolated to find the next function value. The
method has first-order accuracy.

y)

t t t
x X X3 x

Figure 16.1.2. Midpoint method. Second-order accuracy is obtained by using the initial derivative at
each step to find a point halfway across the interval, then using the midpoint derivative across the full
width of the interval. In the figure, filled dots represent final function values, while open dots represent
function values that are discarded once their derivatives have been calculated and used.

idea. By far the most often used is the classical fourth-order Runge-Kutta formula,
which has a certain sleekness of organization about it:

ky =hf (an, yn)

h k
hea = Nf (an + 51 Yn + >)
h k:
keg = hf (an + 51 Yn + >)
kg =hf (an +h, yn + kg)
ky ko kg
Ynv1 = Yn 4 5 3 3 7 + O(h°) (16.1.3)

The fourth-order Runge-Kutta method requires four evaluations of the right-
hand side per step h (see Figure 16.1.3). This will be superior to the midpoint
method (16.1.2) if at least twice as large a step is possible with (16.1.3) for the same
accuracy. Is that so? The answer is: often, perhaps even usually, but surely not
always! This takes us back to a central theme, namely that high order does not always
mean high accuracy. The statement “fourth-order Runge-Kutta is generally superior
to second-order” is a true one, but you should recognize it as a statement about the

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

712 Chapter 16. —_ Integration of Ordinary Differential Equations

Figure 16.1.3. Fourth-order Runge-Kutta method. In each step the derivative is evaluated four times:
once at the initial point, twice at trial midpoints, and once at a trial endpoint. From these derivatives the
final function value (shown as a filled dot) is calculated. (See text for details.)

contemporary practice of science rather than as a statement about strict mathematics.
That is, it reflects the nature of the problems that contemporary scientists like to solve.

For many scientific users, fourth-order Runge-Kutta is not just the first word on
ODE integrators, but the last word as well. In fact, you can get pretty far on this old
workhorse, especially if you combine it with an adaptive stepsize algorithm. Keep
in mind, however, that the old workhorse’s last trip may well be to take you to the
poorhouse: Bulirsch-Stoer or predictor-corrector methods can be very much more
efficient for problems where very high accuracy is a requirement. Those methods
are the high-strung racehorses. Runge-Kutta is for ploughing the fields. However,
even the old workhorse is more nimble with new horseshoes. In §16.2 we will give
a modern implementation of a Runge-Kutta method that is quite competitive as long
as very high accuracy is not required. An excellent discussion of the pitfalls in
constructing a good Runge-Kutta code is given in [3].

Here is the routine for carrying out one classical Runge-Kutta step on a set
of n differential equations. You input the values of the independent variables, and
you get out new values which are stepped by a stepsize h (which can be positive or
negative). You will notice that the routine requires you to supply not only function
derivs for calculating the right-hand side, but also values of the derivatives at the
starting point. Why not let the routine call derivs for this first value? The answer
will become clear only in the next section, but in brief is this: This call may not be
your only one with these starting conditions. You may have taken a previous step
with too large a stepsize, and this is your replacement. In that case, you do not
want to call derivs unnecessarily at the start. Note that the routine that follows
has, therefore, only three calls to derivs.

#include "nrutil.h"

void rk4(float y[], float dydx[], int n, float x, float h, float yout[],

void (*derivs) (float, float [], float []))
Given values for the variables y[1..n] and their derivatives dydx[1..n] known at x, use the
fourth-order Runge-Kutta method to advance the solution over an interval h and return the
incremented variables as yout [1..n], which need not be a distinct array from y. The user
supplies the routine derivs(x,y,dydx), which returns derivatives dydx at x.
{

int i;

float xh,hh,h6,*dym, *dyt,*yt;

dym=vector(1,n);
dyt=vector(1,n);

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.1 Runge-Kutta Method 713

yt=vector(1,n);

hh=h+0.5;

h6=h/6.0;

xh=xthh

for (i=1;i<=n;i++) yt[i]=y [i] +hh*dydx [i] ; First step.
(*derivs) (xh,yt,dyt) ; Second step.
for (i=1;i<=n;i++) yt(i]=y[i]+hh*dyt [i] ;

(*derivs) (xh, yt ,dym) ; Third step.
for (i=1;i<=n;i++) {

yt Ci] =y [i] +h*dym [il ;
dym(i] += dyt[il;
}
(*derivs) (xth,yt,dyt) ; Fourth step.
for (i=1;i jit+) Accumulate increments with proper
yout [i] =y [i] +h6* (dydx [i] +dyt [4]+2.0*dym[i]); weights.
free_vector(yt,1,n);
free_vector(dyt,1,n);
free_vector(dym,1,n);

The Runge-Kutta method treats every step in a sequence of steps in identical
manner. Prior behavior of a solution is not used in its propagation. This is
mathematically proper, since any point along the trajectory of an ordinary differential
equation can serve as an initial point. The fact that all steps are treated identically also
makes it easy to incorporate Runge-Kutta into relatively simple “driver” schemes.

We consider adaptive stepsize control, discussed in the next section, an essential
for serious computing. Occasionally, however, you just want to tabulate a function at
equally spaced intervals, and without particularly high accuracy. In the most common
case, you want to produce a graph of the function. Then all you need may be a
simple driver program that goes from an initial 2 , to a final a in a specified number
of steps. To check accuracy, double the number of steps, repeat the integration, and
compare results. This approach surely does not minimize computer time, and it can
fail for problems whose nature requires a variable stepsize, but it may well minimize
user effort. On small problems, this may be the paramount consideration.

Here is such a driver, self-explanatory, which tabulates the integrated functions
in the global arrays *x and **y; be sure to allocate memory for them with the
routines vector () and matrix(), respectively.

#include "nrutil.h"
float *+y,*xx; For communication back to main.

void rkdumb(float vstart[], int nvar, float x1, float x2, int nstep,

void (*derivs) (float, float [], float []))
Starting from initial values vstart[1..nvar] known at x1 use fourth-order Runge-Kutta
to advance nstep equal increments to x2. The user-supplied routine derivs(x,v,dvdx)
evaluates derivatives. Results are stored in the global variables y[1..nvar][1..nstept+1]
and xx[1..nstep+1].
{

void rk4(float y[], float dydx[], int n, float x, float h, float yout[],

void (*derivs) (float, float [], float []));

int i,k;

float x,h;

float *v,*vout,*dv;

v=vector(1,nvar) ;
vout=vector(1,nvar) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

714 Chapter 16. Integration of Ordinary Differential Equations

dv=vector(1,nvar) ;

for (i=1;i<=nvar;i++) { Load starting values.
v(il=vstart [i] ;
ylil [1]=v lil;

+
xx[1]=x1;
x=x1;
h=(x2-x1)/nstep;
for (k=1;k<=nstep;kt++) { Take nstep steps.
(*derivs) (x,v,dv);
rk4(v,dv,nvar,x,h,vout ,derivs) ;
if ((float)(x+h) == x) nrerror("Step size too small in routine rkdumb") ;
x += h;
xx [k+1] =x; Store intermediate steps.
for (i=1;i<=nvar;it++) {
v([i]=vout [i];
y li] [k+1]=v[il;
}
}

free_vector(dv,1,nvar) ;
free_vector(vout,1,nvar) ;
free_vector(v,1,nvar) ;

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §25.5. 1]

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), Chapter 2. [2]

Shampine, L.F., and Watts, H.A. 1977, in Mathematical Software III, J.R. Rice, ed. (New York: Aca-
demic Press), pp. 257-275; 1979, Applied Mathematics and Computation, vol. 5, pp. 93—
121. [3]

Rice, J.R. 1983, Numerical Methods, Software, and Analysis (New York: McGraw-Hill), §9.2.

16.2 Adaptive Stepsize Control for Runge-Kutta

A good ODE integrator should exert some adaptive control over its own progress,
making frequent changes in its stepsize. Usually the purpose of this adaptive stepsize
control is to achieve some predetermined accuracy in the solution with minimum
computational effort. Many small steps should tiptoe through treacherous terrain,
while a few great strides should speed through smooth uninteresting countryside.
The resulting gains in efficiency are not mere tens of percents or factors of two;
they can sometimes be factors of ten, a hundred, or more. Sometimes accuracy
may be demanded not directly in the solution itself, but in some related conserved
quantity that can be monitored.

Implementation of adaptive stepsize control requires that the stepping algorithm
signal information about its performance, most important, an estimate of its truncation
error. In this section we will learn how such information can be obtained. Obviously,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.2 Adaptive Stepsize Control for Runge-Kutta 715

the calculation of this information will add to the computational overhead, but the
investment will generally be repaid handsomely.

With fourth-order Runge-Kutta, the most straightforward technique by far is
step doubling (see, e.g.,[1]). We take each step twice, once as a full step, then,
independently, as two half steps (see Figure 16.2.1). How much overhead is this,
say in terms of the number of evaluations of the right-hand sides? Each of the three
separate Runge-Kutta steps in the procedure requires 4 evaluations, but the single
and double sequences share a starting point, so the total is 11. This is to be compared
not to 4, but to 8 (the two half-steps), since — stepsize control aside — we are
achieving the accuracy of the smaller (half) stepsize. The overhead cost is therefore
a factor 1.375. What does it buy us?

Let us denote the exact solution for an advance from x to x + 2h by y(x + 2h)
and the two approximate solutions by y1 (one step 2h) and y2 (2 steps each of size
h). Since the basic method is fourth order, the true solution and the two numerical
approximations are related by

y(a + 2h) = ys. + (2h)°6 + O(AS) +...

y(a + 2h) = yo + 2(h)b + O(R®) +... (16.2.1)

where, to order h®, the value ¢ remains constant over the step. [Taylor series
expansion tells us the ¢ is a number whose order of magnitude is y ‘) (x) /5!.] The
first expression in (16.2.1) involves (2h)° since the stepsize is 2h, while the second
expression involves 2(/.°) since the error on each step is h°. The difference between
the two numerical estimates is a convenient indicator of truncation error

A=y-m (16.2.2)

It is this difference that we shall endeavor to keep to a desired degree of accuracy,
neither too large nor too small. We do this by adjusting h.

It might also occur to you that, ignoring terms of order h® and higher, we can
solve the two equations in (16.2.1) to improve our numerical estimate of the true
solution y(x + 2h), namely,

A
y(a + 2h) = y+ Fe + O(h®) (16.2.3)
od

This estimate is accurate to fifth order, one order higher than the original Runge-
Kutta steps. However, we can’t have our cake and eat it: (16.2.3) may be fifth-order
accurate, but we have no way of monitoring its truncation error. Higher order is
not always higher accuracy! Use of (16.2.3) rarely does harm, but we have no
way of directly knowing whether it is doing any good. Therefore we should use
A as the error estimate and take as “gravy” any additional accuracy gain derived
from (16.2.3). In the technical literature, use of a procedure like (16.2.3) is called
“local extrapolation.”

An alternative stepsize adjustment algorithm is based on the embedded Runge-
Kutta formulas, originally invented by Fehlberg. An interesting fact about Runge-
Kutta formulas is that for orders M higher than four, more than M function
evaluations (though never more than M + 2) are required. This accounts for the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

716 Chapter 16. Integration of Ordinary Differential Equations

° .

oe  ——— big step
e
O_o
e } two small steps
* ad ad
—_—
x
Figure 16.2.1. | Step-doubling as a means for adaptive stepsize control in fourth-order Runge-Kutta.

Points where the derivative is evaluated are shown as filled circles. The open circle represents the same
derivatives as the filled circle immediately above it, so the total number of evaluations is 11 per two steps.
Comparing the accuracy of the big step with the two small steps gives a criterion for adjusting the stepsize
on the next step, or for rejecting the current step as inaccurate.

popularity of the classical fourth-order method: It seems to give the most bang
for the buck. However, Fehlberg discovered a fifth-order method with six function
evaluations where another combination of the six functions gives a fourth-order
method. The difference between the two estimates of y(a + h) can then be used as
an estimate of the truncation error to adjust the stepsize. Since Fehlberg’s original
formula, several other embedded Runge-Kutta formulas have been found.

Many practitioners were at one time wary of the robustness of Runge-Kutta-
Fehlberg methods. The feeling was that using the same evaluation points to advance
the function and to estimate the error was riskier than step-doubling, where the error
estimate is based on independent function evaluations. However, experience has
shown that this concern is not a problem in practice. Accordingly, embedded Runge-
Kutta formulas, which are roughly a factor of two more efficient, have superseded
algorithms based on step-doubling.

The general form of a fifth-order Runge-Kutta formula is

ky =hf (an, Yn)
ko =hf (an + a2h, yn + boik1)

(16.2.4)

ke = hf (an + ach, yn + berki +--+ + desks)

Yn+1 = Yn + erky + coke + egkg + caka + e5ks + cgke + O(h°)

The embedded fourth-order formula is

Ung = Yn + jkr + chko + chkg + chka + cks + cSkg +O(h°) (16.2.5)

and so the error estimate is

6

A= Ynt1 — Ung = ei — Gj ki (16.2.6)

i=1

The particular values of the various constants that we favor are those found by Cash
and Karp [2], and given in the accompanying table. These give a more efficient
method than Fehlberg’s original values, with somewhat better error properties.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.2 Adaptive Stepsize Control for Runge-Kutta 717

Cash-Karp Parameters for Embedded Runga-Kutta Method
i ai bij Ci os
1 37. 2825
378 2764
i 1
2 = 5 0 0
3 ) 3 3 a 250 18575
10 40 40 621 48384
4 | 3 3 9 6 125 13525
5 10 10 5 594 55296
5 —u 5 — 70 35 277
2 1 54 3 27 a7 9 14336
6 7 1631 175 575 44275 253 512 1
8 55296 512 13824 110592 7096 1771 zl
j= 1 2 3 4 5

Now that we know, at least approximately, what our error is, we need to
consider how to keep it within desired bounds. What is the relation between A
and h? According to (16.2.4) — (16.2.5), A scales as h>. If we take a step hy
and produce an error Aj, therefore, the step ho that would have given some other
value Ao is readily estimated as

0.2
Ao

ho =hi A

(16.2.7)

Henceforth we will let Ap denote the desired accuracy. Then equation (16.2.7) is
used in two ways: If A; is larger than Ap in magnitude, the equation tells how
much to decrease the stepsize when we retry the present (failed) step. If Aj is
smaller than Ao, on the other hand, then the equation tells how much we can safely
increase the stepsize for the next step. Local extrapolation consists in accepting
the fifth order value yn+1, even though the error estimate actually applies to the
fourth order value y;.1.

Our notation hides the fact that Ao is actually a vector of desired accuracies,
one for each equation in the set of ODEs. In general, our accuracy requirement will
be that all equations are within their respective allowed errors. In other words, we
will rescale the stepsize according to the needs of the “worst-offender’” equation.

How is Ao, the desired accuracy, related to some looser prescription like “get a
solution good to one part in 10°”? That can be a subtle question, and it depends on
exactly what your application is! You may be dealing with a set of equations whose
dependent variables differ enormously in magnitude. In that case, you probably
want to use fractional errors, Ao = ey, where ¢ is the number like 10~® or whatever.
On the other hand, you may have oscillatory functions that pass through zero but
are bounded by some maximum values. In that case you probably want to set Ao
equal to € times those maximum values.

A convenient way to fold these considerations into a generally useful stepper
routine is this: One of the arguments of the routine will of course be the vector of
dependent variables at the beginning of a proposed step. Call that y[1..n]. Let
us require the user to specify for each step another, corresponding, vector argument
yscal[1..n], and also an overall tolerance level eps. Then the desired accuracy

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

718 Chapter 16. —_ Integration of Ordinary Differential Equations

for the ith equation will be taken to be
Ao = eps x yscal [i] (16.2.8)

If you desire constant fractional errors, plug a pointer to y into the pointer to yscal
calling slot (no need to copy the values into a different array). If you desire constant
absolute errors relative to some maximum values, set the elements of yscal equal to
those maximum values. A useful “trick” for getting constant fractional errors except
“very” near zero crossings is to set yscal[i] equal to |y[i]| + |h x dydx[i]].
(The routine odeint, below, does this.)

Here is a more technical point. We have to consider one additional possibility
for yscal. The error criteria mentioned thus far are “local,” in that they bound the
error of each step individually. In some applications you may be unusually sensitive
about a “global” accumulation of errors, from beginning to end of the integration
and in the worst possible case where the errors all are presumed to add with the
same sign. Then, the smaller the stepsize h, the smaller the value Ao that you will
need to impose. Why? Because there will be more steps between your starting
and ending values of x. In such cases you will want to set yscal proportional to
h, typically to something like

Ao = ch x dydx [i] (16.2.9)

This enforces fractional accuracy € not on the values of y but (much more stringently)
on the increments to those values at each step. But now look back at (16.2.7). If Ao
has an implicit scaling with h, then the exponent 0.20 is no longer correct: When
the stepsize is reduced from a too-large value, the new predicted value h ; will fail to
meet the desired accuracy when yscal is also altered to this new h; value. Instead
of 0.20 = 1/5, we must scale by the exponent 0.25 = 1/4 for things to work out.

The exponents 0.20 and 0.25 are not really very different. This motivates us
to adopt the following pragmatic approach, one that frees us from having to know
in advance whether or not you, the user, plan to scale your yscal’s with stepsize.
Whenever we decrease a stepsize, let us use the larger value of the exponent (whether
we need it or not!), and whenever we increase a stepsize, let us use the smaller
exponent. Furthermore, because our estimates of error are not exact, but only
accurate to the leading order in h, we are advised to put in a safety factor S which is
a few percent smaller than unity. Equation (16.2.7) is thus replaced by

0.20
Shy ~ Ao > Ai
ho = A 2s (16.2.10)
Shy |= Ao < Ai
1

We have found this prescription to be a reliable one in practice.
Here, then, is a stepper program that takes one “quality-controlled” Runge-
Kutta step.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.2 Adaptive Stepsize Control for Runge-Kutta 719

#include <math.h>

#include "nrutil.h"

#define SAFETY 0.9

#define PGROW -0.2

#define PSHRNK -0.25

#define ERRCON 1.89e-4

The value ERRCON equals (5/SAFETY) raised to the power (1/PGROW), see use below.

void rkqs(float y[], float dydx[], int n, float *x, float htry, float eps,
float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []))
Fifth-order Runge-Kutta step with monitoring of local truncation error to ensure accuracy and
adjust stepsize. Input are the dependent variable vector y[1. .n] and its derivative dydx[1. .n]
at the starting value of the independent variable x. Also input are the stepsize to be attempted
htry, the required accuracy eps, and the vector yscal[1..n] against which the error is
scaled. On output, y and x are replaced by their new values, hdid is the stepsize that was
actually accomplished, and hnext is the estimated next stepsize. derivs is the user-supplied
routine that computes the right-hand side derivatives.

{
void rkck(float y[], float dydx[], int n, float x, float h,
float yout[], float yerr[], void (*derivs) (float, float [], float []));
int i;
float errmax,h,htemp,xnew,*yerr,*ytemp;
yerr=vector(1,n);
ytemp=vector(1,n);
h=htry; Set stepsize to the initial trial value.
for (;;) {
rkck(y,dydx,n,*x,h,ytemp,yerr,derivs) ; Take a step.
errmax=0.0; Evaluate accuracy.
for (i=1;i<=n;it++) errmax=FMAX(errmax,fabs(yerr[i]/yscal[i]));
errmax /= eps; Scale relative to required tolerance.
if (errmax <= 1.0) break; Step succeeded. Compute size of next step.
htemp=SAFETY*h*pow(errmax, PSHRNK) ;
Truncation error too large, reduce stepsize.
h=(h >= 0.0 ? FMAX(htemp,0.1*h) : FMIN(htemp,0.1*h));
No more than a factor of 10.
xnew=(*x)+h;
if (xnew == *x) nrerror("stepsize underflow in rkqs");
+
if (errmax > ERRCON) *hnext=SAFETY*h*pow(errmax ,PGROW) ;
else *hnext=5.0*h; No more than a factor of 5 increase.
*x += (*hdi 3
for (i=1;i<=n;i++) y[i]=ytemp [i];
free_vector(ytemp,1,n);
free_vector(yerr,1,n);
3

The routine rkqs calls the routine rkck to take a Cash-Karp Runge-Kutta step:

#include "nrutil.h"

void rkck(float y[], float dydx[], int n, float x, float h, float yout[],

float yerr[], void (*derivs)(float, float [], float []))
Given values for n variables y[1..n] and their derivatives dydx[1..n] known at x, use
the fifth-order Cash-Karp Runge-Kutta method to advance the solution over an interval h
and return the incremented variables as yout [1..n]. Also return an estimate of the local
truncation error in yout using the embedded fourth-order method. The user supplies the routine
derivs(x,y,dydx), which returns derivatives dydx at x.

{

int i;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

720 Chapter 16. Integration of Ordinary Differential Equations

static float a2=0.2,a3=0.3,a4=0.6,a5=1.0,a6=0.875,b21=0.2,
b31=3.0/40.0,b32=9.0/40.0,b41=0.3,b42 = -0.9,b43=1.2,
b51 = -11.0/54.0, b52=2.5,b53 = -70.0/27.0,b54=35.0/27.0,
b61=1631.0/55296.0,b62=175.0/512.0,b63=575 .0/13824.0,
b64=44275 .0/110592.0,b65=253 .0/4096 .0,c1=37.0/378.0,
c3=250 .0/621.0,c4=125.0/594.0,c6=512.0/1771.0,
dc5 = -277.00/14336.0;

float dc1=c1-2825.0/27648.0,dc3=c3-18575 .0/48384.0,
dc4=c4-13525 .0/55296 .0,dc6=c6-0.25;

float *ak2,+*ak3,*ak4,*ak5,*ak6,*ytemp;

ak2=vector(1,n);
ak3=vector(1,n);
ak4=vector(1,n);
ak5=vector(1,n);
ak6=vector(1,n) ;
ytemp=vector(1,n);

for (i=1;i<=n;it++) First step.
ytemp [i] =y [i] +b21*h*dydx [i] ;
(*derivs) (xta2*h,ytemp,ak2) ; Second step.
for (i=1;i<=n;it++)
ytemp [i] =y [i] +h* (b31*dydx [i] +b32*ak2[i]);

(*derivs) (xta3*h,ytemp,ak3) ; Third step.

for (i=1;i<=n;it++)
ytemp [i] =y [i] +h* (b41*dydx [i] +b42*ak2[i] +b43*ak3[i]) ;

(*derivs) (xta4*h,ytemp,ak4) ; Fourth step.

for (i=1;i<=n;it++)
ytemp [i] =y [i] +h (b51*dydx [i] +b52*ak2 [i] +b53*ak3 [i] +b54*ak4[i]);

(*derivs) (xta5*h,ytemp,ak5) ; Fifth step.

for (i=1;i<=n;it++)
ytemp [i] =y [i] +h* (b61*dydx [i] +b62*ak2[i] +b63*ak3 [i] +b64*ak4[i]+b65*ak5 [i] ) ;

(*derivs) (xt+ta6*h, ytemp,ak6) ; Sixth step.
for (i=1;i pit+) Accumulate increments with proper weights.
yout [i] =y [i] +h (c1*dydx [i] +c3*ak3 [i] +c4*ak4 [i] +c6*ak6 [i] ) ;

for (i=1;i pitt)
yerr [i] =h* (dc1*dydx [i] +dc3*ak3 [i] +dc4*ak4 [i] +dc5*ak5 [i] +dc6*ak6 [i] ) ;
Estimate error as difference between fourth and fifth order methods.

free_vector(ytemp,1,n);

free_vector(ak6,1,n);

free_vector(ak5,1,n);

free_vector(ak4,1,n);

free_vector(ak3,1,n);

free_vector(ak2,1,n);

Noting that the above routines are all in single precision, don’t be too greedy in
specifying eps. The punishment for excessive greediness is interesting and worthy of
Gilbert and Sullivan’s Mikado: The routine can always achieve an apparent zero error
by making the stepsize so small that quantities of order hy’ add to quantities of order
y as if they were zero. Then the routine chugs happily along taking infinitely many
infinitesimal steps and never changing the dependent variables one iota. (You guard
against this catastrophic loss of your computer budget by signaling on abnormally
small stepsizes or on the dependent variable vector remaining unchanged from step
to step. On a personal workstation you guard against it by not taking too long a
lunch hour while your program is running.)

Here is a full-fledged “driver” for Runge-Kutta with adaptive stepsize control.
We warmly recommend this routine, or one like it, for a variety of problems, notably

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.2 Adaptive Stepsize Control for Runge-Kutta 721

including garden-variety ODEs or sets of ODEs, and definite integrals (augmenting
the methods of Chapter 4). For storage of intermediate results (if you desire to
inspect them) we assume that the top-level pointer references *xp and **yp have been
validly initialized (e.g., by the utilities vector() and matrix()). Because steps
occur at unequal intervals results are only stored at intervals greater than dxsav. The
top-level variable kmax indicates the maximum number of steps that can be stored.
If kmax=0 there is no intermediate storage, and the pointers *xp and **yp need not
point to valid memory. Storage of steps stops if kmax is exceeded, except that the
ending values are always stored. Again, these controls are merely indicative of what
you might need. The routine odeint should be customized to the problem at hand.

#include <math.h>

#include "nrutil.h"
#define MAXSTP 10000
#define TINY 1.0e-30

extern int kmax,kount;

extern float *xp,**yp,dxsav;

User storage for intermediate results. Preset kmax and dxsav in the calling program. If kmax #
0 results are stored at approximate intervals dxsav in the arrays xp[1..kount], yp[1. .nvar]
(1. .kount], where kount is output by odeint. Defining declarations for these variables, with
memory allocations xp[1..kmax] and yp[1..nvar] [1..kmax] for the arrays, should be in
the calling program.

void odeint(float ystart[], int nvar, float x1, float x2, float eps, float hi,

float hmin, int *nok, int *nbad,

void (*derivs) (float, float [], float []),

void (*rkqs) (float [], float [], int, float *, float, float, float [],

float *, float *, void (*)(float, float [], float [])))
Runge-Kutta driver with adaptive stepsize control. Integrate starting values ystart [1. .nvar]
from x1 to x2 with accuracy eps, storing intermediate results in global variables. h1 should
be set as a guessed first stepsize, hmin as the minimum allowed stepsize (can be zero). On
output nok and nbad are the number of good and bad (but retried and fixed) steps taken, and
ystart is replaced by values at the end of the integration interval. derivs is the user-supplied
routine for calculating the right-hand side derivative, while rkqs is the name of the stepper
routine to be used.
{

int nstp,i;

float xsav,x,hnext,hdid,h;

float *yscal,*y,*dydx;

yscal=vector(1,nvar) ;
y=vector(1,nvar) ;
dydx=vector(1,nvar) ;
x=x1;
h=SIGN(h1,x2-x1);
nok = (*nbad) = kount = 0;
for (i=1;i<=nvar;it++) yli]=ystart [il];
if (kmax > 0) xsav=x-dxsav*2.0;
for (nstp=1;nstp<=MAXSTP;nstp++) {
(*derivs) (x,y,dydx) ;
for (i=1;i<=nvar;i++)
Scaling used to monitor accuracy. This general-purpose choice can be modified
if need be.
yscal [i] =fabs (y [i] )+fabs (dydx [i] *h)+TINY;
if (kmax > 0 && kount < kmax-1 && fabs(x-xsav) > fabs(dxsav)) {

Assures storage of first step.
Take at most MAXSTP steps.

xp [+t+kount] =x; Store intermediate results.
for (i=1;i<=nvar;i++) yp[i] [kount]=y[il];
xSav=x;

+

if ((x+h-x2)*(xth-x1) > 0.0) h=x2-x; If stepsize can overshoot, decrease.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

722 Chapter 16. Integration of Ordinary Differential Equations

(*rkqs) (y,dydx,nvar,&x,h,eps,yscal,&hdid, &hnext ,derivs) ;
if (hdid == h) ++(*nok); else ++(*nbad) ;

if ((x-x2)*(x2-x1) >= 0.0) { Are we done?
for (i=1;i<=nvar;it++) ystart[iJ]=y[il;
if (kmax) {
xp[++kount] =x; Save final step.

for (i=1;i<=nvar;i++) yp([i] [kount]=y [i];
3
free_vector(dydx,1,nvar) ;
free_vector(y,1,nvar) ;
free_vector(yscal,1,nvar) ;

return; Normal exit.
}
if (fabs(hnext) <= hmin) nrerror("Step size too small in odeint");
h=hnext;

+

nrerror("Too many steps in routine odeint");

CITED REFERENCES AND FURTHER READING:

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall). [1]

Cash, J.R., and Karp, A.H. 1990, ACM Transactions on Mathematical Software, vol. 16, pp. 201—
222. [2]

Shampine, L.F., and Watts, H.A. 1977, in Mathematical Software III, J.R. Rice, ed. (New York: Aca-
demic Press), pp. 257-275; 1979, Applied Mathematics and Computation, vol. 5, pp. 93—
121.

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for Mathematical
Computations (Englewood Cliffs, NJ: Prentice-Hall).

16.3 Modified Midpoint Method

This section discusses the modified midpoint method, which advances a vector
of dependent variables y(x) from a point x to a point x + H by a sequence of n
substeps each of size h,

h=H/n (16.3.1)

In principle, one could use the modified midpoint method in its own right as an ODE
integrator. In practice, the method finds its most important application as a part of
the more powerful Bulirsch-Stoer technique, treated in §16.4. You can therefore
consider this section as a preamble to $16.4.

The number of right-hand side evaluations required by the modified midpoint
method is n + 1. The formulas for the method are

zo +hf (x, zo)

a
Zm+1 = %m—1 + 2hf (x + mh, zm) for m=1,2,...,n—1

1
y(a+H) yn = pln + Zn-1 +hf(« + H, z)|

(16.3.2)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.3 Modified Midpoint Method 723

Here the z’s are intermediate approximations which march along in steps of h, while
Yn is the final approximation to y(z + H). The method is basically a “centered
difference” or “midpoint” method (compare equation 16.1.2), except at the first and
last points. Those give the qualifier “modified.”

The modified midpoint method is a second-order method, like (16.1.2), but with
the advantage of requiring (asymptotically for large n) only one derivative evaluation
per step h instead of the two required by second-order Runge-Kutta. Perhaps there
are applications where the simplicity of (16.3.2), easily coded in-line in some other
program, recommends it. In general, however, use of the modified midpoint method
by itself will be dominated by the embedded Runge-Kutta method with adaptive
stepsize control, as implemented in the preceding section.

The usefulness of the modified midpoint method to the Bulirsch-Stoer technique
(816.4) derives from a “deep” result about equations (16.3.2), due to Gragg. It turns
out that the error of (16.3.2), expressed as a power series in h, the stepsize, contains
only even powers of h,

oo
Yn — y(a + H) = Yo agh™ (16.3.3)
i=1

where H is held constant, but h changes by varying n in (16.3.1). The importance
of this even power series is that, if we play our usual tricks of combining steps to
knock out higher-order error terms, we can gain two orders at a time!

For example, suppose 7 is even, and let y,/ denote the result of applying
(16.3.1) and (16.3.2) with half as many steps, n — n/2. Then the estimate

y(e + H) © Ante (16.3.4)
is fourth-order accurate, the same as fourth-order Runge-Kutta, but requires only
about 1.5 derivative evaluations per step h instead of Runge-Kutta’s 4 evaluations.
Don’t be too anxious to implement (16.3.4), since we will soon do even better.
Now would be a good time to look back at the routine qsimp in $4.2, and
especially to compare equation (4.2.4) with equation (16.3.4) above. You will see
that the transition in Chapter 4 to the idea of Richardson extrapolation, as embodied
in Romberg integration of §4.3, is exactly analogous to the transition in going from

this section to the next one.

Here is the routine that implements the modified midpoint method, which will
be used below.

#include "nrutil.h"

void mmid(float y[], float dydx[], int nvar, float xs, float htot, int nstep,

float yout[], void (*derivs) (float, float[], float[]))
Modified midpoint step. At xs, input the dependent variable vector y[1..nvar] and its deriva-
tive vector dydx[1..nvar]. Also input is htot, the total step to be made, and nstep, the
number of substeps to be used. The output is returned as yout [1..nvar], which need not
be a distinct array from y; if it is distinct, however, then y and dydx are returned undamaged.
t

int n,i;

float x,swap,h2,h,*ym,*yn;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

724 Chapter 16. Integration of Ordinary Differential Equations

ym=vector (1,nvar) ;
yn=vector (1,nvar) ;

h=htot/nstep; Stepsize this trip.
for (i=1;i<=nvar;it+) {
ym(il=y[il;
yn(i]=y [i] +h*dydx [i] ; First step.
x=xsth;
(xderivs) (x, yn, yout) ; Will use yout for temporary storage of deriva-
h2=2.0*h; tives.
for (n=2;n<=nstep;nt+) { General step.

for (i=1;i<=nvar;it++) {
swap=ym [i] +h2*yout [i] ;

ym[il=yn[il] ;
yn[i]=swap;
}
x += h;
(*derivs) (x,yn, yout) ;
+
for (i=1;i<=nvar; i++) Last step.

yout [i] =0.5*(ym[i]+yn[i]+h*yout [i]) ;
free_vector(yn,1,nvar) ;
free_vector(ym,1,nvar) ;

CITED REFERENCES AND FURTHER READING:

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), 86.1.4.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§7.2.12.

16.4 Richardson Extrapolation and the
Bulirsch-Stoer Method

The techniques described in this section are not for differential equations
containing nonsmooth functions. For example, you might have a differential
equation whose right-hand side involves a function that is evaluated by table look-up
and interpolation. If so, go back to Runge-Kutta with adaptive stepsize choice:
That method does an excellent job of feeling its way through rocky or discontinuous
terrain. It is also an excellent choice for quick-and-dirty, low-accuracy solution
of a set of equations. A second warning is that the techniques in this section are
not particularly good for differential equations that have singular points inside the
interval of integration. A regular solution must tiptoe very carefully across such
points. Runge-Kutta with adaptive stepsize can sometimes effect this; more generally,
there are special techniques available for such problems, beyond our scope here.

Apart from those two caveats, we believe that the Bulirsch-Stoer method,
discussed in this section, is the best known way to obtain high-accuracy solutions
to ordinary differential equations with minimal computational effort. (A possible
exception, infrequently encountered in practice, is discussed in $16.7.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.4 Richardson Extrapolation and the Bulirsch-Stoer Method 725

extrapolation
to © steps

6 steps

n \ n \ n |
x x+H

Figure 16.4.1. Richardson extrapolation as used in the Bulirsch-Stoer method. A large interval H is
spanned by different sequences of finer and finer substeps. Their results are extrapolated to an answer
that is supposed to correspond to infinitely fine substeps. In the Bulirsch-Stoer method, the integrations
are done by the modified midpoint method, and the extrapolation technique is rational function or
polynomial extrapolation.

Three key ideas are involved. The first is Richardson’s deferred approach
to the limit, which we already met in §4.3 on Romberg integration. The idea is
to consider the final answer of a numerical calculation as itself being an analytic
function (if a complicated one) of an adjustable parameter like the stepsize h. That
analytic function can be probed by performing the calculation with various values
of h, none of them being necessarily small enough to yield the accuracy that we
desire. When we know enough about the function, we fit it to some analytic form,
and then evaluate it at that mythical and golden point h = 0 (see Figure 16.4.1).
Richardson extrapolation is a method for turning straw into gold! (Lead into gold
for alchemist readers.)

The second idea has to do with what kind of fitting function is used. Bulirsch and
Stoer first recognized the strength of rational function extrapolation in Richardson-
type applications. That strength is to break the shackles of the power series and its
limited radius of convergence, out only to the distance of the first pole in the complex
plane. Rational function fits can remain good approximations to analytic functions
even after the various terms in powers of h all have comparable magnitudes. In
other words, h can be so large as to make the whole notion of the “order” of the
method meaningless — and the method can still work superbly. Nevertheless, more
recent experience suggests that for smooth problems straightforward polynomial
extrapolation is slightly more efficient than rational function extrapolation. We will
accordingly adopt polynomial extrapolation as the default, but the routine bsstep
below allows easy substitution of one kind of extrapolation for the other. You
might wish at this point to review §3.1-§3.2, where polynomial and rational function
extrapolation were already discussed.

The third idea was discussed in the section before this one, namely to use
a method whose error function is strictly even, allowing the rational function or
polynomial approximation to be in terms of the variable / instead of just h.

Put these ideas together and you have the Bulirsch-Stoer method [1]. A single
Bulirsch-Stoer step takes us from x to « + H, where H is supposed to be quite a large

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

726 Chapter 16. — Integration of Ordinary Differential Equations

— not at all infinitesimal — distance. That single step is a grand leap consisting
of many (e.g., dozens to hundreds) substeps of modified midpoint method, which
are then extrapolated to zero stepsize.

The sequence of separate attempts to cross the interval H is made with increasing
values of n, the number of substeps. Bulirsch and Stoer originally proposed the
sequence

n = 2,4, 6,8, 12, 16, 24, 32, 48, 64,96,..., [nj = 2nj—2],... (16.4.1)
More recent work by Deuflhard [2,3] suggests that the sequence

n= 2,4,6,8, 10,12, 14,..., [nj = 2j],... (16.4.2)

is usually more efficient. For each step, we do not know in advance how far up this
sequence we will go. After each successive n is tried, a polynomial extrapolation
is attempted. That extrapolation gives both extrapolated values and error estimates.
If the errors are not satisfactory, we go higher in n. If they are satisfactory, we go
on to the next step and begin anew with n = 2.

Of course there must be some upper limit, beyond which we conclude that there
is some obstacle in our path in the interval H, so that we must reduce H rather than
just subdivide it more finely. In the implementations below, the maximum number
of n’s to be tried is called KMAXX. For reasons described below we usually take this
equal to 8; the 8th value of the sequence (16.4.2) is 16, so this is the maximum
number of subdivisions of H that we allow.

We enforce error control, as in the Runge-Kutta method, by monitoring internal
consistency, and adapting stepsize to match a prescribed bound on the local truncation
error. Each new result from the sequence of modified midpoint integrations allows a
tableau like that in §3.1 to be extended by one additional set of diagonals. The size of
the new correction added at each stage is taken as the (conservative) error estimate.
How should we use this error estimate to adjust the stepsize? The best strategy now
known is due to Deuflhard [2,3]. For completeness we describe it here:

Suppose the absolute value of the error estimate returned from the kth column (and hence
the & + 1st row) of the extrapolation tableau is €,+1,,. Error control is enforced by requiring
Ek+1k <€ (16.4.3)

as the criterion for accepting the current step, where ¢ is the required tolerance. For the even
sequence (16.4.2) the order of the method is 2k + 1:

2k+1
€kpiye ~ HORT (16.4.4)
Thus a simple estimate of a new stepsize H;, to obtain convergence ina fixed column k would be

c yen

Ek+1,k

=H ( (16.45)

Which column k should we aim to achieve convergence in? Let’s compare the work
required for different k. Suppose Aj, is the work to obtain row k of the extrapolation tableau,
so Ax41 is the work to obtain column &. We will assume the work is dominated by the cost
of evaluating the functions defining the right-hand s of the differential equations. For nz
subdivisions in H, the number of function evaluations can be found from the recurrence

Ai=m+1

(16.4.6)
Apyi = Ap t+ neti

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.4 Richardson Extrapolation and the Bulirsch-Stoer Method 727

The work per unit step to get column k is Ay41/H,, which we nondimensionalize with a
factor of H and write as

. Apu
Wi. = H 1647
k= Sr ( )
7  \1/(2k+1)
= Apt (+) (16.4.8)

The quantities W;, can be calculated during the integration. The optimal column index q
is then defined by

W,

I
E
i=]
=
=

(16.4.9)

where ky is the final column, in which the error criterion (16.4.3) was satisfied. The q
determined from (16.4.9) defines the stepsize H, to be used as the next basic stepsize, so that
we can expect to get convergence in the optimal column q.

Two important refinements have to be made to the strategy outlined so far:

e If the current H is “too small,” then ky will be “too small,” and so q remains
“too small.’ It may be desirable to increase H and aim for convergence in a
column q > ky.

e Ifthe current H is “too big,” we may not converge at all on the current step and we
will have to decrease H. We would like to detect this by monitoring the quantities
€41,k for each k so we can stop the current step as soon as possible.

Deuflhard’s prescription for dealing with these two problems uses ideas from communi-

cation theory to determine the “average expected convergence behavior” of the extrapolation.

His model produces certain correction factors a(k,q) by which H;, is to be multiplied to try
to get convergence in column q. The factors a(k, q) depend only on € and the sequence {nj}
and so can be computed once during initialization:

AgpirtAgti
a(k,q) = € CRM Ag+ a1) for k<q (16.4.10)

with a(q,q) = 1.

Now to handle the first problem, suppose convergence occurs in column g = ky. Then
rather than taking H/, for the next step, we might aim to increase the stepsize to get convergence
in column q + 1. Since we don’t have H,+1 available from the computation, we estimate it as

Ho41 = Haa(q,q+1) (16.4.11)

By equation (16.4.7) this replacement is efficient, i.e., reduces the work per unit step, if

Agri 5 Agte
> 16.4.12
or
Aq+ia(q,qg+1) > Aq+2 (16.4.13)

During initialization, this inequality can be checked for q = 1,2,... to determine kmax, the
largest allowed column. Then when (16.4.12) is satisfied it will always be efficient to use
Aq+1. (In practice we limit kmax to 8 even when ¢ is very small as there is very little further
gain in efficiency whereas roundoff can become a problem.)

The problem of stepsize reduction is handled by computing stepsize estimates

Ay, = Hralk,q); k=1

.q-l (16.4.14)

during the current step. The H’s are estimates of the stepsize to get convergence in the optimal
column q. If any Hy is “too small?’ we abandon the current step and restart using H;,. The
criterion of being “too small” is taken to be

Hya(k,q+1) <H (16.4.15)
The a’s satisfy a(k,qg + 1) > a(k,q).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

728 Chapter 16. —_ Integration of Ordinary Differential Equations

During the first step, when we have no information about the solution, the stepsize
reduction check is made for all k. Afterwards, we test for convergence and for possible
stepsize reduction only in an “order window”

max(1,q—1) <k < min(kmax,q + 1) (16.4.16)

The rationale for the order window is that if convergence appears to occur for k < q — 1 it
is often spurious, resulting from some fortuitously small error estimate in the extrapolation.
On the other hand, if you need to go beyond k = q + 1 to obtain convergence, your local
model of the convergence behavior is obviously not very good and you need to cut the
stepsize and reestablish it.

In the routine bsstep, these various tests are actually carried out using quantities

c+1k \1/(2k+1)
e(k) = H (Se)

HA, €
called err [k] in the code. As usual, we include a “safety factor” in the stepsize selection.
This is implemented by replacing € by 0.25¢. Other safety factors are explained in the
program comments.

Note that while the optimal convergence column is restricted to increase by at most one
on each step, a sudden drop in order is allowed by equation (16.4.9). This gives the method
a degree of robustness for problems with discontinuities.

(16.4.17)

Let us remind you once again that scaling of the variables is often crucial for
successful integration of differential equations. The scaling “trick” suggested in
the discussion following equation (16.2.8) is a good general purpose choice, but
not foolproof. Scaling by the maximum values of the variables is more robust, but
requires you to have some prior information.

The following implementation of a Bulirsch-Stoer step has exactly the same
calling sequence as the quality-controlled Runge-Kutta stepper rkqs. This means
that the driver odeint in §16.2 can be used for Bulirsch-Stoer as well as Runge-
Kutta: Just substitute bsstep for rkqs in odeint’s argument list. The routine
bsstep calls mmid to take the modified midpoint sequences, and calls pzextr, given
below, to do the polynomial extrapolation.

#include <math.h>
#include "nrutil.h"

#define KMAXX 8 Maximum row number used in the extrapola-
#define IMAXX (KMAXX+1) tion.

#define SAFE1 0.25 Safety factors.

#define SAFE2 0.7

#define REDMAX 1.0e-5 Maximum factor for stepsize reduction.
#define REDMIN 0.7 Minimum factor for stepsize reduction.
#define TINY 1.0e-30 Prevents division by zero.

#define SCALMX 0.1 1/SCALMX is the maximum factor by which a

stepsize can be increased.
float **d,*x;
Pointers to matrix and vector used by pzextr or rzextr.

void bsstep(float y[], float dydx[], int nv, float *xx, float htry, float eps,
float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []))
Bulirsch-Stoer step with monitoring of local truncation error to ensure accuracy and adjust
stepsize. Input are the dependent variable vector y[1..nv] and its derivative dydx[1. .nv]
at the starting value of the independent variable x. Also input are the stepsize to be attempted
htry, the required accuracy eps, and the vector yscal[1..nv] against which the error is
scaled. On output, y and x are replaced by their new values, hdid is the stepsize that was
actually accomplished, and hnext is the estimated next stepsize. derivs is the user-supplied
routine that computes the right-hand side derivatives. Be sure to set htry on successive steps

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.4 Richardson Extrapolation and the Bulirsch-Stoer Method 729

to the value of hnext returned from the previous step, as is the case if the routine is called
by odeint.
{
void mmid(float y[], float dydx[], int nvar, float xs, float htot,
int nstep, float yout[], void (*derivs) (float, float[], float[]));
void pzextr(int iest, float xest, float yest[], float yz[], float dy[],
int nv);
int i,iq,k,kk,km;
static int first=1,kmax,kopt;
static float epsold = -1.0,xnew;
float epsi,errmax,fact,h,red,scale,work,wrkmin,xest ;
float *err,*yerr,*ysav,*yseq;
static float a[IMAXx+1];
static float alf[KMAXX+1] [KMAXX+1] ;
static int nseq[IMAXX+1]={0,2,4,6,8,10,12,14,16,18};
int reduct ,exitflag=0;

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

d=matrix(1,nv,1,KMAXX) ;
err=vector(1,KMAXX) ;
x=vector (1,KMAXX) ;
yerr=vector(1,nv);
ysav=vector(1,nv);
yseq=vector(1,nv);

if (eps != epsold) { A new tolerance, so reinitialize.
*hnext = xnew = -1.0e29; “Impossible” values.
eps1=SAFE1*eps;
a(1]=nseq[1]+1; Compute work coefficients Aj.
for (k=1;k<=KMAXX;k++) a[k+1]=a[k] +nseq[k+1] ;
for (iq=2;iq<=KMAXX;iq++) { Compute a(k,q).
for (k=1;k<iq;k++)
alf [k] [iq] =pow(eps1, (a[k+1]-aliqt1])/
((aligqti]-a[1]+1.0)*(2#k+1)));
}
epsold=eps;
for (kopt=2;kopt<KMAXX;koptt++) Determine optimal row number for
if (a[koptt+1] > alkopt]*alf[kopt-1] [kopt]) break; convergence.
kmax=kopt ;

jisenv;it+) ysav[il=y(lil; Save the starting values.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

xnew || h != (*hnext)) { A new stepsize or a new integration:
3 re-establish the order window.
kopt=kmax ;
+
reduct=0;
for (;;) {
for (k=1;k<=kmax;k++) { Evaluate the sequence of modified
xnew=(*xx) +h; midpoint integrations.
if (xnew == (*xx)) nrerror("step size underflow in bsstep");
mmid(ysav,dydx,nv,*xx,h,nseq[k] ,yseq,derivs) ;
xest=SQR(h/nseq[k]) ; Squared, since error series is even.
pzextr(k,xest ,yseq,y,yerr,nv) ; Perform extrapolation.
if (k != 1) { Compute normalized error estimate
errmax=TINY; €(k).
for (i=1;i<=nv;it++) errmax=FMAX(errmax,fabs(yerr[i]/yscal[i]));
errmax /= eps; Scale error relative to tolerance.
km=k-1;
err [km] =pow(errmax/SAFE1,1.0/(2+*km+1)) ;
3
if (k != 1 && (k >= kopt-1 || first)) { In order window.
if (errmax < 1.0) { Converged.
exitflag=1;
break;

730

Chapter 16. Integration of Ordinary Differential Equations

if (k == kmax || k == kopt+1) { Check for possible stepsize
red=SAFE2/err [km] ; reduction.
break;

3
else if (k == kopt && alf[kopt-1] [kopt] < err[km]) {
red=1.0/err [kn] ;
break;
+
else if (kopt == kmax && alf[km] [kmax-1] < err[km]) {
red=alf [km] [kmax-1] *SAFE2/err [km] ;
break;
+
else if (alf[km] [kopt] < err[km]) {
red=alf [km] [kopt-1] /err [km] ;
break;

3
}
if (exitflag) break;

red=FMIN(red,REDMIN) ; Reduce stepsize by at least REDMIN

red=FMAX (red, REDMAX) ; and at most REDMAX.
h *= red;
reduct=1;

} Try again.

*XX=xNeW; Successful step taken.

-0e35; Compute optimal row for convergence
for (kk=1;kk<=km;kk++) { and corresponding stepsize.
fact=FMAX (err [kk] , SCALMX) ;
work=fact*a[kk+1] ;
if (work < wrkmin) {
scale=fact;
wrkmin=work;
kopt=kk+1;
}
+
*hnext=h/scale;
if (kopt >= k && kopt != kmax && !reduct) {
Check for possible order increase, but not if stepsize was just reduced.
fact=FMAX(scale/alf [kopt-1] [kopt] , SCALMX) ;
if (alkopt+1]*fact <= wrkmin) {
*hnext=h/fact;
kopt++;
}
}
free_vector(yseq,1,nv);
free_vector(ysav,1,nv);
free_vector(yerr,1,nv);
free_vector(x,1,KMAXX) ;
free_vector (err, 1,KMAXX) ;
free_matrix(d,1,nv,1,KMAXX) ;

The polynomial extrapolation routine is based on the same algorithm as polint

§3.1. It is simpler in that it is always extrapolating to zero, rather than to an arbitrary
value. However, it is more complicated in that it must individually extrapolate each
component of a vector of quantities.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.4 Richardson Extrapolation and the Bulirsch-Stoer Method 731

#include "nrutil.h"
extern float **d,*x; Defined in bsstep.

void pzextr(int iest, float xest, float yest[], float yz[], float dy[], int nv)
Use polynomial extrapolation to evaluate nv functions at a = 0 by fitting a polynomial to a
sequence of estimates with progressively smaller values 2 = xest, and corresponding function
vectors yest [1..nv]. This call is number iest in the sequence of calls. Extrapolated function
values are output as yz[1..nv], and their estimated error is output as dy[1. .nv].
{

int k1,j;

float q,f2,f1,delta,*c;

c=vector(1,nv);

x[iest]=xest; Save current independent variable.

for (j=1;j<=nv;j++) dy([jl=yz[jl=yest [jl];

if Ciest 1) { Store first estimate in first column.
for (j=1;j<=nv;jt+) d[j] [1]=yest [jl];

} else {

for (j=1;j<=nv;jt++) cljl=yest[j];
for (k1=1;k1l<iest;ki++) {
delta=1.0/(x[iest-k1]-xest) ;
fi=xest*delta;
f£2=x [iest-k1] *delta;
for (

Propagate tableau 1 diagonal more.

a(j] [k1]=dy [5];
delta=c[j]-q;
dy [j]=f1*delta;
c[j]=f2*delta;
yz[j] += day[j];
}
+
for (j=1;j<=nv;j++) d[j] [iest]=dy[j];
+

free_vector(c,1,nv);

Current wisdom favors polynomial extrapolation over rational function extrap-
olation in the Bulirsch-Stoer method. However, our feeling is that this view is guided
more by the kinds of problems used for tests than by one method being actually
“better.” Accordingly, we provide the optional routine rzextr for rational function
extrapolation, an exact substitution for pzextr above.

#include "nrutil.h"
extern float **d,*x; Defined in bsstep.

void rzextr(int iest, float xest, float yest[], float yz[], float dy[], int nv)
Exact substitute for pzextr, but uses diagonal rational function extrapolation instead of poly-
nomial extrapolation.
{

int k,j;

float yy,v,ddy,c,b1,b,*fx;

fx=vector(1,iest);
x[iest]=xest; Save current independent variable.
if (iest == 1)
for (j=1;j<=nv;jt+) {
yz[jl=yest[j];
a[jJ] (1]=yest [5];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

732 Chapter 16. — Integration of Ordinary Differential Equations

dy [j]=yest [j];

else {
for (k=1;k<iest ;k++)
fx [k+1] =x [iest-k] /xest;

for (j=1;j<env;jt+) { Evaluate next diagonal in tableau.
v=d(j] (11;
a(jJ] [1] =y: yest [jl];

for (k=2;k<=iest;k++) {

pi=fx[k] *v;

b=bi-c;

if (b) {
b=(c-v)/b;
ddy=c*b;
c=b1*b;

} else Care needed to avoid division by 0.
ddy=v;

if (k != iest) v=d[j] [kl];
d(j] [k]=ddy;
yy += ddy;
3
dy [jl=ddy;
yz[jl=yy;
}

free_vector(fx,1,iest);

CITED REFERENCES AND FURTHER READING:

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§7.2.14. [1]

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), §6.2.

Deuflhard, P. 1983, Numerische Mathematik, vol. 41, pp. 399-422. [2]
Deuflhard, P. 1985, SIAM Review, vol. 27, pp. 505-535. [3]

16.5 Second-Order Conservative Equations

Usually when you have a system of high-order differential equations to solve it is best
to reformulate them as a system of first-order equations, as discussed in §16.0. There is
a particular class of equations that occurs quite frequently in practice where you can gain
about a factor of two in efficiency by differencing the equations directly. The equations are
second-order systems where the derivative does not appear on the right-hand side:

y"=f(2,y), — y(%o0) =o, (wo) = 20 (16.5.1)

As usual, y can denote a vector of values.
Stoermer’s rule, dating back to 1907, has been a popular method for discretizing such
systems. With h = H/m we have

y1 = yo + hizo + 5hf (20, yo)
Yui — 2yn + yr—1 = h? f(ao + kh, yr), k=1,...,m-1 (16.5.2)
2m = (Ym — Ym—1)/h+ Shf (xo + Hy ym)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.5 Second-Order Conservative Equations 733

Here Zn is y’(xo + H). Henrici showed how to rewrite equations (16.5.2) to reduce roundoff
error by using the quantities Ay = yr41 — ye. Start with

Ao = h{zo + 3hf (x0, yo)

(16.5.3)
y1 = yo + Ao
Then for k = 1,...,m—1, set
Ag = An-1 +h? f (xo + kh, yr)
(16.5.4)
Yr+i = Yk + An
Finally compute the derivative from
tm = Am-i/h+ dhf(xo + Hy ym) (16.5.5)

Gragg again showed that the error series for equations (16.5.3)-(16.5.5) contains only
even powers of h, and so the method is a logical candidate for extrapolation a la Bulirsch-Stoer.
We replace mmid by the following routine stoerm:

#include "nrutil.h"

void stoerm(float y[], float d2y[], int nv, float xs, float htot, int nstep,

float yout[], void (*derivs) (float, float [], float []))
Stoermer's rule for integrating y’’ = f(x,y) for a system of nm = nv/2 equations. On input
y(1..nv] contains y in its first n elements and y’ in its second n elements, all evaluated at
xs. d2y[1..nv] contains the right-hand side function f (also evaluated at xs) in its first n
elements. Its second n elements are not referenced. Also input is htot, the total step to be
taken, and nstep, the number of substeps to be used. The output is returned as yout (1..nv],
with the same storage arrangement as y. derivs is the user-supplied routine that calculates f.
{

int i,n,neqns,nn;

float h,h2,halfh,x,*ytemp;

ytemp=vector(1,nv) ;

h=htot/nstep; Stepsize this trip.
halfh=0.5*h;
Number of equations.
for (i=1;i<=neqns;it++) { First step.
n=neqns+i;
ytemp [i] =y [i] +(ytemp [n] =h* (y [n] thalfh*d2y[i]));
+
x=xsth;
(*derivs) (x,ytemp, yout) ; Use yout for temporary storage of derivatives.
h2=h*h;
for (nn=2;nn<=nstep;nnt++) { General step.
for (i=1;i<=neqns;it++)
ytemp[i] += (ytemp[(n=neqns+i)] += h2*yout[i]);
x += h;
(*derivs) (x,ytemp, yout) ;
}
for (i=1;i<=neqns;it++) { Last step.
n=neqns+i;
yout [n] =ytemp [n] /hthalfh*yout [i] ;
yout [i]=ytemp [i] ;
}

free_vector(ytemp,1,nv) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

734 Chapter 16. Integration of Ordinary Differential Equations

Note that for compatibility with bsstep the arrays y and d2y are of length 2n for a
system of n second-order equations. The values of y are stored in the first n elements of y,
while the first derivatives are stored in the second n elements. The right-hand side f is stored
in the first n elements of the array d2y; the second n elements are unused. With this storage
arrangement you can use bsstep simply by replacing the call to mmid with one to stoerm
using the same arguments; just be sure that the argument nv of bsstep is set to 2n. You
should also use the more efficient sequence of stepsizes suggested by Deuflhard:

n=1,2,

4,5... (16.5.6)

and set KMAXX = 12 in bsstep.

CITED REFERENCES AND FURTHER READING:
Deuflhard, P. 1985, SIAM Review, vol. 27, pp. 505-535.

16.6 Stiff Sets of Equations

As soon as one deals with more than one first-order differential equation, the
possibility of a stiff set of equations arises. Stiffness occurs in a problem where
there are two or more very different scales of the independent variable on which
the dependent variables are changing. For example, consider the following set
of equations [1]:

u’ = 998u + 1998v

16.6.1
v’ = —999u — 1999v ( )
with boundary conditions

u(0) =1 v(0) =0 (16.6.2)
By means of the transformation

u=2’y-—z v=-ytz (16.6.3)

we find the solution

u—2Qen® — e7 10008

y= en 4g 10002 (16.6.4)

If we integrated the system (16.6.1) with any of the methods given so far in this
chapter, the presence of the e~ 10°” term would require a stepsize h < 1/1000 for
the method to be stable (the reason for this is explained below). This is so even

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.6 Stiff Sets of Equations 735

Figure 16.6.1. Example of an instability encountered in integrating a stiff equation (schematic). Here
it is supposed that the equation has two solutions, shown as solid and dashed lines. Although the initial
conditions are such as to give the solid solution, the stability of the integration (shown as the unstable
dotted sequence of segments) is determined by the more rapidly varying dashed solution, even after that
solution has effectively died away to zero. Implicit integration methods are the cure.

though the e~ 1°" term is completely negligible in determining the values of wu and
v as soon as one is away from the origin (see Figure 16.6.1).

This is the generic disease of stiff equations: we are required to follow the
variation in the solution on the shortest length scale to maintain stability of the
integration, even though accuracy requirements allow a much larger stepsize.

To see how we might cure this problem, consider the single equation

y’ = -cy (16.6.5)

where c > 0 is a constant. The explicit (or forward) Euler scheme for integrating
this equation with stepsize h is

Yn+1 = Yn + hy, = (L—ch)yn (16.6.6)

The method is called explicit because the new value y,4+1 is given explicitly in
terms of the old value y,. Clearly the method is unstable if h > 2/c, for then
lyn| ~ co as n > co.

The simplest cure is to resort to implicit differencing, where the right-hand side
is evaluated at the new y location. In this case, we get the backward Euler scheme:

Ung = Yn thy (16.6.7)
or
Yn
= 16.6.8
Yotd l+ch ( )

The method is absolutely stable: even as h — 00, yn41 — 0, which is in fact the
correct solution of the differential equation. If we think of x as representing time,
then the implicit method converges to the true equilibrium solution (i.e., the solution
at late times) for large stepsizes. This nice feature of implicit methods holds only
for linear systems, but even in the general case implicit methods give better stability.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

736 Chapter 16. Integration of Ordinary Differential Equations

Of course, we give up accuracy in following the evolution towards equilibrium if
we use large stepsizes, but we maintain stability.

These considerations can easily be generalized to sets of linear equations with
constant coefficients:

y'=-C-y (16.6.9)
where C is a positive definite matrix. Explicit differencing gives
Ynti = (1= CA) +n (16.6.10)

Now a matrix A” tends to zero as n — oo only if the largest eigenvalue of A
has magnitude less than unity. Thus y,, is bounded as n — oo only if the largest
eigenvalue of | — Ch is less than 1, or in other words

2

h< 16.6.11
Nina (16.6.1)
where Amax is the largest eigenvalue of C.
On the other hand, implicit differencing gives
Yntt =Yn +h nga (16.6.12)
or
Yn = (1+Ch)"}-y, (16.6.13)

If the eigenvalues of C are 4, then the eigenvalues of (1 + Ch) ~1 are (1 + Ah)~!,
which has magnitude less than one for all h. (Recall that all the eigenvalues of a
positive definite matrix are nonnegative.) Thus the method is stable for all stepsizes
h. The penalty we pay for this stability is that we are required to invert a matrix
at each step.

Not all equations are linear with constant coefficients, unfortunately! For
the system

y' =fy) (16.6.14)
implicit differencing gives
Ynta =Yn + h£Yn41) (16.6.15)

In general this is some nasty set of nonlinear equations that has to be solved iteratively
at each step. Suppose we try linearizing the equations, as in Newton’s method:

Ynt1 =Yn th

of
fy) + a “(Yagi — +) (16.6.16)

Here Of/Oy is the matrix of the partial derivatives of the right-hand side (the Jacobian
matrix). Rearrange equation (16.6.16) into the form

af] *
Ynu1 =Y¥n th bohe “f(y n) (16.6.17)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

16.6 Stiff Sets of Equations 737

If h is not too big, only one iteration of Newton’s method may be accurate enough
to solve equation (16.6.15) using equation (16.6.17). In other words, at each step
we have to invert the matrix

1h (16.6.18)

to find y,,,,. Solving implicit methods by linearization is called a “semi-implicit”
method, so equation (16.6.17) is the semi-implicit Euler method. It is not guaranteed
to be stable, but it usually is, because the behavior is locally similar to the case of
a constant matrix C described above.

So far we have dealt only with implicit methods that are first-order accurate.
While these are very robust, most problems will benefit from higher-order methods.
There are three important classes of higher-order methods for stiff systems:

e Generalizations of the Runge-Kutta method, of which the most useful
are the Rosenbrock methods. The first practical implementation of these
ideas was by Kaps and Rentrop, and so these methods are also called
Kaps-Rentrop methods.

e Generalizations of the Bulirsch-Stoer method, in particular a semi-implicit
extrapolation method due to Bader and Deuflhard.

e Predictor-corrector methods, most of which are descendants of Gear’s
backward differentiation method.

We shall give implementations of the first two methods. Note that systems where
the right-hand side depends explicitly on x, f(y, 2), can be handled by adding x to
the list of dependent variables so that the system to be solved is

(2) - (1) (166.19)

In both the routines to be given in this section, we have explicitly carried out this
replacement for you, so the routines can handle right-hand sides of the form f(y, x)
without any special effort on your part.

We now mention an important point: It is absolutely crucial to scale your vari-
ables properly when integrating stiff problems with automatic stepsize adjustment.
As in our nonstiff routines, you will be asked to supply a vector y,.,; with which
the error is to be scaled. For example, to get constant fractional errors, simply set
Yscal = ly|. You can get constant absolute errors relative to some maximum values
by setting y,,,) equal to those maximum values. In stiff problems, there are often
strongly decreasing pieces of the solution which you are not particularly intereste
in following once they are small. You can control the relative error above some
threshold C and the absolute error below the threshold by setting

Yscat = max(C, |y|) (16.6.20)

If you are using appropriate nondimensional units, then each component of C should
be of order unity. If you are not sure what values to take for C, simply try
setting each component equal to unity. We strongly advocate the choice (16.6.20)
for stiff problems.

One final warning: Solving stiff problems can sometimes lead to catastrophic
precision loss. Be alert for situations where double precision is necessary.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

738 Chapter 16. — Integration of Ordinary Differential Equations

Rosenbrock Methods

These methods have the advantage of being relatively simple to understand and imple-
ment. For moderate accuracies (¢ < 107+ — 10~° in the error criterion) and moderate-sized
systems (N < 10), they are competitive with the more complicated algorithms. For more
stringent parameters, Rosenbrock methods remain reliable; they merely become less efficient
than competitors like the semi-implicit extrapolation method (see below).

A Rosenbrock method seeks a solution of the form

y(wo +h) = yo + So ciki (16.6.21)

i=l

where the corrections k; are found by solving s linear equations that generalize the structure
in (16.6.17):

i-l i-1
(1 —-yhf’) -ki = hf (» + Souk, ) +E So yijkj, = (16.6.22)
j=l j=l

Here we denote the Jacobian matrix by f’. The coefficients y, ci, aij, and 7; are fixed
constants independent of the problem. If 7 = 7; = 0, this is simply a Runge-Kutta scheme.
Equations (16.6.22) can be solved successively for ki, k2,....

Crucial to the success of a stiff integration scheme is an automatic stepsize adjustment
algorithm. Kaps and Rentrop [2] discovered an embedded or Runge-Kutta-Fehlberg method
as described in §16.2: Two estimates of the form (16.6.21) are computed, the “real” one y and
a lower-order estimate Y with different coefficients é;,i = 1,...,8, where § < s but the k;
are the same. The difference between y and ¥ leads to an estimate of the local truncation error,
which can then be used for stepsize control. Kaps and Rentrop showed that the smallest value
of s for which embedding is possible is s = 4, § = 3, leading to a fourth-order method.

To minimize the matrix-vector multiplications on the right-hand side of (16.6.22), we
rewrite the equations in terms of quantities

i-l
8, = So wks + 7k: (166.23)
j=l

The equations then take the form
(1/7h - f") - 81 = £(¥0)
(1/yh —£') - 82 = Flyg + a21g,) + c21g,/h
(1/yh —£') +g = Flyo + asig, + as282) + (e318; + cs282)/h
(

I/yh —£') +84 = flyo + aaig, + G428> + G4agg) + (C118, + Ca2By + C4sy)/h
(16.6.24)

In our implementation stiff of the Kaps-Rentrop algorithm, we have carried out the
replacement (16.6.19) explicitly in equations (16.6.24), so you need not concern yourself
about it. Simply provide a routine (called derivs in stiff) that returns f (called dydx) as a
function of x and y. Also supply a routine jacobn that returns f’ (dfdy) and Of/Ox (dfdx)
as functions of x and y. If x does not occur explicitly on the right-hand side, then dfdx will
be zero. Usually the Jacobian matrix will be available to you by analytic differentiation of
the right-hand side f. If not, your routine will have to compute it by numerical differencing
with appropriate increments Ay.

Kaps and Rentrop gave two different sets of parameters, which have slightly different
stability properties. Several other sets have been proposed. Our default choice is that of
Shampine [3], but we also give you one of the Kaps-Rentrop sets as an option. Some proposed
parameter sets require function evaluations outside the domain of integration; we prefer to
avoid that complication.

The calling sequence of stiff is exactly the same as the nonstiff routines given earlier
in this chapter. It is thus “plug-compatible” with them in the general ODE integrating routine

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.6 Stiff Sets of Equations 739

odeint. This compatibility requires, unfortunately, one slight anomaly: While the user-
supplied routine derivs is a dummy argument (which can therefore have any actual name),
the other user-supplied routine is not an argument and must be named (exactly) jacobn.

stiff begins by saving the initial values, in case the step has to be repeated because
the error tolerance is exceeded. The linear equations (16.6.24) are solved by first computing
the LU decomposition of the matrix 1/yh — f' using the routine ludcmp. Then the four
g, are found by back-substitution of the four different right-hand sides using lubksb. Note
that each step of the integration requires one call to jacobn and three calls to derivs (one
call to get dydx before calling stiff, and two calls inside stiff). The reason only three
calls are needed and not four is that the parameters have been chosen so that the last two
calls in equation (16.6.24) are done with the same arguments. Counting the evaluation of
the Jacobian matrix as roughly equivalent to N evaluations of the right-hand side f, we see
that the Kaps-Rentrop scheme involves about N + 3 function evaluations per step. Note that
if N is large and the Jacobian matrix is sparse, you should replace the LU decomposition
by a suitable sparse matrix procedure.

Stepsize control depends on the fact that

Yexact = y + O(h?)
(16.6.25)
Yexact =
Thus
ly 9] = O(n") (16.6.26)

Referring back to the steps leading from equation (16.2.4) to equation (16.2.10), we see that
the new stepsize should be chosen as in equation (16.2.10) but with the exponents 1/4 and 1/5
replaced by 1/3 and 1/4, respectively. Also, experience shows that it is wise to prevent too large
a stepsize change in one step, otherwise we will probably have to undo the large change in the
next step. We adopt 0.5 and 1.5 as the maximum allowed decrease and increase of h in one step.

#include <math.h>

#include "nrutil.h"

#define SAFETY 0.9

#define GROW 1.5

#define PGROW -0.25

#define SHRNK 0.5

#define PSHRNK (-1.0/3.0)

#define ERRCON 0.1296

#define MAXTRY 40

Here NMAX is the maximum value of n; GROW and SHRNK are the largest and smallest factors
by which stepsize can change in one step; ERRCON equals (GROW/SAFETY) raised to the power
(1/PGROW) and handles the case when errmax ~ 0.
#tdefine GAM (1.0/2.0)

#define A21 2.0

#define A31 (48.0/25.0)

#define A32 (6.0/25.0)

#define C21 -8.0

#define C31 (372.0/25.0)

#define C32 (12.0/5.0)

#define C41 (-112.0/125.0)

#define C42 (-54.0/125.0)

#define C43 (-2.0/5.0)

#define B1 (19.0/9.0)

#define B2 (1.0/2.0)

#define B3 (25.0/108.0)

#define B4 (125.0/108.0)

#define E1 (17.0/54.0)

#define E2 (7.0/36.0)

#define E3 0.0

#define E4 (125.0/108.0)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

740 Chapter 16. —_ Integration of Ordinary Differential Equations

#define C1X (1.0/2.0)
#define C2X (-3.0/2.0)
#define C3X (121.0/50.0)
#define C4X (29.0/250.0)
#define A2X 1.0

#define A3X (3.0/5.0)

void stiff(float y[], float dydx[], int n, float *x, float htry, float eps,

float yscal[], float *hdid, float *hnext,

void (*derivs) (float, float [], float []))
Fourth-order Rosenbrock step for integrating stiff o.d.e.'s, with monitoring of local truncation
error to adjust stepsize. Input are the dependent variable vector y[1..n] and its derivative
dydx[1..n] at the starting value of the independent variable x. Also input are the stepsize to
be attempted htry, the required accuracy eps, and the vector yscal[1..n] against which
the error is scaled. On output, y and x are replaced by their new values, hdid is the stepsize
that was actually accomplished, and hnext is the estimated next stepsize. derivs is a user-
supplied routine that computes the derivatives of the right-hand side with respect to x, while
jacobn (a fixed name) is a user-supplied routine that computes the Jacobi matrix of derivatives
of the right-hand side with respect to the components of y.
t

void jacobn(float x, float y[], float dfdx[], float *+#dfdy, int n);

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

int i,j,jtry,*indx;

float d,errmax,h,xsav,**a,*dfdx,**dfdy,*dysav,*err;

float *g1,*g2,*g3,*g4,*ysav;

indx=ivector(1,n);

a=matrix(1,n,1,n);

dfdx=vector(1,n);

dfdy=matrix(1,n,1,n);

dysav=vector(1,n);

err=vector(1,n);

gl=vector(1,n);

g2=vector(1,n);

g3=vector(1,n) ;

g4=vector(1,n);

ysav=vector(1,n);

xsav=(*x) ; Save initial values.

for (i=1;i
ysav[i. 3
dysav [i] =dydx [i] ;

+
jacobn(xsav,ysav,dfdx,dfdy,n) ;
The user must supply this routine to return the n-by-n matrix dfdy and the vector dfdx.

h=htry; Set stepsize to the initial trial value.
for (jtry=1; jtry<=MAXTRY; jtry++) {
for (i=1;i<=n;it+) { Set up the matrix 1 — yhf’.
for (j=1;j<=n;j++) alil(j] = -dfdy[il] [j];
ali] [i] += 1.0/(GAM*h) ;
}
ludcmp(a,n,indx,&d) ; LU decomposition of the matrix.
for (i=1;i<=n;it++) Set up right-hand side for gy.
gili]=dysav [i] +h*C1X+dfdx [4] ;
lubksb(a,n, indx,g1); Solve for gy.
for (i=1;i<=n;it++) Compute intermediate values of y and x.

ylil=ysav [i]+A21*g1 [i];
*x=xsav+A2X*h;

(*derivs) (*x,y,dydx) ; Compute dydx at the intermediate values.
for (i=1;i<=n;it++) Set up right-hand side for go.
g2[i] =dydx [i] +hCox*dfdx [i] +C21*g1 [i] /h;
lubksb(a,n, indx,g2); Solve for go.
for (i=1;i<=n;it++) Compute intermediate values of y and x.

y (i]=ysav [i] +A31*g1 [i] +A32*g2 [i];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.6 Stiff Sets of Equations 741

+

*x=xsav+A3X*h;
(*derivs) (*x,y,dydx) ; Compute dydx at the intermediate values.
for (i=1;i<=n;it++) Set up right-hand side for g3.
g3 [i] =dydx [i] +h*C3X*dfdx [i] +(C31*g1 [i] +C32*g2[i])/h;
lubksb(a,n, indx, g3) ; Solve for g3.
for (i=1;i<=n;it++) Set up right-hand side for gy.
g4 [i] =dydx [i] +h*C4x*dfdx [i] +(C41*g1 [i] +C42*g2 [1] +C43+*g3[i]) /h;
lubksb(a,n, indx, g4); Solve for gy.
for (i=1;i<=n;it++) { Get fourth-order estimate of y and error estimate.
y[il=ysav [i] +B1*g1 [i] +B2*g2[i]+B3+*g3 [i] +B4*g4[il];
err [i] =E1*g1 [i] +E2*g2 [i] +E3*g3[i]+E4*g4[i] ;
}
*x=xsavth;
if (+x xsav) nrerror("stepsize not significant in stiff");
errmax=0.0; Evaluate accuracy.
for (i=1;i<=n;i++) errmax=FMAX(errmax,fabs(err[i]/yscal[i]));
errmax /= eps; Scale relative to required tolerance.
if (errmax <= 1.0) { Step succeeded. Compute size of next step and re-
*hdid=h; turn.
*hnext=(errmax > ERRCON ? SAFETY*h*pow(errmax,PGROW) : GROW*h) ;
free_vector(ysav,1,n);
free_vector(g4,1,n);
free_vector(g3,1,n);
free_vector(g2,1,n);
free_vector(gi,1,n);
free_vector(err,1,n);
free_vector(dysav,1,n);
free_matrix(dfdy,1,n,1,n);
free_vector(dfdx,1,n);
free_matrix(a,i,n,1,n);
free_ivector(indx,1,n);
return;
} else { Truncation error too large, reduce stepsize.
*hnext=SAFETY*h*pow (errmax , PSHRNK) ;
h=(h >= 0.0 ? FMAX(*hnext,SHRNK*h) : FMIN(*hnext ,SHRNK*h)) ;

}
Go back and re-try step.

nrerror("exceeded MAXTRY in stiff");

Here are the Kaps-Rentrop parameters, which can be substituted for those of Shampine

simply by replacing the #define statements:

#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define
#define

0.2

A21 2.0

A31 4.52470820736
A32 4.16352878860
C21 -5.07167533877
C31 6.02015272865
C32 0.159750684673
C41 -1.856343618677
C42 -8.50538085819
C43 -2.08407513602
B1 3.95750374663
B2 4.62489238836
B3 0.617477263873
B4 1.282612945268
E1 -2.30215540292
E2 -3.07363448539
E3 0.873280801802
E4 1.282612945268
C1X GAM

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

742 Chapter 16. — Integration of Ordinary Differential Equations

#define C2X -0.396296677520e-01
#define C3X 0.550778939579
#define C4X -0.553509845700e-01
#define A2X 0.462

#define A3X 0.880208333333

As an example of how stiff is used, one can solve the system
yi = —-013y1 — 1000y1 y3
yo = —2500y2ys (16.6.27)
ys = —.013y1 — 1000y1y3 — 2500y2y3

with initial conditions
m(0)=1, (0) =1, —-ys(0) =0 (166.28)

(This is test problem D4 in [4].) We integrate the system up to x = 50 with an initial stepsize
of h = 2.9 x 10-4 using odeint. The components of C in (16.6.20) are all set to unity.
The routines derivs and jacobn for this problem are given below. Even though the ratio
of largest to smallest decay constants for this problem is around 10°, stiff succeeds in
integrating this set in only 29 steps with « = 10~*. By contrast, the Runge-Kutta routine
rkqgs requires 51,012 steps!

void jacobn(float x, float y[], float dfdx[], float **dfdy, int n)
{

int i;

for (i=1;i<=n;i++) dfdx[i]=0.0;
dfdy(1] [1] = -0.013-1000.0*y [3];

dfdy [1] [2]=0.0;

dfdy(1][3] = -1000.0y[1];

dfdy [2] [1]=0.0;

dfdy(2] [2] = -2500.0*y[3];

dfdy(2][3] = -2500.0*y [2];

dfdy(3] [1] = -0.013-1000.0+y 3];

dfdy(3] [2] = -2500.0*y [3];

dfdy([3] [3] = -1000.0+*y[1]-2500.0*y [2];
}
void derivs(float x, float y[], float dydx[])
{

dydx[1] = -0.013*y[1]-1000.0*y [1] *y[3] ;

dydx[2] = -2500.0*y [2] *y [3];

dydx[3] = -0.013*y[1]-1000.0xy [1] *y [3] -2500.0*y [2] *y [31 ;
}

Semi-implicit Extrapolation Method

The Bulirsch-Stoer method, which discretizes the differential equation using the modified
midpoint rule, does not work for stiff problems. Bader and Deuflhard [5] discovered a semi-
implicit discretization that works very well and that lends itself to extrapolation exactly as
in the original Bulirsch-Stoer method.

The starting point is an implicit form of the midpoint rule:

Ynti + Yn-1 )

Ynti ~Yn-1 = ant ( 5) (16.6.29)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.6 Stiff Sets of Equations 743

Convert this equation into semi-implicit form by linearizing the right-hand side about f(y,,).
The result is the semi-implicit midpoint rule:
of of of
1-h]- = |1l+ha—|- Qh Jf -=: 16.6.30
>| Ynt1 + x Yn-1 + (Yn) = By va ( )

It is used with a special first step, the semi-implicit Euler step (16.6.17), and a special
“smoothing” last step in which the last y,, is replaced by

Yn = 5(Ynt1 +¥n—1) (16.6.31)

Bader and Deuflhard showed that the error series for this method once again involves only
even powers of h.

For practical implementation, it is better to rewrite the equations using Ay = y,44 —Yx-
With h = H/m, start by calculating

A ! no 7 hf(y)
o= |l-A— .
ay ° (16.6.32)
Y1 =Yo + Ao
Then for k = 1,...,m—1, set
at}~*
Apa Ana +21 =e] + [hf(y,) — An—a] (166.33)
Yeu =Ye + Av
Finally compute
ay"
Am = |1—h=] — -[hf(y,,) —Am—
| oy Um) 4 (16.6.34)
Yin = Ym + Am

It is easy to incorporate the replacement (16.6.19) in the above formulas. The additional
terms in the Jacobian that come from Of/0z all cancel out of the semi-implicit midpoint rule
(16.6.30). In the special first step (16.6.17), and in the corresponding equation (16.6.32), the
term hf becomes hf + h?9f/Ox. The remaining equations are all unchanged.

This algorithm is implemented in the routine simpr:

#include "nrutil.h"

void simpr(float y[], float dydx[], float dfdx[], float *+#dfdy, int n,

float xs, float htot, int nstep, float yout[],

void (*derivs) (float, float [], float []))
Performs one step of semi-implicit midpoint rule. Input are the dependent variable y[1. .n], its
derivative dydx[1..n], the derivative of the right-hand side with respect to x, dfdx[1..n],
and the Jacobian dfdy[1..n][1..n] at xs. Also input are htot, the total step to be taken,
and nstep, the number of substeps to be used. The output is returned as yout[1..n].
derivs is the user-supplied routine that calculates dydx.
{

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

int i,j,nn,*indx;

float d,h,x,**a,*del,*ytemp;

indx=ivector(1,n);

a=matrix(1,n,1,n);

del=vector(1,n);

ytemp=vector(1,n);

h=htot/nstep; Stepsize this trip.

for (i=1;i<=n;i++) { Set up the matrix | — hf’.
for (j=1;j<=n;jt++) ali] [j] = -h*dfdy[i] [jl];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

744 Chapter 16. —_ Integration of Ordinary Differential Equations

++a(il C4];
+
ludcemp(a,n, indx, &d) ; LU decomposition of the matrix.
for (i=1;i jit+) Set up right-hand side for first step. Use yout

yout [i] =h* (dydx [i] +h*dfdx[i]) ; for temporary storage.
lubksb(a,n,indx, yout) ;
for (i=1;i<=n;it++) First step.

ytemp[i]=y [i]+(del [i]=yout [i]) ;
x=xsth;
(*derivs) (x,ytemp, yout) ; Use yout for temporary storage of derivatives.
for (nn=2;nn<=nstep;nn++) { General step.

for (i=1;i<=n;it++) Set up right-hand side for general step.

yout [i] =h*yout [i]-del [i] ;
lubksb(a,n,indx, yout) ;
for (i=1;i<=n;it++)
ytemp[i] += (del[i] += 2.0*yout[i]);

x += h;

(*derivs) (x,ytemp, yout) ;
}
for (i=1;i<=n;it++) Set up right-hand side for last step.

yout [i] =h*yout [i] -del [i] ;

lubksb(a,n,indx, yout) ;

for (i=1;i<=n;it++) Take last step.
yout [i] += ytemp[il];

free_vector(ytemp,1,n);

free_vector(del,1,n);

free_matrix(a,i,n,1,n);

free_ivector(indx,1,n);

The routine simpr is intended to be used in a routine stifbs that is almost exactly the

same as bsstep. The only differences are:
e The stepsize sequence is

n = 2,6,10, 14, 22, 34, 50,..., (16.6.35)

where each member differs from its predecessor by the smallest multiple of 4 that
makes the ratio of successive terms be < 8. The parameter KMAXX is taken to be 7.

e The work per unit step now includes the cost of Jacobian evaluations as well
as function evaluations. We count one Jacobian evaluation as equivalent to N
function evaluations, where N is the number of equations.

e Once again the user-supplied routine derivs is adummy argument and so can have
any name. However, to maintain “plug-compatibility” with rkqs, bsstep and
stiff, the routine jacobn is not an argument and must have exactly this name. It
is called once per step to return f’ (dfdy) and Of/x (dfdx) as functions of x and y.

Here is the routine, with comments pointing out only the differences from bsstep:

#include <math.h>
#include "nrutil.h"
#define KMAXX 7
#tdefine IMAXX (KMAXX+1)
#define SAFE1 0.25
#define SAFE2 0.7
#define REDMAX 1.0e-5
#define REDMIN 0.7
#define TINY 1.0e-30
#define SCALMX 0.1

float **d,*x;
void stifbs(float y[], float dydx[], int nv, float *xx, float htry, float eps,

float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []))

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.6 Stiff Sets of Equations 745

Semi-implicit extrapolation step for integrating stiff o.d.e.’s, with monitoring of local truncation
error to adjust stepsize. Input are the dependent variable vector y[1..nv] and its derivative
dydx[1..nv] at the starting value of the independent variable x. Also input are the stepsize
to be attempted htry, the required accuracy eps, and the vector yscal[1..nv] against
which the error is scaled. On output, y and x are replaced by their new values, hdid is the
stepsize that was actually accomplished, and hnext is the estimated next stepsize. derivs
is a user-supplied routine that computes the derivatives of the right-hand side with respect to
x, while jacobn (a fixed name) is a user-supplied routine that computes the Jacobi matrix of
derivatives of the right-hand side with respect to the components of y. Be sure to set htry
on successive steps to the value of hnext returned from the previous step, as is the case if the
routine is called by odeint.

{

void jacobn(float x, float y[], float dfdx[], float *+#dfdy, int n);
void simpr(float y[], float dydx[], float dfdx[], float **dfdy,

int n, float xs, float htot, int nstep, float yout[],
void (*derivs) (float, float [], float []));

void pzextr(int iest, float xest, float yest[], float yz[], float dy[],

int nv);
int i,iq,k,kk,km;
static int first=1,kmax,kopt,nvold = -1;
static float epsold = -1.0,xnew;

float epsi,errmax,fact,h,red,scale,work,wrkmin,xest ;

float *dfdx,**dfdy,*err,*yerr,*ysav,*yseq;

static float a[IMAXX+1];

static float alf[KMAXX+1] [KMAXX+1] ;

static int nseq[IMAXX+1]={0,2,6,10,14,22,34,50,70}; Sequence is different from
int reduct ,exitflag=0; bsstep.

d=matrix(1,nv,1,KMAXX) ;
dfdx=vector(1,nv);
dfdy=matrix(1,nv,1,nv);
err=vector(1,KMAXX) ;
x=vector(1,KMAXX) ;
yerr=vector(1,nv);
ysav=vector(1,nv);
yseq=vector(1,nv);

if(eps != epsold || nv != nvold) {

+

Reinitialize also if nv has changed.
*hnext = xnew = -1.0e29;
eps1=SAFE1*eps;
a(1]=nseq[1]+1;
for (k=1;k<=KMAXX;k++) a[k+1]=a[k]+nseq[k+1] ;
for (iq=2;iq<=KMAXX;iqt+) {
for (k=1;k<iq;k++)
alf [k] [iq] =pow(eps1, ((a[k+1]-aliqti])/
((aliqti]-a[1]+1.0)*(2*k+1))));

}

epsold=eps;

nvold=nv; Save nv.

afi] += nv; Add cost of Jacobian evaluations to work
for (k=1;k<=KMAXX;k++) al[k+1]=a[k]+nseq[k+1] ; coefficients.

for (kopt=2;kopt<KMAXX;kopt++)
if (a[koptt+1] > alkopt]*alf[kopt-1] [kopt]) break;
kmax=kopt ;

h=htry;
for (i=1;i<=nv;it++) ysav[i]=y(il];

jacobn(*xx,y,dfdx,dfdy ,nv) ;

Evaluate Jacobian.

if (#xx != xnew || h != (*hnext)) {

+

first=1;
kopt=kmax ;

reduct=0;
for G3) f

for (k=1;k<=kmax;kt+) {

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

746

Chapter 16. —_ Integration of Ordinary Differential Equations

xnew=(*xx) +h;
if (xnew == (*xx)) nrerror("step size underflow in stifbs");
simpr (ysav,dydx,dfdx,dfdy ,nv,*xx,h,nseq[k] ,yseq,derivs) ;
Semi-implicit midpoint rule.
xest=SQR(h/nseq[k]) ; The rest of the routine is identical to
pzextr(k,xest ,yseq,y,yerr,nv) ; bsstep.
if (kK !=1) {
errmax=TINY;
for (i=1;i<=nv;it++) errmax=FMAX(errmax,fabs(yerr[i]/yscal[i]));
errmax /= eps;
km=k-1;
err [km] =pow(errmax/SAFE1,1.0/(2+*km+1)) ;

}
if (k != 1 && (k >= kopt-1 || first)) {
if (errmax < 1.0) {

exitflag=1;
break;
}
if (k == kmax || k == kopt+1) f
red=SAFE2/err [km] ;
break;
}

else if (k == kopt && alf[kopt-1] [kopt] < err[km]) {
red=1.0/err [kn] ;
break;
+
else if (kopt == kmax && alf[km] [kmax-1] < err[km]) {
red=alf [km] [kmax-1] *SAFE2/err [km] ;
break;
+
else if (alf[km] [kopt] < err[km]) {
red=alf [km] [kopt-1] /err [km] ;
break;

}
}
if (exitflag) break;
red=FMIN(red,REDMIN) ;
red=FMAX (red, REDMAX) ;

h *= red;
reduct=1;
+
*XX=xNeW}

.0e35;
for (kk=1;kk<=km;kkt++) {
fact=FMAX (err [kk] , SCALMX) ;
work=fact*a[kk+1] ;
if (work < wrkmin) {
scale=fact;
wrkmin=work;
kopt=kk+1;
}
+
*hnext=h/scale;
if (kopt >= k && kopt != kmax && !reduct) {
fact=FMAX(scale/alf [kopt-1] [kopt] , SCALMX) ;
if (alkopt+1]*fact <= wrkmin) {
*hnext=h/fact ;
kopt++;
}
+

free_vector(yseq,1,nv);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.7 Multistep, Multivalue, and Predictor-Corrector Methods 747

free_vector(ysav,1,nv);
free_vector(yerr,1,nv);
free_vector(x,1,KMAXX) ;
free_vector (err, 1,KMAXX) ;
free_matrix(dfdy,1,nv,1,nv);
free_vector (dfdx,1,nv);
free_matrix(d,1,nv,1,KMAXX) ;

The routine stifbs is an excellent routine for all stiff problems, competitive with
the best Gear-type routines. stiff is comparable in execution time for moderate N and
€ S$ 10“. By the time « ~ 1078, stifbs is roughly an order of magnitude faster. There
are further improvements that could be applied to stifbs to make it even more robust. For
example, very occasionally ludemp in simpr will encounter a singular matrix. You could
arrange for the stepsize to be reduced, say by a factor of the current nseq[k]. There are
also certain stability restrictions on the stepsize that come into play on some problems. For
a discussion of how to implement these automatically, see [6].

CITED REFERENCES AND FURTHER READING:

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall). [1]

Kaps, P., and Rentrop, P. 1979, Numerische Mathematik, vol. 33, pp. 55-68. [2]

Shampine, L.F. 1982, ACM Transactions on Mathematical Software, vol. 8, pp. 93-113. [3]

Enright, W.H., and Pryce, J.D. 1987, ACM Transactions on Mathematical Software, vol. 13,
pp. 1-27. [4]

Bader, G., and Deuflhard, P. 1983, Numerische Mathematik, vol. 41, pp. 373-398. [5]

Deuflhard, P. 1983, Numerische Mathematik, vol. 41, pp. 399-422.

Deuflhard, P. 1985, SIAM Review, vol. 27, pp. 505-535.

Deuflhard, P. 1987, “Uniqueness Theorems for Stiff ODE Initial Value Problems,” Preprint SC-
87-3 (Berlin: Konrad Zuse Zentrum fur Informationstechnik). [6]

Enright, W.H., Hull, T.E., and Lindberg, B. 1975, BIT, vol. 15, pp. 10-48.

Wanner, G. 1988, in Numerical Analysis 1987, Pitman Research Notes in Mathematics, vol. 170,
D.F. Griffiths and G.A. Watson, eds. (Harlow, Essex, U.K.: Longman Scientific and Tech-
nical).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag).

16.7 Multistep, Multivalue, and
Predictor-Corrector Methods

The terms multistep and multivalue describe two different ways of implementing
essentially the same integration technique for ODEs. Predictor-corrector is a partic-
ular subcategrory of these methods — in fact, the most widely used. Accordingly,
the name predictor-corrector is often loosely used to denote all these methods.

We suspect that predictor-corrector integrators have had their day, and that they
are no longer the method of choice for most problems in ODEs. For high-precision
applications, or applications where evaluations of the right-hand sides are expensive,
Bulirsch-Stoer dominates. For convenience, or for low precision, adaptive-stepsize
Runge-Kutta dominates. Predictor-corrector methods have been, we think, squeezed

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

748 Chapter 16. Integration of Ordinary Differential Equations

out in the middle. There is possibly only one exceptional case: high-precision
solution of very smooth equations with very complicated right-hand sides, as we
will describe later.

Nevertheless, these methods have had a long historical run. Textbooks are
full of information on them, and there are a lot of standard ODE programs around
that are based on predictor-corrector methods. Many capable researchers have a
lot of experience with predictor-corrector routines, and they see no reason to make
a precipitous change of habit. It is not a bad idea for you to be familiar with the
principles involved, and even with the sorts of bookkeeping details that are the bane
of these methods. Otherwise there will be a big surprise in store when you first have
to fix a problem in a predictor-corrector routine.

Let us first consider the multistep approach. Think about how integrating an
ODE is different from finding the integral of a function: For a function, the integrand
has a known dependence on the independent variable x, and can be evaluated at
will. For an ODE, the “integrand” is the right-hand side, which depends both on
x and on the dependent variables y. Thus to advance the solution of y’ = f(z, y)
from x, to x, we have

y(@) = Yn + [ f(a’, y) dx" (16.7.1)

In a single-step method like Runge-Kutta or Bulirsch-Stoer, the value yn+1 at 241
depends only on y,,. In a multistep method, we approximate f(z, y) by a polynomial
passing through several previous points 7,2, _1,... and possibly also through
n41. The result of evaluating the integral (16.7.1) at x = x p41 is then of the form

Yn+1 = Yn + (Bonga + B1Yn + B2Yn—1 + B3syn—2 + +++) (16.7.2)

where y/, denotes f (tn, Yn), and so on. If 39 = 0, the method is explicit; otherwise
it is implicit. The order of the method depends on how many previous steps we
use to get each new value of y.

Consider how we might solve an implicit formula of the form (16.7.2) for y n41-
Two methods suggest themselves: functional iteration and Newton’s method. In
functional iteration, we take some initial guess for y,+1, insert it into the right-hand
side of (16.7.2) to get an updated value of y,,41, insert this updated value back into
the right-hand side, and continue iterating. But how are we to get an initial guess for
Yn+1? Easy! Just use some explicit formula of the same form as (16.7.2). This is
called the predictor step. In the predictor step we are essentially extrapolating the
polynomial fit to the derivative from the previous points to the new point x 1 and
then doing the integral (16.7.1) in a Simpson-like manner from x, to 41. The
subsequent Simpson-like integration, using the prediction step’s value of yn+1 to
interpolate the derivative, is called the corrector step. The difference between the
predicted and corrected function values supplies information on the local truncation
error that can be used to control accuracy and to adjust stepsize.

If one corrector step is good, aren’t many better? Why not use each corrector
as an improved predictor and iterate to convergence on each step? Answer: Even if
you had a perfect predictor, the step would still be accurate only to the finite order
of the corrector. This incurable error term is on the same order as that which your
iteration is supposed to cure, so you are at best changing only the coefficient in front

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.7 Multistep, Multivalue, and Predictor-Corrector Methods 749

of the error term by a fractional amount. So dubious an improvement is certainly not
worth the effort. Your extra effort would be better spent in taking a smaller stepsize.

As described so far, you might think it desirable or necessary to predict several
intervals ahead at each step, then to use all these intervals, with various weights, in
a Simpson-like corrector step. That is not a good idea. Extrapolation is the least
stable part of the procedure, and it is desirable to minimize its effect. Therefore, the
integration steps of a predictor-corrector method are overlapping, each one involving
several stepsize intervals h, but extending just one such interval farther than the
previous ones. Only that one extended interval is extrapolated by each predictor step.

The most popular predictor-corrector methods are probably the Adams-
Bashforth-Moulton schemes, which have good stability properties. The Adams-
Bashforth part is the predictor. For example, the third-order case is

predictor: Yn+1 = Yn + Tp (23H)n — 16y,-1 + 5yn—2) + O(h*) (16.7.3)

Here information at the current point x, , together with the two previous points x ,_1
and t»_2 (assumed equally spaced), is used to predict the value y,,, at the next
point, 7,41. The Adams-Moulton part is the corrector. The third-order case is

corrector: Ynt1 = Yn + # (Buh +89 —Yn-1) FO(R*) — (16.7.4)
Without the trial value of y,+41 from the predictor step to insert on the right-hand
side, the corrector would be a nasty implicit equation for yn4+1.

There are actually three separate processes occurring in a predictor-corrector
method: the predictor step, which we call P, the evaluation of the derivative y/, , 1
from the latest value of y, which we call E, and the corrector step, which we call
C. In this notation, iterating m times with the corrector (a practice we inveighed
against earlier) would be written P(EC)". One also has the choice of finishing with
aC or anE step. The lore is that a final E is superior, so the strategy usually
recommended is PECE.

Notice that a PC method with a fixed number of iterations (say, one) is an
explicit method! When we fix the number of iterations in advance, then the final
value of y,41 can be written as some complicated function of known quantities. Thus
fixed iteration PC methods lose the strong stability properties of implicit methods
and should only be used for nonstiff problems.

For stiff problems we must use an implicit method if we want to avoid having
tiny stepsizes. (Not all implicit methods are good for stiff problems, but fortunately
some good ones such as the Gear formulas are known.) We then appear to have two
choices for solving the implicit equations: functional iteration to convergence, or
Newton iteration. However, it turns out that for stiff problems functional iteration will
not even converge unless we use tiny stepsizes, no matter how close our prediction
is! Thus Newton iteration is usually an essential part of a multistep stiff solver. For
convergence, Newton’s method doesn’t particularly care what the stepsize is, as long
as the prediction is accurate enough.

Multistep methods, as we have described them so far, suffer from two serious
difficulties when one tries to implement them:

e Since the formulas require results from equally spaced steps, adjusting

the stepsize is difficult.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

750 Chapter 16. Integration of Ordinary Differential Equations

e Starting and stopping present problems. For starting, we need the initial
values plus several previous steps to prime the pump. Stopping is a
problem because equal steps are unlikely to land directly on the desired
termination point.

Older implementations of PC methods have various cumbersome ways of
dealing with these problems. For example, they might use Runge-Kutta to start
and stop. Changing the stepsize requires considerable bookkeeping to do some
kind of interpolation procedure. Fortunately both these drawbacks disappear with
the multivalue approach.

For multivalue methods the basic data available to the integrator are the first
few terms of the Taylor series expansion of the solution at the current point x ,,. The
aim is to advance the solution and obtain the expansion coefficients at the next point
n41. This is in contrast to multistep methods, where the data are the values of
the solution at 7,,%,_1,.... We’ll illustrate the idea by considering a four-value
method, for which the basic data are

= hy},
Yn= (n2 /2)y!! (16.7.5)
(h3/6) yn

It is also conventional to scale the derivatives with the powers of h = p41 — Lp as
shown. Note that here we use the vector notation y to denote the solution and its
first few derivatives at a point, not the fact that we are solving a system of equations
with many components y.

In terms of the data in (16.7.5), we can approximate the value of the solution
y at some point x:

(2) = gn +(e —ay)gh, + FY yy 4 CAO yr

(16.7.6)

Set x = Xn+1 in equation (16.7.6) to get an approximation to y,+41. Differentiate
equation (16.7.6) and set x = x41 to get an approximation to y/, , ,,and similarly for
Yny1 and yi”, ,. Call the resulting approximation y,,41, where the tilde is a reminder
that all we have done so far is a polynomial extrapolation of the solution and its

derivatives; we have not yet used the differential equation. You can easily verify that

Ynti =B-y, (16.7.7)
where the matrix B is
111i
0 12 3
B= 0013 (16.7.8)
000 1

We now write the actual approximation to y,,,, that we will use by adding a
correction to Yn41:

Yn = Yn¢1 + ar (16.7.9)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

16.7 Multistep, Multivalue, and Predictor-Corrector Methods 751

Here r will be a fixed vector of numbers, in the same way that B is a fixed matrix.
We fix a by requiring that the differential equation

Una = F (n+ Yn+1) (16.7.10)
be satisfied. The second of the equations in (16.7.9) is
Unser = n4r + 72 (16.7.11)
and this will be consistent with (16.7.10) provided
re=1, a= hf (ent, Yt) — G4 (16.7.12)

The values of 71, 73, and 14 are free for the inventor of a given four-value method to
choose. Different choices give different orders of method (i.e., through what order
in h the final expression 16.7.9 actually approximates the solution), and different
stability properties.

An interesting result, not obvious from our presentation, is that multivalue and
multistep methods are entirely equivalent. In other words, the value y +41 given by
a multivalue method with given B and r is exactly the same value given by some
multistep method with given (’s in equation (16.7.2). For example, it turns out
that the Adams-Bashforth formula (16.7.3) corresponds to a four-value method with
ry = 0, r3 = 3/4, and r, = 1/6. The method is explicit because r} = 0. The
Adams-Moulton method (16.7.4) corresponds to the implicit four-value method with
ry = 5/12, r3 = 3/4, and ra = 1/6. Implicit multivalue methods are solved the
same way as implicit multistep methods: either by a predictor-corrector approach
using an explicit method for the predictor, or by Newton iteration for stiff systems.

Why go to all the trouble of introducing a whole new method that turns out
to be equivalent to a method you already knew? The reason is that multivalue
methods allow an easy solution to the two difficulties we mentioned above in actually
implementing multistep methods.

Consider first the question of stepsize adjustment. To change stepsize from h
to h’ at some point x,,, simply multiply the components of y,, in (16.7.5) by the
appropriate powers of h’/h, and you are ready to continue to x, + h’.

Multivalue methods also allow a relatively easy change in the order of the
method: Simply change r. The usual strategy for this is first to determine the new
stepsize with the current order from the error estimate. Then check what stepsize
would be predicted using an order one greater and one smaller than the current
order. Choose the order that allows you to take the biggest next step. Being able to
change order also allows an easy solution to the starting problem: Simply start with
a first-order method and let the order automatically increase to the appropriate level.

For low accuracy requirements, a Runge-Kutta routine like rkqs is almost
always the most efficient choice. For high accuracy, bsstep is both robust and
efficient. For very smooth functions, a variable-order PC method can invoke very
high orders. If the right-hand side of the equation is relatively complicated, so that
the expense of evaluating it outweighs the bookkeeping expense, then the best PC
packages can outperform Bulirsch-Stoer on such problems. As you can imagine,
however, such a variable-stepsize, variable-order method is not trivial to program. If

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

752 Chapter 16. Integration of Ordinary Differential Equations

you suspect that your problem is suitable for this treatment, we recommend use of a
canned PC package. For further details consult Gear [1] or Shampine and Gordon [2].

Our prediction, nevertheless, is that, as extrapolation methods like Bulirsch-
Stoer continue to gain sophistication, they will eventually beat out PC methods in
all applications. We are willing, however, to be corrected.

CITED REFERENCES AND FURTHER READING:

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equations (Englewood
Cliffs, NJ: Prentice-Hall), Chapter 9. [1]

Shampine, L.F., and Gordon, M.K. 1975, Computer Solution of Ordinary Differential Equations.
The Initial Value Problem. (San Francisco: W.H Freeman). [2]

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America), Chapter 5.

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software (Englewood Cliffs,
NJ: Prentice Hall), Chapter 8.

Hamming, R.W. 1962, Numerical Methods for Engineers and Scientists; reprinted 1986 (New
York: Dover), Chapters 14-15.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
Chapter 7.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 17. Two Point Boundary
Value Problems

17.0 Introduction

When ordinary differential equations are required to satisfy boundary conditions
at more than one value of the independent variable, the resulting problem is called a
two point boundary value problem. As the terminology indicates, the most common
case by far is where boundary conditions are supposed to be satisfied at two points —
usually the starting and ending values of the integration. However, the phrase “two
point boundary value problem” is also used loosely to include more complicated
cases, e.g., where some conditions are specified at endpoints, others at interior
(usually singular) points.

The crucial distinction between initial value problems (Chapter 16) and two
point boundary value problems (this chapter) is that in the former case we are able
to start an acceptable solution at its beginning (initial values) and just march it along
by numerical integration to its end (final values); while in the present case, the
boundary conditions at the starting point do not determine a unique solution to start
with — and a “random” choice among the solutions that satisfy these (incomplete)
starting boundary conditions is almost certain not to satisfy the boundary conditions
at the other specified point(s).

It should not surprise you that iteration is in general required to meld these
spatially scattered boundary conditions into a single global solution of the differential
equations. For this reason, two point boundary value problems require considerably
more effort to solve than do initial value problems. You have to integrate your dif-
ferential equations over the interval of interest, or perform an analogous “relaxation”
procedure (see below), at least several, and sometimes very many, times. Only in
the special case of linear differential equations can you say in advance just how
many such iterations will be required.

The “standard” two point boundary value problem has the following form: We
desire the solution to a set of N coupled first-order ordinary differential equations,
satisfying n; boundary conditions at the starting point 2;, and a remaining set of
nz = N — n, boundary conditions at the final point x2. (Recall that all differential
equations of order higher than first can be written as coupled sets of first-order
equations, cf. §16.0.)

The differential equations are

dx

= gi(t,41,y2,---,yn) t= 1,2,...,N (17.0.1)

753

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

754 Chapter 17. Two Point Boundary Value Problems

desired
+4 boundary
value

required
boundary
value x

Figure 17.0.1. Shooting method (schematic). Trial integrations that satisfy the boundary condition at one
endpoint are “launched.” The discrepancies from the desired boundary condition at the other endpoint are
used to adjust the starting conditions, until boundary conditions at both endpoints are ultimately satisfied.

At 21, the solution is supposed to satisfy
Byj (21,41, y2,--.,yn) =0 j=l,...,m (17.0.2)
while at x2, it is supposed to satisfy

Boj. (%2, 91, Y2,---,yn) =9 k=1,...,n2 (17.0.3)

There are two distinct classes of numerical methods for solving two point
boundary value problems. In the shooting method (§17.1) we choose values for all
of the dependent variables at one boundary. These values must be consistent with
any boundary conditions for that boundary, but otherwise are arranged to depend
on arbitrary free parameters whose values we initially “randomly” guess. We then
integrate the ODEs by initial value methods, arriving at the other boundary (and/or any
interior points with boundary conditions specified). In general, we find discrepancies
from the desired boundary values there. Now we have a multidimensional root-
finding problem, as was treated in §9.6 and §9.7: Find the adjustment of the free
parameters at the starting point that zeros the discrepancies at the other boundary
point(s). If we liken integrating the differential equations to following the trajectory
of a shot from gun to target, then picking the initial conditions corresponds to aiming
(see Figure 17.0.1). The shooting method provides a systematic approach to taking
a set of “ranging” shots that allow us to improve our “aim” systematically.

As another variant of the shooting method (§17.2), we can guess unknown free
parameters at both ends of the domain, integrate the equations to a common midpoint,
and seek to adjust the guessed parameters so that the solution joins “smoothly” at
the fitting point. In all shooting methods, trial solutions satisfy the differential
equations “exactly” (or as exactly as we care to make our numerical integration),
but the trial solutions come to satisfy the required boundary conditions only after
the iterations are finished.

Relaxation methods use a different approach. The differential equations are
replaced by finite-difference equations on a mesh of points that covers the range of

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.0 Introduction 755

\st iteration
nd iteration

required
boundary
value

required
boundary
value

Figure 17.0.2. Relaxation method (schematic). An initial solution is guessed that approximately satisfies
the differential equation and boundary conditions. An iterative process adjusts the function to bring it
into close agreement with the true solution.

the integration. A trial solution consists of values for the dependent variables at each
mesh point, not satisfying the desired finite-difference equations, nor necessarily even
satisfying the required boundary conditions. The iteration, now called relaxation,
consists of adjusting all the values on the mesh so as to bring them into successively
closer agreement with the finite-difference equations and, simultaneously, with the
boundary conditions (see Figure 17.0.2). For example, if the problem involves three
coupled equations and a mesh of one hundred points, we must guess and improve
three hundred variables representing the solution.

With all this adjustment, you may be surprised that relaxation is ever an efficient
method, but (for the right problems) it really is! Relaxation works better than
shooting when the boundary conditions are especially delicate or subtle, or where
they involve complicated algebraic relations that cannot easily be solved in closed
form. Relaxation works best when the solution is smooth and not highly oscillatory.
Such oscillations would require many grid points for accurate representation. The
number and position of required points may not be known a priori. Shooting methods
are usually preferred in such cases, because their variable stepsize integrations adjust
naturally to a solution’s peculiarities.

Relaxation methods are often preferred when the ODEs have extraneous
solutions which, while not appearing in the final solution satisfying all boundary
conditions, may wreak havoc on the initial value integrations required by shooting.
The typical case is that of trying to maintain a dying exponential in the presence
of growing exponentials.

Good initial guesses are the secret of efficient relaxation methods. Often one
has to solve a problem many times, each time with a slightly different value of some
parameter. In that case, the previous solution is usually a good initial guess when
the parameter is changed, and relaxation will work well.

Until you have enough experience to make your own judgment between the two
methods, you might wish to follow the advice of your authors, who are notorious
computer gunslingers: We always shoot first, and only then relax.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

756 Chapter 17. Two Point Boundary Value Problems

Problems Reducible to the Standard Boundary Problem

There are two important problems that can be reduced to the standard boundary
value problem described by equations (17.0.1) — (17.0.3). The first is the eigenvalue
problem for differential equations. Here the right-hand side of the system of
differential equations depends on a parameter A,

dyi(2)
dx

= gilt. y1,-.., yn, A) (17.0.4)

and one has to satisfy N + 1 boundary conditions instead of just N. The problem
is overdetermined and in general there is no solution for arbitrary values of A. For
certain special values of A, the eigenvalues, equation (17.0.4) does have a solution.

We reduce this problem to the standard case by introducing a new dependent
variable

yN41 =A (17.0.5)
and another differential equation
dyn+1
—— =0 17.0.6
de (17.06)

An example of this trick is given in §17.4.

The other case that can be put in the standard form is a free boundary problem.
Here only one boundary abscissa x, is specified, while the other boundary x2 is to
be determined so that the system (17.0.1) has a solution satisfying a total of N + 1
boundary conditions. Here we again add an extra constant dependent variable:

YN41 =2@2- 21 (17.0.7)
dyn+1

= 17.0.8

ano ( )

We also define a new independent variable t by setting
wa, =tynii, 0<t<l1 (17.0.9)

The system of N + 1 differential equations for dy;/dt is now in the standard form,
with ¢ varying between the known limits 0 and 1.

CITED REFERENCES AND FURTHER READING:

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems (Waltham, MA:
Blaisdell).

Kippenhan, R., Weigert, A., and Hofmeister, E. 1968, in Methods in Computational Physics,
vol. 7 (New York: Academic Press), pp. 129ff.

Eggleton, P.P. 1971, Monthly Notices of the Royal Astronomical Society, vol. 151, pp. 351-364.
London, R.A., and Flannery, B.P. 1982, Astrophysical Journal, vol. 258, pp. 260-269.

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§87.3-7.4.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.1 The Shooting Method 757

17.1 The Shooting Method

In this section we discuss “pure” shooting, where the integration proceeds from
x to x2, and we try to match boundary conditions at the end of the integration. In
the next section, we describe shooting to an intermediate fitting point, where the
solution to the equations and boundary conditions is found by launching “shots”
from both sides of the interval and trying to match continuity conditions at some
intermediate point.

Our implementation of the shooting method exactly implements multidimen-
sional, globally convergent Newton-Raphson (§9.7). It seeks to zero n2 functions
of nz variables. The functions are obtained by integrating N differential equations
from x; to x2. Let us see how this works:

At the starting point x; there are N starting values y; to be specified, but
subject to 1 conditions. Therefore there are nz = N — 7, freely specifiable starting
values. Let us imagine that these freely specifiable values are the components of a
vector V that lives in a vector space of dimension n2. Then you, the user, knowing
the functional form of the boundary conditions (17.0.2), can write a function that
generates a complete set of NV starting values y, satisfying the boundary conditions
at x,, from an arbitrary vector value of V in which there are no restrictions on the n 2
component values. In other words, (17.0.2) converts to a prescription

yilt1) = yi(tisM,---.Vns) i=1,...,N (17.1.1)

Below, the function that implements (17.1.1) will be called load.

Notice that the components of V might be exactly the values of certain “free”
components of y, with the other components of y determined by the boundary
conditions. Alternatively, the components of V might parametrize the solutions that
satisfy the starting boundary conditions in some other convenient way. Boundary
conditions often impose algebraic relations among the y ;, rather than specific values
for each of them. Using some auxiliary set of parameters often makes it easier to
“solve” the boundary relations for a consistent set of y;’s. It makes no difference
which way you go, as long as your vector space of V’s generates (through 17.1.1)
all allowed starting vectors y.

Given a particular V, a particular y(x1) is thus generated. It can then be turned
into a y(x2) by integrating the ODEs to x2 as an initial value problem (e.g., using
Chapter 16’s odeint). Now, at x2, let us define a discrepancy vector F, also of
dimension n2, whose components measure how far we are from satisfying the n 2
boundary conditions at x2 (17.0.3). Simplest of all is just to use the right-hand
sides of (17.0.3),

Fy, = Box(%2,y) k=1,...,n2 (17.1.2)

As in the case of V, however, you can use any other convenient parametrization,
as long as your space of F’s spans the space of possible discrepancies from the
desired boundary conditions, with all components of F equal to zero if and only if
the boundary conditions at 4 are satisfied. Below, you will be asked to supply a
user-written function score which uses (17.0.3) to convert an N-vector of ending
values y(x2) into an n2-vector of discrepancies F.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

758 Chapter 17. Two Point Boundary Value Problems

Now, as far as Newton-Raphson is concerned, we are nearly in business. We
want to find a vector value of V that zeros the vector value of F. We do this
by invoking the globally convergent Newton’s method implemented in the routine
newt of §9.7. Recall that the heart of Newton’s method involves solving the set
of ng linear equations

J-5V=-F (17.1.3)
and then adding the correction back,
yrew = volt 4 §V (17.1.4)

In (17.1.3), the Jacobian matrix J has components given by

OF;

Ja = oy,

(17.1.5)

It is not feasible to compute these partial derivatives analytically. Rather, each
requires a separate integration of the N ODEs, followed by the evaluation of

OF, Fi(V,...,Vj + AV...) — Fi(Y,-.., Vin.)
av; AV,

(17.1.6)

This is done automatically for you in the routine fdjac that comes with newt. The
only input to newt that you have to provide is the routine vecfunc that calculates
F by integrating the ODEs. Here is the appropriate routine, called shoot, that is
to be passed as the actual argument in newt:

#include "nrutil.h"
#define EPS 1.0e-6

extern int nvar; Variables that you must define and set in your main pro-
extern float x1,x2; gram.
int kmax,kount; Communicates with odeint.

float *xp,**yp,dxsav;

void shoot(int n, float v[], float f[])
Routine for use with newt to solve a two point boundary value problem for nvar coupled ODEs
by shooting from x1 to x2. Initial values for the nvar ODEs at x1 are generated from the n2
input coefficients v[1..n2], using the user-supplied routine load. The routine integrates the
ODEs to x2 using the Runge-Kutta method with tolerance EPS, initial stepsize h1, and minimum
stepsize hmin. At x2 it calls the user-supplied routine score to evaluate the n2 functions
£[1..n2] that ought to be zero to satisfy the boundary conditions at x2. The functions f
are returned on output. newt uses a globally convergent Newton's method to adjust the values
of v until the functions f are zero. The user-supplied routine derivs(x,y,dydx) supplies
derivative information to the ODE integrator (see Chapter 16). The first set of global variables
above receives its values from the main program so that shoot can have the syntax required
for it to be the argument vecfunc of newt.
{
void derivs(float x, float y[], float dydx[]);
void load(float x1, float v[], float y[]);
void odeint(float ystart[], int nvar, float x1, float x2,
float eps, float hi, float hmin, int *nok, int *nbad,
void (*derivs) (float, float [], float []),
void (*rkqs)(float [], float [], int, float *, float, float,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.1 The Shooting Method 759

float [], float *, float *, void (*)(float, float [], float [])));
void rkqs(float y[], float dydx[], int n, float *x,
float htry, float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []));
void score(float xf, float y[], float f£[]);
int nbad,nok;
float hi,hmin=0.0,*y;

y=vector(1,nvar) ;

kmax=0;

hi=(x2-x1)/100.0;

load(x1,v,y);

odeint (y,nvar,x1,x2,EPS,hi,hmin,&nok,&nbad,derivs,rkqs) ;
score(x2,y,f);

free_vector(y,1,nvar);

For some problems the initial stepsize AV might depend sensitively upon the
initial conditions. It is straightforward to alter load to include a suggested stepsize
hi as another output variable and feed it to fdjac via a global variable.

A complete cycle of the shooting method thus requires n2 + 1 integrations of
the N coupled ODEs: one integration to evaluate the current degree of mismatch,
and mz for the partial derivatives. Each new cycle requires a new round of n2 + 1
integrations. This illustrates the enormous extra effort involved in solving two point
boundary value problems compared with initial value problems.

If the differential equations are linear, then only one complete cycle is required,
since (17.1.3)-(17.1.4) should take us right to the solution. A second round can be
useful, however, in mopping up some (never all) of the roundoff error.

As given here, shoot uses the quality controlled Runge-Kutta method of §16.2
to integrate the ODEs, but any of the other methods of Chapter 16 could just as
well be used.

You, the user, must supply shoot with: (i) a function load(x1,v,y) which
calculates the n-vector y[1..n] (satisfying the starting boundary conditions, of
course), given the freely specifiable variables of v[1..n2] at the initial point x1;
(ii) a function score(x2,y,f) which calculates the discrepancy vector f[1. .n2]
of the ending boundary conditions, given the vector y[1..n] at the endpoint x2;
(iii) a starting vector v [1. .n2]; (iv) a function derivs for the ODE integration; and
other obvious parameters as described in the header comment above.

In 817.4 we give a sample program illustrating how to use shoot.

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America).

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems (Waltham, MA:
Blaisdell).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

760 Chapter 17. Two Point Boundary Value Problems

17.2 Shooting to a Fitting Point

The shooting method described in §17.1 tacitly assumed that the “shots” would
be able to traverse the entire domain of integration, even at the early stages of
convergence to a correct solution. In some problems it can happen that, for very
wrong starting conditions, an initial solution can’t even get from x1 to x2 without
encountering some incalculable, or catastrophic, result. For example, the argument
of a square root might go negative, causing the numerical code to crash. Simple
shooting would be stymied.

A different, but related, case is where the endpoints are both singular points
of the set of ODEs. One frequently needs to use special methods to integrate near
the singular points, analytic asymptotic expansions, for example. In such cases it is
feasible to integrate in the direction away from a singular point, using the special
method to get through the first little bit and then reading off “initial” values for
further numerical integration. However it is usually not feasible to integrate into
a singular point, if only because one has not usually expended the same analytic
effort to obtain expansions of “wrong” solutions near the singular point (those not
satisfying the desired boundary condition).

The solution to the above mentioned difficulties is shooting to a fitting point.
Instead of integrating from x1 to 22, we integrate first from x1 to some point x that
is between x, and x2; and second from «v2 (in the opposite direction) to x s.

If (as before) the number of boundary conditions imposed at x; is n;, and the
number imposed at x2 is m2, then there are nz freely specifiable starting values at
x, and n, freely specifiable starting values at x2. (If you are confused by this, go
back to §17.1.) We can therefore define an n-vector V1) of starting parameters
at x1, and a prescription load1(x1,v1,y) for mapping V1) into a y that satisfies
the boundary conditions at x,

yilt1) = yi(tisVayis-+->Vayne) = 1... N (17.2.1)

Likewise we can define an n ,-vector V2) of starting parameters at x2, and a
prescription load2(x2,v2,y) for mapping Vz) into a y that satisfies the boundary
conditions at x2,

yi(w2) = yi(v2;Viay1.---,Vayn,) = t= 1... N (17.2.2)

We thus have a total of N freely adjustable parameters in the combination of
Va) and V(2). The N conditions that must be satisfied are that there be agreement
in N components of y at 2 ¢ between the values obtained integrating from one side
and from the other,

yi(ap;s Vay) = yilre; Vea) i=1,...,N (17.2.3)
In some problems, the N matching conditions can be better described (physically,
mathematically, or numerically) by using NV different functions F';, i = 1...N,each

possibly depending on the NV components y;. In those cases, (17.2.3) is replaced by

Fily(xs; Vay)] = Fily(zs; Via) i=1,...,N (17.2.4)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.2 Shooting to a Fitting Point 761

In the program below, the user-supplied function score (xf ,y,f) is supposed
to map an input N-vector y into an output N-vector F. In most cases, you can
dummy this function as the identity mapping.

Shooting to a fitting point uses globally convergent Newton-Raphson exactly
as in §17.1. Comparing closely with the routine shoot of the previous section, you
should have no difficulty in understanding the following routine shootf. The main
differences in use are that you have to supply both load1 and load2. Also, in the
calling program you must supply initial guesses for vi[1..n2] and v2[1..n1].
Once again a sample program illustrating shooting to a fitting point is given in §17.4.

#include "nrutil.h"
#define EPS 1.0e-6

extern int nn2,nvar; Variables that you must define and set in your main pro-
extern float x1,x2,xf; gram.
int kmax,kount; Communicates with odeint.

float *xp,**yp,dxsav;

void shootf(int n, float v[], float f[])
Routine for use with newt to solve a two point boundary value problem for nvar coupled
ODEs by shooting from x1 and x2 to a fitting point xf. Initial values for the nvar ODEs at
x1 (x2) are generated from then2 (n1) coefficients vl (v2), using the user-supplied routine
load1 (load2). The coefficients v1 and v2 should be stored in a single array v[1. .ni+n2]
in the main program by statements of the form vi=v; and v2 = &v[n2];. The input param-
eter n = ni + n2 = nvar. The routine integrates the ODEs to xf using the Runge-Kutta
method with tolerance EPS, initial stepsize h1, and minimum stepsize hmin. At xf it calls the
user-supplied routine score to evaluate the nvar functions £1 and £2 that ought to match
at xf. The differences f are returned on output. newt uses a globally convergent Newton's
method to adjust the values of v until the functions f are zero. The user-supplied routine
derivs(x,y,dydx) supplies derivative information to the ODE integrator (see Chapter 16).
The first set of global variables above receives its values from the main program so that shoot
can have the syntax required for it to be the argument vecfunc of newt. Set nn2 = n2 in
the main program.
{
void derivs(float x, float y[], float dydx[]);
void loadi(float x1, float vif], float y[]);
void load2(float x2, float v2[], float y[]);
void odeint(float ystart[], int nvar, float x1, float x2,
float eps, float hi, float hmin, int *nok, int *nbad,
void (*derivs) (float, float [], float []),
void (*rkqs)(float [], float [], int, float *, float, float,
float [], float *, float *, void (*)(float, float [], float [])));
void rkqs(float y[], float dydx[], int n, float *x,
float htry, float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []));
void score(float xf, float y[], float f£[]);
int i,nbad,nok;
float hi,hmin=0.0,*f1,¥*f2,*y;

f1=vector(1,nvar) ;

f£2=vector(1,nvar) ;

y=vector(1,nvar) ;

kmax=0;

hi=(x2-x1)/100.0;

loadi(x1,v,y); Path from x1 to xf with best trial values v1.
odeint (y,nvar,x1,xf,EPS,hi,hmin,&nok,&nbad,derivs,rkqs) ;
score(xf,y,f1);

load2(x2,&v[nn2] ,y); Path from x2 to xf with best trial values v2.
odeint (y,nvar,x2,xf,EPS,hi,hmin,&nok,&nbad,derivs,rkqs) ;
score(xf,y,£2);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

762 Chapter 17. Two Point Boundary Value Problems

for (i=1;i<=n;it++) f[i]=f1[i]-f2[i];
free_vector(y,1,nvar);
free_vector(f2,1,nvar) ;
free_vector(f1,1,nvar) ;

There are boundary value problems where even shooting to a fitting point fails
— the integration interval has to be partitioned by several fitting points with the
solution being matched at each such point. For more details see [1].

CITED REFERENCES AND FURTHER READING:

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition (Washington: Mathe-
matical Association of America).

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems (Waltham, MA:
Blaisdell).

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§87.3.5-7.3.6. [1]

17.3 Relaxation Methods

In relaxation methods we replace ODEs by approximate finite-difference equations
(FDEs) on a grid or mesh of points that spans the domain of interest. As a typical example,
we could replace a general first-order differential equation

dy
—2 = g(z, 17.3.1
Te 7 I(@Y) ( )
with an algebraic equation relating function values at two points k,k — 1:
Yk — Yk—-1 — (te — ee-1) 9 [3 (ae + 2-1), $ (yu + yr—1)] =0 (17.3.2)

The form of the FDE in (17.3.2) illustrates the idea, but not uniquely: There are many
ways to turn the ODE into an FDE. When the problem involves N coupled first-order ODEs
represented by FDEs on a mesh of IM points, a solution consists of values for N dependent
functions given at each of the MM mesh points, or N x M variables in all. The relaxation
method determines the solution by starting with a guess and improving it, iteratively. As the
iterations improve the solution, the result is said to relax to the true solution.

While several iteration schemes are possible, for most problems our old standby, multi-
dimensional Newton’s method, works well. The method produces a matrix equation that
must be solved, but the matrix takes a special, “block diagonal” form, that allows it to be
inverted far more economically both in time and storage than would be possible for a general
matrix of size (MN) x (MN). Since MN can easily be several thousand, this is crucial
for the feasibility of the method.

Our implementation couples at most pairs of points, as in equation
(17.3.2). More points can be coupled, but then the method becomes more complex.
We will provide enough background so that you can write a more general scheme if you
have the patience to do so.

Let us develop a general set of algebraic equations that represent the ODEs by FDEs. The
ODE problem is exactly identical to that expressed in equations (17.0.1)-(17.0.3) where we had
NN coupled first-order equations that satisfy n; boundary conditions at 7, and ng = N — ny
boundary conditions at x2. We first define a mesh or grid by a set of k = 1,2 M points
at which we supply values for the independent variable z,. In particular, x, is the initial
boundary, and zyz is the final boundary. We use the notation y, to refer to the entire set of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.3 Relaxation Methods 763

dependent variables y1, y2,..., yn at point x. At an arbitrary point k in the middle of the
mesh, we approximate the set of N first-order ODEs by algebraic relations of the form

0 = Ex = yp —Yp—1 — (@k — Ce-1) 8p (Tk, Tk-1, Yes Yai) k= 2,3,...,M (17.3.3)

The notation signifies that g, can be evaluated using information from both points k,k — 1.
The FDEs labeled by E;, provide N equations coupling 2N variables at points k, k — 1. There
are M — 1 points, k = 2,3,...,M, at which difference equations of the form (17.3.3) apply.
Thus the FDEs provide a total of (14 — 1) N equations for the MN unknowns. The remaining
NN equations come from the boundary conditions.

At the first boundary we have

0=E; =B(r1,y;) (17.34)
while at the second boundary
0=Emy1 =C(amu,yy) (17.3.5)

The vectors E; and B have only n; nonzero components, corresponding to the n, boundary
conditions at x;. It will turn out to be useful to take these nonzero components to be the
last n1 components. In other words, E£j,1 4 0 only for 7 = n2 + 1,n2 +2 N. At
the other boundary, only the first n2 components of Eyy+1 and C are nonzero: Ej,741 #0
only for 7 = 1,2,..., m2.

The “solution” of the FDE problem in (17.3.3)-(17.3.5) consists of a set of variables
yj,k the values of the N variables y; at the M points x. The algorithm we describe
below requires an initial guess for the y;,,. We then determine increments Ay;,, such that
yj,k + Ayj,« is an improved approximation to the solution.

Equations for the increments are developed by expanding the FDEs in first-order Taylor
series with respect to small changes Ay,. At an interior point, k = 2,3,...,.M this gives:

Ex(¥, + Avg, Yn + AY,=1) © Ee (Yes Ye—-1)

N

N
OE, OE, (17.3.6)
f Ayn k—1 4 AYn,k
' > OYn,k—-1 Yuka OYn,ke Uruk

n=1 n=1

For a solution we want the updated value E(y + Ay) to be zero, so the general set of equations
at an interior point can be written in matrix form as

N 2N
SO SjnAyne-at S> SjnAyn—ne =—Eje, f= 1,2,...,N (17.3.7)
n=1 n=N+4+1
where
OE; OEjn
Sin =p, Sintn =z, n=1,2,..., N 1738
” Oynea PN Oy WS Atos ( )

The quantity S;,, is an N x 2N matrix at each point k. Each interior point thus supplies a
block of N equations coupling 2.N corrections to the solution variables at the points k, k — 1.

Similarly, the algebraic relations at the boundaries can be expanded in a first-order
Taylor series for increments that improve the solution. Since E; depends only on y,, we
find at the first boundary:

N
SO SinAgnr =—Eja, j=n2+1,n2+2,...,N (17.3.9)
n=1
where
OB;,.
in = , n=1,2,...,N 173.10
Buna ( )

At the second boundary,

N
> SjnAynm = —Ejmyi, gj =1,2,...,n2 (17.3.11)

n=1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
22
ao
so
es
az
oe
$3
22
5
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
go
§5
an
gd
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

9
o
3
Ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

764 Chapter 17. Two Point Boundary Value Problems

where
_ OF j,mM+1

Sin = =1,2,...,N 17.3.12
ia Oyn,M " . ( )

We thus have in equations (17.3.7)-(17.3.12) a set of linear equations to be solved for
the corrections Ay, iterating until the corrections are sufficiently small. The equations have
a special structure, because each $j,» couples only points k, k — 1. Figure 17.3.1 illustrates
the typical structure of the complete matrix equation for the case of 5 variables and 4 mesh
points, with 3 boundary conditions at the first boundary and 2 at the second. The 3 x 5
block of nonzero entries in the top left-hand corner of the matrix comes from the boundary
condition Sj,, at point k = 1. The next three 5 x 10 blocks are the Sj, at the interior
points, coupling variables at mesh points (2,1), (3,2), and (4,3). Finally we have the block
corresponding to the second boundary condition.

We can solve equations (17.3.7)-(17.3.12) for the increments Ay using a form of
Gaussian elimination that exploits the special structure of the matrix to minimize the total
number of operations, and that minimizes storage of matrix coefficients by packing the
elements in a special blocked structure. (You might wish to review Chapter 2, especially
§2.2, if you are unfamiliar with the steps involved in Gaussian elimination.) Recall that
Gaussian elimination consists of manipulating the equations by elementary operations such
as dividing rows of coefficients by a common factor to produce unity in diagonal elements,
and adding appropriate multiples of other rows to produce zeros below the diagonal. Here
we take advantage of the block structure by performing a bit more reduction than in pure
Gaussian elimination, so that the storage of coefficients is minimized. Figure 17.3.2 shows
the form that we wish to achieve by elimination, just prior to the backsubstitution step. Only a
small subset of the reduced MN x MN matrix elements needs to be stored as the elimination
progresses. Once the matrix elements reach the stage in Figure 17.3.2, the solution follows
quickly by a backsubstitution procedure.

Furthermore, the entire procedure, except the backsubstitution step, operates only on
one block of the matrix at a time. The procedure contains four types of operations: (1)
partial reduction to zero of certain elements of a block using results from a previous step,
(2) elimination of the square structure of the remaining block elements such that the square
section contains unity along the diagonal, and zero in off-diagonal elements, (3) storage of the
remaining nonzero coefficients for use in later steps, and (4) backsubstitution. We illustrate
the steps schematically by figures.

Consider the block of equations describing corrections available from the initial boundary
conditions. We have n; equations for N unknown corrections. We wish to transform the first
block so that its left-hand n1 x m1 square section becomes unity along the diagonal, and zero
in off-diagonal elements. Figure 17.3.3 shows the original and final form of the first block
of the matrix. In the figure we designate matrix elements that are subject to diagonalization
by “D”, and elements that will be altered by “A”; in the final block, elements that are stored
are labeled by “S”. We get from start to finish by selecting in turn m1 “pivot” elements from
among the first n; columns, normalizing the pivot row so that the value of the “pivot” element
is unity, and adding appropriate multiples of this row to the remaining rows so that they
contain zeros in the pivot column. In its final form, the reduced block expresses values for the
corrections to the first n; variables at mesh point 1 in terms of values for the remaining nz
unknown corrections at point 1, i.e., we now know what the first m1 elements are in terms of
the remaining n2 elements. We store only the final set of n2 nonzero columns from the initial
block, plus the column for the altered right-hand side of the matrix equation.

We must emphasize here an important detail of the method. To exploit the reduced
storage allowed by operating on blocks, it is essential that the ordering of columns in the
s matrix of derivatives be such that pivot elements can be found among the first n, rows
of the matrix. This means that the n; boundary conditions at the first point must contain
some dependence on the first j=1,2,...,71 dependent variables, y[j] [1]. If not, then the
original square n X 7 subsection of the first block will appear to be singular, and the method
will fail. Alternatively, we would have to allow the search for pivot elements to involve all
N columns of the block, and this would require column swapping and far more bookkeeping.
The code provides a simple method of reordering the variables, i.e., the columns of the s
matrix, so that this can be done easily. End of important detail.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

765

17.3 Relaxation Methods

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.
Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

ana A

Pr rr reer ere

XXXXXXKXXXX
XXXXXXKXXXX
XXXXXXKXXXX
XXXXXXKXXXX
XXXXXXKXXXX

xx x
xx x
xx xX

xx x
xx xX

XX XXXXXXXX
XX XXXXXXXX
XX XXXXXXXX
XX XXXXXXXX
XX XXXXXXXX

Vv
Vv
Vv
Vv
Vv

XXXXXKXKXXXX

XXXXXKXKXXXX

XXXXXKXKXXXX

XXXXXKXKXXXX

XXXXXKXKXXXX

xx
xx
xx
xx
xx

Matrix structure of a set of linear finite-difference equations (FDEs) with boundary

Figure 17.3.1.

Here X represents a coefficient of the FDEs, V represents a

component of the unknown solution vector, and B is a component of the known right-hand side. Empty
spaces represent zeros. The matrix equation is to be solved by a special form of Gaussian elimination.

conditions imposed at both endpoints.
(See text for details.)

ama e

>>> rp >>ee>

aman

> >>>

[-ai-)

>>

xx
xx

1

aaa es

>>>

~ x
xx

>

ian elimination. Once the matrix of Figure 17.3.1 has been

Figure 17.3.2. Target structure of the Gau:

reduced to this form, the solution follows quickly by backsubstitution.

Two Point Boundary Value Problems

Chapter 17.

766

Vv
Vv
Vv

(a) DDDAA

DDDAA
DDDAA

(b) 100SS8

0o10SS8S

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

Ss a
€ Fe
: ‘»
a 6
= S
2 :
i =
Aa tag)
mo i
| °°
z g
Ee nNnnnatatatactcd NANNNNNNN 2 NANnNnnnat<ct NANNNNN
5 £
2
Ss PPrPrrrr>r SP >r>r>r>r>r> =£ PrPr rr rr FP err rr
° 83 q
4g 5
3 E
> 22 tiactaatatc NAnNnNNnNN pa ANNNNADA ANNNNS
55 atetcc< ANNnNnN ¥ ANMNNHAAQ NHHNN=
sé qaqaqgqa ocoooon 3 CSCOCOATNN cooo7r°o
ao Ss
n £5 aaaaa coorOo yg SOOT ONN OOOASCSO
n $38 aaqaqaa coxnceo 3e SOMA OONN OCOHOSCS
= osc
= €@ NNNAARAAAQA NNNOASCSCS EE — oncooco
eo. os
o &¢ ANNHAAAADA uannacccc BE moose s~ccooce
nw aa
o €& SCOAFNNNNN OOM OSCCCO 85 coooo coooo
= &
2&8 SAONNNNN OCOrHoocoococeo 2s coooo coooo
& A
fa -=SONNNNN ASOCCCOO Bg ooooo ooooo
=e a
“4 > ~~ 23 > ~~
22 s i) go 5 3S
PE 2 hear) = na hear
3s 3.
32 SE
“as BE
% &
as ae
ce Ca
“3 ~eé
28 2
3 "bh a¢
Be Be
HO He

Vv

00001

(a)

Reduction process for the last (lower right) block of the matrix in Figure 17.3.1.

Figure 17.3.5.

Original form, (b) final form. (See text for explanation.)

17.3 Relaxation Methods 767

Next consider the block of N equations representing the FDEs that describe the relation
between the 2N corrections at points 2 and 1. The elements of that block, together with results
from the previous step, are illustrated in Figure 17.3.4. Note that by adding suitable multiples
of rows from the first block we can reduce to zero the first n1 columns of the block (labeled
by “Z’), and, to do so, we will need to alter only the columns from mi + 1 to N and the
vector element on the right-hand side. Of the remaining columns we can diagonalize a square
subsection of N x N elements, labeled by “D” in the figure. In the process we alter the final
set of n2 + 1 columns, denoted “A” in the figure. The second half of the figure shows the
block when we finish operating on it, with the stored (nz + 1) x N elements labeled by “S.”

If we operate on the next set of equations corresponding to the FDEs coupling corrections
at points 3 and 2, we see that the state of available results and new equations exactly reproduces
the situation described in the previous paragraph. Thus, we can carry out those steps again
for each block in turn through block M. Finally on block M + 1 we encounter the remaining
boundary conditions.

Figure 17.3.5 shows the final block of nz FDEs relating the NV corrections for variables
at mesh point /, together with the result of reducing the previous block. Again, we can first
use the prior results to zero the first m1 columns of the block. Now, when we diagonalize
the remaining square section, we strike gold: We get values for the final nz corrections
at mesh point M.

With the final block reduced, the matrix has the desired form shown previously in
Figure 17.3.2, and the matrix is ripe for backsubstitution. Starting with the bottom row and
working up towards the top, at each stage we can simply determine one unknown correction
in terms of known quantities.

The function solvde organizes the steps described above. The principal procedures
used in the algorithm are performed by functions called internally by solvde. The function
red eliminates leading columns of the s matrix using results from prior blocks. pinvs
diagonalizes the square subsection of s and stores unreduced coefficients. bksub carries
out the backsubstitution step. The user of solvde must understand the calling arguments,
as described below, and supply a function difeq, called by solvde, that evaluates the s
matrix for each block.

Most of the arguments in the call to solvde have already been described, but some
require discussion. Array y[j] [k] contains the initial guess for the solution, with j labeling
the dependent variables at mesh points k. The problem involves ne FDEs spanning points
k=1,..., m. nb boundary conditions apply at the first point k=1. The array indexv[j]
establishes the correspondence between columns of the s matrix, equations (17.3.8), (17.3.10),
and (17.3.12), and the dependent variables. As described above it is essential that the nb
boundary conditions at k=1 involve the dependent variables referenced by the first nb columns
of the s matrix. Thus, columns j of the s matrix can be ordered by the user in difeq to refer
to derivatives with respect to the dependent variable indexv[j].

The function only attempts itmax correction cycles before returning, even if the solution
has not converged. The parameters conv, slowc, scalv relate to convergence. Each
inversion of the matrix produces corrections for ne variables at m mesh points. We want these
to become vanishingly small as the iterations proceed, but we must define a measure for the
size of corrections. This error “norm” is very problem specific, so the user might wish to
rewrite this section of the code as appropriate. In the program below we compute a value
for the average correction err by summing the absolute value of all corrections, weighted by
a scale factor appropriate to each type of variable:

1 AY [3] [«]|
err ey scaivijl (173.13)

When err<conv, the method has converged. Note that the user gets to supply an array scalv
which measures the typical size of each variable.

Obviously, if err is large, we are far from a solution, and perhaps it is a bad idea
to believe that the corrections generated from a first-order Taylor series are accurate. The
number slowc modulates application of corrections. After each iteration we apply only a

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

768 Chapter 17. Two Point Boundary Value Problems

fraction of the corrections found by matrix inversion:

YOGI0k + YOj) 0d 4 slows _ ay 3) Da) (173.14)
max(slowc, err)

Thus, when err>slowc only a fraction of the corrections are used, but when err<slowc
the entire correction gets applied.

The call statement also supplies solvde with the array y[1..nyj] [1..nyk] con-
taining the initial trial solution, and workspace arrays c[1..ne] [1..ne-nbt+1] [1..m+1],
s[1..ne][1..2*net+1]. The array c is the blockbuster: It stores the unreduced elements
of the matrix built up for the backsubstitution step. If there are m mesh points, then there
will be m+1 blocks, each requiring ne rows and ne-nb+1 columns. Although large, this
is small compared with (nexm)* elements required for the whole matrix if we did not
break it into blocks.

We now describe the workings of the user-supplied function difeq. The synopsis of
the function is

void difeq(int k, int ki, int k2, int jsf, int isi, int isf,
int indexv[], int ne, float **s, float **y);

The only information passed from difeq to solvde is the matrix of derivatives
s[1..ne] [1..2*ne+1]; all other arguments are input to difeq and should not be altered.
k indicates the current mesh point, or block number. k1,k2 label the first and last point in
the mesh. If k=k1 or k>k2, the block involves the boundary conditions at the first or final
points; otherwise the block acts on FDEs coupling variables at points k-1, k.

The convention on storing information into the array s[i][j] follows that used in
equations (17.3.8), (17.3.10), and (17.3.12): Rows i label equations, columns j refer to
derivatives with respect to dependent variables in the solution. Recall that each equation will
depend on the ne dependent variables at either one or two points. Thus, j runs from 1 to
either ne or 2*ne. The column ordering for dependent variables at each point must agree
with the list supplied in indexv[j]. Thus, for a block not at a boundary, the first column
multiplies AY (1=indexv[1] ,k-1), and the column ne+1 multiplies AY (1=indexv[1] ,k).
is1,isf give the numbers of the starting and final rows that need to be filled in the s matrix
for this block. jsf labels the column in which the difference equations EF; ;, of equations
(17.3.3)+(17.3.5) are stored. Thus, —s[i] [jsf] is the vector on the right-hand side of the
matrix. The reason for the minus sign is that difeq supplies the actual difference equation,
£Ej,~, not its negative. Note that solvde supplies a value for jsf such that the difference
equation is put in the column just after all derivatives in the s matrix. Thus, difeq expects to
find values entered into s[i] [j] for rows ist <i < isf and1 < j < jsf.

Finally, s{1..nsi][1..nsj] and y(1..nyj][1..nyk] supply difeq with storage
for s and the solution variables y for this iteration. An example of how to use this routine
is given in the next section.

Many ideas in the following code are due to Eggleton [1].

#include <stdio.h>
#include <math.h>
#include "nrutil.h"

void solvde(int itmax, float conv, float slowc, float scalv[], int indexv[],
int ne, int nb, int m, float **y, float ***c, float **s)

Driver routine for solution of two point boundary value problems by relaxation. itmax is the
maximum number of iterations. conv is the convergence criterion (see text). slowc controls
the fraction of corrections actually used after each iteration. scalv[1..ne] contains typical
sizes for each dependent variable, used to weight errors. indexv[1..ne] lists the column
ordering of variables used to construct the matrix s[1..ne] [1..2*ne+1] of derivatives. (The
nb boundary conditions at the first mesh point must contain some dependence on the first nb
variables listed in indexv.) The problem involves ne equations for ne adjustable dependent
variables at each point. At the first mesh point there are nb boundary conditions. There are a
total of m mesh points. y[1..ne] [1..m] is the two-dimensional array that contains the initial
guess for all the dependent variables at each mesh point. On each iteration, it is updated by the

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.3 Relaxation Methods 769

calculated correction. The arrays c[1..ne][1..ne-nb+1] [1..m+1] and s supply dummy
storage used by the relaxation code.
{
void bksub(int ne, int nb, int jf, int ki, int k2, float ***c);
void difeq(int k, int ki, int k2, int jsf, int isi, int isf,
int indexv[], int ne, float **s, float **y);
void pinvs(int ie1, int ie2, int je1, int jsf, int jcl, int k,
float ***c, float **s);
void red(int izi, int iz2, int jz1, int jz2, int jm1, int jm2, int jmf,
int icl, int jc1, int jcf, int kc, float ***c, float **s);
int ici,ic2,ic3,ic4,it,j,j1,j2,j3,54,55, 46,57. 48,593
int jc1,jcf,jv,k,k1,k2,km,kp,nvars,*kmax;
float err,errj,fac,vmax,vz,*ermax;

doa,

kmax=ivector(1,ne);
ermax=vector(1,ne);
k1=1;

k2=m;

nvars=ne*m;

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

Set up row and column markers.

for (it=1;it<=itmax;it++) { Primary iteration loop.
k=k1; Boundary conditions at first point.
difeq(k,k1,k2,j9,ic3,ic4,indexv,ne,s,y);
pinvs(ic3,ic4,j5,j9,jc1,k1,c,s);
for (k=kit+1;k<=k2;k++) { Finite difference equations at all point pairs.
kp=k-1;
difeq(k,k1,k2,j9,ic1,ic4,indexv,ne,s,y);
red(icl,ic4,j1,j2,j3,j4,j9,ic3,jc1,jcf,kp,c,s);
pinvs(ic1,ic4,j3,j9,jc1,k,c,s);
}
k=k2+1; Final boundary conditions.
difeq(k,k1,k2,j9,icl,ic2,indexv,ne,s,y);
red(ic1,ic2,j5,j6,j7,j8,j9,ic3,jc1,jcf,k2,c,s);
pinvs(icl,ic2,j7,j9,jcf,k2+1,c,s);

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

bksub(ne,nb, jcf ,k1,k2,c); Backsubstitution.
err=0.0;
for (j=1;j<=sne;jt+) { Convergence check, accumulate average er-
jv=indexv[j]; ror.
errj=vmax=0.0;
km=0;
for (k=k1;k<=k2;k++) { Find point with largest error, for each de-
vz=fabs(c[jv] [1] [k]); pendent variable.
if (vz > vmax) {
vmaXx=vz;
km=k;
3
errj += vz;
3
err += errj/scalv[j]; Note weighting for each dependent variable.

ermax[j]=c[jv] [1] (km]/scalv[j];

770

Chapter 17. Two Point Boundary Value Problems

kmax [j]=km;
}
err /= nvars;
fac=(err > slowc ? slowc/err : 1.0);
Reduce correction applied when error is large.
for (j=1;j<sne;jt+) { Apply corrections.
jv=indexv[j];
for (k=k1;k<=k2;k++)
yljl{k] -= fac*c([jv] [1] [kl];

printf("\n48s 49s %9s\n","Iter.","Error","FAC"); Summary of corrections
printf("%6d %12.6f %11.6f\n",it,err,fac) ; for this step.
if (err < conv) { Point with largest error for each variable can
free_vector(ermax,1,ne) ; be monitored by writing out kmax and
free_ivector(kmax,1,ne) ; ermax.
return;
}
+
nrerror("Too many iterations in solvde"); Convergence failed.

void bksub(int ne, int nb, int jf, int ki, int k2, float ***c)
Backsubstitution, used internally by solvde.

{

int nbf,im,kp,k,j,i;

float xx;

nbf=ne-nb;

im=1;

for (k=k2;k>=k1;k--) { Use recurrence relations to eliminate remaining de-
if (k k1) im=nbf+1; pendences.

kp=k+1;
for (j=1;j<=nbf; j++) {
xx=c[j] [jf] [kp];

for (i=im;i<=ne; i++)
efi] (jf) 0k] -= cli) (j] [k] xx;
+
+
for (k=k1;k<=k2;k++) { Reorder corrections to be in column 1.
kp=k+1;
for (i=1;i<=nb;i++) c(i] [1] [k]=c[i+nbf] [jf] [kl];
for (i=1;i<=nbf;it++) cli+nb] [1] [k]=c[i] [jf] [kp];
+

#include <math.h>
#include "nrutil.h"

void pinvs(int ie1, int ie2, int jel, int jsf, int jc1, int k, float **«c,

float **s)

Diagonalize the square subsection of the s matrix, and store the recursion coefficients in c;
used internally by solvde.

{

int jsi,jpiv,jp,je2,jcoff,j,irow,ipiv,id,icoff,i,*indxr;
float pivinv,piv,dum,big,*pscl;

indxr=ivector(ie1l,ie2);
pscl=vector(ie1,ie2);
j ei+ie2-iel;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.3 Relaxation Methods 771

for (i=iel;i<=ie2;it++) {
big=0.0;
for (j=je1;j<=je2; j++)
if (fabs(s[i][j]) > big) big=fabs(s[i] [j]);
if (big == 0.0) nrerror("Singular matrix - row all 0, in pinvs");
pscl[i]=1.0/big;
indxr [i]=0;

Implicit pivoting, as in §2.1.

}
for (id=ie1;id<=ie2;id++) {
piv=0.0;
for (i=iel;i<=ie2;i++) { Find pivot element.
if (indxr[i] == 0) f
big=0.0;

for (j=je1;j<=je2;j++) {
if (fabs(s[i][j]) > big)
jp=is
big=fabs(s[i][j]);

+
3
if (big*pscl[i] > piv) {
ipiv=i;
jpiv=jp;
piv=big*pscl [i] ;
3
}
if (s[ipiv] [jpiv] == 0.0) nrerror("Singular matrix in routine pinvs") ;
indxr [ipiv]=jpiv; In place reduction. Save column ordering.
pivinv=1.0/s[ipiv] [jpiv] ;
for (j=je1;j<=jsf;j++) s[ipiv][j] *= pivinv; Normalize pivot row.
s[ipiv] [jpiv]=1.0;
for (i=iel;i<=ie2;i++) { Reduce nonpivot elements in column.

if (indxr[i] != jpiv) {
if (sfil(jpiv]) f
dum=s [i] [jpiv] ;
for (j=je1;j<=jsf; j++)
s[i][j] -= dum+s[ipiv] [j];
s[i] [jpiv]=0.0;

Sort and store unreduced coefficients.

for (j=js1;j<=jsf; j++) clirow] [j+jcoff] [k]=s[il] [jl];

}
free_vector(pscl,ie1,ie2) ;
free_ivector(indxr,ie1,ie2);

void red(int iz1, int iz2, int jzi, int jz2, int jm1, int jm2, int jmf,
int ici, int jc1, int jcf, int kc, float ***c, float **s)

Reduce columns jz1-jz2 of the s matrix, using previous results as stored in the c matrix. Only

columns jm1-jm2,jmf are affected by the prior results. red is used internally by solvde.

int loff,1,j,ic,i;
float vx;

loff=jc1-jm1;
ic=icl;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

7712 Chapter 17. Two Point Boundary Value Problems

for (j=jz1;j<=jz2;jt++) { Loop over columns to be zeroed.

for (1=jm1;1<=jm2;1++) { Loop over columns altered.
vx=c [ic] [1+loff] [kc] ;
for (i=iz1;i<=iz2;it++) s{i]Q] -= s[i] [j]*vx; Loop over rows.
}
vx=c [ic] [jcf] [kc];
for (i=izi1;i<=iz2;i++) s[i][jmf] -= s[i] [j]*vx; Plus final element.
ic += 1;

“Algebraically Difficult” Sets of Differential Equations

Relaxation methods allow you to take advantage of an additional opportunity that, while
not obvious, can speed up some calculations enormously. It is not necessary that the set
of variables y;,, correspond exactly with the dependent variables of the original differential
equations. They can be related to those variables through algebraic equations. Obviously, it
is necessary only that the solution variables allow us to evaluate the functions y, g, B,C that
are used to construct the FDEs from the ODEs. In some problems g depends on functions of
y that are known only implicitly, so that iterative solutions are necessary to evaluate functions
in the ODEs. Often one can dispense with this “internal” nonlinear problem by defining
a new set of variables from which both y,g and the boundary conditions can be obtained
directly. A typical example occurs in physical problems where the equations require solution
of a complex equation of state that can be expressed in more convenient terms using variables
other than the original dependent variables in the ODE. While this approach is analogous to
performing an analytic change of variables directly on the original ODEs, such an analytic
transformation might be prohibitively complicated. The change of variables in the relaxation
method is easy and requires no analytic manipulations.

CITED REFERENCES AND FURTHER READING:
Eggleton, PP. 1971, Monthly Notices of the Royal Astronomical Society, vol. 151, pp. 351-364. [1]

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems (Waltham, MA:
Blaisdell).

Kippenhan, R., Weigert, A., and Hofmeister, E. 1968, in Methods in Computational Physics,
vol. 7 (New York: Academic Press), pp. 129ff.

17.4 A Worked Example: Spheroidal Harmonics

The best way to understand the algorithms of the previous sections is to see
them employed to solve an actual problem. As a sample problem, we have selected
the computation of spheroidal harmonics. (The more common name is spheroidal
angle functions, but we prefer the explicit reminder of the kinship with spherical
harmonics.) We will show how to find spheroidal harmonics, first by the method
of relaxation ($17.3), and then by the methods of shooting ($17.1) and shooting
to a fitting point (§17.2).

Spheroidal harmonics typically arise when certain partial differential
equations are solved by separation of variables in spheroidal coordinates. They
satisfy the following differential equation on the interval —1 < x < 1:

as 2
a [a-yE] + (a- ee - _ )s=0 (17.4.1)

— 72

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.4 A Worked Example: Spheroidal Harmonics 773

Here m is an integer, c is the “oblateness parameter,” and \ is the eigenvalue. Despite
the notation, c? can be positive or negative. For c? > 0 the functions are called
“prolate,” while if c? < 0 they are called “oblate.” The equation has singular points
at = +1 and is to be solved subject to the boundary conditions that the solution be
regular at x = +1. Only for certain values of A, the eigenvalues, will this be possible.

If we consider first the spherical case, where c = 0, we recognize the differential
equation for Legendre functions P’"(x). In this case the eigenvalues are Amn =
n(n+1),n=m,m+1,.... The integer n labels successive eigenvalues for
fixed m: When n = m we have the lowest eigenvalue, and the corresponding
eigenfunction has no nodes in the interval —1 < 2 < 1; whenn = m+ 1 we have
the next eigenvalue, and the eigenfunction has one node inside (—1, 1); and so on.

A similar situation holds for the general case c? 4 0. We write the eigenvalues
of (17.4.1) as Amn(c) and the eigenfunctions as Smn(x;c). For fixed m,n =
m,m + 1,... labels the successive eigenvalues.

The computation of Amn(c) and Sinn (a; c) traditionally has been quite difficult.
Complicated recurrence relations, power series expansions, etc., can be found
in references [1-3]. Cheap computing makes evaluation by direct solution of the
differential equation quite feasible.

The first step is to investigate the behavior of the solution near the singular

points x = +1. Substituting a power series expansion of the form
oo
S=(1+ a)? So ax( + ax)* (17.4.2)
k=0

in equation (17.4.1), we find that the regular solution has a = m/2. (Without loss
of generality we can take m > 0 since m — —m is a symmetry of the equation.)
We get an equation that is numerically more tractable if we factor out this behavior.
Accordingly we set

S=(1l-27)™/?y (17.4.3)
We then find from (17.4.1) that y satisfies the equation

a—27)54 —2(m + 1)2 + (W—22%)y =0 (17.4.4)

where
p=A-—m(m+1) (17.4.5)

Both equations (17.4.1) and (17.4.4) are invariant under the replacement
x — —a. Thus the functions S and y must also be invariant, except possibly for an
overall scale factor. (Since the equations are linear, a constant multiple of a solution
is also a solution.) Because the solutions will be normalized, the scale factor can
only be +1. If n — mis odd, there are an odd number of zeros in the interval (—1, 1).
Thus we must choose the antisymmetric solution y(—x) = —y(x) which has a zero
at x = 0. Conversely, if n — m is even we must have the symmetric solution. Thus

Ymn(—2) = (-1)"""'Ynn(2) (174.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

774 Chapter 17. Two Point Boundary Value Problems

and similarly for Sinn.
The boundary conditions on (17.4.4) require that y be regular at x = +1. In
other words, near the endpoints the solution takes the form

y = ao +a1(1 — 27) +ag(1—a?)? +... (17.4.7)
Substituting this expansion in equation (17.4.4) and letting x — 1, we find that

u-e

a = ima” (17.4.8)
Equivalently,
, poe
y= am+ yl?) (17.4.9)
A similar equation holds at x = —1 with a minus sign on the right-hand side.

The irregular solution has a different relation between function and derivative at
the endpoints.

Instead of integrating the equation from —1 to 1, we can exploit the symmetry
(17.4.6) to integrate from 0 to 1. The boundary condition at x = 0 is

n —m odd

=0,
(17.4.10)
=0, n—meven

y(0)
y'(0)

A third boundary condition comes from the fact that any constant multiple
of a solution y is a solution. We can thus normalize the solution. We adopt the

normalization that the function Smn has the same limiting behavior as P?” at x = 1:

lim (1 — 07)? Sian (a3 e) = lim (1 — a?)-™/? P™ (x) (17.4.11)

rs 2

Various normalization conventions in the literature are tabulated by Flammer [1].
Imposing three boundary conditions for the second-order equation (17.4.4)

turns it into an eigenvalue problem for \ or equivalently for u. We write it in the

standard form by setting

yw=y (174.12)
y=y’ (17.4.13)
y3 = bh (174.14)
Then
vA = ye (17.4.15)
1
Y= To [2x(m + 1)y2 — (ys — ea?) y1] (174.16)
ys =0 (17.4.17)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

17.4 A Worked Example: Spheroidal Harmonics

7715

The boundary condition at 2 = 0 in this notation is
yi =0, m—m odd

yo=0, n—m even

At x = 1 we have two conditions:

y3 — 2
yt

Y= ome)
3 _ p2)—m/2 pm/..)
me im(l v) Pr'(@) = 2™mIl(n — m)!

(-1)™(n +m)!

y

(17.4.18)

(17.4.19)

(17.4.20)

We are now ready to illustrate the use of the methods of previous sections

on this problem.

Relaxation

If we just want a few isolated values of \ or S,, shooting is probably the quickest
method. However, if we want values for a large sequence of values of c, relaxation
is better. Relaxation rewards a good initial guess with rapid convergence, and the
previous solution should be a good initial guess if c is changed only slightly.

For simplicity, we choose a uniform grid on the interval 0 < # < 1. Fora

total of MM mesh points, we have

ol
~ M-1
tp = (k-Wh, k=1,2,...,M

h

At interior points k = 2,3,..., M, equation (17.4.15) gives

h
Ek = Yk — Yijk—-1 — 5 Wak + Y2,b-1)
Equation (17.4.16) gives

Eon = Yok — Y2,k—-1 — Br

(ve + &e-1)(m + 1) (Y2,4 +.Y2,k-1) o (yak + yir—1)

k

2 2

where

Y3,k + Y3,k-1 _ (xp + ©R-1)”
2 4
h

1- 4 (ap + @-1)?

Be =

Finally, equation (17.4.17) gives

E3,k = Y3,k — Y3,k-1

(17.421)

(17.4.22)

(17.4.23)

(17.4.24)

(17.4.25)

(17.4.26)

(17.427)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

716 Chapter 17. Two Point Boundary Value Problems

Now recall that the matrix of partial derivatives S;,; of equation (17.3.8) is
defined so that 7 labels the equation and j the variable. In our case, j runs from 1 to
3 for y; at k — 1 and from 4 to 6 for y; at k. Thus equation (17.4.23) gives

h
Sii=-l, Si2= =) Si3=0
h (17.4.28)
Sia=1, Sis= 3 Si¢6=0
Similarly equation (17.4.24) yields
S21 = an 3/2, S20 = —1— Be(ae + ep-1)(m + 1)/2,
S23 = Br(ys.n + y1,h—1)/4 Soa = S21,
S25 =2+ S22, S26 = S23
(17.4.29)
while from equation (17.4.27) we find
S31 =0, S32 =0, $3.3 =—1
3,1 , 8.2 , 3,3 (17.4.30)
S34 = 0, 53.5 = 0, S36=1
At x = 0 we have the boundary condition
,» n—modd
Fga = {ee (17.431)
Y2,1, M—meven

Recall the convention adopted in the solvde routine that for one boundary condition
at k = 1 only $3; can be nonzero. Also, j takes on the values 4 to 6 since the
boundary condition involves only yx, not yx—-1. Accordingly, the only nonzero
values of S3,; at 2 = O are

S34=1, n—m odd

(17.4.32)
S35 =1, n— meven
At x = 1 we have
yam — ©
Fy mai. = Y2.mM — 2 174.33
1,M+1 = Y2,M Xm + D Y1,M ( )
Fa,u41 = Yim — 7 (17.4.34)
Thus
2

7 ¥3,M — C¢ 7 Y1M
=- ; =1, 5 = — | 17435
Sia 20m FI) Sis ; S16 Xm +1) ( )
Soa =1, So5=0, S26 =0 (174.36)

Here now is the sample program that implements the above algorithm. We
need a main program, sfroid, that calls the routine solvde, and we must supply
the function difeq called by solvde. For simplicity we choose an equally spaced

Jeo JO Woo suMMy/:dy

“(Aluo eouewy YON) €%pZ-228-008-
“sayndwiod Jansas Aue 0} (aU0 S

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

a
=
oO
fog
g
oe

4 J@YLNY “asn jeuosied u:

n
Ey
3
3
o
3
Ey
&
3
fei
3
Zz
Cc
=
m
2
fe)
>
=
D
m
is}
v0
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
a
5
Q
fe}
=
vu
Cc
a
2
fo)
@D
jes)
Zz
2
a
8
k
g
3
i
2

oS
2
2
o
3S
nn
os
<
i?)
2
3
Ey
g
a
o
Cc
Es
Fe
g
g
z
3
3
S
9
2
3
Q
3
3
a
9
S
z
E
2
3S
g
2
o
3S
nn
z
<
Zz
é
3
g
3.
Ea
DD
2
Q.
3
3
8
n
g
a
8

3
Qa
2
2.
ce}
3
Q
2
eI
2
3
3
2g
5
2
3
3B
8
Ea
5
°

17.4 A Worked Example: Spheroidal Harmonics 777

mesh of m = 41 points, that is, h = .025. As we shall see, this gives good accuracy

for the eigenvalues up to moderate values of n — m.

Since the boundary condition at x = 0 does not involve y; if n — m is even,
we have to use the indexv feature of solvde. Recall that the value of indexv[j]
describes which column of s [i] [j] the variable y[j] has been put in. If n — m
is even, we need to interchange the columns for y; and yz so that there is not a

zero pivot element in s[i] [j].

The program prompts for values of m and n. It then computes an initial guess
for y based on the Legendre function P’”. It next prompts for c?, solves for y,

prompts for c?, solves for y using the previous values as an initial guess, and so on.

#include <stdio.h>
#include <math.h>
#include "nrutil.h"
#define NE 3
#define M 41
#define NB 1
#define NSI NE
#define NYJ NE
#define NYK M
#define NCI NE
#define NCJ (NE-NB+1)
#define NCK (M+1)
#define NSJ (2*NE+1)

int mm,n,mpt=M;
float h,c2=0.0,anorm,x[M+1];

Global variables communicating with difeq.

int main(void) /* Program sfroid */

Sample program using solvde. Computes eigenvalues of spheroidal harmonics Sinn(x;c) for

m >0Oandn>m. In the program, m is mm, c? is c2, and + of equation (17.4.20) is anorm.
{

float plgndr(int 1, int m, float x);

void solvde(int itmax, float conv, float slowc, float scalv[],

int indexv[], int ne, int nb, int m, float **y, float ***c, float **s);

int i,itmax,k,indexv([NE+1] ;

float conv,deriv,faci1,fac2,q1,slowc,scalv[NE+1] ;

float *ty,**s,*#*C;

y=matrix(1,NYJ,1,NYK);

s=matrix(1,NSI,1,NSJ);

c=£3tensor(1,NCI,1,NCJ,1,NCK) ;

itmax=100;

conv=5.0e-6;

slowc=1.0;

h=1.0/(M-1);

printf("\nenter m n\n");

scanf("%d %d" , &mm , &n) ;

if (ntmm & 1) { No interchanges necessary.
indexv[1]=1;
indexv [2]
indexv [3]=3;

} else { Interchange y1 and yo.
indexv[1]=2;
indexv[2]=1;
indexv [3]=3;

+
anorm=1.0; Compute +.
if (om) {

qi=n;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

718 Chapter 17.

Two Point Boundary Value Problems

for (i=1;i<=mm;i++) anorm =
for (k=1;k<=(M-1);k++) {
x [k]=(k-1) *h;
faci=1.0-x[k] *x[k] ;
fac2=exp((-mm/2.0)*log(fac1));
y(1] [k]=plgndr (n,mm,x[k] )*fac2;
deriv =

=

y(2
y [3] [k] =n* (n+1) -mm+ (mm+1) ;
+
x(M]=1.0;
y(1] [M]=anorm;

y [3] (M]=n* (n+1) -mm* (mm+1) ;

-((n-mmt+1) *plgndr (n+1,mm,x[k] )-
(n+1) *x [k] *plgndr (n,mm,x([k]))/fac1;
(k] =mm*x [k] *y [1] [k] /faci+deriv+fac2;

-0.5*anorm*(n+i)*(q1--/i) ;

Initial guess.

Pi” from §6.8.
Derivative of P7” from a recur-
rence relation.

Initial guess at 2 = 1 done sep-
arately.

y [2] (M]=(y [3] (M] -c2) *y [1] [M]/(2.0*(mm+1.0)) ;

scalv[1]=fabs(anorm) ;
scalv(2]=(y[2][M] > scalv[1] ? y([2] [MJ
scalv(3]=(y[3] [M] > 1.0 ? y[3] [M]

for (53) f

: scalv[1]);

1.0);

printf("\nEnter c**2 or 999 to end.\n");

scanf("%f",&c2) ;
if (c2 == 999) {

free_f3tensor(c,1,NCI,1,NCJ,1,NCK) ;

free_matrix(s,1,NSI,1,NSJ);
free_matrix(y,1,NYJ,1,NYK);
return 0;

+

solvde(itmax,conv,slowc,scalv,indexv,NE,NB,M,y,c,s);
printf("\n %s %2d %s %2d hs %7.3f Zs 710.6£\n",

‘m =",mm," n ="yn," cee2 =",c2,
"lamda =",y[3] (1]+mm*(mm+1)) ;

extern int mm,n,mpt;
extern float h,c2,anorm,x[];

void difeq(int k, int ki, int k2,
int ne, float **s, float *#*y)
Returns matrix s for solvde.

{

int jsf,

float temp,temp1,temp2;

if (k == k1) {

if (ntmm & 1) {
s[3] [3+indexv[1]]=1.0
s[3] [3+indexv[2] :
s [3] [3+indexv[3]]=0.0;
s(3] [jsf]=y[1] [1];

} else {
s [3] [3+indexv[1]]=0.0;
s(3] [3+indexv[2]]=1.0;
s [3] [3+indexv[3]]=0.0;
s[3] (jsf]=y([2] [1];

}

} else if (k > k2) {

s[1] [3+indexv[1]]
s([1] [(3+indexv[2]]=
s[1] [3+indexv[3]]
s[1] [jsf
s[2] [3+indexv[1.

meu

J=1.0;

-(y[3] [mpt]-c2) /(2.0* (mm+1.0)) ;

Return for another value of c?.

Defined in sfroid.

int isi, int isf, int indexv[],

Boundary condition at first point.

Equation (17.4.32).

Equation (17.4.31).

Equation (17.4.32).

Equation (17.4.31).

Boundary conditions at last point.
(17.4.35).

-y(1] (mpt] /(2.0*(mm+1.0)) ;
=y [2] [mpt]-(y [3] [mpt]-c2)*y [1] [mpt] /(2.0* (mm+1.0)) ;

(17.4.33).
Equation (17.4.36).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.4 A Worked Example: Spheroidal Harmonics

7719

s(2] [3+indexv
s(2] [3+indexv
s(2] [(jsfl=y(1

indexv[1
indexv [2
indexv [3
3+indexv
3+indexv
3+indexv

©

NVEWWOWWWWNHUNNNNNE

p2=0.5*(y [3
indexv[1
indexv[2.
indexv[3.
3+indexv
3+indexv
3+indexv
indexv[1
indexv[2.
indexv[3.
3+indexv
3+indexv
3+indexv
jsfl=y([1
jsfl=y (2
*0.5%(mmt+1
*0.5*(y[1.
s[3] [jsf]=y (3)

HDHOAHHHHHAHHAHHHAt¢H+Ht+HHHABHAD

emp1=x [k] +x [k-1] ;
emp=h/(1.0-temp1*temp1*0.25) ;

2]]=0.0;

3]]=0.0;

mpt]-anorm; Equation (17.4.34).

Interior point.

= -1.0; Equation (17.4.28).
= -0.5*h;

=0.0;

1]]=1.0;

2 = -0.5*h;

3]]=0.0;

k] +y [3] [k-1] )-c2*0.25*temp1*temp1;
=temp*temp2*0.5;
= -1.0-0.5*temp*(mmt+1.0)+*temp1;
=0.25*temp* (y[1] [k]+y[1] [k-1]);

1]]=s [2] [indexv[1]];
2]]=2.0+s [2] [indexv[2]] ;
3]]=s[2] [indexv[3]] ;

=0.0; Equation (17.4.30).
=0.0;
= -1.0;
1]]=0.0;
2]]=0.0;
3]]=1.0;
k]-y [1] [k-1] -0.5¥*h* (y [2] [k] +y [2] [k-1]) ;
k] -y [2] [k-1] -temp* ( (x [k] +x [k-1])
-0)*(y [2] [k] +y [2] [k-1] )-temp2
k]+y(1] [k-1]));
k]-y [3] [k-1]; Equation (17.4.27).

Equation (17.4.29).

(17.4.23).
(17.4.24).

You can run the program and check it against values of A mn (c) given in the tables
at the back of Flammer’s book [1] or in Table 21.1 of Abramowitz and Stegun [2].
Typically it converges in about 3 iterations. The table below gives a few comparisons.

Selected Output of sfroid

mon € Aexact Asfroid

2

2 0.1 6.01427 6.01427
1.0 6.14095 6.14095
4.0 6.54250 6.54253

1.0 30.4361 30.4372
16.0 36.9963 37.0135
11 —-1.0 131.560 131.554

on

Shooting

To solve the same problem via shooting (§17.1), we supply a function derivs
that implements equations (17.4.15)-(17.4.17). We will integrate the equations over
the range —1 < x < 0. We provide the function load which sets the eigenvalue
y3 to its current best estimate, v[1]. It also sets the boundary values of y, and
y2 using equations (17.4.20) and (17.4.19) (with a minus sign corresponding to
x = —1). Note that the boundary condition is actually applied a distance dx from

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

780 Chapter 17. Two Point Boundary Value Problems

the boundary to avoid having to evaluate y} right on the boundary. The function
score follows from equation (17.4.18).

#include <stdio.h>
#include "nrutil.h"
#define N2 1

doa,

int m,n; Communicates with load, score, and derivs.
float c2,dx,gmma;

int nvar; Communicates with shoot.
float x1,x2;

int main(void) /* Program sphoot */

Sample program using shoot. Computes eigenvalues of spheroidal harmonics Simn(x;c) for
m > 0 and n > m. Note how the routine vecfunc for newt is provided by shoot (§17.1).

{

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

void newt(float x[], int n, int *check,
void (*vecfunc) (int, float [], float []));
void shoot(int n, float v[], float f[]);
int check,i;
float qi,*v;

v=vector (1,N2);

dx=1.00-4; Avoid evaluating derivatives exactly at « = —1.
nvar=3; Number of equations.
for G3) f

printf("input m,n,c-squared\n") ;
if (scanf("%d %d %f",&m,&n,&c2) == EOF) break;

if (1 <_m I| m < 0) continue;

gmma=1.0; Compute 7 of equation (17.4.20).
qi=n;

for (i=1;i<=m;it++) gmma *= -0.5*(nti)*(q1--/i) ;

v[1]=n*(n+1)-m* (m+1)+c2/2.0; Initial guess for eigenvalue.

x1 = -1.0+dx; Set range of integration.

x2=0.0;

newt (v,N2,&check, shoot) ; Find v that zeros function f in score.

if (check) {
printf("shoot failed; bad initial guess\n");
} else {
printf ("\tmu(m,n)\n") ;
printf("%12.6£\n",v([1]);

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

}

}

free_vector(v,1,N2);

return 0;
3
void load(float x1, float v[], float y[])
Supplies starting values for integration at « = —1+ dz.
{

float y1 = (n-m & 1 ? -gmma : gmma);
y(3]=v[1];

yl2] = -(y[3]-c2)*y1/(2*(m+1));
y[1]=y1+y [2] *dx;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

}

void score(float xf, float y[], float f[])
Tests whether boundary condition at « = 0 is satisfied.
{

f[t1]=@-m & 1? yfi] : yl2]);
3

17.4 A Worked Example: Spheroidal Harmonics 781

void derivs(float x, float y[], float dydx[])
Evaluates derivatives for odeint.

{
dydx [1] =y [2] ;
dydx [2]=(2.0+#x* (m+1 .0) *y [2] - Cy [3] -c2*x+x) *y [1] ) /(1.0-x*x) ;
dydx[3]=0.0;

3

Shooting to a Fitting Point

For variety we illustrate shootf from §17.2 by integrating over the whole range
—1+dzx < x < 1 -dz, with the fitting point chosen to be at x = 0. The routine
derivs is identical to the one for shoot. Now, however, there are two load routines.
The routine load1 for x = —1 is essentially identical to load above. At x = 1,
load2 sets the function value y; and the eigenvalue y3 to their best current estimates,
v2[1] and v2[2], respectively. If you quite sensibly make your initial guess of
the eigenvalue the same in the two intervals, then v1 [1] will stay equal to v2[2]
during the iteration. The function score simply checks whether all three function
values match at the fitting point.

#include <stdio.h>
#include <math.h>
#include "nrutil.h"
#define N1 2
#define N2 1
#tdefine NTOT (N1+N2)
#define DXX 1.0e-4

int m,n; Communicates with loadi, load2, score,
float c2,dx,gmma; and derivs.
int nn2,nvar; Communicates with shootf.

float x1,x2,xf;

int main(void) /* Program sphfpt */
Sample program using shootf. Computes eigenvalues of spheroidal harmonics Sinn(a;c) for
m > 0 and n> m. Note how the routine vecfunc for newt is provided by shootf (§17.2).
The routine derivs is the same as for sphoot.
{

void newt(float x[], int n, int *check,

void (*vecfunc) (int, float [], float []));

void shootf(int n, float v[], float f[]);

int check,i;

float qi,*v1,*v2,*v;

v=vector(1,NTOT) ;

vi=v;

v2 = &v[N2];

nvar=NTOT; Number of equations.

nn2=N2;

dx=DXX; Avoid evaluating derivatives exactly at 2 =
for (;;) { +1.

printf("input m,n,c-squared\n") ;

if (scanf("%d %d %f",&m,&n,&c2) == EOF) break;

if (n < m || m< 0) continue;

gmma=1.0; Compute 7 of equation (17.4.20).
qi=n;

for (i=1;i<=m;it++) gmma *= -0.5*(nti)*(q1--/i) ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

782 Chapter 17. Two Point Boundary Value Problems

vi [1] =n*(n+1)-m* (m+1)+c2/2.0; Initial guess for eigenvalue and function value.
v2(2]=v1if1];
v2[1]=gmma*(1.0-(v2[2]-c2) *dx/(2*(m+1)));

xl = -1.0+dx; Set range of integration.
x2=1.0-dx;
xf=0.0; Fitting point.

newt (v,NTOT,&check, shootf) ; Find v that zeros function f in score.
if (check) {

printf("shootf failed; bad initial guess\n");
} else {

printf("\tmu(m,n)\n") ;

printf("%12.6£\n",v([1]);

}

}

free_vector(v,1,NTOT) ;

return 0;
}
void loadi(float x1, float vif], float y[])
Supplies starting values for integration at 2 = —1+ dz.
t

float y1 = (n-m & 1 ? -gmma : gmma);
y(3]=vi(11;
yl2] = -(y[3]-c2)*y1/(2*(m+1)) ;
y(1]=yi+y [2] *dx;

}

void load2(float x2, float v2[], float y[])
Supplies starting values for integration at « = 1 — da.

{

y(3]=v2(2];

ylil=v2(11;

y(2]=(y[3]-c2) *y [1] /(2*(m+1)) ;
}

void score(float xf, float y[], float f[])
Tests whether solutions match at fitting point « = 0.
{

int i;

for (i=1;i<=3;i++) flil=ylil;

CITED REFERENCES AND FURTHER READING:

Flammer, C. 1957, Spheroidal Wave Functions (Stanford, CA: Stanford University Press). [1]

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York), §21. [2]

Morse, P.M., and Feshbach, H. 1953, Methods of Theoretical Physics, Part Il (New York: McGraw-
Hill), pp. 1502ff. [3]

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.5 Automated Allocation of Mesh Points 783

17.5 Automated Allocation of Mesh Points

In relaxation problems, you have to choose values for the independent variable at the
mesh points. This is called allocating the grid or mesh. The usual procedure is to pick
a plausible set of values and, if it works, to be content. If it doesn’t work, increasing the
number of points usually cures the problem.

If we know ahead of time where our solutions will be rapidly varying, we can put more
grid points there and less elsewhere. Alternatively, we can solve the problem first on a uniform
mesh and then examine the solution to see where we should add more points. We then repeat
the solution with the improved grid. The object of the exercise is to allocate points in such
a way as to represent the solution accurately.

It is also possible to automate the allocation of mesh points, so that it is done
“dynamically” during the relaxation process. This powerful technique not only improves
the accuracy of the relaxation method, but also (as we will see in the next section) allows
internal singularities to be handled in quite a neat way. Here we learn how to accomplish
the automatic allocation.

We want to focus attention on the independent variable x, and consider two alternative
reparametrizations of it. The first, we term q; this is just the coordinate corresponding to the
mesh points themselves, so that g = 1 atk = 1,q = 2 atk = 2, and so on. Between any two
mesh points we have Ag = 1. In the change of independent variable in the ODEs from z to q,

dy
> = 17.5.1
as ( )
becomes
dy dx
> =s-_ 17.5.2
dq 8 dq ( )
In terms of q, equation (17.5.2) as an FDE might be written
dx dx
Ye Yea 3] leq) + (eq =0 (17.5.3)
T),, TY yy

or some related version. Note that dx/dq should accompany g. The transformation between
x and q depends only on the Jacobian dx/dg. Its reciprocal dq/dzx is proportional to the
density of mesh points.

Now, given the function y(x), or its approximation at the current stage of relaxation,
we are supposed to have some idea of how we want to specify the density of mesh points.
For example, we might want dq/dx to be larger where y is changing rapidly, or near to the
boundaries, or both. In fact, we can probably make up a formula for what we would like
dq/dzx to be proportional to. The problem is that we do not know the proportionality constant.
That is, the formula that we might invent would not have the correct integral over the whole
range of x so as to make g vary from 1 to M, according to its definition. To solve this problem
we introduce a second reparametrization Q(q), where Q is a new independent variable. The
relation between Q and q is taken to be linear, so that a mesh spacing formula for dQ/dx
differs only in its unknown proportionality constant. A linear relation implies

#Q
+= 17.5.4
ip 7° (17.54)
or, expressed in the usual manner as coupled first-order equations,
aQ(e) as
= = 17.5.5
Hy ao (1755)

where y is a new intermediate variable. We add these two equations to the set of ODEs
being solved.

Completing the prescription, we add a third ODE that is just our desired mesh-density
function, namely

= 2Q _ dQ dq
~ dx dq dx

(17.5.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

784 Chapter 17. Two Point Boundary Value Problems

where ¢(2) is chosen by us. Written in terms of the mesh variable q, this equation is

dz __¥ (175.7)

dq (x)

Notice that ¢(a) should be chosen to be positive definite, so that the density of mesh points
is everywhere positive. Otherwise (17.5.7) can have a zero in its denominator.

To use automated mesh spacing, you add the three ODEs (17.5.5) and (17.5.7) to your
set of equations, i.e., to the array y[j] [k]. Now x becomes a dependent variable! Q and w
also become new dependent variables. Normally, evaluating @ requires little extra work since
it will be composed from pieces of the g’s that exist anyway. The automated procedure allows
one to investigate quickly how the numerical results might be affected by various strategies
for mesh spacing. (A special case occurs if the desired mesh spacing function Q can be found
analytically, i.e., dQ/dz is directly integrable. Then, you need to add only two equations,
those in 17.5.5, and two new variables 2, 2.)

As an example of a typical strategy for implementing this scheme, consider a system
with one dependent variable y(x). We could set

_ dx , |diny|
dg=F+u (17.58)
or
_dQ_i dy/dx
oa) = 2 = 5+ 5 (175.9)

where A and 6 are constants that we choose. The first term would give a uniform spacing
in x if it alone were present. The second term forces more grid points to be used where y is
changing rapidly. The constants act to make every logarithmic change in y of an amount 6
about as “attractive” to a grid point as a change in x of amount A. You adjust the constants
according to taste. Other strategies are possible, such as a logarithmic spacing in a, replacing
dz in the first term with dlnz.

CITED REFERENCES AND FURTHER READING:
Eggleton, P. P. 1971, Monthly Notices of the Royal Astronomical Society, vol. 151, pp. 351-364.

Kippenhan, R., Weigert, A., and Hofmeister, E. 1968, in Methods in Computational Physics,
vol. 7 (New York: Academic Press), pp. 129ff.

17.6 Handling Internal Boundary Conditions
or Singular Points

Singularities can occur in the interiors of two point boundary value problems. Typically,
there is a point x, at which a derivative must be evaluated by an expression of the form
>.) — N(@s,y)

S(as) D(as.y) (17.6.1)
where the denominator D(xs,y) = 0. In physical problems with finite answers, singular
points usually come with their own cure: Where D — 0, there the physical solution y must
be such as to make N — 0 simultaneously, in such a way that the ratio takes on a meaningful
value. This constraint on the solution y is often called a regularity condition. The condition
that D(x, y) satisfy some special constraint at x, is entirely analogous to an extra boundary
condition, an algebraic relation among the dependent variables that must hold at a point.

We discussed a related situation earlier, in §17.2, when we described the “fitting point
method” to handle the task of integrating equations with singular behavior at the boundaries.
In those problems you are unable to integrate from one side of the domain to the other.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.6 Handling Internal Boundary Conditions or Singular Points 785

(a) 1 XXX vB

1X XxX vB
XXXXXXXXXX S vB
XXXXXXXXXX » vB
XXXXXXXXXX & vB
XXXXXXXXXX vB
XXXXXXXXXX vB
vB
XXXXXXXXXX vB
XXXXXXXXXX vB
XXXXXXXXXX vB
XXXXXXXXXX vB
XXXXXXXXXX vB
(bt) 1 XXX vB
1X XxX vB
1 xxx se vB
1 XXX mS vB
1 XXX vB

Ss
1 Xxx vB
txxx/ vB
vB
1 xX xX vB
1 xX xX vB
1 xX xX vB
1 xXx vB
1xXxX vB

Figure 17.6.1. | FDE matrix structure with an internal boundary condition. The internal condition

introduces a special block. (a) Original form, compare with Figure 17.3.1; (b) final form, compare
with Figure 17.3.2.

However, the ODEs do have well-behaved derivatives and solutions in the neighborhood of
the singularity, so it is readily possible to integrate away from the point. Both the relaxation
method and the method of “shooting” to a fitting point handle such problems easily. Also,
in those problems the presence of singular behavior served to isolate some special boundary
values that had to be satisfied to solve the equations.

The difference here is that we are concerned with singularities arising at intermediate
points, where the location of the singular point depends on the solution, so is not known a
priori. Consequently, we face a circular task: The singularity prevents us from finding a
numerical solution, but we need a numerical solution to find its location. Such singularities
are also associated with selecting a special value for some variable which allows the solution
to satisfy the regularity condition at the singular point. Thus, internal singularities take on
aspects of being internal boundary conditions.

One way of handling internal singularities is to treat the problem as a free boundary
problem, as discussed at the end of §17.0. Suppose, as a simple example, we consider
the equation

dy _ N(z,y)

dz D(x,y)

where N and D are required to pass through zero at some unknown point 2;. We add
the equation

(17.6.2)

_ dz
2=%s.-21 in =0 (17.6.3)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

786 Chapter 17. Two Point Boundary Value Problems

where x, is the unknown location of the singularity, and change the independent variable
to t by setting

v—2, =tz, O0<t<l (17.64)
The boundary conditions at t = 1 become
N(a,y) =0, D(x,y) =0 (17.6.5)

Use of an adaptive mesh as discussed in the previous section is another way to overcome
the difficulties of an internal singularity. For the problem (17.6.2), we add the mesh spacing
equations

dQ _

Gane (17.6.6)
dy

dq 7° (17.6.7)

with a simple mesh spacing function that maps x uniformly into q, where g runs from 1 to
M, the number of mesh points:

dQ
v)=a-%, —~=1 768
Q(x) =e-a1, FE (17.68)
Having added three first-order differential equations, we must also add their corresponding
boundary conditions. If there were no singularity, these could simply be
at g=1: T=21, Q=0 (17.6.9)
at q=M: c= x2 (17.6.10)
and a total of N values y; specified at q = 1. In this case the problem is essentially an
initial value problem with all boundary conditions specified at 21 and the mesh spacing
function is superfluous.
However, in the actual case at hand we impose the conditions
at q=1: t=271, Q=0 (17.6.11)
at q=M: N(a,y)=0, D(a,y)=0 (17.6.12)

and N — 1 values y: at g = 1. The “missing” yi is to be adjusted, in other words, so as
to make the solution go through the singular point in a regular (zero-over-zero) rather than
irregular (finite-over-zero) manner. Notice also that these boundary conditions do not directly
impose a value for x2, which becomes an adjustable parameter that the code varies in an
attempt to match the regularity condition.

In this example the singularity occurred at a boundary, and the complication arose
because the location of the boundary was unknown. In other problems we might wish to
continue the integration beyond the internal singularity. For the example given above, we
could simply integrate the ODEs to the singular point, then as a separate problem recommence
the integration from the singular point on as far we care to go. However, in other cases the
singularity occurs internally, but does not completely determine the problem: There are still
some more boundary conditions to be satisfied further along in the mesh. Such cases present
no difficulty in principle, but do require some adaptation of the relaxation code given in §17.3.
In effect all you need to do is to add a “special” block of equations at the mesh point where
the internal boundary conditions occur, and do the proper bookkeeping.

Figure 17.6.1 illustrates a concrete example where the overall problem contains 5
equations with 2 boundary conditions at the first point, one “internal” boundary condition, and
two final boundary conditions. The figure shows the structure of the overall matrix equations
along the diagonal in the vicinity of the special block. In the middle of the domain, blocks
typically involve 5 equations (rows) in 10 unknowns (columns). For each block prior to the
special block, the initial boundary conditions provided enough information to zero the first
two columns of the blocks. The five FDEs eliminate five more columns, and the final three
columns need to be stored for the backsubstitution step (as described in 817.3). To handle the
extra condition we break the normal cycle and add a special block with only one equation:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

17.6 Handling Internal Boundary Conditions or Singular Points 787

the internal boundary condition. This effectively reduces the required storage of unreduced
coefficients by one column for the rest of the grid, and allows us to reduce to zero the first
three columns of subsequent blocks. The functions red, pinvs, bksub can readily handle
these cases with minor recoding, but each problem makes for a special case, and you will
have to make the modifications as required.

CITED REFERENCES AND FURTHER READING:
London, R.A., and Flannery, B.P. 1982, Astrophysical Journal, vol. 258, pp. 260-269.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

Chapter 18. Integral Equations
and Inverse Theory

18.0 Introduction

Many people, otherwise numerically knowledgable, imagine that the numerical
solution of integral equations must be an extremely arcane topic, since, until recently,
it was almost never treated in numerical analysis textbooks. Actually there is a
large and growing literature on the numerical solution of integral equations; several
monographs have by now appeared [1-3]. One reason for the sheer volume of this
activity is that there are many different kinds of equations, each with many different
possible pitfalls; often many different algorithms have been proposed to deal with
a single case.

There is a close correspondence between linear integral equations, which specify
linear, integral relations among functions in an infinite-dimensional function space,
and plain old linear equations, which specify analogous relations among vectors
in a finite-dimensional vector space. Because this correspondence lies at the heart
of most computational algorithms, it is worth making it explicit as we recall how
integral equations are classified.

Fredholm equations involve definite integrals with fixed upper and lower limits.
An inhomogeneous Fredholm equation of the first kind has the form

b
g(t)= | K(t,s)f(s) ds (18.0.1)
a
Here f(t) is the unknown function to be solved for, while g(t) is a known “right-hand
side.” (In integral equations, for some odd reason, the familiar “right-hand side” is
conventionally written on the left!) The function of two variables, K(t, s) is called
the kernel. Equation (18.0.1) is analogous to the matrix equation

K-f=g (18.0.2)

whose solution is f = K~!-g, where K ~! is the matrix inverse. Like equation (18.0.2),
equation (18.0.1) has a unique solution whenever g is nonzero (the homogeneous
case with g = 0 is almost never useful) and K is invertible. However, as we shall
see, this latter condition is as often the exception as the rule.

The analog of the finite-dimensional eigenvalue problem

(K —ol)-f=g (18.0.3)

788

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

18.0 Introduction 789

is called a Fredholm equation of the second kind, usually written

b

f()= | K(t,s)f(s) ds + g(t) (18.0.4)

a

Again, the notational conventions do not exactly correspond: in equation (18.0.4)
is 1/o in (18.0.3), while g is —g/X. If g (or g) is zero, then the equation is said
to be homogeneous. If the kernel A(t, s) is bounded, then, like equation (18.0.3),
equation (18.0.4) has the property that its homogeneous form has solutions for
at most a denumerably infinite set \ = A,, n = 1,2,..., the eigenvalues. The
corresponding solutions f,,(t) are the eigenfunctions. The eigenvalues are real if
the kernel is symmetric.

In the inhomogeneous case of nonzero g (or g), equations (18.0.3) and (18.0.4)
are soluble except when A (or a) is an eigenvalue — because the integral operator
(or matrix) is singular then. In integral equations this dichotomy is called the
Fredholm alternative.

Fredholm equations of the first kind are often extremely ill-conditioned. Ap-
plying the kernel to a function is generally a smoothing operation, so the solution,
which requires inverting the operator, will be extremely sensitive to small changes
or errors in the input. Smoothing often actually loses information, and there is no
way to get it back in an inverse operation. Specialized methods have been developed
for such equations, which are often called inverse problems. In general, a method
must augment the information given with some prior knowledge of the nature of the
solution. This prior knowledge is then used, in one way or another, to restore lost
information. We will introduce such techniques in §18.4.

Inhomogeneous Fredholm equations of the second kind are much less often
ill-conditioned. Equation (18.0.4) can be rewritten as

IK(t, s) — 06(t — s)| f(s) ds = —og(t) (18.0.5)

where J(t — s) is a Dirac delta function (and where we have changed from \ to its
reciprocal o for clarity). If o is large enough in magnitude, then equation (18.0.5)
is, in effect, diagonally dominant and thus well-conditioned. Only if a is small do
we go back to the ill-conditioned case.

Homogeneous Fredholm equations of the second kind are likewise not partic-
ularly ill-posed. If K is a smoothing operator, then it will map many f’s to zero,
or near-zero; there will thus be a large number of degenerate or nearly degenerate
eigenvalues around 0 = 0 (A — oo), but this will cause no particular computational
difficulties. In fact, we can now see that the magnitude of o needed to rescue the
inhomogeneous equation (18.0.5) from an ill-conditioned fate is generally much less
than that required for diagonal dominance. Since the o term shifts all eigenvalues,
it is enough that it be large enough to shift a smoothing operator’s forest of near-
zero eigenvalues away from zero, so that the resulting operator becomes invertible
(except, of course, at the discrete eigenvalues).

Volterra equations are a special case of Fredholm equations with K(t, s) = 0
for s > t. Chopping off the unnecessary part of the integration, Volterra equations are
written in a form where the upper limit of integration is the independent variable t.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

790 Chapter 18. Integral Equations and Inverse Theory

The Volterra equation of the first kind

has as its analog the matrix equation (now written out in components)

k
SO Kaj fi = oe (18.0.7)
j=l

Comparing with equation (18.0.2), we see that the Volterra equation corresponds to
a matrix K that is lower (i.e., left) triangular, with zero entries above the diagonal.
As we know from Chapter 2, such matrix equations are trivially soluble by forward
substitution. Techniques for solving Volterra equations are similarly straightforward.
When experimental measurement noise does not dominate, Volterra equations of the
first kind tend not to be ill-conditioned; the upper limit to the integral introduces a
sharp step that conveniently spoils any smoothing properties of the kernel.
The Volterra equation of the second kind is written

t
f(t)= i K(t,s) f(s) ds + g(t) (18.0.8)
a
whose matrix analog is the equation
(K-1)-f=g (18.0.9)

with K lower triangular. The reason there is no in these equations is that (i) in
the inhomogeneous case (nonzero g) it can be absorbed into K’, while (ii) in the
homogeneous case (g = 0), it is a theorem that Volterra equations of the second kind
with bounded kernels have no eigenvalues with square-integrable eigenfunctions.

We have specialized our definitions to the case of linear integral equations.
The integrand in a nonlinear version of equation (18.0.1) or (18.0.6) would be
K(t,s, f(s)) instead of K(t,s)f(s); a nonlinear version of equation (18.0.4) or
(18.0.8) would have an integrand K(t, s, f(t), f(s)). Nonlinear Fredholm equations
are considerably more complicated than their linear counterparts. Fortunately, they
do not occur as frequently in practice and we shall by and large ignore them in this
chapter. By contrast, solving nonlinear Volterra equations usually involves only a
slight modification of the algorithm for linear equations, as we shall see.

Almost all methods for solving integral equations numerically make use of
quadrature rules, frequently Gaussian quadratures. This would be a good time
for you to go back and review 84.5, especially the advanced material towards the
end of that section.

In the sections that follow, we first discuss Fredholm equations of the second kind
with smooth kernels (§18.1). Nontrivial quadrature rules come into the discussion,
but we will be dealing with well-conditioned systems of equations. We then return
to Volterra equations (§18.2), and find that simple and straightforward methods are
generally satisfactory for these equations.

In §18.3 we discuss how to proceed in the case of singular kernels, focusing
largely on Fredholm equations (both first and second kinds). Singularities require

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.1 Fredholm Equations of the Second Kind 791

special quadrature rules, but they are also sometimes blessings in disguise, since they
can spoil a kernel’s smoothing and make problems well-conditioned.

In §§18.4-18.7 we face up to the issues of inverse problems. §18.4 is an
introduction to this large subject.

We should note here that wavelet transforms, already discussed in §13.10, are
applicable not only to data compression and signal processing, but can also be used
to transform some classes of integral equations into sparse linear problems that allow
fast solution. You may wish to review §13.10 as part of reading this chapter.

Some subjects, such as integro-differential equations, we must simply declare
to be beyond our scope. For a review of methods for integro-differential equations,
see Brunner [4].

It should go without saying that this one short chapter can only barely touch on
a few of the most basic methods involved in this complicated subject.

CITED REFERENCES AND FURTHER READING:

Delves, L.M., and Mohamed, J.L. 1985, Computational Methods for Integral Equations (Cam-
bridge, U.K.: Cambridge University Press). [1]

Linz, P. 1985, Analytical and Numerical Methods for Volterra Equations (Philadelphia: S.1.A.M.).
[2]

Atkinson, K.E. 1976, A Survey of Numerical Methods for the Solution of Fredholm Integral
Equations of the Second Kind (Philadelphia: S.I.A.M.). [3]

Brunner, H. 1988, in Numerical Analysis 1987, Pitman Research Notes in Mathematics vol. 170,
D.F. Griffiths and G.A. Watson, eds. (Harlow, Essex, U.K.: Longman Scientific and Tech-
nical), pp. 18-38. [4]

Smithies, F. 1958, Integral Equations (Cambridge, U.K.: Cambridge University Press).

Kanwal, R.P. 1971, Linear Integral Equations (New York: Academic Press).

Green, C.D. 1969, Integral Equation Methods (New York: Barnes & Noble).

18.1 Fredholm Equations of the Second Kind

We desire a numerical solution for f(¢) in the equation

b

f(t)= | K(t,s)f(s) ds + g(t) (18.1.1)

a

The method we describe, a very basic one, is called the Nystrom method. It requires
the choice of some approximate quadrature rule:

N

[uw ds = So wiy(s;) (18.1.2)

j=l

Here the set {w,;} are the weights of the quadrature rule, while the N points {s ;}
are the abscissas.

What quadrature rule should we use? It is certainly possible to solve integral
equations with low-order quadrature rules like the repeated trapezoidal or Simpson’s

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

792 Chapter 18. Integral Equations and Inverse Theory

rules. We will see, however, that the solution method involves O(N 3) operations,
and so the most efficient methods tend to use high-order quadrature rules to keep
N as small as possible. For smooth, nonsingular problems, nothing beats Gaussian
quadrature (e.g., Gauss-Legendre quadrature, §4.5). (For non-smooth or singular
kernels, see $18.3.)

Delves and Mohamed [1] investigated methods more complicated than the
Nystrom method. For straightforward Fredholm equations of the second kind, they
concluded“. . . the clear winner of this contest has been the Nystrom routine . . . with
the N-point Gauss-Legendre rule. This routine is extremely simple... . Such results
are enough to make a numerical analyst weep.”

If we apply the quadrature rule (18.1.2) to equation (18.1.1), we get

N
FQ) = ADT wjK (t, sy) F(sy) + (8) (18.1.3)
j=l
Evaluate equation (18.1.3) at the quadrature points:
N
F (ti) = D7 Wj K (ti, 85). (83) + gti) (18.1.4)
j=l

Let f; be the vector f(t;), gi the vector g(t;), Ki; the matrix K(t;, s;), and define
Kij = Kijw; (18.1.5)

Then in matrix notation equation (18.1.4) becomes

(1—AK)-f=g (18.1.6)

This is a set of N linear algebraic equations in N unknowns that can be solved
by standard triangular decomposition techniques (§2.3) — that is where the O(N 3)
operations count comes in. The solution is usually well-conditioned, unless \ is
very close to an eigenvalue.

Having obtained the solution at the quadrature points {t;}, how do you get the
solution at some other point ¢? You do not simply use polynomial interpolation.
This destroys all the accuracy you have worked so hard to achieve. Nystrom’s key
observation was that you should use equation (18.1.3) as an interpolatory formula,
maintaining the accuracy of the solution.

We here give two routines for use with linear Fredholm equations of the second
kind. The routine fred2 sets up equation (18.1.6) and then solves it by LU
decomposition with calls to the routines ludcmp and lubksb. The Gauss-Legendre
quadrature is implemented by first getting the weights and abscissas with a call to
gauleg. Routine fred2 requires that you provide an external function that returns
g(t) and another that returns A/‘;;. It then returns the solution f at the quadrature
points. It also returns the quadrature points and weights. These are used by the
second routine fredin to carry out the Nystrom interpolation of equation (18.1.3)
and return the value of f at any point in the interval (a, b].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.1 Fredholm Equations of the Second Kind 793

#include "nrutil.h"

void fred2(int n, float a, float b, float t[], float f[], float wf],
float (*g) (float), float (*ak) (float, float))

Solves a linear Fredholm equation of the second kind. On input, a and b are the limits of
integration, and n is the number of points to use in the Gaussian quadrature. g and ak are
user-supplied external functions that respectively return g(t) and AK (t,s). The routine returns
arrays t[1..n] and £[1..n] containing the abscissas t; of the Gaussian quadrature and the
solution f at these abscissas. Also returned is the array w[1..n] of Gaussian weights for use
with the Nystrom interpolation routine fredin.

t
void gauleg(float x1, float x2, float x[], float wf], int n);
void lubksb(float **a, int n, int *indx, float b[]);
void ludemp(float **a, int n, int *indx, float *d);
int i,j,*indx;
float d,**omk;
indx=ivector(1,n);
omk=matrix(1,n,1,n);
gauleg(a,b,t,w,n); Replace gauleg with another routine if not using
for (i=1;i<=n;i++) { Gauss-Legendre quadrature.
for (j=1;j<=n; j++) Form I — AK.
omk [i] [j]=(float) (i == j)-(*ak) (t [i] ,t[j])*wljl;
f[il=(*g) (t[i]);
+
ludcmp(omk,n, indx,&d) ; Solve linear equations.
lubksb(omk,n, indx,f) ;
free_matrix(omk,1,n,1,n);
free_ivector(indx,1,n);
3

float fredin(float x, int n, float a, float b, float t{], float f[],

float wl], float (*g)(float), float (*ak)(float, float))
Given arrays t[1..n] and w[1..n] containing the abscissas and weights of the Gaussian
quadrature, and given the solution array f [1..n] from fred2, this function returns the value of
f at x using the Nystrom interpolation formula. On input, a and b are the limits of integration,
and n is the number of points used in the Gaussian quadrature. g and ak are user-supplied
external functions that respectively return g(t) and AK(t,s).

t
int i;
float sum=0.0;
for (i=1;i<=n;it++) sum += (*ak) (x,t [i])*wlil*f (il;
return (*g) (x)+sum;
}

One disadvantage of a method based on Gaussian quadrature is that there is no
simple way to obtain an estimate of the error in the result. The best practical method
is to increase N by 50%, say, and treat the difference between the two estimates as a
conservative estimate of the error in the result obtained with the larger value of NV.

Turn now to solutions of the homogeneous equation. If we set \ = 1/o and
g = 0, then equation (18.1.6) becomes a standard eigenvalue equation

K-f=of (18.1.7)

which we can solve with any convenient matrix eigenvalue routine (see Chapter
11). Note that if our original problem had a symmetric kernel, then the matrix K

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

794 Chapter 18. Integral Equations and Inverse Theory

is symmetric. However, since the weights w; are not equal for most quadrature
rules, the matrix K (equation 18.1.5) is not symmetric. The matrix eigenvalue
problem is much easier for symmetric matrices, and so we should restore the
symmetry if possible. Provided the weights are positive (which they are for Gaussian
quadrature), we can define the diagonal matrix D = diag(w,) and its square root,
Di? = diag(,/wj). Then equation (18.1.7) becomes

K-D-f=of
Multiplying by D!/?, we get

(pYv?-K -p'/?) -h=oh (18.1.8)

where h = D'/? -f, Equation (18.1.8) is now in the form of a symmetric eigenvalue
problem.

Solution of equations (18.1.7) or (18.1.8) will in general give N eigenvalues,
where N is the number of quadrature points used. For square-integrable kernels,
these will provide good approximations to the lowest N eigenvalues of the integral
equation. Kernels of finite rank (also called degenerate or separable kernels) have
only a finite number of nonzero eigenvalues (possibly none). You can diagnose
this situation by a cluster of eigenvalues o that are zero to machine precision. The
number of nonzero eigenvalues will stay constant as you increase N to improve
their accuracy. Some care is required here: A nondegenerate kernel can have an
infinite number of eigenvalues that have an accumulation point at o = 0. You
distinguish the two cases by the behavior of the solution as you increase NV. If you
suspect a degenerate kernel, you will usually be able to solve the problem by analytic
techniques described in all the textbooks.

CITED REFERENCES AND FURTHER READING:

Delves, L.M., and Mohamed, J.L. 1985, Computational Methods for Integral Equations (Cam-
bridge, U.K.: Cambridge University Press). [1]

Atkinson, K.E. 1976, A Survey of Numerical Methods for the Solution of Fredholm Integral
Equations of the Second Kind (Philadelphia: S.1.A.M.).

18.2 Volterra Equations

Let us now turn to Volterra equations, of which our prototype is the Volterra
equation of the second kind,

A= | K(t,s)f(s) ds + g(t) (18.2.1)

Most algorithms for Volterra equations march out from t = a, building up the solution
as they go. In this sense they resemble not only forward substitution (as discussed

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.2 Volterra Equations 795

in §18.0), but also initial-value problems for ordinary differential equations. In fact,
many algorithms for ODEs have counterparts for Volterra equations.

The simplest way to proceed is to solve the equation on a mesh with uniform
spacing:

b-a
N

tj =a+ih, i1=0,1,..., N, h (18.2.2)

To do so, we must choose a quadrature rule. For a uniform mesh, the simplest
scheme is the trapezoidal rule, equation (4.1.11):

ti i-1
| K(ti.s)f(s)ds=h | £Kiofo +S) Kijfj + 2 Kili (18.2.3)

j=l

Thus the trapezoidal method for equation (18.2.1) is:

fo = 90
it (18.2.4)
(1 4hKu)fi=h ale PKS | tg, t=1,...,N
j=l

(For a Volterra equation of the first kind, the leading 1 on the left would be absent,
and g would have opposite sign, with corresponding straightforward changes in the
rest of the discussion.)

Equation (18.2.4) is an explicit prescription that gives the solution in O(N ?)
operations. Unlike Fredholm equations, it is not necessary to solve a system of linear
equations. Volterra equations thus usually involve less work than the corresponding
Fredholm equations which, as we have seen, do involve the inversion of, sometimes
large, linear systems.

The efficiency of solving Volterra equations is somewhat counterbalanced by
the fact that systems of these equations occur more frequently in practice. If we
interpret equation (18.2.1) as a vector equation for the vector of m functions f(t),
then the kernel A(t, s) is an m x m matrix. Equation (18.2.4) must now also be
understood as a vector equation. For each i, we have to solve the m x m set of
linear algebraic equations by Gaussian elimination.

The routine voltra below implements this algorithm. You must supply an
external function that returns the kth function of the vector g(t) at the point t, and
another that returns the (k,1) element of the matrix K(t,s) at (t,s). The routine
voltra then returns the vector f(t) at the regularly spaced points t;.

#include "nrutil.h"

void voltra(int n, int m, float tO, float h, float *t, float **f,

float (*g)(int, float), float (*ak)(int, int, float, float))
Solves a set of m linear Volterra equations of the second kind using the extended trapezoidal rule.
On input, tO is the starting point of the integration and n-1 is the number of steps of size h to
be taken. g(k,t) is a user-supplied external function that returns g;(t), while ak(k,1,t,s)
is another user-supplied external function that returns the (k,/) element of the matrix A(t, s).
The solution is returned in £[1..m] [1..n], with the corresponding abscissas in t[1..n].
{

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

796 Chapter 18. Integral Equations and Inverse Theory

void lubksb(float **a, int n, int *indx, float b[]);
void ludemp(float **a, int n, int *indx, float *d);
int i,j,k,1,*indx;
float d,sum,**a,*b;

indx=ivector(1,m) ;
a=matrix(1,m,1,m);
b=vector(1,m);
t[1]=t0;
for ¢ pk<=m;k++) f[k] [1]=(*g) (k,t[1]); Initialize.
for (i=2; pitt) { Take a step h.
t[il=t[i-1]+h;
for (k=1;k<=m;k++) {
sum=(*g) (k,t[i]); Accumulate right-hand side of linear
for (1=1;1<=m;1++) { equations in sum.
sum += 0.5*h*(*ak) (k,1,t(i] ,t[1])*£(1] [1];
for (j=2;j<i;j++)
sum += he (ak) (ke,1,t Ci] ,t05])*£ (1) [5];
alk] (1J=(k == 1)-0.5*h*(*ak) (k,1,t [i] ,t[i]); Left-hand side goes
} in matrix a.
b[k]=sum;

}

ludemp(a,m, indx, &d) ;

lubksb(a,m, indx,b) ;

for (k=1;k<=m;k++) f[k] [i]=b[k];

Solve linear equations.

}

free_vector(b,1,m);
free_matrix(a,1,m,1,m);
free_ivector(indx,1,m);

For nonlinear Volterra equations, equation (18.2.4) holds with the product K ;; f;
replaced by Kj; (f;), and similarly for the other two products of A’’s and f’s. Thus
for each i we solve a nonlinear equation for f; with a known right-hand side.
Newton’s method (§9.4 or §9.6) with an initial guess of f;-1 usually works very
well provided the stepsize is not too big.

Higher-order methods for solving Volterra equations are, in our opinion, not as
important as for Fredholm equations, since Volterra equations are relatively easy to
solve. However, there is an extensive literature on the subject. Several difficulties
arise. First, any method that achieves higher order by operating on several quadrature
points simultaneously will need a special method to get started, when values at the
first few points are not yet known.

Second, stable quadrature rules can give rise to unexpected instabilities in
integral equations. For example, suppose we try to replace the trapezoidal rule in
the algorithm above with Simpson’s rule. Simpson’s rule naturally integrates over
an interval 2h, so we easily get the function values at the even mesh points. For the
odd mesh points, we could try appending one panel of trapezoidal rule. But to which
end of the integration should we append it? We could do one step of trapezoidal rule
followed by all Simpson’s rule, or Simpson’s rule with one step of trapezoidal rule
at the end. Surprisingly, the former scheme is unstable, while the latter is fine!

A simple approach that can be used with the trapezoidal method given above
is Richardson extrapolation: Compute the solution with stepsize h and h/2. Then,
assuming the error scales with h?, compute

= Af(h/2) = Hh) (18.2.5)

fe 3

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.3 Integral Equations with Singular Kernels 797

This procedure can be repeated as with Romberg integration.

The general consensus is that the best of the higher order methods is the
block-by-block method (see [1]). Another important topic is the use of variable
stepsize methods, which are much more efficient if there are sharp features in K or
f. Variable stepsize methods are quite a bit more complicated than their counterparts
for differential equations; we refer you to the literature [1,2] for a discussion.

You should also be on the lookout for singularities in the integrand. If you find
them, then look to §18.3 for additional ideas.

CITED REFERENCES AND FURTHER READING:

Linz, P. 1985, Analytical and Numerical Methods for Volterra Equations (Philadelphia: S.1.A.M.).
(1)

Delves, L.M., and Mohamed, J.L. 1985, Computational Methods for Integral Equations (Cam-
bridge, U.K.: Cambridge University Press). [2]

18.3 Integral Equations with Singular Kernels

Many integral equations have singularities in either the kernel or the solution or both.
A simple quadrature method will show poor convergence with N if such singularities are
ignored. There is sometimes art in how singularities are best handled.

We start with a few straightforward suggestions:

1. Integrable singularities can often be removed by a change of variable. For example, the
singular behavior K(t, s) ~ sil? ~1/2 near s = 0 can be removed by the transformation
z = s'/?. Note that we are assuming that the singular behavior is confined to K’, whereas
the quadrature actually involves the product K(t,s)f(s), and it is this product that must be
“fixed.” Ideally, you must deduce the singular nature of the product before you try a numerical
solution, and take the appropriate action. Commonly, however, a singular kernel does not
produce a singular solution f(t). (The highly singular kernel K(t,s) = 6(t — s) is simply
the identity operator, for example.)

2. If K(t,s) can be factored as w(s)K(t,s), where w(s) is singular and K(t, s) is
smooth, then a Gaussian quadrature based on w(s) as a weight function will work well. Even
if the factorization is only approximate, the convergence is often improved dramatically. All
you have to do is replace gauleg in the routine fred2 by another quadrature routine. Section
4.5 explained how to construct such quadratures; or you can find tabulated abscissas and
weights in the standard references [1,2]. You must of course supply K instead of K.

This method is a special case of the product Nystrom method [3,4], where one factors out
a singular term p(t, s) depending on both ¢ and s from K and constructs suitable weights for
its Gaussian quadrature. The calculations in the general case are quite cumbersome, because
the weights depend on the chosen {t;} as well as the form of p(t, s).

We prefer to implement the product Nystrom method on a uniform grid, with a quadrature
scheme that generalizes the extended Simpson’s 3/8 rule (equation 4.1.5) to arbitrary weight
functions. We discuss this in the subsections below.

3. Special quadrature formulas are also useful when the kernel is not strictly singular,
but is “almost” so. One example is when the kernel is concentrated near t = s on a scale much
smaller than the scale on which the solution f(t) varies. In that case, a quadrature formula
can be based on locally approximating f(s) by a polynomial or spline, while calculating the
first few moments of the kernel A(t, s) at the tabulation points t;. In such a scheme the
narrow width of the kernel becomes an asset, rather than a liability: The quadrature becomes
exact as the width of the kernel goes to zero.

4. An infinite range of integration is also a form of singularity. Truncating the range at a
large finite value should be used only as a last resort. If the kernel goes rapidly to zero, then

ors

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

798 Chapter 18. Integral Equations and Inverse Theory

a Gauss-Laguerre [w ~ exp(—as)] or Gauss-Hermite [w ~ exp(—s?)] quadrature should
work well. Long-tailed functions often succumb to the transformation
2a
s=—-a 18.3.1
z+1 ( )
which maps 0 < s < co tol > z > —1 so that Gauss-Legendre integration can be used.
Here a > 0 is a constant that you adjust to improve the convergence.
5. A common situation in practice is that A(t, s) is singular along the diagonal line
t = s. Here the Nystrom method fails completely because the kernel gets evaluated at (t;, si).
Subtraction of the singularity is one possible cure:

b b b
[ Kestoas= [ KOs) - F@)as+ [ K(s) Fas
ee se ee (18.3.2)

b
= [ Kes) ~ FO) ds-+ OF

where r(t) = pr K(t, s) ds is computed analytically or numerically. If the first term on
the right-hand side is now regular, we can use the Nystrom method. Instead of equation
(18.1.4), we get

N
h=AD w/Kylfy— Ab rihi +9 (18.33)
j=l
j4i
Sometimes the subtraction process must be repeated before the kernel is completely regularized.
See [3] for details. (And read on for a different, we think better, way to handle diagonal
singularities.)

Quadrature on a Uniform Mesh with Arbitrary Weight

It is possible in general to find n-point linear quadrature rules that approximate the
integral of a function f(z), times an arbitrary weight function w(zx), over an arbitrary range
of integration (a, b), as the sum of weights times n evenly spaced values of the function f(z),
say atx = kh,(k+1)h,...,(k+n—1)h. The general scheme for deriving such quadrature
tules is to write down the n linear equations that must be satisfied if the quadrature rule is
to be exact for the n functions f(x) = const ™~1 and then solve these for the
coefficients. This can be done analytically, once and for all, if the moments of the weight
function over the same range of integration,

W, = — | v"w(x)dx (18.3.4)

are assumed to be known. Here the prefactor h~” is chosen to make Wn scale as h if (as
in the usual case) b — a is proportional to h.
Carrying out this prescription for the four-point case gives the result

afeh) [« + 1)(k + 2)(k + 3)Wo — (3k? + 12k + 11)Wi + 3(k + 2)Wo — |
= f({k + Wh) )|- k(k + 2)(k + 3)Wo + (3k? + 10k + 6)Wi — (3k +5)Wo + ws|
af ({k + 2]h) fac k+1(k+3)Wo — (3k? + 8k +3)Wi + (3k + 4)Wo = wa]

=f ((k + 3]h)| — kk + 1)(k + 2)Wo + (3k? + 6k + 2)W1 — 3(k + 1I)Wo + w|

(18.3.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.3 Integral Equations with Singular Kernels 799

While the terms in brackets superficially appear to scale as k?, there is typically cancellation
at both O(k?) and O(k).

Equation (18.3.5) can be specialized to various choices of (a,b). The obvious choice
isa = kh, b = (k + 3)h, in which case we get a four-point quadrature rule that generalizes
Simpson’s 3/8 rule (equation 4.1.5). In fact, we can recover this special case by setting
w(x) = 1, in which case (18.3.4) becomes

h
W, = ——|(k +: n+l pntl
—le+3)
The four terms in square brackets equation (18.3.5) each become independent of k, and
(18.3.5) in fact reduces to

(k+3)h ¢
[flea = Fh) Seth)

kh

(18.3.6)

LE
g (kr Ih) +3

Back to the case of general w(x), some other choices for a and 6 are also useful. For

f((k+3]h) (18.3.7)

example, we may want to choose (a, b) to be ({k
allowing us to finish off an extended rule who
of three, without loss of accuracy: The integra

+ Ih, [k + 3]h) or ([k + 2]h, [k + 3]h),
se number of intervals is not a multiple
will be estimated using the four values

f(kh),..., f({k + 3]h). Even more useful is to choose (a, b) to be ({k + 1h, [k + 2]h), thus
using four points to integrate a centered single interval. These weights, when sewed together
into an extended formula, give quadrature schemes that have smooth coefficients, i.e., without
the Simpson-like 2, 4, 2, 4, 2 alternation. (In fact, this was the technique that we used to derive
equation 4.1.14, which you may now wish to reexamine.)

All these rules are of the same order as the extended Simpson’s rule, that is, exact
for f(x) a cubic polynomial. Rules of lower order, if desired, are similarly obtained. The
three point formula is

[ * w(e) f(a)de = 2 fh) G

1)(k + 2)Wo — (2k +3)M. 4 w,]

f({k-4 1m)| — ae + 2)Wo + 2(k + 1)Wi aid (18.3.8)

5f(lk + 2h) |k(k + 1)Wo — (2k + Wi +m

Here the simple special case is to take, w(x) = 1, so that
h

ro 5)\ntl _ pnt
Wr = ay [(k + 2) 7} (18.3.9)
Then equation (18.3.8) becomes Simpson’s rule,
(k+2)h h Ah h
[, fade =F f(eh) + Efe + Uh) + EF + Ih) (183.10)

For nonconstant weight functions w(), however, equation (18.3.8) gives rules of one order
less than Simpson, since they do not benefit from the extra symmetry of the constant case.
The two point formula is simply

(k+1)h
[, w(x) f(a)dx = f(kh)[(k +1)Wo — Wi] + f([k + 1h) [—kWo + Wi] (18.3.1)

Here is a routine wwghts that uses the above formulas to return an extended N-point
quadrature rule for the interval (a,b) = (0,[N — 1]h). Input to wwghts is a user-supplied
routine, kermon, that is called to get the first four indefinite-integral moments of w(x), namely

y
Fn(y) = / s™w(s)ds m =0,1,2,3 (18.3.12)
(The lower limit is arbitrary and can be chosen for convenience.) Cautionary note: When
called with N < 4, wwghts returns a rule of lower order than Simpson; you should structure
your problem to avoid this.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

800 Chapter 18. Integral Equations and Inverse Theory

void wwghts(float wghts[], int n, float h,

void (*kermom) (double [], double ,int))
Constructs in wghts[1..n] weights for the n-point equal-interval quadrature from 0 to (n—1)h
of a function f(x) times an arbitrary (possibly singular) weight function w(x) whose indefinite-
integral moments F;,(y) are provided by the user-supplied routine kermom.

{
int j,k;
double wold[5] ,wnew[5] ,w[5] ,hh,hi,c,fac,a,b;
Double precision on internal calculations even though the interface is in single precision. ZSPoH
PI is! gle Pl oe $33
Sa 3
hh=h; ze8ee
hi=1.0/hh; £25 -8
for (j=1;j<=n; j++) wghts[j]=0.0; aga
Zero all the weights so we can sum into them. gs asses
(*kermom) (wold,0.0,4) ; Evaluate indefinite integrals at lower end. esses
if (>= 4) { Use highest available order. Bas8S
b=0.0; For another problem, you might change = ES . q
for (j=1;j<=n-3;j++) { this lower limit. S655
c=j-1; This is called k in equation (18.3.5). %2a8 2
a=b; Set upper and lower limits for this step. rs © gz
b=athh; WEEEO
if (j == n-3) b=(n-1)*hh; Last interval: go all the way to end. Seen
(*kermom) (wnew,b,4) ; ss 32 a
for (fac=1.0,k=1;k<=4;k++,fac*=hi) Equation (18.3.4). SgRegZ
wlk] =(wnew [k] -wold [k] ) *fac; 5 8 8 3 2
wghts[j] += ( Equation (18.3.5). 333 vz
((o#4 0) *(c#2.0) #(c+3.0) w [1] 35R SS
=(11.0+e%(12.0+c%3.0)) #w[2] Sogn
+3. 0#(c+2.0) #w[3]-w[4])/6.0) ; 22889
wghts[j+1] += ¢ Faes a
(-c#(c#2.0)*(c#3.0) 4 [1] SSg39
+(6.0+c*(10.0+c*3.0)) *w[2] 83so08
-(3.0«c+5.0)*w[3]+w[4])*0.5); ezSsa
wghts[j+2] += ( 33285
(c#(c+1.0)*(c+3.0) 4w [1] SP 3RO
=(3.0+c#(8.0+c*3.0)) 4 [2] SIBGS
+(3.0*c+4.0) *w[3]-w[4])*0.5); aes ze
wghts[j+3] += ( 29 DOS
(ce (c#1 .0)*(c#2.0) #wL1] ZR SS
+(2.0+c*(6.0+c#3.0)) *w[2] 82586
-3.0* (c#1.0) #w[3]+w[4])/6.0); assge
for (k=1;k<=4;k++) wold{k]=wnew[k] ; Reset lower limits for moments. 3 tcl ZS
} 20538
} else if (n == 3) { Lower-order cases; not recommended. 238 a2
(*kermom) (wnew, hhthh, 3) ; Geeka
w1]=wnew[1]-wold [1] ; 2 g 2 8
w([2]=hi*(wnew[2]-wold[2]) ; S 2325
w([3]=hithi* (wnew[3]-wold[3]) ; 5 eo8
wghts [1] =w[1]-1.5+w[2]+0.5+w([3] ; a9eg
ughts [2]=2.0*w(2]-w/3) ; 2688
wghts [3] =0.5*(w(3]-w(2]) ; 8233
} else if (n == 2) { 32
(*kermom) (wnew, hh, 2) ; 2 Bo
wghts [1] =wnew[1]-wold([1] -(wghts [2] =hi*(wnew[2]-wold[2])); 3 23
+ =
3

We will now give an example of how to apply wwghts to a singular integral equation.

18.3 Integral Equations with Singular Kernels 801

Worked Example: A Diagonally Singular Kernel
As a particular example, consider the integral equation
f(a) + [Ke fly)ay = sine (18.3.13)
with the (arbitrarily chosen) nasty emel

K(«,y) = cos x cosy X { (18.3.14)

which has a logarithmic singularity on the left of the diagonal, combined with a square-root
discontinuity on the right.

The first step is to do (analytically, in this case) the required moment integrals over
the singular part of the kernel, equation (18.3.12). Since these integrals are done at a fixed
value of x, we can use x as the lower limit. For any specified value of y, the required
indefinite integral is then either

y yx
Fn (ys -[ sm 0)'as= | (a+t)t/?dt = ify>a — (18.3.15)
or

y oy
Fin(y; 2) = — [ s™ In(a — s)ds = [ (x —t)” Intdt ify<a  (18.3.16)
(where a change of variable has been made in the second equality in each case). Doing these
integrals analytically (actually, we used a symbolic integration package!), we package the
resulting formulas in the following routine. Note that w(j + 1) returns Fj(y; 2).

#include <math.h>
extern double x; Defined in quadmx.

void kermom(double w[], double y, int m)
Returns in w[1..m] the first m indefinite-integral moments of one row of the singular part of
the kernel. (For this example, m is hard-wired to be 4.) The input variable y labels the column,
while the global variable x is the row. We can take x as the lower limit of integration. Thus,
we return the moment integrals either purely to the left or purely to the right of the diagonal.
{

double d,df,clog,x2,x3,x4,y2;

if (y >= x) {

d=y-x;
df=2.0*sqrt (d) *d;
wl1]=df£/3.0;

w[2]=df*(x/3.0+d/5.0);
w([3]=df*((x/3.0 + 0.4*d)*x + d*d/7.0);

wl4]=df*(((x/3.0 + 0.6*d)*x + 3.0*d*d/7.0) *x+d*d*d/9.0);
} else {

x3=(x2=x*x) *x;

x4=x2*x2;

y2-y*ys

d=x-y;

w[1]=d* ((clog=log(d))-1.0);

w[2] = -0.25*(3.0*x+y-2.0*clog* (x+y) ) *d;

w(3]=(-11.0*x3+y*(6.0#x2+y*(3, 0#x+2.0*y))

+6 .0*clog* (x3-y*y2))/18.0;
w[4]=(-25.0*x4+y* (12. 0*«x3+y* (6. 0*x2+y*
(4.04x+3.0*y))) 412. 0xclogs (x4-(y2*y2))) /48.0;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

802 Chapter 18. Integral Equations and Inverse Theory

Next, we write a routine that constructs the quadrature matrix.

#include <math.h>
#include "nrutil.h"
#define PI 3.14159265

double x; Communicates with kermom.

void quadmx(float **a, int n)

Constructs in a{1..n][1..n] the quadrature matrix for an example Fredholm equation of
the second kind. The nonsingular part of the kernel is computed within this routine, while
the quadrature weights which integrate the singular part of the kernel are obtained via calls
to wwghts. An external routine kermom, which supplies indefinite-integral moments of the
singular part of the kernel, is passed to wwghts.

t
void kermom(double w[], double y, int m);
void wwghts(float wghts[], int n, float h,
void (*kermom) (double [], double ,int));
int j,k;
float h,*wt,xx,cx;
wt=vector(1,n);
h=PI/(n-1);
for (j j<en;jt+) {
X=: j-1)*h; Put x in global variable for use by kermom.
wwghts (wt ,n,h,kermom) ;
cx=cos (xx) ; Part of nonsingular kernel.
for (k=1;k<=n;k++) alj] [k]=wt [k] *cx*cos((k-1) *h) ;
Put together all the pieces of the kernel.
t+alj] [5]; Since equation of the second kind, there is diagonal piece
} independent of h.
free_vector(wt,1,n);
}

Finally, we solve the linear system for any particular right-hand side, here sin x.

#include <stdio.h>

#include <math.h>

#include "nrutil.h"

#define PI 3.14159265

#define N 40 Here the size of the grid is specified.

int main(void) /* Program fredex */
This sample program shows how to solve a Fredholm equation of the second kind using the
product Nystrom method and a quadrature rule especially constructed for a particular, singular,
kernel.
{

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float *d);

void quadmx(float **a, int n);

float **a,d,¥*g,x;

int *indx,j;

indx=ivector(1,N);
a=matrix(1,N,1,N);
g=vector(1,N);
quadmx(a,N) ; Make the quadrature matrix; all the action is here.
ludcemp(a,N,indx, &d) ; Decompose the matrix.
for (j=1;j<=N;j++) glj]=sin((j-1)*PI/(N-1));
Construct the right hand side, here sina.
lubksb(a,N,indx,g) ; Backsubstitute.
for ( <=N; j++) { Write out the solution.
x=(j-1)*PI/(N-1);

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

18.3 Integral Equations with Singular Kernels 803

0 5 1 1.5 2 25 3
xX

Figure 18.3.1. Solution of the example integral equation (18.3.14) with grid sizes N = 10, 20, and 40.
The tabulated solution values have been connected by straight lines; in practice one would interpolate
a small N solution more smoothly.

printf("%6.2d %12.6f %12.6f\n",j,x,g[j]);
}
free_vector(g,1,N);
free_matrix(a,1,N,1,N);
free_ivector(indx,1,N);
return 0;

With N = 40, this program gives accuracy at about the 10~° level. The accuracy
increases as N* (as it should for our Simpson-order quadrature scheme) despite the highly
singular kernel. Figure 18.3.1 shows the solution obtained, also plotting the solution for
smaller values of NY, which are themselves seen to be remarkably faithful. Notice that the
solution is smooth, even though the kernel is singular, a common occurrence.

CITED REFERENCES AND FURTHER READING:

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Functions, Applied Mathe-
matics Series, Volume 55 (Washington: National Bureau of Standards; reprinted 1968 by
Dover Publications, New York). [1]

Stroud, A.H., and Secrest, D. 1966, Gaussian Quadrature Formulas (Englewood Cliffs, Nu:
Prentice-Hall). [2]

Delves, L.M., and Mohamed, J.L. 1985, Computational Methods for Integral Equations (Cam-
bridge, U.K.: Cambridge University Press). [3]

Atkinson, K.E. 1976, A Survey of Numerical Methods for the Solution of Fredholm Integral
Equations of the Second Kind (Philadelphia: S.I.A.M.). [4]

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

804 Chapter 18. Integral Equations and Inverse Theory

18.4 Inverse Problems and the Use of A Priori
Information

Later discussion will be facilitated by some preliminary mention of a couple
of mathematical points. Suppose that u is an “unknown” vector that we plan to
determine by some minimization principle. Let A[u] > 0 and B[u] > 0 be two
positive functionals of u, so that we can try to determine u by either

minimize: Alu] or minimize: Blu] (18.4.1)

(Of course these will generally give different answers for u.) As another possibility,
now suppose that we want to minimize A[u] subject to the constraint that B[u] have
some particular value, say b. The method of Lagrange multipliers gives the variation

a (Alu) +1 (Blu] — b)} = 2 (Alu) + Blu}) = 0 (18.4.2)
where \; is a Lagrange multiplier. Notice that b is absent in the second equality,
since it doesn’t depend on u.

Next, suppose that we change our minds and decide to minimize B[u] subject
to the constraint that A{u] have a particular value, a. Instead of equation (18.4.2)
we have

2 (Blu) + Ao(Alu] — a)} = 2 (Blu] + AxAlu]) = 0 (18.43)
with, this time, Ag the Lagrange multiplier. Multiplying equation (18.4.3) by the
constant 1/2, and identifying 1/A2 with A, we see that the actual variations are
exactly the same in the two cases. Both cases will yield the same one-parameter
family of solutions, say, u(A;). As A; varies from 0 to oo, the solution u(A1)
varies along a so-called trade-off curve between the problem of minimizing A and
the problem of minimizing B. Any solution along this curve can equally well
be thought of as either (i) a minimization of A for some constrained value of B,
or (ii) a minimization of B for some constrained value of A, or (iii) a weighted
minimization of the sum A+ A,B.

The second preliminary point has to do with degenerate minimization principles.
In the example above, now suppose that A[u] has the particular form

Afu] = |A-u—c|? (18.4.4)

for some matrix A and vector c. If A has fewer rows than columns, or if A is square
but degenerate (has a nontrivial nullspace, see §2.6, especially Figure 2.6.1), then
minimizing A{u] will not give a unique solution for u. (To see why, review §15.4,
and note that for a “design matrix” A with fewer rows than columns, the matrix
A’ - A in the normal equations 15.4.10 is degenerate.) However, if we add any
multiple \ times a nondegenerate quadratic form Bu], for example u - H - u with H
a positive definite matrix, then minimization of A[u] + \B[u] will lead to a unique
solution for u. (The sum of two quadratic forms is itself a quadratic form, with the
second piece guaranteeing nondegeneracy.)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

18.4 Inverse Problems and the Use of A Priori Information 805

We can combine these two points, for this conclusion: When a quadratic
minimization principle is combined with a quadratic constraint, and both are positive,
only one of the two need be nondegenerate for the overall problem to be well-posed.
We are now equipped to face the subject of inverse problems.

The Inverse Problem with Zeroth-Order Regularization

Suppose that u(z) is some unknown or underlying (u stands for both unknown
and underlying!) physical process, which we hope to determine by a set of NV
measurements c;, i = 1,2,...,.N. The relation between u(2) and the c;’s is that

each c; measures a (hopefully distinct) aspect of u(a) through its own linear response
kernel r;, and with its own measurement error n;. In other words,

G=stn= [reyuca)az + nj (18.4.5)

(compare this to equations 13.3.1 and 13.3.2). Within the assumption of linearity,
this is quite a general formulation. The c;’s might approximate values of u(x) at
certain locations x;, in which case r;(x) would have the form of a more or less
narrow instrumental response centered around x = x;. Or, the c;’s might “live” in an
entirely different function space from u(x), measuring different Fourier components
of u(a) for example.

The inverse problemis, given the c;’s, the r;(x)’s, and perhaps some information
about the errors n; such as their covariance matrix

Siz = Covar[n;,n;] (18.4.6)

how do we find a good statistical estimator of u(x), call it U(x)?

It should be obvious that this is an ill-posed problem. After all, how can we
reconstruct a whole function t(x) from only a finite number of discrete values c ;?
Yet, whether formally or informally, we do this all the time in science. We routinely
measure “enough points” and then “draw a curve through them.” In doing so, we
are making some assumptions, either about the underlying function u(x), or about
the nature of the response functions r;(2), or both. Our purpose now is to formalize
these assumptions, and to extend our abilities to cases where the measurements and
underlying function live in quite different function spaces. (How do you “draw a
curve” through a scattering of Fourier coefficients?)

We can’t really want every point x of the function ti). We do want some
large number M of discrete points v,,, 4 = 1,2,...,.M, where M is sufficiently
large, and the x,,’s are sufficiently evenly spaced, that neither u(a) nor r;(x) varies
much between any «,, and 2,1. (Here and following we will use Greek letters like
i to denote values in the space of the underlying process, and Roman letters like i
to denote values of immediate observables.) For such a dense set of x ,,’s, we can
replace equation (18.4.5) by a quadrature like

a= > Riu(tp) + nj (18.4.7)

be

where the N x M matrix R has components

Rip = 7i(@p)(@pp1 — Tp—1)/2 (18.4.8)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

806 Chapter 18. Integral Equations and Inverse Theory

(or any other simple quadrature — it rarely matters which). We will view equations
(18.4.5) and (18.4.7) as being equivalent for practical purposes.

How do you solve a set of equations like equation (18.4.7) for the unknown
u(a,)’s? Here is a bad way, but one that contains the germ of some correct ideas:
Form a x? measure of how well a model w(x) agrees with the measured data,

le 3 > Ripti( on] 8 ‘fs - Pyite|

j=l

= 7 Det ae °

(18.4.9)

v

O%

i=1

(compare with equation 15.1.5). Here S~? is the inverse of the covariance matrix,
and the approximate equality holds if you can neglect the off-diagonal covariances,
with o; = (Covar[i, é])!/?.

Now you can use the method of singular value decomposition (SVD) in §15.4
to find the vector @ that minimizes equation (18.4.9). Don’t try to use the method
of normal equations; since / is greater than N they will be singular, as we already
discussed. The SVD process will thus surely find a large number of zero singular
values, indicative of a highly non-unique solution. Among the infinity of degenerate
solutions (most of them badly behaved with arbitrarily large (a ,,)’s) SVD will
select the one with smallest || in the sense of

Sea)? a minimum (18.4.10)

“

(look at Figure 2.6.1). This solution is often called the principal solution. It
is a limiting case of what is called zeroth-order regularization, corresponding to
minimizing the sum of the two positive functionals

minimize: y?[G] + \(@- 7) (18.4.11)

in the limit of small 1. Below, we will learn how to do such minimizations, as well
as more general ones, without the ad hoc use of SVD.

What happens if we determine U by equation (18.4.11) with a non-infinitesimal
value of X? First, note that if 1 >> N (many more unknowns than equations), then
u will often have enough freedom to be able to make y? (equation 18.4.9) quite
unrealistically small, if not zero. In the language of §15.1, the number of degrees of
freedom v = N — M, which is approximately the expected value of x? when v is
large, is being driven down to zero (and, not meaningfully, beyond). Yet, we know
that for the true underlying function u(2), which has no adjustable parameters, the
number of degrees of freedom and the expected value of y? should be about v + N.

Increasing ) pulls the solution away from minimizing x? in favor of minimizing
t-@. From the preliminary discussion above, we can view this as minimizing 0 - 0
subject to the constraint that y? have some constant nonzero value. A popular
choice, in fact, is to find that value of A which yields x? = N, that is, to get about as
much extra regularization as a plausible value of x? dictates. The resulting @(a) is
called the solution of the inverse problem with zeroth-order regularization.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.4 Inverse Problems and the Use of A Priori Information 807

{ (independent of agreement)

6 '
< i
ss i]
Ss 3 !
8) 32)
By oc
< 1
3
3 achievable solutions
[a2]
1
|
1
1
f
6 1
Sy, vA
ey, . XX best agreement
Ws, (independent of smoothness)

<— Better Smoothness B

Figure 18.4.1. Almost all inverse problem methods involve a trade-off between two optimizations:
agreement between data and solution, or “sharpness” of mapping between true and estimated solution (here
denoted A), and smoothness or stability of the solution (here denoted B). Among all possible solutions,
shown here schematically as the shaded region, those on the boundary connecting the unconstrained
minimum of A and the unconstrained minimum of B are the “best” solutions, in the sense that every
other solution is dominated by at least one solution on the curve.

The value N is actually a surrogate for any value drawn from a Gaussian
distribution with mean N and standard deviation (2N)1‘/? (the asymptotic \?
distribution). One might equally plausibly try two values of \, one giving y? =
N + (2N)'/?, the other N — (2N)!/?.

Zeroth-order regularization, though dominated by better methods, demonstrates
most of the basic ideas that are used in inverse problem theory. In general, there are
two positive functionals, call them A and B. The first, A, measures something like
the agreement of a model to the data (e.g., x7), or sometimes a related quantity like
the “sharpness” of the mapping between the solution and the underlying function.
When A by itself is minimized, the agreement or sharpness becomes very good
(often impossibly good), but the solution becomes unstable, wildly oscillating, or in
other ways unrealistic, reflecting that A alone typically defines a highly degenerate
minimization problem.

That is where B comes in. It measures something like the “smoothness” of the
desired solution, or sometimes a related quantity that parametrizes the stability of
the solution with respect to variations in the data, or sometimes a quantity reflecting
a priori judgments about the likelihood of a solution. B is called the stabilizing
functional or regularizing operator. In any case, minimizing B by itself is supposed
to give a solution that is “smooth” or “stable” or “likely” — and that has nothing
at all to do with the measured data.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

808 Chapter 18. Integral Equations and Inverse Theory

The single central idea in inverse theory is the prescription

minimize: A+AB (18.4.12)

for various values of 0 < \ < oo along the so-called trade-off curve (see Figure
18.4.1), and then to settle on a “best” value of \ by one or another criterion, ranging
from fairly objective (e.g., making y? = N) to entirely subjective. Successful
methods, several of which we will now describe, differ as to their choices of A and
B, as to whether the prescription (18.4.12) yields linear or nonlinear equations, as
to their recommended method for selecting a final A, and as to their practicality for
computer-intensive two-dimensional problems like image processing.

They also differ as to the philosophical baggage that they (or rather, their
proponents) carry. We have thus far avoided the word “Bayesian.” (Courts have
consistently held that academic license does not extend to shouting “Bayesian” in a
crowded lecture hall.) But it is hard, nor have we any wish, to disguise the fact that
B has something to do with a priori expectation, or knowledge, of a solution, while
A has something to do with a posteriori knowledge. The constant A adjudicates a
delicate compromise between the two. Some inverse methods have acquired a more
Bayesian stamp than others, but we think that this is purely an accident of history.
An outsider looking only at the equations that are actually solved, and not at the
accompanying philosophical justifications, would have a difficult time separating the
so-called Bayesian methods from the so-called empirical ones, we think.

The next three sections discuss three different approaches to the problem of
inversion, which have had considerable success in different fields. All three fit
within the general framework that we have outlined, but they are quite different in
detail and in implementation.

CITED REFERENCES AND FURTHER READING:
Craig, |.J.D., and Brown, J.C. 1986, Inverse Problems in Astronomy (Bristol, U.K.: Adam Hilger).

Twomey, S. 1977, Introduction to the Mathematics of Inversion in Remote Sensing and Indirect
Measurements (Amsterdam: Elsevier).

Tikhonov, A.N., and Arsenin, V.Y. 1977, Solutions of Ill-Posed Problems (New York: Wiley).

Tikhonov, A.N., and Goncharsky, A.V. (eds.) 1987, Ill-Posed Problems in the Natural Sciences
(Moscow: MIR).

Parker, R.L. 1977, Annual Review of Earth and Planetary Science, vol. 5, pp. 35-64.

Frieden, B.R. 1975, in Picture Processing and Digital Filtering, T.S. Huang, ed. (New York:
Springer-Verlag).

Tarantola, A. 1987, Inverse Problem Theory (Amsterdam: Elsevier).

Baumeister, J. 1987, Stable Solution of Inverse Problems (Braunschweig, Germany: Friedr. Vieweg
& Sohn) [mathematically oriented].

Titterington, D.M. 1985, Astronomy and Astrophysics, vol. 144, pp. 381-387.
Jeffrey, W., and Rosner, R. 1986, Astrophysical Journal, vol. 310, pp. 463-472.

18.5 Linear Regularization Methods

What we will call linear regularization is also called the Phillips-Twomey
method [1,2], the constrained linear inversion method [3], the method of regulariza-
tion [4], and Tikhonov-Miller regularization [5-7]. (It probably has other names also,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.5 Linear Regularization Methods 809

since it is so obviously a good idea.) In its simplest form, the method is an immediate
generalization of zeroth-order regularization (equation 18.4.11, above). As before,
the functional A is taken to be the \? deviation, equation (18.4.9), but the functional
B is replaced by more sophisticated measures of smoothness that derive from first
or higher derivatives.

For example, suppose that your a priori belief is that a credible u(x) is not too
different from a constant. Then a reasonable functional to minimize is

M-1

Bx [i@wPae x Ss (Gy. — Wey]? (18.5.1)
p=

since it is nonnegative and equal to zero only when w(x) is constant. Here
ty, = U(x,), and the second equality (proportionality) assumes that the x ,,’s are
uniformly spaced. We can write the second form of B as

B= |B-0/? =t-(B’-B)-@=0-H-t (18.5.2)
where @ is the vector of components @,,, js = 1,...,M, B is the (MM — 1) x M
first difference matrix
-1 1 0 0 0 0 0 0
0 -1 1 0 0 0 --- 0
B= : on : (18.5.3)
0 --- 0 0 0 0 -1 1 0
0 --- 0 0 0 0 0 -1 1
and H is the M x M matrix
1 -l 0 0 0 0 O --- 0
-1 2 -1 0 0 0 O --- 0
0 -1 2 -1 0 0 O --- 0
H=B!.B= : m : (18.5.4)
O + 0 0 0 -1 2 -l 0
O + 0 0 0 0 -1 2 -1
O + 0 0 0 0 0 -1 1

Note that B has one fewer row than column. It follows that the symmetric H
is degenerate; it has exactly one zero eigenvalue corresponding to the value of a
constant function, any one of which makes B exactly zero.

If, just as in §15.4, we write

Ain = Rip /oi b = ci /oi (18.5.5)
then, using equation (18.4.9), the minimization principle (18.4.12) is
minimize: A+AB=|A-7-b/?+\0-H-O (18.5.6)

This can readily be reduced to a linear set of normal equations, just as in §15.4: The
components %,, of the solution satisfy the set of M equations in M unknowns,

Ss (x Ai Ap) + Min tip = SO Aindb) = w= 1,2,...,M_ (18.5.7)

p

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

810 Chapter 18. Integral Equations and Inverse Theory

or, in vector notation,
(A-A+AH)-@=A™-b (18.5.8)

Equations (18.5.7) or (18.5.8) can be solved by the standard techniques of
Chapter 2, e.g., LU decomposition. The usual warnings about normal equations
being ill-conditioned do not apply, since the whole purpose of the \ term is to cure
that same ill-conditioning. Note, however, that the A term by itself is ill-conditioned,
since it does not select a preferred constant value. You hope your data can at
least do that!

Although inversion of the matrix (A7 - A + AH) is not generally the best way to
solve for @, let us digress to write the solution to equation (18.5.8) schematically as

a= Gases AT -a) Al-b (schematic only!) (18.5.9)
A’ -A+AH

where the identity matrix in the form A - A~! has been inserted. This is schematic
not only because the matrix inverse is fancifully written as a denominator, but
also because, in general, the inverse matrix A~! does not exist. However, it is
illuminating to compare equation (18.5.9) with equation (13.3.6) for optimal or
Wiener filtering, or with equation (13.6.6) for general linear prediction. One sees
that A? - A plays the role of $2, the signal power or autocorrelation, while \H
plays the role of N’, the noise power or autocorrelation. The term in parentheses
in equation (18.5.9) is something like an optimal filter, whose effect is to pass the
ill-posed inverse A~! - b through unmodified when A7 - A is sufficiently large, but
to suppress it when A’ - A is small.

The above choices of B and H are only the simplest in an obvious sequence of
derivatives. If your a priori belief is that a linear function is a good approximation
to u(x), then minimize

M-2

Bx [irae SO [A tiy + tings — Tyo)? (18.5.10)
pal
implying
-l1 2-1 0 0 0 0+. 0
0-1 2-1 0 0 0+. 0
B=| : ne : (18.5.11)
O-. 0 0 0-1 2-1 0
0 0 0 0 0 -1 2 -1
and
1-2 1 0 0 0 0 0
- 5 4 1 0 0 0 0
1-4 6-4 1 0 0. 0
0 1-4 6 -4 1°05. 0
H=B’.B= : m : (18.5.12)
O-. 0 1-4 6-4 1 0
O- 0 0 1-4 6-4 1
O- 0 0 0 1-4 5 -2

0 -:- 0 0 0 0 1 -2 1

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

18.5 Linear Regularization Methods 811

This H has two zero eigenvalues, corresponding to the two undetermined parameters
of a linear function.
If your a priori belief is that a quadratic function is preferable, then minimize

M-3

Bx [(@"erPac x So [Hy + 8G p41 — 8b p42 + Buys]? (18.5.13)
pal
with
-1 30-3 1 0 0 O --- 0
0-1 3-3 1 0 0. 0
B= : _ : (18.5.14)
0 0 0-1 3 - 0
0 0 0 0 -1 301
and now
1-3 3 -1 0 60 60 0 0 0
-3 10 -12 6 -1 O 6 0 0 0
3-12 19 -15 6 -1 0 0 0 0
-l1 6 -15 2 -15 6 -1 oO 0 0
0 -l 6 -15 20 -15 6 -1 0 0
0 0 -1 6 -15 20 -15 6 -1 O
0 0 0 -1 5 -15 20 —15 6 -1
0 0 0 0 -l 6 —-15 19 —12 3
0 0 oO 0 O -1 6 -12 10 -3
0 0 0 0 0 OO -1 38 -3 1

(18.5.15)
(We’Il leave the calculation of cubics and above to the compulsive reader.)

Notice that you can regularize with “closeness to a differential equation,” if
you want. Just pick B to be the appropriate sum of finite-difference operators (the
coefficients can depend on ), and calculate H = B” - B. You don’t need to know
the values of your boundary conditions, since B can have fewer rows than columns,
as above; hopefully, your data will determine them. Of course, if you do know some
boundary conditions, you can build these into B too.

With all the proportionality signs above, you may have lost track of what actual
value of X to try first. A simple trick for at least getting “on the map” is to first try

= Tr(A? - A)/Tr(H) (18.5.16)

where Tr is the trace of the matrix (sum of diagonal components). This choice
will tend to make the two parts of the minimization have comparable weights, and
you can adjust from there.

As for what is the “correct” value of A, an objective criterion, if you know
your errors o; with reasonable accuracy, is to make x? (that is, |A - @ — b|?) equal
to N, the number of measurements. We remarked above on the twin acceptable
choices N + (2N)'/2. A subjective criterion is to pick any value that you like in the

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

812 Chapter 18. Integral Equations and Inverse Theory

range 0 < \ < ov, depending on your relative degree of belief in the a priori and a
posteriori evidence. (Yes, people actually do that. Don’t blame us.)

Two-Dimensional Problems and Iterative Methods

Up to now our notation has been indicative of a one-dimensional problem,
finding w(x) or t,, = %(x,,). However, all of the discussion easily generalizes to the
problem of estimating a two-dimensional set of unknowns t,x, /4 = 1,..., M, k=
1,..., , K, corresponding, say, to the pixel intensities of a measured image. In this
case, equation (18.5.8) is still the one we want to solve.

In image processing, it is usual to have the same number of input pixels in a
measured “raw” or “dirty” image as desired “clean” pixels in the processed output
image, so the matrices R and A (equation 18.5.5) are square and of size MK x MK.
A is typically much too large to represent as a full matrix, but often it is either (i)
sparse, with coefficients blurring an underlying pixel (i, 7) only into measurements
(i+ few, j+few), or (ii) translationally invariant, so that A (;_j)(4,) = A(i-, j-V).
Both of these situations lead to tractable problems.

In the case of translational invariance, fast Fourier transforms (FFTs) are the
obvious method of choice. The general linear relation between underlying function
and measured values (18.4.7) now becomes a discrete convolution like equation
(13.1.1). If k denotes a two-dimensional wave-vector, then the two-dimensional FFT
takes us back and forth between the transform pairs

A(@i-p,j—v) <= Alk) bag) <> dK) Guy) —S Tk) (185.17)

We also need a regularization or smoothing operator B and the derived H = B7 - B.
One popular choice for B is the five-point finite-difference approximation of the
Laplacian operator, that is, the difference between the value of each point and the
average of its four Cartesian neighbors. In Fourier space, this choice implies,

B(k) x sin? (7k /M) sin? (wk2/K)

~ (18.5.18)
H(k) x sin4(ak,/M) sin*(tk2/K)
In Fourier space, equation (18.5.7) is merely algebraic, with solution
*(k)b(k
u(k) = (k)O(k) (18.5.19)

|A(k)? + AH(k)

where asterisk denotes complex conjugation. You can make use of the FFT routines
for real data in §12.5.

Turn now to the case where A is not translationally invariant. Direct solution
of (18.5.8) is now hopeless, since the matrix A is just too large. We need some
kind of iterative scheme.

One way to proceed is to use the full machinery of the conjugate gradient
method in §10.6 to find the minimum of A + AB, equation (18.5.6). Of the various
methods in Chapter 10, conjugate gradient is the unique best choice because (i)
it does not require storage of a Hessian matrix, which would be infeasible here,

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

18.5 Linear Regularization Methods 813

and (ii) it does exploit gradient information, which we can readily compute: The
gradient of equation (18.5.6) is

V(A+ AB) = 2[(A7 -A + AH)-— A™- bd] (18.5.20)

(cf. 18.5.8). Evaluation of both the function and the gradient should of course take
advantage of the sparsity of A, for example via the routines sprsax and sprstx
in §2.7. We will discuss the conjugate gradient technique further in $18.7, in the
context of the (nonlinear) maximum entropy method. Some of that discussion can
apply here as well.

The conjugate gradient method notwithstanding, application of the unsophis-
ticated steepest descent method (see $10.6) can sometimes produce useful results,
particularly when combined with projections onto convex sets (see below). If the
solution after k iterations is denoted a" , then after k + 1 iterations we have

a) = [1 —e(AT- A+ AH) -O™ 4+ AT -d (18.5.21)

Here ¢ is a parameter that dictates how far to move in the downhill gradient direction.
The method converges when € is small enough, in particular satisfying

2
0<e< - F (18.5.22)
max eigenvalue (A~ - A+ AH)

There exist complicated schemes for finding optimal values or sequences for e€,
see [7]; or, one can adopt an experimental approach, evaluating (18.5.6) to be sure
that downhill steps are in fact being taken.

In those image processing problems where the final measure of success is
somewhat subjective (e.g., “how good does the picture look?”), iteration (18.5.21)
sometimes produces significantly improved images long before convergence is
achieved. This probably accounts for much of its use, since its mathematical
convergence is extremely slow. In fact, (18.5.21) can be used with H = 0, in which
case the solution is not regularized at all, and full convergence would be disastrous!
This is called Van Cittert’s method and goes back to the 1930s. A number of
iterations the order of 1000 is not uncommon [7].

Deterministic Constraints: Projections onto Convex Sets

A set of possible underlying functions (or images) {t} is said to be convex if,
for any two elements t, and U, in the set, all the linearly interpolated combinations

(L-n)ta+nt, O<n<1 (18.5.23)

are also in the set. Many deterministic constraints that one might want to impose on
the solution 0 to an inverse problem in fact define convex sets, for example:

© positivity

© compact support (i.e., zero value outside of a certain region)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

814 Chapter 18. Integral Equations and Inverse Theory

e known bounds (ie., wp (x) < U(x) < uy(x) for specified functions uz,
and uy).

(In this last case, the bounds might be related to an initial estimate and its error bars,
e.g., Uo(a) + yo(x), where ¥ is of order 1 or 2.) Notice that these, and similar,
constraints can be either in the image space, or in the Fourier transform space, or (in
fact) in the space of any linear transformation of 0.

If C; is a convex set, then P; is called a nonexpansive projection operator onto
that set if (i) P; leaves unchanged any @ already in C;, and (ii) P; maps any & outside
C; to the closest element of C;, in the sense that

|Pa@ —U| <a, —| forall t, inC; (18.5.24)

While this definition sounds complicated, examples are very simple: A nonexpansive
projection onto the set of positive 0’s is “set all negative components of 0 equal
to zero.” A nonexpansive projection onto the set of w(x)’s bounded by u p(x) <
U(x) < uy(2) is “set all values less than the lower bound equal to that bound, and
set all values greater than the upper bound equal to that bound.” A nonexpansive
projection onto functions with compact support is “zero the values outside of the
region of support.”

The usefulness of these definitions is the following remarkable theorem: Let C
be the intersection of m convex sets C',, C2,..., Cm. Then the iteration

AY) = (PyPy--- Pa” (18.5.25)

will converge to C' from all starting points, as k — oo. Also, if C is empty (there
is no intersection), then the iteration will have no limit point. Application of this
theorem is called the method of projections onto convex sets or sometimes POCS [7].

A generalization of the POCS theorem is that the P;’s can be replaced by
a set of J;’s,

T.=14+6i(Pi-1) 0<6 <2 (18.5.26)

A well-chosen set of (;’s can accelerate the convergence to the intersection set C.

Some inverse problems can be completely solved by iteration (18.5.25) alone!
For example, a problem that occurs in both astronomical imaging and X-ray
diffraction work is to recover an image given only the modulus of its Fourier
transform (equivalent to its power spectrum or autocorrelation) and not the phase.
Here three convex sets can be utilized: the set of all images whose Fourier transform
has the specified modulus to within specified error bounds; the set of all positive
images; and the set of all images with zero intensity outside of some specified region.
In this case the POCS iteration (18.5.25) cycles among these three, imposing each
constraint in turn; FFTs are used to get in and out of Fourier space each time the
Fourier constraint is imposed.

The specific application of POCS to constraints alternately in the spatial and
Fourier domains is also known as the Gerchberg-Saxton algorithm [8]. While this
algorithm is non-expansive, and is frequently convergent in practice, it has not been
proved to converge in all cases [9]. In the phase-retrieval problem mentioned above,
the algorithm often “gets stuck” on a plateau for many iterations before making
sudden, dramatic improvements. As many as 104 to 10° iterations are sometimes

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.6 Backus-Gilbert Method 815

necessary. (For “unsticking” procedures, see [10].) The uniqueness of the solution
is also not well understood, although for two-dimensional images of reasonable
complexity it is believed to be unique.

Deterministic constraints can be incorporated, via projection operators, into
iterative methods of linear regularization. In particular, rearranging terms somewhat,
we can write the iteration (18.5.21) as

a) = 1 — dH) a 4 eAT- (b-A- a) (18.5.27)
If the iteration is modified by the insertion of projection operators at each step
aS) = (Py P2-++Pm)[(1 — AH) - 8%) + eAT-(b—A-G™)] — (18.5.28)

(or, instead of P;’s, the J; operators of equation 18.5.26), then it can be shown that
the convergence condition (18.5.22) is unmodified, and the iteration will converge
to minimize the quadratic functional (18.5.6) subject to the desired nonlinear
deterministic constraints. See [7] for references to more sophisticated, and faster
converging, iterations along these lines.

CITED REFERENCES AND FURTHER READING:
Phillips, D.L. 1962, Journal of the Association for Computing Machinery, vol. 9, pp. 84-97. [1]
Twomey, S. 1963, Journal of the Association for Computing Machinery, vol. 10, pp. 97-101. [2]

Twomey, S. 1977, Introduction to the Mathematics of Inversion in Remote Sensing and Indirect
Measurements (Amsterdam: Elsevier). [3]

Craig, |.J.D., and Brown, J.C. 1986, Inverse Problems in Astronomy (Bristol, U.K.: Adam Hilger).
[4]

Tikhonov, A.N., and Arsenin, V.Y. 1977, Solutions of Ill-Posed Problems (New York: Wiley). [5]

Tikhonov, A.N., and Goncharsky, A.V. (eds.) 1987, Ill-Posed Problems in the Natural Sciences
(Moscow: MIR).

Miller, K. 1970, SIAM Journal on Mathematical Analysis, vol. 1, pp. 52-74. [6]

Schafer, R.W., Mersereau, R.M., and Richards, M.A. 1981, Proceedings of the IEEE, vol. 69,
pp. 432-450.

Biemond, J., Lagendijk, R.L., and Mersereau, R.M. 1990, Proceedings of the IEEE, vol. 78,
pp. 856-883. [7]

Gerchberg, R.W., and Saxton, W.O. 1972, Optik, vol. 35, pp. 237-246. [8]
Fienup, J.R. 1982, Applied Optics, vol. 15, pp. 2758-2769. [9]

Fienup, J.R., and Wackerman, C.C. 1986, Journal of the Optical Society of America A, vol. 3,
pp. 1897-1907. [10]

18.6 Backus-Gilbert Method

The Backus-Gilbert method [1.2] (see, e.g., [3] or [4] for summaries) differs from
other regularization methods in the nature of its functionals A and B. For B, the
method seeks to maximize the stability of the solution w(x) rather than, in the first
instance, its smoothness. That is,

B = Var{ti(x)| (18.6.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

816 Chapter 18. Integral Equations and Inverse Theory

is used as a measure of how much the solution @(x) varies as the data vary within
their measurement errors. Note that this variance is not the expected deviation of
U(x) from the true u(x) — that will be constrained by A — but rather measures
the expected experiment-to-experiment scatter among estimates w(x) if the whole
experiment were to be repeated many times.

For A the Backus-Gilbert method looks at the relationship between the solution
u(x) and the true function u(x), and seeks to make the mapping between these as
close to the identity map as possible in the limit of error-free data. The method is
linear, so the relationship between (x) and u(x) can be written as

for some so-called resolution function or averaging kernel 5(a, x’). The Backus-
Gilbert method seeks to minimize the width or spread of 5 (that is, maximize the
resolving power). A is chosen to be some positive measure of the spread.

While Backus-Gilbert’s philosophy is thus rather different from that of Phillips-
Twomey and related methods, in practice the differences between the methods are
less than one might think. A stable solution is almost inevitably bound to be
smooth: The wild, unstable oscillations that result from an unregularized solution
are always exquisitely sensitive to small changes in the data. Likewise, making
U(x) close to u(x) inevitably will bring error-free data into agreement with the
model. Thus A and B play roles closely analogous to their corresponding roles
in the previous two sections.

The principal advantage of the Backus-Gilbert formulation is that it gives good
control over just those properties that it seeks to measure, namely stability and
resolving power. Moreover, in the Backus-Gilbert method, the choice of \ (playing
its usual role of compromise between A and 8) is conventionally made, or at least
can easily be made, before any actual data are processed. One’s uneasiness at making
a post hoc, and therefore potentially subjectively biased, choice of \ is thus removed.
Backus-Gilbert is often recommended as the method of choice for designing, and
predicting the performance of, experiments that require data inversion.

Let’s see how this all works. Starting with equation (18.4.5),

G=stu= [r@yueiae + ny (18.6.3)

and building in linearity from the start, we seek a set of inverse response kernels
qi(z) such that

is the desired estimator of w
kernels for each data point,

It is useful to define the integrals of the response

(18.6.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.6 Backus-Gilbert Method 817

Substituting equation (18.6.4) into equation (18.6.3), and comparing with equation
(18.6.2), we see that

S(a, 2") = S>ai(x)ri(a’) (18.6.6)
i
We can require this averaging kernel to have unit area at every x, giving

where q() and R are each vectors of length NV, the number of measurements.
Standard propagation of errors, and equation (18.6.1), give

B = Var(ii(x)] = S> >> ai(w)Sigqi(x) = a(a) -S- a(x) (18.6.8)

i

where S;; is the covariance matrix (equation 18.4.6). If one can neglect off-diagonal
covariances (as when the errors on the c,’s are independent), then Sij = dijo?
is diagonal.

We now need to define a measure of the width or spread of 5(x,2’) at each
value of x. While many choices are possible, Backus and Gilbert choose the second
moment of its square. This measure becomes the functional A,

18.6.9
=P alas (aa(o) =ale)-Weay-aa) 8°?
ij

where we have here used equation (18.6.6) and defined the spread matrix W(x) by

The functions g;() are now determined by the minimization principle
minimize: A+ AB = q(x) - [W(x) + AS] - q(x) (18.6.11)

subject to the constraint (18.6.7) that q(a)-R = 1.
The solution of equation (18.6.11) is

[W(a) + AS]7!-R

a2) = RWS OR (18.6.12)

(Reference [4] gives an accessible proof.) For any particular data set c (set of
measurements c;), the solution (x) is thus

c-[W(x) + AS]~!-R

Ue) = RW) 4S) oR

(18.6.13)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

818 Chapter 18. Integral Equations and Inverse Theory

(Don’t let this notation mislead you into inverting the full matrix W(a) + AS. You
only need to solve for some y the linear system (W(x) + AS) -y = R, and then
substitute y into both the numerators and denominators of 18.6.12 or 18.6.13.)

Equations (18.6.12) and (18.6.13) have a completely different character from
the linearly regularized solutions to (18.5.7) and (18.5.8). The vectors and matrices in
(18.6.12) all have size N, the number of measurements. There is no discretization of
the underlying variable x, so M does not come into play at all. One solves a different
N x N set of linear equations for each desired value of x. By contrast, in (18.5.8),
one solves an / x M linear set, but only once. In general, the computational burden
of repeatedly solving linear systems makes the Backus-Gilbert method unsuitable
for other than one-dimensional problems.

How does one choose A within the Backus-Gilbert scheme? As already
mentioned, you can (in some cases should) make the choice before you see any actual
data. For a given trial value of \, and for a sequence of x’s, use equation (18.6.12)
to calculate q(z); then use equation (18.6.6) to plot the resolution functions O(a, x’)
as a function of x’. These plots will exhibit the amplitude with which different
underlying values x’ contribute to the point w(x) of your estimate. For the same
value of , also plot the function ,/Var[t(x)| using equation (18.6.8). (You need an
estimate of your measurement covariance matrix for this.)

As you change 2 you will see very explicitly the trade-off between resolution
and stability. Pick the value that meets your needs. You can even choose to be a
function of x, A = (x), in equations (18.6.12) and (18.6.13), should you desire to
do so. (This is one benefit of solving a separate set of equations for each x.) For
the chosen value or values of A, you now have a quantitative understanding of your
inverse solution procedure. This can prove invaluable if — once you are processing
real data — you need to judge whether a particular feature, a spike or jump for
example, is genuine, and/or is actually resolved. The Backus-Gilbert method has
found particular success among geophysicists, who use it to obtain information about
the structure of the Earth (e.g., density run with depth) from seismic travel time data.

CITED REFERENCES AND FURTHER READING:

Backus, G.E., and Gilbert, F. 1968, Geophysical Journal of the Royal Astronomical Society,
vol. 16, pp. 169-205. [1]

Backus, G.E., and Gilbert, F. 1970, Philosophical Transactions of the Royal Society of London
A, vol. 266, pp. 123-192. [2]

Parker, R.L. 1977, Annual Review of Earth and Planetary Science, vol. 5, pp. 35-64. [3]
Loredo, T.J., and Epstein, R.I. 1989, Astrophysical Journal, vol. 336, pp. 896-919. [4]

18.7 Maximum Entropy Image Restoration

Above, we commented that the association of certain inversion methodsbreak
with Bayesian arguments is more historical accident than intellectual imperative.
Maximum entropy methods, so-called, are notorious in this regard; to summarize
these methods without some, at least introductory, Bayesian invocations would be
to serve a steak without the sizzle, or a sundae without the cherry. We should

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.7 Maximum Entropy Image Restoration 819

also comment in passing that the connection between maximum entropy inversion
methods, considered here, and maximum entropy spectral estimation, discussed in
§13.7, is rather abstract. For practical purposes the two techniques, though both
named maximum entropy method or MEM, are unrelated.

Bayes’ Theorem, which follows from the standard axioms of probability, relates
the conditional probabilities of two events, say A and B:

Prob(B|A)

Prob( |B) = Prob(A) So cpy

(18.7.1)

Here Prob(A|B) is the probability of A given that B has occurred, and similarly for
Prob(B|A), while Prob(.A) and Prob(B) are unconditional probabilities.

“Bayesians” (so-called) adopt a broader interpretation of probabilities than do
so-called “frequentists.” To a Bayesian, P(A|B) is a measure of the degree of
plausibility of A (given B) ona scale ranging from zero to one. In this broader view,
A and B need not be repeatable events; they can be propositions or hypotheses.
The equations of probability theory then become a set of consistent rules for
conducting inference [1,2]. Since plausibility is itself always conditioned on some,
perhaps unarticulated, set of assumptions, all Bayesian probabilities are viewed as
conditional on some collective background information J.

Suppose H is some hypothesis. Even before there exist any explicit data,
a Bayesian can assign to H some degree of plausibility Prob(H|I), called the
“Bayesian prior.” Now, when some data D; comes along, Bayes theorem tells how
to reassess the plausibility of H,

Prob(D|HI)
Prob(D,|Z)

Prob(H|D,I) = Prob(H|J) (18.7.2)

The factor in the numerator on the right of equation (18.7.2) is calculable as the
probability of a data set given the hypothesis (compare with “likelihood” in §15.1).
The denominator, called the “prior predictive probability” of the data, is in this case
merely a normalization constant which can be calculated by the requirement that
the probability of all hypotheses should sum to unity. (In other Bayesian contexts,
the prior predictive probabilities of two qualitatively different models can be used
to assess their relative plausibility.)

If some additional data D2 comes along tomorrow, we can further refine our
estimate of H’s probability, as

Prob(D2|HD;1)

Prob(H|D2DI) = Prob(H|D1I
rob(H|D2DiT) = Prob(H| 1) Prob(Da|Dil)

(18.7.3)

Using the product rule for probabilities, Prob(AB|C) = Prob(A|C)Prob(B| AC),
we find that equations (18.7.2) and (18.7.3) imply

Prob(D2D,|H1)
Prob(D2D;|I)

Prob(H|D2D,I) = Prob(H|Z) (18.7.4)

which shows that we would have gotten the same answer if all the data DD»
had been taken together.

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

820 Chapter 18. Integral Equations and Inverse Theory

From a Bayesian perspective, inverse problems are inference problems [3,4]. The
underlying parameter set u is a hypothesis whose probability, given the measured
data values c, and the Bayesian prior Prob(u|J) can be calculated. We might want
to report a single “best” inverse u, the one that maximizes

Prob(u|Z)

Prob(u|cI) = Prob(clul) Prob(clZ)

(18.7.5)
over all possible choices of u. Bayesian analysis also admits the possibility of
reporting additional information that characterizes the region of possible u’s with
high relative probability, the so-called “posterior bubble” in u.

The calculation of the probability of the data c, given the hypothesis u proceeds
exactly as in the maximum likelihood method. For Gaussian errors, e.g., it is given by

Prob(clul) = exp(—5x7) Au Aue -+ Aum (18.7.6)

where x? is calculated from u and c using equation (18.4.9), and the Au,,’s are
constant, small ranges of the components of u whose actual magnitude is irrelevant,
because they do not depend on u (compare equations 15.1.3 and 15.1.4).

In maximum likelihood estimation we, in effect, chose the prior Prob(u|I) to

be constant. That was a luxury that we could afford when estimating a small number
of parameters from a large amount of data. Here, the number of “parameters”
(components of u) is comparable to or larger than the number of measured values
(components of c); we need to have a nontrivial prior, Prob(u|J), to resolve the
degeneracy of the solution.
In maximum entropy image restoration, that is where entropy comes in. The
entropy of a physical system in some macroscopic state, usually denoted S, is the
logarithm of the number of microscopically distinct configurations that all have
the same macroscopic observables (i.e., consistent with the observed macroscopic
state). Actually, we will find it useful to denote the negative of the entropy, also
called the negentropy, by H = —S (a notation that goes back to Boltzmann). In
situations where there is reason to believe that the a priori probabilities of the
microscopic configurations are all the same (these situations are called ergodic), then
the Bayesian prior Prob(u|J) for a macroscopic state with entropy S is proportional
to exp(S) or exp(—H).

MEM uses this concept to assign a prior probability to any given underlying
function u. For example [5-7], suppose that the measurement of luminance in each
pixel is quantized to (in some units) an integer value. Let

M

U= Sou (18.7.7)

p=

be the total number of luminance quanta in the whole image. Then we can base our
“prior” on the notion that each luminance quantum has an equal a priori chance of
being in any pixel. (See [8] for a more abstract justification of this idea.) The number
of ways of getting a particular configuration u is

U! 1
Sa a - 1 s{l - Inu, 18.7.8
hol ug) OP You n(u,/U) + 3 (we ye ve) ( )

“

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.7 Maximum Entropy Image Restoration 821

Here the left side can be understood as the number of distinct orderings of all
the luminance quanta, divided by the numbers of equivalent reorderings within
each pixel, while the right side follows by Stirling’s approximation to the factorial
function. Taking the negative of the logarithm, and neglecting terms of order log U
in the presence of terms of order U, we get the negentropy

M
H(u) = S> uy In(u,/U) (18.7.9)

p=1

From equations (18.7.5), (18.7.6), and (18.7.9) we now seek to maximize

1
Prob(u|c) x exp -3] exp[—H(u)] (18.7.10)
or, equivalently,
1 1 M
minimize: — In [Prob(u|c) | 5X by) + H(u) 5X by) + You In(u,/U)
y=1
(18.7.1)

This ought to remind you of equation (18.4.11), or equation (18.5.6), or in fact any of
our previous minimization principles along the lines of A + AB, where \B = H(u)
is a regularizing operator. Where is \? We need to put it in for exactly the reason
discussed following equation (18.4.11): Degenerate inversions are likely to be able
to achieve unrealistically small values of y?. We need an adjustable parameter to
bring x? into its expected narrow statistical range of N + (2) !/?. The discussion at
the beginning of §18.4 showed that it makes no difference which term we attach the
to. For consistency in notation, we absorb a factor 2 into \ and put it on the entropy
term. (Another way to see the necessity of an undetermined factor is to note that it
is necessary if our minimization principle is to be invariant under changing the units
in which u is quantized, e.g., if an 8-bit analog-to-digital converter is replaced by a
12-bit one.) We can now also put “hats” back to indicate that this is the procedure
for obtaining our chosen statistical estimator:

M
minimize: A+B = x7[f] +AH(@) = x7[0] +S 0G, m(@,) — (18.7.12)
pal

(Formally, we might also add a second Lagrange multiplier \’U, to constrain the
total intensity U to be constant.)

It is not hard to see that the negentropy, H (1), is in fact a regularizing operator,
similar to 0 - @ (equation 18.4.11) or 0-H - 0 (equation 18.5.6). The following of
its properties are noteworthy:

1. When U is held constant, H(t) is minimized for t,, = U/M = constant, so it
smooths in the sense of trying to achieve a constant solution, similar to equation
(18.5.4). The fact that the constant solution is a minimum follows from the fact
that the second derivative of wu In u is positive.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

822 Chapter 18. Integral Equations and Inverse Theory

2. Unlike equation (18.5.4), however, H(a) is local, in the sense that it does not
difference neighboring pixels. It simply sums some function f, here

f(u) =ulnu (18.7.13)

over all pixels; it is invariant, in fact, under a complete scrambling of the pixels

in an image. This form implies that H(t) is not seriously increased by the

occurrence of a small number of very bright pixels (point sources) embedded
in a low-intensity smooth background.

3. H(@) goes to infinite slope as any one pixel goes to zero. This causes it to
enforce positivity of the image, without the necessity of additional deterministic
constraints.

4. The biggest difference between H(i) and the other regularizing operators that
we have met is that H(t) is not a quadratic functional of 0, so the equations
obtained by varying equation (18.7.12) are nonlinear. This fact is itself worthy
of some additional discussion.

Nonlinear equations are harder to solve than linear equations. For image
processing, however, the large number of equations usually dictates an iterative
solution procedure, even for linear equations, so the practical effect of the nonlinearity
is somewhat mitigated. Below, we will summarize some of the methods that are
successfully used for MEM inverse problems.

For some problems, notably the problem in radio-astronomy of image recovery
from an incomplete set of Fourier coefficients, the superior performance of MEM
inversion can be, in part, traced to the nonlinearity of H(t). One way to see this [5]
is to consider the limit of perfect measurements o; — 0. In this case the x? term in
the minimization principle (18.7.12) gets replaced by a set of constraints, each with
its own Lagrange multiplier, requiring agreement between model and data; that is,

minimize: >; Je; — S> Rin tin | + H(@) (18.7.14)
j be

(cf. equation 18.4.7). Setting the formal derivative with respect to @,, to zero gives

OH hm
aa, = f' (Gu) = yy AjRin (18.7.15)
or defining a function G as the inverse function of f’,

tip =G{ SOAR, (18.7.16)
j

This solution is only formal, since the \;’s must be found by requiring that equation
(8.7.16) satisfy all the constraints built into equation (18.7.14). However, equation
(18.7.16) does show the crucial fact that if G is linear , then the solution 0 contains only
a linear combination of basis functions R ;,, corresponding to actual measurements
Jj. This is equivalent to setting unmeasured c,;’s to zero. Notice that the principal
solution obtained from equation (18.4.11) in fact has a linear G.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

18.7 Maximum Entropy Image Restoration 823

In the problem of incomplete Fourier image reconstruction, the typical R j,,
has the form exp(—27ik, - x,,), where x,, is a two-dimensional vector in the image
space and k,, is a two-dimensional wave-vector. If an image contains strong point
sources, then the effect of setting unmeasured c;’s to zero is to produce sidelobe
ripples throughout the image plane. These ripples can mask any actual extended,
low-intensity image features lying between the point sources. If, however, the slope
of G is smaller for small values of its argument, larger for large values, then ripples
in low-intensity portions of the image are relatively suppressed, while strong point
sources will be relatively sharpened (‘“‘superresolution”). This behavior on the slope
of G is equivalent to requiring f’”(u) < 0. For f(u) = ulnu, we in fact have
f"(u) = -1/u? < 0.

In more picturesque language, the nonlinearity acts to “create” nonzero values
for the unmeasured c;’s, so as to suppress the low-intensity ripple and sharpen the
point sources.

Is MEM Really Magical?

How unique is the negentropy functional (18.7.9)? Recall that that equation is
based on the assumption that luminance elements are a priori distributed over the
pixels uniformly. If we instead had some other preferred a priori image in mind, one
with pixel intensities m,,, then it is easy to show that the negentropy becomes

M
H(u) = > Up n(uz/m,) + constant (18.7.17)
y=1

(the constant can then be ignored). All the rest of the discussion then goes through.

More fundamentally, and despite statements by zealots to the contrary [7], there
is actually nothing universal about the functional form f(u) = ulnu. In some
other physical situations (for example, the entropy of an electromagnetic field in the
limit of many photons per mode, as in radio-astronomy) the physical negentropy
functional is actually f(w) = — Inu (see [5] for other examples). In general, the
question, “Entropy of what?” is not uniquely answerable in any particular situation.
(See reference [9] for an attempt at articulating a more general principle that reduces
to one or another entropy functional under appropriate circumstances.)

The four numbered properties summarized above, plus the desirable sign for
nonlinearity, f’”(w) < 0, are all as true for f(u) = —Inwas for f(u) = ulnu. In
fact these properties are shared by a nonlinear function as simple as f(u) = — //u,
which has no information theoretic justification at all (no logarithms!). MEM
reconstructions of test images using any of these entropy forms are virtually
indistinguishable [5].

By all available evidence, MEM seems to be neither more nor less than one
usefully nonlinear version of the general regularization scheme A + B that we have
by now considered in many forms. Its peculiarities become strengths when applied
to the reconstruction from incomplete Fourier data of images that are expected
to be dominated by very bright point sources, but which also contain interesting
low-intensity, extended sources. For images of some other character, there is no
reason to suppose that MEM methods will generally dominate other regularization
schemes, either ones already known or yet to be invented.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

824 Chapter 18. Integral Equations and Inverse Theory

Algorithms for MEM

The goal is to find the vector 0 that minimizes A + \B where in the notation
of equations (18.5.5), (18.5.6), and (18.7.13),

A=|b-A-t? B= 5° f(@,) (18.7.18)
Me
Compared with a “general” minimization problem, we have the advantage that

we can compute the gradients and the second partial derivative matrices (Hessian
matrices) explicitly,

2
VA=2(AT A-G-AT.b) 2A = pat ay,
OU, Ip (18.7.19)
~ PB ni “
[VB]. = fy) ———— = bp f(y)
pty

It is important to note that while A’s second partial derivative matrix cannot be stored
(its size is the square of the number of pixels), it can be applied to any vector by
first applying A, then A7. In the case of reconstruction from incomplete Fourier
data, or in the case of convolution with a translation invariant point spread function,
these applications will typically involve several FFTs. Likewise, the calculation of
the gradient VA will involve FFTs in the application of A and A’.

While some success has been achieved with the classical conjugate gradient
method (§10.6), it is often found that the nonlinearity in f(u) = ulnwu causes
problems. Attempted steps that give 0 with even one negative value must be cut in
magnitude, sometimes so severely as to slow the solution to a crawl. The underlying
problem is that the conjugate gradient method develops its information about the
inverse of the Hessian matrix a bit at a time, while changing its location in the search
space. When a nonlinear function is quite different from a pure quadratic form, the
old information becomes obsolete before it gets usefully exploited.

Skilling and collaborators [6,7,10,11] developed a complicated but highly suc-
cessful scheme, wherein a minimum is repeatedly sought not along a single search
direction, but in a small- (typically three-) dimensional subspace, spanned by vectors
that are calculated anew at each landing point. The subspace basis vectors are
chosen in such a way as to avoid directions leading to negative values. One of the
most successful choices is the three-dimensional subspace spanned by the vectors
with components given by

) = ty VA]
ey = [VB],

ey _ ML p(PA/II,I%,)tip[VB]p A, ,(0°A/I7,0%,)a,[VA]p
: ¥, tp ((VB]p)° YB (VAI)?

(18.7.20)

(In these equations there is no sum over ju.) The form of the e “°) has some justification
if one views dot products as occurring in a space with the metric guy = Our /ty,
chosen to make zero values “far away’; see [6].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

18.7 Maximum Entropy Image Restoration 825

Within the three-dimensional subspace, the three-component gradient and nine-
component Hessian matrix are computed by projection from the large space, and
the minimum in the subspace is estimated by (trivially) solving three simultaneous
linear equations, as in §10.7, equation (10.7.4). The size of a step A@ is required
to be limited by the inequality

SO (AG,)?/ti, < (0.1 to 0.5)U (18.7.21)
in
Because the gradient directions VA and VB are separately available, it is possible
to combine the minimum search with a simultaneous adjustment of \ so as finally to
satisfy the desired constraint. There are various further tricks employed.

A less general, but in practice often equally satisfactory, approach is due to
Cornwell and Evans [12]. Here, noting that B’s Hessian (second partial derivative)
matrix is diagonal, one asks whether there is a useful diagonal approximation to
A’s Hessian, namely 2A7 - A. If A, denotes the diagonal components of such an
approximation, then a useful step in 1 would be

1
“RAF Gy)
(again compare equation 10.7.4). Even more extreme, one might seek an approx-
imation with constant diagonal elements, A,, = A, so that

1
A+ AF" in)
Since A? - A has something of the nature of a doubly convolved point spread
function, and since in real cases one often has a point spread function with a sharp

central peak, even the more extreme of these approximations is often fruitful. One
starts with a rough estimate of A obtained from the A j,,’s, e.g.,

Aw (Stat (18.7.24)

An accurate value is not important, since in practice A is adjusted adaptively: If A
is too large, then equation (18.7.23)’s steps will be too small (that is, larger steps in
the same direction will produce even greater decrease in .A + B). If A is too small,
then attempted steps will land in an unfeasible region (negative values of % ,,), or will
result in an increased A+ \B. There is an obvious similarity between the adjustment
of A here and the Levenberg-Marquardt method of §15.5; this should not be too
surprising, since MEM is closely akin to the problem of nonlinear least-squares
fitting. Reference [12] also discusses how the value of A + Af’ (ti,,) can be used to
adjust the Lagrange multiplier \ so as to converge to the desired value of y 7.

All practical MEM algorithms are found to require on the order of 30 to 50
iterations to converge. This convergence behavior is not now understood in any
fundamental way.

Ati, (VA +AVB) (18.7.2)

Atiy = (VA + AVB) (18.7.23)

“Bayesian” versus “Historic” Maximum Entropy

Several more recent developments in maximum entropy image restoration
go under the rubric “Bayesian” to distinguish them from the previous “historic”
methods. See [13] for details and references.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

826 Chapter 18. Integral Equations and Inverse Theory

e Better priors: We already noted that the entropy functional (equation
18.7.13) is invariant under scrambling all pixels and has no notion of
smoothness. The so-called “intrinsic correlation function” (ICF) model
(Ref. [13], where it is called “New MaxEnt”) is similar enough to the
entropy functional to allow similar algorithms, but it makes the values of
neighboring pixels correlated, enforcing smoothness.

Better estimation of \: Above we chose ) to bring y? into its expected
narrow statistical range of N + (2N)'/?. This in effect overestimates y?,
however, since some effective number ¥ of parameters are being “fitted” in
doing the reconstruction. A Bayesian approach leads to a self-consistent
estimate of this and an objectively better choice for X.

CITED REFERENCES AND FURTHER READING:

Jaynes, E.T. 1976, in Foundations of Probability Theory, Statistical Inference, and Statistical
Theories of Science, W.L. Harper and C.A. Hooker, eds. (Dordrecht: Reidel). [1]

Jaynes, E.T. 1985, in Maximum-Entropy and Bayesian Methods in Inverse Problems, C.R. Smith
and W.T. Grandy, Jr., eds. (Dordrecht: Reidel). [2]

Jaynes, E.T. 1984, in SIAM-AMS Proceedings, vol. 14, D.W. McLaughlin, ed. (Providence, RI:
American Mathematical Society). [3]

Titterington, D.M. 1985, Astronomy and Astrophysics, vol. 144, 381-387. [4]

Narayan, R., and Nityananda, R. 1986, Annual Review of Astronomy and Astrophysics, vol. 24,
pp. 127-170. [5]

Skilling, J., and Bryan, R.K. 1984, Monthly Notices of the Royal Astronomical Society, vol. 211,
pp. 111-124. [6]

Burch, S.F., Gull, S.F., and Skilling, J. 1983, Computer Vision, Graphics and Image Processing,
vol. 23, pp. 113-128. [7]

Skilling, J. 1989, in Maximum Entropy and Bayesian Methods, J. Skilling, ed. (Boston: Kluwer). [8]
Frieden, B.R. 1983, Journal of the Optical Society of America, vol. 73, pp. 927-938. [9]

Skilling, J., and Gull, S.F. 1985, in Maximum-Entropy and Bayesian Methods in Inverse Problems,
C.R. Smith and W.T. Grandy, Jr., eds. (Dordrecht: Reidel). [10]

Skilling, J. 1986, in Maximum Entropy and Bayesian Methods in Applied Statistics, J.H. Justice,
ed. (Cambridge: Cambridge University Press). [11]

Cornwell, T.J., and Evans, K.F. 1985, Astronomy and Astrophysics, vol. 143, pp. 77-83. [12]
Gull, S.F. 1989, in Maximum Entropy and Bayesian Methods, J. Skilling, ed. (Boston: Kluwer). [13]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 19. Partial Differential
Equations

19.0 Introduction

The numerical treatment of partial differential equations is, by itself, a vast
subject. Partial differential equations are at the heart of many, if not most,
computer analyses or simulations of continuous physical systems, such as fluids,
electromagnetic fields, the human body, and so on. The intent of this chapter is to
give the briefest possible useful introduction. Ideally, there would be an entire second
volume of Numerical Recipes dealing with partial differential equations alone. (The
references [1-4] provide, of course, available alternatives.)

In most mathematics books, partial differential equations (PDEs) are classified
into the three categories, hyperbolic, parabolic, and elliptic, on the basis of their
characteristics, or curves of information propagation. The prototypical example of
a hyperbolic equation is the one-dimensional wave equation

Ou 207u
— =" 19.0.1
ae” Bae (19.0.1)
where v = constant is the velocity of wave propagation. The prototypical parabolic
equation is the diffusion equation
du Oo Ou
—= | D— 19.0.2

where D is the diffusion coefficient. The prototypical elliptic equation is the
Poisson equation

Ou Ou

where the source term p is given. If the source term is equal to zero, the equation
is Laplace’s equation.

From a computational point of view, the classification into these three canonical
types is not very meaningful — or at least not as important as some other essential
distinctions. Equations (19.0.1) and (19.0.2) both define initial value or Cauchy
problems: If information on u (perhaps including time derivative information) is

827

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

828 Chapter 19. Partial Differential Equations

@ O° fe) [e) [e) [e) [e) [e) @
boundary
® ° ° ° ° ° ° ° ® / conditions
@ [e) fe) [e) [e) [e) [e) [e) @
e e e e e e e
initial values
(a)
bd bd bd boundary
values
e e [e) e v4
e [e) fe) [e) e e
e O° fe) [e} [e) [e) e
e fe) [e) [e) [e) e
e [e) [e) [e) e
e e e e
(b)
Figure 19.0.1. Initial value problem (a) and boundary value problem (b) are contrasted. In (a) initial

values are given on one “time slice,” and it is desired to advance the solution in time, computing successive
rows of open dots in the direction shown by the arrows. Boundary conditions at the left and right edges
of each row (@) must also be supplied, but only one row at a time. Only one, or a few, previous rows
need be maintained in memory. In (b), boundary values are specified around the edge of a grid, and an
iterative process is employed to find the values of all the internal points (open circles). All grid points
must be maintained in memory.

given at some initial time to for all x, then the equations describe how u(x,t)
propagates itself forward in time. In other words, equations (19.0.1) and (19.0.2)
describe time evolution. The goal of a numerical code should be to track that time
evolution with some desired accuracy.

By contrast, equation (19.0.3) directs us to find a single “static” function u(a, y)
which satisfies the equation within some (, y) region of interest, and which — one
must also specify — has some desired behavior on the boundary of the region of
interest. These problems are called boundary value problems. In general it is not

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.0 Introduction 829

possible stably to just “integrate in from the boundary” in the same sense that an
initial value problem can be “integrated forward in time.” Therefore, the goal of a
numerical code is somehow to converge on the correct solution everywhere at once.

This, then, is the most important classification from a computational point
of view: Is the problem at hand an initial value (time evolution) problem? or
is it a boundary value (static solution) problem? Figure 19.0.1 emphasizes the
distinction. Notice that while the italicized terminology is standard, the terminology
in parentheses is a much better description of the dichotomy from a computational
perspective. The subclassification of initial value problems into parabolic and
hyperbolic is much less important because (i) many actual problems are of a mixed
type, and (ii) as we will see, most hyperbolic problems get parabolic pieces mixed
into them by the time one is discussing practical computational schemes.

Initial Value Problems

An initial value problem is defined by answers to the following questions:

e What are the dependent variables to be propagated forward in time?

e What is the evolution equation for each variable? Usually the evolution
equations will all be coupled, with more than one dependent variable
appearing on the right-hand side of each equation.

e What is the highest time derivative that occurs in each variable’s evolution
equation? If possible, this time derivative should be put alone on the
equation’s left-hand side. Not only the value of a variable, but also the
value of all its time derivatives — up to the highest one — must be
specified to define the evolution.

e What special equations (boundary conditions) govern the evolution in time
of points on the boundary of the spatial region of interest? Examples:
Dirichlet conditions specify the values of the boundary points as a function
of time; Neumann conditions specify the values of the normal gradients on
the boundary; outgoing-wave boundary conditions are just what they say.

Sections 19.1—-19.3 of this chapter deal with initial value problems of several

different forms. We make no pretence of completeness, but rather hope to convey a
certain amount of generalizable information through a few carefully chosen model
examples. These examples will illustrate an important point: One’s principal
computational concern must be the stability of the algorithm. Many reasonable-
looking algorithms for initial value problems just don’t work — they are numerically
unstable.

Boundary Value Problems

The questions that define a boundary value problem are:

e What are the variables?

e What equations are satisfied in the interior of the region of interest?

e What equations are satisfied by points on the boundary of the region of
interest? (Here Dirichlet and Neumann conditions are possible choices for
elliptic second-order equations, but more complicated boundary conditions
can also be encountered.)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

830 Chapter 19. Partial Differential Equations

In contrast to initial value problems, stability is relatively easy to achieve
for boundary value problems. Thus, the efficiency of the algorithms, both in
computational load and storage requirements, becomes the principal concern.

Because all the conditions on a boundary value problem must be satisfied
“simultaneously,” these problems usually boil down, at least conceptually, to the
solution of large numbers of simultaneous algebraic equations. When such equations
are nonlinear, they are usually solved by linearization and iteration; so without much
loss of generality we can view the problem as being the solution of special, large
linear sets of equations.

As an example, one which we will refer to in §§19.4-19.6 as our “model
problem,” let us consider the solution of equation (19.0.3) by the finite-difference
method. We represent the function u(x, y) by its values at the discrete set of points

tj =a+jA, j=90,1,..,7

19.0.4
yw=yo +l, 1=0,1,...,2 ( )

where A is the grid spacing. From now on, we will write w,, for u(aj,y), and
pj for p(x;, yr). For (19.0.3) we substitute a finite-difference representation (see
Figure 19.0.2),

Uji — 2ujr t+ Uji | Uyl4ea — 2Uj + Uj -1 7
A2 ' A2 Pj,l

(19.0.5)

or equivalently
2
Ujti + Uj-1 + Ugg + Ug i—1 — 4uz = A* pj (19.0.6)
To write this system of linear equations in matrix form we need to make a
vector out of u. Let us number the two dimensions of grid points in a single
one-dimensional sequence by defining

i=j(L+1)+l for j=0,1,..,J, 1=0,1,..,L (19.0.7)

In other words, 7 increases most rapidly along the columns representing y values.
Equation (19.0.6) now becomes

UitL+i + Ui—(L41) + Uit1 + Ui-1 — 4ui = A? 0; (19.0.8)
This equation holds only at the interior points 7 = 1,2,...,J — 1;1 = 1,2,...,

L-1.
The points where

j=0 ie.,¢ =0,..., Z]

jad fie.,i = J(L+1),..., J(L+1) +L]

1=0 fie, =0,L41,...,J(L+ 1) (19.0.9)
l=L fie, i =L,L+141,...,J(24+1) +1]

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.0 Introduction 831

> <—
YL
A
<®
y1
yo
Xo x eee xy

Figure 19.0.2. Finite-difference representation of a second-order elliptic equation on a two-dimensional
grid. The second derivatives at the point A are evaluated using the points to which A is shown connected.
The second derivatives at point B are evaluated using the connected points and also using “right-hand
side” boundary information, shown schematically as ®.

are boundary points where either wu or its derivative has been specified. If we pull
all this “known” information over to the right-hand side of equation (19.0.8), then
the equation takes the form

A-u=b (19.0.10)

where A has the form shown in Figure 19.0.3. The matrix A is called “tridiagonal
with fringes.” A general linear second-order elliptic equation

Pu Ou Pu Ou
a(x, 9) a5 + W(x. w) a 4 oe." a td wa, so.
Pu ~
OxOy

will lead to a matrix of similar structure except that the nonzero entries will not
be constants.

As a rough classification, there are three different approaches to the solution
of equation (19.0.10), not all applicable in all cases: relaxation methods, “rapid”
methods (e.g., Fourier methods), and direct matrix methods.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

832 Chapter 19. Partial Differential Equations

increasing j} ——>

Tt |-4 1 1 7] ft
2 1-4 1 1 each
4 eee . block
5 eee . (L+1)x
& 1-41 . (L+1)
| 1-4 1
1 -41 1
1 1-4 1 1
. see . J+1
. eee . blocks
1 1-4 1 1
1 1-4 1
1 -4 1
. 1-4 1
. 1-4 1
1 1-4
< J +1 blocks

Figure 19.0.3. Matrix structure derived from a second-order elliptic equation (here equation 19.0.6). All
elements not shown are zero. The matrix has diagonal blocks that are themselves tridiagonal, and sub-
and super-diagonal blocks that are diagonal. This form is called “tridiagonal with fringes.” A matrix this
sparse would never be stored in its full form as shown here.

Relaxation methods make immediate use of the structure of the sparse matrix
A. The matrix is split into two parts

A=E-F (19.0.12)
where E is easily invertible and F is the remainder. Then (19.0.10) becomes
E-u=F-u+b (19.0.13)

The relaxation method involves choosing an initial guess u) and then solving
successively for iterates u(”) from

E-u =F-ul’-) 4b (19.0.14)

Since E is chosen to be easily invertible, each iteration is fast. We will discuss
relaxation methods in some detail in §19.5 and §19.6.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.0 Introduction 833

So-called rapid methods [5] apply for only a rather special class of equations:
those with constant coefficients, or, more generally, those that are separable in the
chosen coordinates. In addition, the boundaries must coincide with coordinate lines.
This special class of equations is met quite often in practice. We defer detailed
discussion to §19.4. Note, however, that the multigrid relaxation methods discussed
in §19.6 can be faster than “rapid” methods.

Matrix methods attempt to solve the equation

A-x=b (19.0.15)

directly. The degree to which this is practical depends very strongly on the exact
structure of the matrix A for the problem at hand, so our discussion can go no farther
than a few remarks and references at this point.

Sparseness of the matrix must be the guiding force. Otherwise the matrix
problem is prohibitively large. For example, the simplest problem on a 100 x 100
spatial grid would involve 10000 unknown u ;,;’s, implying a 10000 x 10000 matrix
A, containing 10° elements!

As we discussed at the end of §2.7, if A is symmetric and positive definite
(as it usually is in elliptic problems), the conjugate-gradient algorithm can be
used. In practice, rounding error often spoils the effectiveness of the conjugate
gradient algorithm for solving finite-difference equations. However, it is useful when
incorporated in methods that first rewrite the equations so that A is transformed to a
matrix A’ that is close to the identity matrix. The quadratic surface defined by the
equations then has almost spherical contours, and the conjugate gradient algorithm
works very well. In §2.7, in the routine linbcg, an analogous preconditioner
was exploited for non-positive definite problems with the more general biconjugate
gradient method. For the positive definite case that arises in PDEs, an example of
a successful implementation is the incomplete Cholesky conjugate gradient method
(ICCG) (see [6-8]).

Another method that relies on a transformation approach is the strongly implicit
procedure of Stone [9]. A program called SIPSOL that implements this routine has
been published [10].

A third class of matrix methods is the Analyze-Factorize-Operate approach as
described in §2.7.

Generally speaking, when you have the storage available to implement these
methods — not nearly as much as the 10° above, but usually much more than is
required by relaxation methods — then you should consider doing so. Only multigrid
relaxation methods (§19.6) are competitive with the best matrix methods. For grids
larger than, say, 300 x 300, however, it is generally found that only relaxation
methods, or “rapid” methods when they are applicable, are possible.

There Is More to Life than Finite Differencing

Besides finite differencing, there are other methods for solving PDEs. Most
important are finite element, Monte Carlo, spectral, and variational methods. Unfor-
tunately, we shall barely be able to do justice to finite differencing in this chapter,
and so shall not be able to discuss these other methods in this book. Finite element
methods [11-12] are often preferred by practitioners in solid mechanics and structural

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

834 Chapter 19. Partial Differential Equations

engineering; these methods allow considerable freedom in putting computational
elements where you want them, important when dealing with highly irregular geome-
tries. Spectral methods [13-15] are preferred for very regular geometries and smooth
functions; they converge more rapidly than finite-difference methods (cf. §19.4), but
they do not work well for problems with discontinuities.

CITED REFERENCES AND FURTHER READING:

Ames, W.F. 1977, Numerical Methods for Partial Differential Equations, 2nd ed. (New York:
Academic Press). [1]

Richtmyer, R.D., and Morton, K.W. 1967, Difference Methods for Initial Value Problems, 2nd ed.
(New York: Wiley-Interscience). [2]

Roache, P.J. 1976, Computational Fluid Dynamics (Albuquerque: Hermosa). [3]

Mitchell, A.R., and Griffiths, D.F. 1980, The Finite Difference Method in Partial Differential Equa-
tions (New York: Wiley) [includes discussion of finite element methods]. [4]

Dorr, F.W. 1970, SIAM Review, vol. 12, pp. 248-263. [5]

Meijerink, J.A., and van der Vorst, H.A. 1977, Mathematics of Computation, vol. 31, pp. 148—
162. [6]

van der Vorst, H.A. 1981, Journal of Computational Physics, vol. 44, pp. 1-19 [review of sparse
iterative methods]. [7]

Kershaw, D.S. 1970, Journal of Computational Physics, vol. 26, pp. 43-65. [8]

Stone, H.J. 1968, SIAM Journal on Numerical Analysis, vol. 5, pp. 530-558. [9]

Jesshope, C.R. 1979, Computer Physics Communications, vol. 17, pp. 383-391. [10]

Strang, G., and Fix, G. 1973, An Analysis of the Finite Element Method (Englewood Cliffs, Nu:
Prentice-Hall). [11]

Burnett, D.S. 1987, Finite Element Analysis: From Concepts to Applications (Reading, MA:
Addison-Wesley). [12]

Gottlieb, D. and Orszag, S.A. 1977, Numerical Analysis of Spectral Methods: Theory and Appli-
cations (Philadelphia: S.1.A.M.). [13]

Canuto, C., Hussaini, M.Y., Quarteroni, A., and Zang, T.A. 1988, Spectral Methods in Fluid
Dynamics (New York: Springer-Verlag). [14]

Boyd, J.P. 1989, Chebyshev and Fourier Spectral Methods (New York: Springer-Verlag). [15]

19.1 Flux-Conservative Initial Value Problems

A large class of initial value (time-evolution) PDEs in one space dimension can
be cast into the form of a flux-conservative equation,

ou OF(u)
—=-l 19.1.1
ot Ox ( )
where u and F are vectors, and where (in some cases) F may depend not only on u
but also on spatial derivatives of u. The vector F is called the conserved flux.
For example, the prototypical hyperbolic equation, the one-dimensional wave
equation with constant velocity of propagation v

Ou 20°u
;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 835

can be rewritten as a set of two first-order equations

Or Os
a Ox
Os or
ae Dp (19.1.3)
where
_ Ou
r=un
Ou
s= 19.1.4
8 at (19.1.4)

In this case r and s become the two components of u, and the flux is given by
the linear matrix relation

0 -v
F(u) = (°, 0 ) -u (19.1.5)
(The physicist-reader may recognize equations (19.1.3) as analogous to Maxwell’s
equations for one-dimensional propagation of electromagnetic waves.)

We will consider, in this section, a prototypical example of the general flux-
conservative equation (19.1.1), namely the equation for a scalar u,

Ou Ou

on (19.1.6)

with v a constant. As it happens, we already know analytically that the general
solution of this equation is a wave propagating in the positive x-direction,

u= f(x —vt) (19.1.7)

where f is an arbitrary function. However, the numerical strategies that we develop
will be equally applicable to the more general equations represented by (19.1.1). In
some contexts, equation (19.1.6) is called an advective equation, because the quantity
u is transported by a “fluid flow” with a velocity v.

How do we go about finite differencing equation (19.1.6) (or, analogously,
19.1.1)? The straightforward approach is to choose equally spaced points along both
the t- and x-axes. Thus denote

a; =% + jAa, j=0,1,..., J

19.1.8
tn =to + nAt, n=0,1,...,N ( )

Let wu? denote u(tn,2;). We have several choices for representing the time
derivative term. The obvious way is to set

au unt -u?

jan
This is called forward Euler differencing (cf. equation 16.1.1). While forward Euler
is only first-order accurate in At, it has the advantage that one is able to calculate

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

836 Chapter 19. Partial Differential Equations

FTCS
torn

Z

xorj

Figure 19.1.1. Representation of the Forward Time Centered Space (FTCS) differencing scheme. In this
and subsequent figures, the open circle is the new point at which the solution is desired; filled circles are
known points whose function values are used in calculating the new point; the solid lines connect points
that are used to calculate spatial derivatives; the dashed lines connect points that are used to calculate time
derivatives. The FTCS scheme is generally unstable for hyperbolic problems and cannot usually be used.

quantities at timestep n + 1 in terms of only quantities known at timestep n. For the
space derivative, we can use a second-order representation still using only quantities
known at timestep n:

ue

J* + O(Ax?) (19.1.10)

: no
Ou _ Ua

2Ax

jun

The resulting finite-difference approximation to equation (19.1.6) is called the FTCS
representation (Forward Time Centered Space),

unth yn um, — un
j i= o( i =) (19.1.11)

At 2Ax

which can easily be rearranged to be a formula for uyh in terms of the other

quantities. The FTCS scheme is illustrated in Figure 19.1.1. It’s a fine example of
an algorithm that is easy to derive, takes little storage, and executes quickly. Too
bad it doesn’t work! (See below.)

The FTCS representation is an explicit scheme. This means that uy for each
j can be calculated explicitly from the quantities that are already known. Later we
shall meet implicit schemes, which require us to solve implicit equations coupling
the ut for various 7. (Explicit and implicit methods for ordinary differential
equations were discussed in §16.6.) The FTCS algorithm is also an example of
a single-level scheme, since only values at time level n have to be stored to find
values at time level n + 1.

von Neumann Stability Analysis

Unfortunately, equation (19.1.11) is of very limited usefulness. It is an unstable
method, which can be used only (if at all) to study waves for a short fraction of one
oscillation period. To find alternative methods with more general applicability, we
must introduce the von Neumann stability analysis.

The von Neumann analysis is local: We imagine that the coefficients of the
difference equations are so slowly varying as to be considered constant in space
and time. In that case, the independent solutions, or eigenmodes, of the difference
equations are all of the form

u” _ Enetkide (19.1.12)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 837

torn “7 a

xorj

Figure 19.1.2. Representation of the Lax differencing scheme, as in the previous figure. The stability
criterion for this scheme is the Courant condition.

where k is a real spatial wave number (which can have any value) and € = £(k) is
a complex number that depends on k. The key fact is that the time dependence of
a single eigenmode is nothing more than successive integer powers of the complex
number €. Therefore, the difference equations are unstable (have exponentially
growing modes) if |€(k)| > 1 for some k. The number € is called the amplification
factor at a given wave number k.

To find €(k), we simply substitute (19.1.12) back into (19.1.11). Dividing
by €", we get

é(k) =1- ee sin kAz (19.1.13)
we

whose modulus is > 1 for all k; so the FTCS scheme is unconditionally unstable.

If the velocity v were a function of t and «:, then we would write v? in equation
(19.1.11). In the von Neumann stability analysis we would still treat v as a constant,
the idea being that for v slowly varying the analysis is local. In fact, even in the
case of strictly constant v, the von Neumann analysis does not rigorously treat the
end effects at 7 = 0 and j = N.

More generally, if the equation’s right-hand side were nonlinear in uw, then a
von Neumann analysis would linearize by writing wu = wo + du, expanding to linear
order in du. Assuming that the uo quantities already satisfy the difference equation
exactly, the analysis would look for an unstable eigenmode of du.

Despite its lack of rigor, the von Neumann method generally gives valid answers
and is much easier to apply than more careful methods. We accordingly adopt it
exclusively. (See, for example, [1] for a discussion of other methods of stability
analysis.)

Lax Method

The instability in the FTCS method can be cured by a simple change due to Lax.
One replaces the term wu’? in the time derivative term by its average (Figure 19.1.2):

1
uy 5 (ut, +u_,) (19.1.14)

This turns (19.1.11) into

Lon vAt

1
41
" 5 (uty tut) - aAg (uj, — ut_1) (19.1.15)

J

Uu

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

838 Chapter 19. Partial Differential Equations

stable unstable

torn

xorj

(a) (b)

Figure 19.1.3. Courant condition for stability of a differencing scheme. The solution of a hyperbolic
problem at a point depends on information within some domain of dependency to the past, shown here
shaded. The differencing scheme (19.1.15) has its own domain of dependency determined by the choice
of points on one time slice (shown as connected solid dots) whose values are used in determining a new
point (shown connected by dashed lines). A differencing scheme is Courant stable if the differencing
domain of dependency is larger than that of the PDEs, as in (a), and unstable if the relationship is the
reverse, as in (b). For more complicated differencing schemes, the domain of dependency might not be
determined simply by the outermost points.

Substituting equation (19.1.12), we find for the amplification factor

€ = coskr — 12S sin dn (19.1.16)
x

The stability condition |€|? < 1 leads to the requirement

U

At
<

Ag 1 (19.1.17)

This is the famous Courant-Friedrichs-Lewy stability criterion, often
called simply the Courant condition. Intuitively, the stability condition can be
understood as follows (Figure 19.1.3): The quantity u an in equation (19.1.15) is
computed from information at points 7 — 1 and j + 1 at time n. In other words,
xj—1 and aj, are the boundaries of the spatial region that is allowed to communicate
information to unt Now recall that in the continuum wave equation, information
actually propagates with a maximum velocity v. If the point unt? is outside of
the shaded region in Figure 19.1.3, then it requires information from points more
distant than the differencing scheme allows. Lack of that information gives rise to
an instability. Therefore, At cannot be made too large.

The surprising result, that the simple replacement (19.1.14) stabilizes the FTCS
scheme, is our first encounter with the fact that differencing PDEs is an art as much
as a science. To see if we can demystify the art somewhat, let us compare the FTCS
and Lax schemes by rewriting equation (19.1.15) so that it is in the form of equation
(19.1.11) with a remainder term:

unt sup - (4 - 1) wi (= — 2uh + “) (19.1.18)
At : 2Ax At “

But this is exactly the FTCS representation of the equation

du _ Ow (Aa)?
ot "Oe "DAE

2

Vu (19.1.19)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 839

where V? = 07/02? in one dimension. We have, in effect, added a diffusion term to
the equation, or, if you recall the form of the Navier-Stokes equation for viscous fluid
flow, a dissipative term. The Lax scheme is thus said to have numerical dissipation,
or numerical viscosity. We can see this also in the amplification factor. Unless |v|At
is exactly equal to Az, |€| < 1 and the amplitude of the wave decreases spuriously.
Isn’t a spurious decrease as bad as a spurious increase? No. The scales that we
hope to study accurately are those that encompass many grid points, so that they have
kAax <1. (The spatial wave number k is defined by equation 19.1.12.) For these
scales, the amplification factor can be seen to be very close to one, in both the stable
and unstable schemes. The stable and unstable schemes are therefore about equally
accurate. For the unstable scheme, however, short scales with kAz ~ 1, which we
are not interested in, will blow up and swamp the interesting part of the solution.
Much better to have a stable scheme in which these short wavelengths die away
innocuously. Both the stable and the unstable schemes are inaccurate for these short
wavelengths, but the inaccuracy is of a tolerable character when the scheme is stable.
When the independent variable u is a vector, then the von Neumann analysis
is slightly more complicated. For example, we can consider equation (19.1.3),

rewritten as
(a) O fu:
a :] =a [* (19.1.20)

The Lax method for this equation is

1 vAt
PS gia tt RSF)
* 19.1.21
n+l 1 m +84) 4 vAt ( n no)
83 gi 7 85-1) © ong i TTA

The von Neumann stability analysis now proceeds by assuming that the eigenmode
is of the following (vector) form,

ry n pikjda ro
J

Here the vector on the right-hand side is a constant (both in space and in time)
eigenvector, and € is a complex number, as before. Substituting (19.1.22) into
(19.1.21), and dividing by the power €”, gives the homogeneous vector equation

(coskAx) — € vat sinkAx r? 0
Az . = 19.1.23
vAt (19.1.23)
Gy sinkAx (cos kAx) — € 3° 0
x

This admits a solution only if the determinant of the matrix on the left vanishes, a
condition easily shown to yield the two roots €
vAt

€ =coskAx + ae sinkAx (19.1.24)

The stability condition is that both roots satisfy |f| < 1. This again turns out to be
simply the Courant condition (19.1.17).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

840 Chapter 19. Partial Differential Equations

Other Varieties of Error

Thus far we have been concerned with amplitude error, because of its intimate
connection with the stability or instability of a differencing scheme. Other varieties
of error are relevant when we shift our concern to accuracy, rather than stability.

Finite-difference schemes for hyperbolic equations can exhibit dispersion, or
phase errors. For example, equation (19.1.16) can be rewritten as

vAt
Ax

Exe thr 4 5 (1 - ) sinkAa (19.1.25)

An arbitrary initial wave packet is a superposition of modes with different k’s.
At each timestep the modes get multiplied by different phase factors (19.1.25),
depending on their value of k. If At = Az/v, then the exact solution for each mode
of a wave packet f(x — vt) is obtained if each mode gets multiplied by exp(—ikAz).
For this value of At, equation (19.1.25) shows that the finite-difference solution
gives the exact analytic result. However, if vAt/Az is not exactly 1, the phase
relations of the modes can become hopelessly garbled and the wave packet disperses.
Note from (19.1.25) that the dispersion becomes large as soon as the wavelength
becomes comparable to the grid spacing Az.

A third type of error is one associated with nonlinear hyperbolic equations and
is therefore sometimes called nonlinear instability. For example, a piece of the Euler
or Navier-Stokes equations for fluid flow looks like

Ov Ov

R= TYant (19.1.26)

The nonlinear term in v can cause a transfer of energy in Fourier space from
long wavelengths to short wavelengths. This results in a wave profile steepening
until a vertical profile or “shock” develops. Since the von Neumann analysis
suggests that the stability can depend on kKAx, a scheme that was stable for shallow
profiles can become unstable for steep profiles. This kind of difficulty arises in
a differencing scheme where the cascade in Fourier space is halted at the shortest
wavelength representable on the grid, that is, atk ~ 1/Aa. If energy simply
accumulates in these modes, it eventually swamps the energy in the long wavelength
modes of interest.

Nonlinear instability and shock formation is thus somewhat controlled by
numerical viscosity such as that discussed in connection with equation (19.1.18)
above. In some fluid problems, however, shock formation is not merely an annoyance,
but an actual physical behavior of the fluid whose detailed study is a goal. Then,
numerical viscosity alone may not be adequate or sufficiently controllable. This is a
complicated subject which we discuss further in the subsection on fluid dynamics,
below.

For wave equations, propagation errors (amplitude or phase) are usually most
worrisome. For advective equations, on the other hand, transport errors are usually
of greater concern. In the Lax scheme, equation (19.1.15), a disturbance in the
advected quantity u at mesh point j propagates to mesh points 7 + 1 and j — 1 at
the next timestep. In reality, however, if the velocity v is positive then only mesh
point 7 + 1 should be affected.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 841

ere)

upwind

torn

<

a: .

xorj

Figure 19.1.4. Representation of upwind differencing schemes. The upper scheme is stable when the
advection constant v is negative, as shown; the lower scheme is stable when the advection constant v is
positive, also as shown. The Courant condition must, of course, also be satisfied.

The simplest way to model the transport properties “better” is to use upwind
differencing (see Figure 19.1.4):

J jot n
unt _ u? 7 , “7 vf > 0 5
—~r = —UF Y yr — yr (19.1.27)
t hm 0
Av 7 7

Note that this scheme is only first-order, not second-order, accurate in the
calculation of the spatial derivatives. How can it be “better”? The answer is
one that annoys the mathematicians: The goal of numerical simulations is not
always “accuracy” in a strictly mathematical sense, but sometimes “fidelity” to the
underlying physics in a sense that is looser and more pragmatic. In such contexts,
some kinds of error are much more tolerable than others. Upwind differencing
generally adds fidelity to problems where the advected variables are liable to undergo
sudden changes of state, e.g., as they pass through shocks or other discontinuities.
You will have to be guided by the specific nature of your own problem.

For the differencing scheme (19.1.27), the amplification factor (for constant v) is

€=1- vat (1— cos kAz) — 1 sin par (19.1.28)
Ag
(1 - at ) (1 — cos kAc) (19.1.29)
a

So the stability criterion |€|? < 1 is (again) simply the Courant condition (19.1.17).
There are various ways of improving the accuracy of first-order upwind
differencing. In the continuum equation, material originally a distance vAt away

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

842 Chapter 19. Partial Differential Equations

Sree)

:

torn

staggered
leapfrog

e@--------

xorj

Figure 19.1.5. Representation of the staggered leapfrog differencing scheme. Note that information
from two previous time slices is used in obtaining the desired point. This scheme is second-order
accurate in both space and time.

arrives at a given point after a time interval At. In the first-order method, the
material always arrives from Ax away. If vAt < Az (to insure accuracy), this can
cause a large error. One way of reducing this error is to interpolate u between j — 1
and j before transporting it. This gives effectively a second-order method. Various
schemes for second-order upwind differencing are discussed and compared in [2-3].

Second-Order Accuracy in Time

When using a method that is first-order accurate in time but second-order
accurate in space, one generally has to take vAt significantly smaller than Az to
achieve desired accuracy, say, by at least a factor of 5. Thus the Courant condition
is not actually the limiting factor with such schemes in practice. However, there are
schemes that are second-order accurate in both space and time, and these can often be
pushed right to their stability limit, with correspondingly smaller computation times.

For example, the staggered leapfrog method for the conservation equation
(19.1.1) is defined as follows (Figure 19.1.5): Using the values of wu” at time t”,
compute the fluxes F’. Then compute new values u”*+1 using the time-centered
values of the fluxes:

At
ntl unt _ -i,! Hn _ Fry) (19.1.30)

j
The name comes from the fact that the time levels in the time derivative term
“leapfrog” over the time levels in the space derivative term. The method requires
that u”~1 and u” be stored to compute u”*?.

For our simple model equation (19.1.6), staggered leapfrog takes the form

n+1 n-1_ _vAt) » n
up uy = ay ie —ul_y) (19.1.31)
The von Neumann stability analysis now gives a quadratic equation for €, rather than
a linear one, because of the occurrence of three consecutive powers of € when the

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 843

form (19.1.12) for an eigenmode is substituted into equation (19.1.31),
; pAt
@—1=—21¢"* sinkAr (19.1.32)
Ax
whose solution is

vAt
€= i sinkAg +
oa

(19.1.33)

Thus the Courant condition is again required for stability. In fact, in equation
(19.1.33), |€|? = 1 for any vAt < Ax. This is the great advantage of the staggered
leapfrog method: There is no amplitude dissipation.

Staggered leapfrog differencing of equations like (19.1.20) is most transparent
if the variables are centered on appropriate half-mesh points:

n nan
re =v Ou v oe
TH?" dal, Ax
j+1/2 34
9, jntl/2 n+1 _ on (19.1.34)
nt1/2 __ Ow Uj uy
t dt |; At

This is purely a notational convenience: we can think of the mesh on which r and
s are defined as being twice as fine as the mesh on which the original variable u is
defined. The leapfrog differencing of equation (19.1.20) is

ntl _ooan n4+1/2 n+1/2
Miti/2— Nj+1/2 _ Sj+1 7 $5
At At (19.1.35)
n+1/2 n—1/2 pn pn
$5 7 85 py stile = Ni=1/2
At : Ax

If you substitute equation (19.1.22) in equation (19.1.35), you will find that once
again the Courant condition is required for stability, and that there is no amplitude
dissipation when it is satisfied.

If we substitute equation (19.1.34) in equation (19.1.35), we find that equation
(19.1.35) is equivalent to

ntl _ on 4 yn-b n on n
uy Quy buy guy — Quip + uly

(At? . (Az?

(19.1.36)

This is just the “usual” second-order differencing of the wave equation (19.1.2). We
see that it is a two-level scheme, requiring both uw” and u”~! to obtain u”*!. In
equation (19.1.35) this shows up as both s”~!/? and r” being needed to advance
the solution.

For equations more complicated than our simple model equation, especially
nonlinear equations, the leapfrog method usually becomes unstable when the gradi-
ents get large. The instability is related to the fact that odd and even mesh points are
completely decoupled, like the black and white squares of a chess board, as shown

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

844 Chapter 19. Partial Differential Equations

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

Figure 19.1.6. Origin of mesh-drift instabilities in a staggered leapfrog scheme. If the mesh points
are imagined to lie in the squares of a chess board, then white squares couple to themselves, black to
themselves, but there is no coupling between white and black. The fix is to introduce a small diffusive
mesh-coupling piece.

in Figure 19.1.6. This mesh drifting instability is cured by coupling the two meshes
through a numerical viscosity term, e.g., adding to the right side of (19.1.31) a small
coefficient (< 1) times why - Quy + wpa. For more on stabilizing difference
schemes by adding numerical dissipation, see, e.g., [4].

The Two-Step Lax-Wendroff scheme is a second-order in time method that
avoids large numerical dissipation and mesh drifting. One defines intermediate
values uj41/2 at the half timesteps ¢,,1/2 and the half mesh points xj 41/2. These
are calculated by the Lax scheme:

n+1/2 Li r

uty = =5 (ujsy + uz) — Fj, — Fj) (19.1.37)
Using these variables, one calculates the fluxes F’ wae Then the updated values

ut are calculated by the properly centered expression

At
41 n+1/2 n+1/2
wet sup (BIL EL) (19.1.38)
n+1/2

The provisional values u’ j+1/2_ are now discarded. (See Figure 19.1.7.)
Let us investigate the stability of this method for our model advective equation,
where F' = vu. Substitute (19.1.37) in (19.1.38) to get

1 1
ut =u} -a [sews +uj)— yulujea — ul)
(19.1.39)

1 1
~ 9 (4; +ul_y)+ gulu; —uj_y)

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.1 Flux-Conservative Initial Value Problems 845

two-step Lax Wendroff

halfstep points YooooN~\
1

Pas

te)

torn °

xorj

Figure 19.1.7. Representation of the two-step Lax-Wendroff differencing scheme. Two halfstep points
(@) are calculated by the Lax method. These, plus one of the original points, produce the new point via
staggered leapfrog. Halfstep points are used only temporarily and do not require storage allocation on the
grid. This scheme is second-order accurate in both space and time.

where
_ vAt
asi (19.1.40)
Then
€=1-iasinkAr — a?(1— coskAz) (19.1.41)
so
\€? =1—a2(1 — a?)(1 — coskAx)? (19.1.42)

The stability criterion |€|? < 1 is therefore a? < 1, orvAt < Az as usual.
Incidentally, you should not think that the Courant condition is the only stability
requirement that ever turns up in PDEs. It keeps doing so in our model examples
just because those examples are so simple in form. The method of analysis is,
however, general.

Except when a = 1, |€|? < 1 in (19.1.42), so some amplitude damping does
occur. The effect is relatively small, however, for wavelengths large compared with
the mesh size Ax. If we expand (19.1.42) for small kAx, we find

(kAx)!
4

|e? = 1-—a7(1 — a?) be (19.1.43)

The departure from unity occurs only at fourth order in k. This should be contrasted
with equation (19.1.16) for the Lax method, which shows that

le? =1-(1—a?)(kAz)? +... (19.1.44)

for small kA.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

846 Chapter 19. Partial Differential Equations

In summary, our recommendation for initial value problems that can be cast in
flux-conservative form, and especially problems related to the wave equation, is to use
the staggered leapfrog method when possible. We have personally had better success
with it than with the Two-Step Lax-Wendroff method. For problems sensitive to
transport errors, upwind differencing or one of its refinements should be considered.

Fluid Dynamics with Shocks

As we alluded to earlier, the treatment of fluid dynamics problems with shocks
has become a very complicated and very sophisticated subject. All we can attempt
to do here is to guide you to some starting points in the literature.

There are basically three important general methods for handling shocks. The
oldest and simplest method, invented by von Neumann and Richtmyer, is to add
artificial viscosity to the equations, modeling the way Nature uses real viscosity
to smooth discontinuities. A good starting point for trying out this method is the
differencing scheme in §12.11 of [1]. This scheme is excellent for nearly all problems
in one spatial dimension.

The second method combines a high-order differencing scheme that is accurate
for smooth flows with a low order scheme that is very dissipative and can smooth
the shocks. Typically, various upwind differencing schemes are combined using
weights chosen to zero the low order scheme unless steep gradients are present, and
also chosen to enforce various “monotonicity” constraints that prevent nonphysical
oscillations from appearing in the numerical solution. References [2-3,5] are a good
place to start with these methods.

The third, and potentially most powerful method, is Godunov’s approach. Here
one gives up the simple linearization inherent in finite differencing based on Taylor
series and includes the nonlinearity of the equations explicitly. There is an analytic
solution for the evolution of two uniform states of a fluid separated by a discontinuity,
the Riemann shock problem. Godunov’s idea was to approximate the fluid by a
large number of cells of uniform states, and piece them together using the Riemann
solution. There have been many generalizations of Godunov’s approach, of which
the most powerful is probably the PPM method [6].

Readable reviews of all these methods, discussing the difficulties arising when
one-dimensional methods are generalized to multidimensions, are given in [7-9].

CITED REFERENCES AND FURTHER READING:

Ames, W.F. 1977, Numerical Methods for Partial Differential Equations, 2nd ed. (New York:
Academic Press), Chapter 4.

Richtmyer, R.D., and Morton, K.W. 1967, Difference Methods for Initial Value Problems, 2nd ed.
(New York: Wiley-Interscience). [1]

Centrella, J., and Wilson, J.R. 1984, Astrophysical Journal Supplement, vol. 54, pp. 229-249,
Appendix B. [2]

Hawley, J.F., Smarr, L.L., and Wilson, J.R. 1984, Astrophysical Journal Supplement, vol. 55,
pp. 211-246, §2c. [3]

Kreiss, H.-O. 1978, Numerical Methods for Solving Time-Dependent Problems for Partial Differ-
ential Equations (Montreal: University of Montreal Press), pp. 66ff. [4]

Harten, A., Lax, P.D., and Van Leer, B. 1983, SIAM Review, vol. 25, pp. 36-61. [5]

Woodward, P., and Colella, P. 1984, Journal of Computational Physics, vol. 54, pp. 174-201. [6]

Roache, P.J. 1976, Computational Fluid Dynamics (Albuquerque: Hermosa). [7]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

e)

“aleEMYOS Sedioay |eEouEWNN Aq Z66 1-886} (O) UBUAdOD sweiGo1g “ssl AsionluN eBpuquied Aq 2661-886 (

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.2 Diffusive Initial Value Problems 847

Woodward, P., and Colella, P. 1984, Journal of Computational Physics, vol. 54, pp. 115-173. [8]
Rizzi, A., and Engquist, B. 1987, Journal of Computational Physics, vol. 72, pp. 1-69. [9]

19.2 Diffusive Initial Value Problems

Recall the model parabolic equation, the diffusion equation in one space

dimension,
Ou (a) Ou
Ot On (052) (19.2.1)

where D is the diffusion coefficient. Actually, this equation is a flux-conservative
equation of the form considered in the previous section, with

Pe _ pdt

an (19.2.2)
the flux in the x-direction. We will assume D > 0, otherwise equation (19.2.1) has
physically unstable solutions: A small disturbance evolves to become more and more
concentrated instead of dispersing. (Don’t make the mistake of trying to find a stable
differencing scheme for a problem whose underlying PDEs are themselves unstable!)

Even though (19.2.1) is of the form already considered, it is useful to consider
it as a model in its own right. The particular form of flux (19.2.2), and its direct
generalizations, occur quite frequently in practice. Moreover, we have already seen
that numerical viscosity and artificial viscosity can introduce diffusive pieces like
the right-hand side of (19.2.1) in many other situations.

Consider first the case when D is a constant. Then the equation

Ou Pu
a Dyn (19.2.3)

can be differenced in the obvious way:
unt —uP D Up, — Quit + uly (19.24)

This is the FTCS scheme again, except that it is a second derivative that has been
differenced on the right-hand side. But this makes a world of difference! The FTCS
scheme was unstable for the hyperbolic equation; however, a quick calculation shows
that the amplification factor for equation (19.2.4) is

ADAt . 4 (kAw
ga 1 OR a (S*) (19.2.5)

The requirement |é| < 1 leads to the stability criterion

2DAt
(AzP

<1 (19.2.6)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

848 Chapter 19. Partial Differential Equations

The physical interpretation of the restriction (19.2.6) is that the maximum
allowed timestep is, up to a numerical factor, the diffusion time across a cell of
width Az.

More generally, the diffusion time 7 across a spatial scale of size is of order

2

TS (19.2.7)
Usually we are interested in modeling accurately the evolution of features with
spatial scales \ >> Ax. If we are limited to timesteps satisfying (19.2.6), we will
need to evolve through of order \?/(Azx)? steps before things start to happen on the
scale of interest. This number of steps is usually prohibitive. We must therefore
find a stable way of taking timesteps comparable to, or perhaps — for accuracy —
somewhat smaller than, the time scale of (19.2.7).

This goal poses an immediate “philosophical” question. Obviously the large
timesteps that we propose to take are going to be woefully inaccurate for the small
scales that we have decided not to be interested in. We want those scales to do
something stable, “innocuous,” and perhaps not too physically unreasonable. We
want to build this innocuous behavior into our differencing scheme. What should
it be?

There are two different answers, each of which has its pros and cons. The
first answer is to seek a differencing scheme that drives small-scale features to their
equilibrium forms, e.g., satisfying equation (19.2.3) with the left-hand side set to
zero. This answer generally makes the best physical sense; but, as we will see, it leads
to a differencing scheme (“‘fully implicit”) that is only first-order accurate in time for
the scales that we are interested in. The second answer is to let small-scale features
maintain their initial amplitudes, so that the evolution of the larger-scale features
of interest takes place superposed with a kind of “frozen in” (though fluctuating)
background of small-scale stuff. This answer gives a differencing scheme (“Crank-
Nicolson’) that is second-order accurate in time. Toward the end of an evolution
calculation, however, one might want to switch over to some steps of the other kind,
to drive the small-scale stuff into equilibrium. Let us now see where these distinct
differencing schemes come from:

Consider the following differencing of (19.2.3),

un nt Qu qyntt

j+1 -1
Al /=pD|— (Axp o (19.2.8)

n+l
Uz

This is exactly like the FTCS scheme (19.2.4), except that the spatial derivatives on
the right-hand side are evaluated at timestep n + 1. Schemes with this character are
called fully implicit or backward time, by contrast with FTCS (which is called fully
explicit). To solve equation (19.2.8) one has to solve a set of simultaneous linear
equations at each timestep for the u oe . Fortunately, this is a simple problem because
the system is tridiagonal: Just group the terms in equation (19.2.8) appropriately:

n+1 n+l ntl _ 4m
saul) + (1+ 2a)jur — aul) = uj,

j=1,2..5-1 (19.2.9)
where

DAt
(Az?

a

(19.2.10)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.2 Diffusive Initial Value Problems 849

Supplemented by Dirichlet or Neumann boundary conditions at 7 = 0 and j = J,
equation (19.2.9) is clearly a tridiagonal system, which can easily be solved at each
timestep by the method of §2.4.

What is the behavior of (19.2.8) for very large timesteps? The answer is seen
most clearly in (19.2.9), in the limit a — oo (At — oo). Dividing by a, we see that
the difference equations are just the finite-difference form of the equilibrium equation

2
a =0 (19.2.1)
iC

What about stability? The amplification factor for equation (19.2.8) is

1
_ 9 (kAa
1+ 4a sin >

Clearly |€| < 1 for any stepsize At. The scheme is unconditionally stable. The details
of the small-scale evolution from the initial conditions are obviously inaccurate for
large At. But, as advertised, the correct equilibrium solution is obtained. This is
the characteristic feature of implicit methods.

Here, on the other hand, is how one gets to the second of our above philosophical
answers, combining the stability of an implicit method with the accuracy of a method
that is second-order in both space and time. Simply form the average of the explicit
and implicit FTCS schemes:

f= (19.2.12)

+1 +1 +1 +1
up sub D | (upfy — Quy + ugty) + (uly, — Qu? + ut)
At 2 (Ax)?

(19.2.13)

Here both the left- and right-hand sides are centered at timestep n + 3, so the method
is second-order accurate in time as claimed. The amplification factor is

1 —2asin? (S*)

kA
1+ 2asin? (=)

so the method is stable for any size At. This scheme is called the Crank-Nicolson
scheme, and is our recommended method for any simple diffusion problem (perhaps
supplemented by a few fully implicit steps at the end). (See Figure 19.2.1.)

Now turn to some generalizations of the simple diffusion equation (19.2.3).
Suppose first that the diffusion coefficient D is not constant, say D = D(x). We can
adopt either of two strategies. First, we can make an analytic change of variable

é= (19.2.14)

dx

'= | Da (19.2.15)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

850 Chapter 19. Partial Differential Equations

°
i
f
f
f
f
f
f
f
f
f
f

o——__§____{_{_o—___

torn
FTCS
(a) xorj
—. cei om < c“_, o—. um
' '
1 1
' 1
' 1
1 1
' 1
' 1
1 1
' 1
' 1
e —__—_—_—_e+—_____
(b) Fully Implicit (c) Crank-Nicolson
Figure 19.2.1. | Three differencing schemes for diffusive problems (shown as in Figure 19.1.2). (a)

Forward Time Center Space is first-order accurate, but stable only for sufficiently small timesteps. (b) Fully
Implicit is stable for arbitrarily large timesteps, but is still only first-order accurate. (c) Crank-Nicolson
is second-order accurate, and is usually stable for large timesteps.

Then

Ou (a) Ou
becomes

Ou 1 Ou

= Dwar (19.2.17)

and we evaluate D at the appropriate y;. Heuristically, the stability criterion (19.2.6)

in an explicit scheme becomes.

A 2
At < min ( wy
J | 2D;

(19.2.18)

Note that constant spacing Ay in y does not imply constant spacing in x.
An alternative method that does not require analytically tractable forms for
D is simply to difference equation (19.2.16) as it stands, centering everything
appropriately. Thus the FTCS method becomes
uptt —uP — Dysryjo(utyy — ut) — Dj-aj2(u? — u®_,)
At (Ax)?

(19.2.19)

where

Djsijo = D(aj+1/2) (19.2.20)

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.2 Diffusive Initial Value Problems 851

and the heuristic stability criterion is

2
At < min [5] (19.2.21)
J j+1/2

The Crank-Nicolson method can be generalized similarly.

The second complication one can consider is a nonlinear diffusion problem,
for example where D = D(u). Explicit schemes can be generalized in the obvious
way. For example, in equation (19.2.19) write

[D(ut,,) + D(u")] (19.2.22)

1
Djsij2 = 2

Implicit schemes are not as easy. The replacement (19.2.22) with n — n + 1 leaves
us with a nasty set of coupled nonlinear equations to solve at each timestep. Often
there is an easier way: If the form of D(w) allows us to integrate

dz = D(u)du (19.2.23)

analytically for z(u), then the right-hand side of (19.2.1) becomes 0?z/0x?, which
we difference implicitly as

gmt] _gyntl 4 yntl
a J _ (19.2.24)

(Ax)?
Now linearize each term on the right-hand side of equation (19.2.24), for example

Oz

Ou

gett = 2(ue*) = 2(u?) + (unt? —ul)
jun (19.2.25)

a) + (unt? —ul)D(u7)

Il
SI
i=

This reduces the problem to tridiagonal form again and in practice usually retains
the stability advantages of fully implicit differencing.

Schrédinger Equation

Sometimes the physical problem being solved imposes constraints on the
differencing scheme that we have not yet taken into account. For example, consider
the time-dependent Schrédinger equation of quantum mechanics. This is basically a
parabolic equation for the evolution of a complex quantity 7. For the scattering of a
wavepacket by a one-dimensional potential V (x), the equation has the form

Ow
~ Oa?

+V(a)y (19.2.26)

(Here we have chosen units so that Planck’s constant i = 1 and the particle mass
m = 1/2.) One is given the initial wavepacket, 7) (x,t = 0), together with boundary

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

852 Chapter 19. Partial Differential Equations

conditions that ~ — 0 at x — +oo. Suppose we content ourselves with first-
order accuracy in time, but want to use an implicit scheme, for stability. A slight
generalization of (19.2.8) leads to

yntl ain jntl — o.j,nt1 ntl
F E Vj | __ [ We + Ui j vj (19.2.27)
At (Ax)? ‘ J ~
for which
l 19.2.28
L+i (Ane sin’ 5 + Vj; At

This is unconditionally stable, but unfortunately is not unitary. The underlying
physical problem requires that the total probability of finding the particle somewhere
remains unity. This is represented formally by the modulus-square norm of w
remaining unity:

| \o|?da =1 (19.2.29)

The initial wave function 7 (x, 0) is normalized to satisfy (19.2.29). The Schr 6dinger
equation (19.2.26) then guarantees that this condition is satisfied at all later times.
Let us write equation (19.2.26) in the form

(19.2.30)
where the operator H is
a2
H=—-yat+V(2) (19.2.31)
The formal solution of equation (19.2.30) is
w(x, t) = eu (a, 0) (19.2.32)
where the exponential of the operator is defined by its power series expansion.

The unstable explicit FTCS scheme approximates (19.2.32) as
yet) = (1— iH At)? (19.2.33)

where H is represented by a centered finite-difference approximation in x. The
stable implicit scheme (19.2.27) is, by contrast,

yet? = (1+ iH At) yr (19.2.34)

These are both first-order accurate in time, as can be seen by expanding equation
(19.2.32). However, neither operator in (19.2.33) or (19.2.34) is unitary.

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.3 Initial Value Problems in Multidimensions 853

The correct way to difference Schrédinger’s equation [1,2] is to use Cayley’s

form for the finite-difference representation of e ~*“', which is second-order accurate
and unitary:
1— siHAt
—iHt 3
e ~ 19.2.35
1+ SiH At ( )
In other words,
(L+ jiH At) ye? = (1- SiH At)? (19.2.36)

On replacing H by its finite-difference approximation in 7, we have a complex
tridiagonal system to solve. The method is stable, unitary, and second-order accurate
in space and time. In fact, it is simply the Crank-Nicolson method once again!

CITED REFERENCES AND FURTHER READING:

Ames, W.F. 1977, Numerical Methods for Partial Differential Equations, 2nd ed. (New York:
Academic Press), Chapter 2.

Goldberg, A., Schey, H.M., and Schwartz, J.L. 1967, American Journal of Physics, vol. 35,
pp. 177-186. [1]

Galbraith, |., Ching, Y.S., and Abraham, E. 1984, American Journal of Physics, vol. 52, pp. 60-
68. [2]

19.3 Initial Value Problems in Multidimensions

The methods described in §19.1 and §19.2 for problems in 1 + 1 dimension
(one space and one time dimension) can easily be generalized to N + 1 dimensions.
However, the computing power necessary to solve the resulting equations is enor-
mous. If you have solved a one-dimensional problem with 100 spatial grid points,
solving the two-dimensional version with 100 x 100 mesh points requires at least
100 times as much computing. You generally have to be content with very modest
spatial resolution in multidimensional problems.

Indulge us in offering a bit of advice about the development and testing of
multidimensional PDE codes: You should always first run your programs on very
small grids, e.g., 8 x 8, even though the resulting accuracy is so poor as to be
useless. When your program is all debugged and demonstrably stable, then you can
increase the grid size to a reasonable one and start looking at the results. We have
actually heard someone protest, “my program would be unstable for a crude grid,
but I am sure the instability will go away on a larger grid.” That is nonsense of a
most pernicious sort, evidencing total confusion between accuracy and stability. In
fact, new instabilities sometimes do show up on larger grids; but old instabilities
never (in our experience) just go away.

Forced to live with modest grid sizes, some people recommend going to higher-
order methods in an attempt to improve accuracy. This is very dangerous. Unless the
solution you are looking for is known to be smooth, and the high-order method you

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

854 Chapter 19. Partial Differential Equations

are using is known to be extremely stable, we do not recommend anything higher
than second-order in time (for sets of first-order equations). For spatial differencing,
we recommend the order of the underlying PDEs, perhaps allowing second-order
spatial differencing for first-order-in-space PDEs. When you increase the order of
a differencing method to greater than the order of the original PDEs, you introduce
spurious solutions to the difference equations. This does not create a problem if they
all happen to decay exponentially; otherwise you are going to see all hell break loose!

Lax Method for a Flux-Conservative Equation

As an example, we show how to generalize the Lax method (19.1.15) to two
dimensions for the conservation equation

Ou OF, OF,
V-F=-— f y 19.3.1
Ot ( Ox oy ) (19.3.1)
Use a spatial grid with
tj =totjA
19.3.2
y= yo + lA ( )
We have chosen Az = Ay = A for simplicity. Then the Lax scheme is
uy? gies FU ae Fup + Up 1)
(19.3.3)

At
- aa Pita ~— Fat Fig — FR)

Note that as an abbreviated notation F'j,; and Fj—; refer to Fy, while Fj,, and
F,_; refer to Fy.

Let us carry out a stability analysis for the model advective equation (analog
of 19.1.6) with

Fy, = vpu, Fy = vyu (19.3.4)

This requires an eigenmode with two dimensions in space, though still only a simple
dependence on powers of € in time,

ue = EM etka IA eikylA (19.3.5)

Substituting in equation (19.3.3), we find

1
é= 3 (cos kA + coskyA) — ia, sink,A — iay sinkyA (19.3.6)
where A A
VU, At VyAt
Or = ty = (19.3.7)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.3 Initial Value Problems in Multidimensions 855

The expression for |€|? can be manipulated into the form

1 :
je? = 1 — (sin? kA + sin? kyA) E (a2 + 03)|
: (19.3.8)
- qos k, A — cos kyA)? —(aysinkzA — ay sinkyA)?

The last two terms are negative, and so the stability requirement ||? < 1 becomes

1
5 2 +a) >0 (19.3.9)

or A
At < ——>___ (19.3.10)

~ Jf2(v2 + u2)t/?

This is an example of the general result for the N-dimensional Courant
condition: If |v| is the maximum propagation velocity in the problem, then

U

A
At< 19.3.11
is the Courant condition.
Diffusion Equation in Multidimensions
Let us consider the two-dimensional diffusion equation,

Ou Oru Pu

—=D|(25+35 19.3.12

ot (Ge + an) (193.12)

An explicit method, such as FTCS, can be generalized from the one-dimensional
case in the obvious way. However, we have seen that diffusive problems are usually
best treated implicitly. Suppose we try to implement the Crank-Nicolson scheme in
two dimensions. This would give us

1
Here A
DAt
ee A=Ar=Ay (19.3.14)
pur = Ups — up + Uj_ay (19.3.15)

and similarly for pun. This is certainly a viable scheme; the problem arises in
solving the coupled linear equations. Whereas in one space dimension the system
was tridiagonal, that is no longer true, though the matrix is still very sparse. One
possibility is to use a suitable sparse matrix technique (see §2.7 and §19.0).
Another possibility, which we generally prefer, is a slightly different way of
generalizing the Crank-Nicolson algorithm. It is still second-order accurate in time

and space, and unconditionally stable, but the equations are easier to solve than

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

856 Chapter 19. Partial Differential Equations

(19.3.13). Called the alternating-direction implicit method (ADI), this embodies the
powerful concept of operator splitting or time splitting, about which we will say
more below. Here, the idea is to divide each timestep into two steps of size At/2.
In each substep, a different dimension is treated implicitly:

ut? ay yt 5° (s2u ute Oyu HD)

(19.3.16)

ith weet ly (02 rtd? 4 52, i)
2

jl 5 y 5

The advantage of this method is that each substep requires only the solution of a
simple tridiagonal system.

Operator Splitting Methods Generally

The basic idea of operator splitting, which is also called time splitting or the
method of fractional steps, is this: Suppose you have an initial value equation of
the form

Ou

aa Lu (19.3.17)

where L is some operator. While L is not necessarily linear, suppose that it can at
least be written as a linear sum of m pieces, which act additively on u,

Lu=Lyut+Lout---+Lyu (19.3.18)

Finally, suppose that for each of the pieces, you already know a differencing scheme
for updating the variable u from timestep n to timestep n + 1, valid if that piece
of the operator were the only one on the right-hand side. We will write these
updatings symbolically as

u™t—Uy(u", At)

ut! = Uo(u", At)
(19.3.19)

ut? =U, (u", At)

Now, one form of operator splitting would be to get from n to n + 1 by the
following sequence of updatings:

unt Gl) =U (u", At)

wrt 2/™) = Yy(urtl/m), At)
(19.3.20)

uth ug, (urter—D/m Ag)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.4 Fourier and Cyclic Reduction Methods 857

For example, a combined advective-diffusion equation, such as

Ou Ou Oru
on + Dra (19.3.21)

might profitably use an explicit scheme for the advective term combined with a
Crank-Nicolson or other implicit scheme for the diffusion term.

The alternating-direction implicit (ADI) method, equation (19.3.16), is an
example of operator splitting with a slightly different twist. Let us reinterpret
(19.3.19) to have a different meaning: Let /; now denote an updating method that
includes algebraically all the pieces of the total operator £, but which is desirably
stable only for the £1 piece; likewise U2,...Um. Then a method of getting from
u™ to u"*t is

untt/m — YU, (u", At/m)

ur t2/™ = Yount At/m)
(19.3.2)

wth =U (urtr-D/™ At/m)

The timestep for each fractional step in (19.3.22) is now only 1/m of the full timestep,
because each partial operation acts with all the terms of the original operator.

Equation (19.3.22) is usually, though not always, stable as a differencing scheme
for the operator £. In fact, as a rule of thumb, it is often sufficient to have stable U/ ;’s
only for the operator pieces having the highest number of spatial derivatives — the
other 7/;’s can be unstable — to make the overall scheme stable!

It is at this point that we turn our attention from initial value problems to
boundary value problems. These will occupy us for the remainder of the chapter.

CITED REFERENCES AND FURTHER READING:

Ames, W.F. 1977, Numerical Methods for Partial Differential Equations, 2nd ed. (New York:

Academic Press).

19.4 Fourier and Cyclic Reduction Methods for
Boundary Value Problems

As discussed in §19.0, most boundary value problems (elliptic equations, for
example) reduce to solving large sparse linear systems of the form

A-u=b (19.4.1)

either once, for boundary value equations that are linear, or iteratively, for boundary
value equations that are nonlinear.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

858 Chapter 19. Partial Differential Equations

Two important techniques lead to “rapid” solution of equation (19.4.1) when
the sparse matrix is of certain frequently occurring forms. The Fourier transform
method is directly applicable when the equations have coefficients that are constant
in space. The cyclic reduction method is somewhat more general; its applicability
is related to the question of whether the equations are separable (in the sense of
“separation of variables”). Both methods require the boundaries to coincide with
the coordinate lines. Finally, for some problems, there is a powerful combination
of these two methods called FACR (Fourier Analysis and Cyclic Reduction). We
now consider each method in turn, using equation (19.0.3), with finite-difference
representation (19.0.6), as a model example. Generally speaking, the methods in this
section are faster, when they apply, than the simpler relaxation methods discussed
in §19.5; but they are not necessarily faster than the more complicated multigrid
methods discussed in §19.6.

Fourier Transform Method

The discrete inverse Fourier transform in both x and y is

J-1L-1

1 ~ -2nij ~2nil
uj = IE > Yo Gnne 2aijm/ J —2niln/L (19.4.2)

m=0 n=0

This can be computed using the FFT independently in each dimension, or else all at
once via the routine fourn of §12.4 or the routine rlft3 of §12.5. Similarly,

J-1L-1
1 ~ ij i
P= FE s S7 Binne 29 Fem Prin L (19.4.3)

m=0 n=0

If we substitute expressions (19.4.2) and (19.4.3) in our model problem (19.0.6),
we find

Finn (erm en emim/I 4 2min/L 4 o—2min/L _ 4) =PinnA? (19.4.4)
or

~ Pinnd?
tmn Pn (19.4.5)
2 | cos 2nm +03 2™ -—2
i “OL
Thus the strategy for solving equation (19.0.6) by FFT techniques is:
e Compute fm, as the Fourier transform
J-1L-1
Pmn = > S py 27 d/F e2mint/L (19.4.6)
j=0 1=0

e Compute @» from equation (19.4.5).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.4 Fourier and Cyclic Reduction Methods 859

e Compute uj, by the inverse Fourier transform (19.4.2).
The above procedure is valid for periodic boundary conditions. In other words,
the solution satisfies

Ujl = Uj4l = Uj EL (19.4.7)

Next consider a Dirichlet boundary condition u = 0 on the rectangular boundary.
Instead of the expansion (19.4.2), we now need an expansion in sine waves:

jm L
/ Tne sin gin (19.4.8)

J L

Ujl =

This satisfies the boundary conditions that u = 0 at 7 = 0, J and at 1 = 0, L. If we
substitute this expansion and the analogous one for pj into equation (19.0.6), we
find that the solution procedure parallels that for periodic boundary conditions:

e Compute Pmn by the sine transform

iM

ae
tS

J-1L=1
~ . mln
Pmn = > > Pjl sin 2" ™ sin rT (19.4.9)

j=l l=1

(A fast sine transform algorithm was given in $12.3.)
e Compute @,,,, from the expression analogous to (19.4.5),

on
A* Pn

finn TM a
2 ( 208 [0S — 2)
cos TJ + cos ZL

(19.4.10)

e Compute u,; by the inverse sine transform (19.4.8).

If we have inhomogeneous boundary conditions, for example u = 0 on all
boundaries except u = f(y) on the boundary x = JA, we have to add to the above
solution a solution uw of the homogeneous equation

Pu au

ae t Bp = 9 (19.4.11)

that satisfies the required boundary conditions. In the continuum case, this would
be an expression of the form

ur = So An sinh TA "=A (19.4.12)

where A,, would be found by requiring that u = f(y) at x = JA. In the discrete
case, we have

1 .
1
= z So An sinh sin (19.4.13)
n=1

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

860 Chapter 19. Partial Differential Equations

If f(y = IA) = fi, then we get A,, from the inverse formula

L-1
1 antl
si 19.4.14
n* sinh (wnJ/L) > fisin ( )
The complete solution to the problem is
us uy t+ uit (19.4.15)

By adding appropriate terms of the form (19.4.12), we can handle inhomogeneous
terms on any boundary surface.

A much simpler procedure for handling inhomogeneous terms is to note that
whenever boundary terms appear on the left-hand side of (19.0.6), they can be taken
over to the right-hand side since they are known. The effective source term is
therefore pj; plus a contribution from the boundary terms. To implement this idea
formally, write the solution as

u=u +u? (19.4.16)

where u’ = 0 on the boundary, while wu? vanishes everywhere except on the

boundary. There it takes on the given boundary value. In the above example, the
only nonzero values of u? would be

uh = hi (19.4.17)
The model equation (19.0.3) becomes
V?ul = -V?u? + p (19.4.18)
or, in finite-difference form,

Usa + Uy + Ub + U1 - dui) =
(19.4.19)

B B B B B 2
— (Up + Ua + Up F Uji — Aug) + A pj,

All the w® terms in equation (19.4.19) vanish except when the equation is evaluated
at 7 = J —1, where

wt yo FU ig Fa — Au = Si + A’pj-i (19.4.20)

Thus the problem is now equivalent to the case of zero boundary conditions, except
that one row of the source term is modified by the replacement

A ps-ip > A’ ps-ia fi (19.4.21)

The case of Neumann boundary conditions Vu = 0 is handled by the cosine
expansion (12.3.17):

a nln
Timn COS

L .
7mm nog Let
Ss 78> (19.4.22)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.4 Fourier and Cyclic Reduction Methods 861

Here the double prime notation means that the terms form = 0 and m = J should
be multiplied by 3, and similarly for n = 0 and n = L. Inhomogeneous terms
Vu = g can be again included by adding a suitable solution of the homogeneous
equation, or more simply by taking boundary terms over to the right-hand side. For
example, the condition

ou = g(y) at x=0 (19.4.23)

becomes
U1, — U-1,1

set =a (19.4.24)

where g; = g(y = IA). Once again we write the solution in the form (19.4.16),
where now Vu’ = 0 on the boundary. This time Vu” takes on the prescribed
value on the boundary, but u? vanishes everywhere except just outside the boundary.
Thus equation (19.4.24) gives

u2, = —2Ag (19.4.25)
All the u® terms in equation (19.4.19) vanish except when j = 0:
uy + wy) + Uo 44 + UO t—4 - dup) = 2Ag + A’ po.1 (19.4.26)

Thus wu’ is the solution of a zero-gradient problem, with the source term modified
by the replacement

A? p01 + Apo. + 2Agi (19.4.27)

Sometimes Neumann boundary conditions are handled by using a staggered
grid, with the u’s defined midway between zone boundaries so that first derivatives
are centered on the mesh points. You can solve such problems using similar
techniques to those described above if you use the alternative form of the cosine
transform, equation (12.3.23).

Cyclic Reduction

Evidently the FFT method works only when the original PDE has constant
coefficients, and boundaries that coincide with the coordinate lines. An alternative
algorithm, which can be used on somewhat more general equations, is called cyclic
reduction (CR).

We illustrate cyclic reduction on the equation

Pu Pu Ou
a2 * Oy Wa, + c(y)u = g(a, y) (19.4.28)

This form arises very often in practice from the Helmholtz or Poisson equations in
polar, cylindrical, or spherical coordinate systems. More general separable equations
are treated in [1].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

862 Chapter 19. Partial Differential Equations

The finite-difference form of equation (19.4.28) can be written as a set of
vector equations

uj-1+T-uj +uj41 =g,h’ (19.4.29)

Here the index 7 comes from differencing in the x-direction, while the y-differencing
(denoted by the index / previously) has been left in vector form. The matrix T
has the form

T=B-21 (19.430)

where the 21 comes from the x-differencing and the matrix B from the y-differencing.
The matrix B, and hence T, is tridiagonal with variable coefficients.

The CR method is derived by writing down three successive equations like
(19.4.29):

uj-2 + T-uj-y t+uj = gid’

ujatT uy tujy1 = gj)? (19.431)

2
uj +T -ujy1 + Uj+2 = 851A

Matrix-multiplying the middle equation by —T and then adding the three equations,
we get

2 $T® «uj + uj 42 = gi A? (19.4.32)
This is an equation of the same form as (19.4.29), with

T® =21-T?
(19.4.33)
1

gi) = = A*(g;_ 177-8; + 8541)

After one level of CR, we have reduced the number of equations by a factor of
two. Since the resulting equations are of the same form as the original equation, we
can repeat the process. Taking the number of mesh points to be a power of 2 for
simplicity, we finally end up with a single equation for the central line of variables:

TY aya = A*gy?, — uo Us (19.4.34)

Here we have moved ug and u, to the right-hand side because they are known
boundary values. Equation (19.4.34) can be solved for u j/2 by the standard
tridiagonal algorithm. The two equations at level f — 1 involve u j/4 and u3;/4. The
equation for u 7/4 involves Uo and u j/2, both of which are known, and hence can be
solved by the usual tridiagonal routine. A similar result holds true at every stage,
so we end up solving J — 1 tridiagonal systems.

In practice, equations (19.4.33) should be rewritten to avoid numerical instabil-
ity. For these and other practical details, refer to [2].

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.5 Relaxation Methods for Boundary Value Problems 863

FACR Method

The best way to solve equations of the form (19.4.28), including the constant
coefficient problem (19.0.3),is a combination of Fourier analysis and cyclic reduction,
the FACR method [3-6]. If at the rth stage of CR we Fourier analyze the equations of
the form (19.4.32) along y, that is, with respect to the suppressed vector index, we
will have a tridiagonal system in the x-direction for each y-Fourier mode:

GE gn + ATE + Ty op = A2g\* (19.4.35)

Here Me”) is the eigenvalue of Tm corresponding to the kth Fourier mode. For

the equation (19.0.3), equation (19.4.5) shows that a will involve terms like
cos(2rk/L) — 2 raised to a power. Solve the tridiagonal systems for 7} at the levels
j = 2",2 x 2",4 x 2",...,J — 2". Fourier synthesize to get the y-values on these
x-lines. Then fill in the intermediate «-lines as in the original CR algorithm.

The trick is to choose the number of levels of CR so as to minimize the total
number of arithmetic operations. One can show that for a typical case of a 128x128
mesh, the optimal level is r = 2; asymptotically, r > log, (log, J).

A rough estimate of running times for these algorithms for equation (19.0.3)
is as follows: The FFT method (in both x and y) and the CR method are roughly
comparable. FACR with r = 0 (that is, FFT in one dimension and solve the
tridiagonal equations by the usual algorithm in the other dimension) gives about a
factor of two gain in speed. The optimal FACR with r = 2 gives another factor
of two gain in speed.

CITED REFERENCES AND FURTHER READING:
Swartzrauber, P.N. 1977, SIAM Review, vol. 19, pp. 490-501. [1]

Buzbee, B.L, Golub, G.H., and Nielson, C.W. 1970, SIAM Journal on Numerical Analysis, vol. 7,
pp. 627-656; see also op. cit. vol. 11, pp. 753-763. [2]

Hockney, R.W. 1965, Journal of the Association for Computing Machinery, vol. 12, pp. 95-113. [3]

Hockney, R.W. 1970, in Methods of Computational Physics, vol. 9 (New York: Academic Press),
pp. 135-211. [4]

Hockney, R.W., and Eastwood, J.W. 1981, Computer Simulation Using Particles (New York:
McGraw-Hill), Chapter 6. [5]

Temperton, C. 1980, Journal of Computational Physics, vol. 34, pp. 314-329. [6]

19.5 Relaxation Methods for Boundary Value
Problems

As we mentioned in §19.0, relaxation methods involve splitting the sparse
matrix that arises from finite differencing and then iterating until a solution is found.

There is another way of thinking about relaxation methods that is somewhat
more physical. Suppose we wish to solve the elliptic equation

Lu=p (19.5.1)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

864 Chapter 19. Partial Differential Equations

where £ represents some elliptic operator and p is the source term. Rewrite the
equation as a diffusion equation,

a Lu-p (19.5.2)
An initial distribution u relaxes to an equilibrium solution as t — oo. This
equilibrium has all time derivatives vanishing. Therefore it is the solution of the
original elliptic problem (19.5.1). We see that all the machinery of §19.2, on diffusive
initial value equations, can be brought to bear on the solution of boundary value
problems by relaxation methods.

Let us apply this idea to our model problem (19.0.3). The diffusion equation is

Ou Pu Pu
a ae t ap (19.5.3)

If we use FTCS differencing (cf. equation 19.2.4), we get

At
ntl n bon n n n n
ul ujat 4 (uta FUP pp Ug Ud — du’) — pjiAt (19.5.4)

Recall from (19.2.6) that FTCS differencing is stable in one spatial dimension only if
At/A? < 3. In two dimensions this becomes At/A? < i Suppose we try to take
the largest possible timestep, and set At = A?/4. Then equation (19.5.4) becomes

1 A?
unyt q (ui FUR yp t Up + Uj 1)- 7 Pal (19.5.5)

Thus the algorithm consists of using the average of u at its four nearest-neighbor
points on the grid (plus the contribution from the source). This procedure is then
iterated until convergence.

This method is in fact a classical method with origins dating back to the
last century, called Jacobi’s method (not to be confused with the Jacobi method
for eigenvalues). The method is not practical because it converges too slowly.
However, it is the basis for understanding the modern methods, which are always
compared with it.

Another classical method is the Gauss-Seidel method, which turns out to be
important in multigrid methods (§19.6). Here we make use of updated values of u on
the right-hand side of (19.5.5) as soon as they become available. In other words, the
averaging is done “in place” instead of being “copied” from an earlier timestep to a
later one. If we are proceeding along the rows, incrementing j for fixed /, we have

1 A?
unyt 7 (wha t ue + UR 4 wt) ~ Pp Pint (19.5.6)
This method is also slowly converging and only of theoretical interest when used by
itself, but some analysis of it will be instructive.

Let us look at the Jacobi and Gauss-Seidel methods in terms of the matrix

splitting concept. We change notation and call u “x,” to conform to standard matrix
notation. To solve

A-x=b (19.5.7)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.5 Relaxation Methods for Boundary Value Problems 865

we can consider splitting A as
A=L+D+U (19.5.8)

where D is the diagonal part of A, L is the lower triangle of A with zeros on the
diagonal, and U is the upper triangle of A with zeros on the diagonal.
In the Jacobi method we write for the rth step of iteration

D- x!) =-(L4U)-x°-) +b (19.5.9)

For our model problem (19.5.5), D is simply the identity matrix. The Jacobi method
converges for matrices A that are “diagonally dominant” in a sense that can be
made mathematically precise. For matrices arising from finite differencing, this
condition is usually met.

What is the rate of convergence of the Jacobi method? A detailed analysis is
beyond our scope, but here is some of the flavor: The matrix —D~! - (L + U) is
the iteration matrix which, apart from an additive term, maps one set of x’s into the
next. The iteration matrix has eigenvalues, each one of which reflects the factor by
which the amplitude of a particular eigenmode of undesired residual is suppressed
during one iteration. Evidently those factors had better all have modulus < 1 for
the relaxation to work at all! The rate of convergence of the method is set by the
rate for the slowest-decaying eigenmode, i.e., the factor with largest modulus. The
modulus of this largest factor, therefore lying between 0 and 1, is called the spectral
radius of the relaxation operator, denoted p..

The number of iterations r required to reduce the overall error by a factor
10” is thus estimated by

i (19.5.10)

In general, the spectral radius p, goes asymptotically to the value 1 as the grid
size J is increased, so that more iterations are required. For any given equation,
grid geometry, and boundary condition, the spectral radius can, in principle, be
computed analytically. For example, for equation (19.5.5) on a J x J grid with
Dirichlet boundary conditions on all four sides, the asymptotic formula for large
J turns out to be

72

a i
Ps 2?

(19.5.11)

The number of iterations r required to reduce the error by a factor of 10 ~” is thus

2pJ?Inl0 1 oy

~ a + - pd 19.5.12

r Zz 5P ( )

In other words, the number of iterations is proportional to the number of mesh points,

J”. Since 100 x 100 and larger problems are common, it is clear that the Jacobi
method is only of theoretical interest.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

866 Chapter 19. Partial Differential Equations

The Gauss-Seidel method, equation (19.5.6), corresponds to the matrix de-
composition

(L+D)-x™ =-u-x" YD +b (19.5.13)

The fact that L is on the left-hand side of the equation follows from the updating
in place, as you can easily check if you write out (19.5.13) in components. One
can show [1-3] that the spectral radius is just the square of the spectral radius of the
Jacobi method. For our model problem, therefore,

72

pei 3 (19.5.14)
2Inl 1
re rns ~ tps? (19.5.15)

The factor of two improvement in the number of iterations over the Jacobi method
still leaves the method impractical.

Successive Overrelaxation (SOR)

We get a better algorithm — one that was the standard algorithm until the 1970s
— if we make an overcorrection to the value of x”) at the rth stage of Gauss-Seidel
iteration, thus anticipating future corrections. Solve (19.5.13) for x"), add and
subtract x("~1) on the right-hand side, and hence write the Gauss-Seidel method as

x) =x") —(L 4D)! - [(L+ D4 U)- x) — b] (19.5.16)
The term in square brackets is just the residual vector €“"—)), so
x) =x") —(L ¢D)yt. e-) (19.5.17)
Now overcorrect, defining
x =x") _a(L4+pD)-t. ee) (19.5.18)

Here w is called the overrelaxation parameter, and the method is called successive
overrelaxation (SOR).

The following theorems can be proved [1-3]:

e The method is convergent only for 0 < w < 2. If0 < w < 1, we speak
of underrelaxation.

e Under certain mathematical restrictions generally satisfied by matrices
arising from finite differencing, only overrelaxation (1 < w < 2) can give
faster convergence than the Gauss-Seidel method.

e If Pyacobi iS the spectral radius of the Jacobi iteration (so that the square
of it is the spectral radius of the Gauss-Seidel iteration), then the optimal
choice for w is given by

2

w = ——_
S$
1+ vi — P3acobi

(19.5.19)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.5 Relaxation Methods for Boundary Value Problems 867

e For this optimal choice, the spectral radius for SOR is

2

PJacobi

psor = | (19.5.20)
(; + Al ~ “—)

As an application of the above results, consider our model problem for which
PJacobi iS given by equation (19.5.11). Then equations (19.5.19) and (19.5.20) give

2

We l+a/J

(19.5.21)
Qn
pson ~1— TF forlarge J (19.5.22)

Equation (19.5.10) gives for the number of iterations to reduce the initial error by
a factor of 107?,

J Ini 1
re pm ~ ppd (19.5.23)

Comparing with equation (19.5.12) or (19.5.15), we see that optimal SOR requires
of order J iterations, as opposed to of order J?. Since J is typically 100 or larger,
this makes a tremendous difference! Equation (19.5.23) leads to the mnemonic
that 3-figure accuracy (p = 3) requires a number of iterations equal to the number
of mesh points along a side of the grid. For 6-figure accuracy, we require about
twice as many iterations.

How do we choose w for a problem for which the answer is not known
analytically? That is just the weak point of SOR! The advantages of SOR obtain
only in a fairly narrow window around the correct value of w. It is better to take w
slightly too large, rather than slightly too small, but best to get it right.

One way to choose w is to map your problem approximately onto a known
problem, replacing the coefficients in the equation by average values. Note, however,
that the known problem must have the same grid size and boundary conditions as the
actual problem. We give for reference purposes the value of p Jacobi for our model
problem on a rectangular J x L grid, allowing for the possibility that Ax #4 Ay:

au Az\? au
cos i + (=) cos L
1 Ar\?
“(3))
Equation (19.5.24) holds for homogeneous Dirichlet or Neumann boundary condi-
tions. For periodic boundary conditions, make the replacement 7 — 27.

A second way, which is especially useful if you plan to solve many similar
elliptic equations each time with slightly different coefficients, is to determine the
optimum value w empirically on the first equation and then use that value for the
remaining equations. Various automated schemes for doing this and for “seeking
out” the best values of w are described in the literature.

While the matrix notation introduced earlier is useful for theoretical analyses,
for practical implementation of the SOR algorithm we need explicit formulas.

(19.5.24)

PJacobi

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

868 Chapter 19. Partial Differential Equations

Consider a general second-order elliptic equation in x and y, finite differenced on
a square as for our model equation. Corresponding to each row of the matrix A
is an equation of the form

a5 Upp + bj Uji + C7,Ug 41 + dj ruj a + 67,105 = fi (19.5.25)

For our model equation, we had a = b = c = d=1,e = —4. The quantity
f is proportional to the source term. The iterative procedure is defined by solving
(19.5.25) for uj:

1,
wi oa (fia — Qjatgj4ie — b5,1Uj-10 — 67,1Uj41 — Gj,1Uj1-1) (9.5.26)
“Js

Then uj is a weighted average

us =wu*s.+ (1 w)urle (19.5.27)

We calculate it as follows: The residual at any stage is

Sj = agua + bj uj-1e + juz iz + dj,uji-1 + e715, — fit (195.28)

and the SOR algorithm (19.5.18) or (19.5.27) is

user = 108 — yy Sit (19.5.29)
5,1

This formulation is very easy to program, and the norm of the residual vector € ;,;
can be used as a criterion for terminating the iteration.

Another practical point concerns the order in which mesh points are processed.
The obvious strategy is simply to proceed in order down the rows (or columns).
Alternatively, suppose we divide the mesh into “odd” and “even” meshes, like the
red and black squares of a checkerboard. Then equation (19.5.26) shows that the
odd points depend only on the even mesh values and vice versa. Accordingly,
we can carry out one half-sweep updating the odd points, say, and then another
half-sweep updating the even points with the new odd values. For the version of
SOR implemented below, we shall adopt odd-even ordering.

The last practical point is that in practice the asymptotic rate of convergence
in SOR is not attained until of order J iterations. The error often grows by a
factor of 20 before convergence sets in. A trivial modification to SOR resolves this
problem. It is based on the observation that, while w is the optimum asymptotic
relaxation parameter, it is not necessarily a good initial choice. In SOR with
Chebyshev acceleration, one uses odd-even ordering and changes w at each half-
sweep according to the following prescription:

wO =]

wl?) — 1/(1— Piacobil 2)

; 19.5.30
WV 1/1 — pean /M), maT 1ynco PP

oo
w) — woptimal

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.5 Relaxation Methods for Boundary Value Problems 869

The beauty of Chebyshev acceleration is that the norm of the error always decreases
with each iteration. (This is the norm of the actual error in uj. The norm of
the residual €;; need not decrease monotonically.) While the asymptotic rate of
convergence is the same as ordinary SOR, there is never any excuse for not using
Chebyshev acceleration to reduce the total number of iterations required.

Here we give a routine for SOR with Chebyshev acceleration.

#include <math.h>
#define MAXITS 1000
#define EPS 1.0e-5

void sor(double **a, double **b, double **c, double **d, double **e,

double **f, double **u, int jmax, double rjac)
Successive overrelaxation solution of equation (19.5.25) with Chebyshev acceleration. a, b, c,
d, e, and f are input as the coefficients of the equation, each dimensioned to the grid size
(1..jmax] [1..jmax]. u is input as the initial guess to the solution, usually zero, and returns
with the final value. rjac is input as the spectral radius of the Jacobi iteration, or an estimate
of it.
t

void nrerror(char error_text[]);

int ipass,j,jsw,1,lsw,n;

double anorm,anormf=0.0,omega=1.0,resid;

Double precision is a good idea for jmax bigger than about 25.

for (j=2;j<jmax; j++)
Compute initial norm of residual and terminate iteration when norm has been reduced by
a factor EPS.
for (1=2;1<jmax;1++)
anormf += fabs(f[j][1]); Assumes initial u is zero.
for (n=1;n<=MAXITS;n++) {
anorm=0.0;
jsw=1;
for (ipass=1;ipass<=2;ipass++) { Odd-even ordering.
lsw=jsw;
for (j=2;j<jmax;jt+) {
for (1=lsw+1;1<jmax;1+=2) {
resid=a[j] [1] *u[j+1] [11]
+b(j] (1] *ufj-1] 1)
t+ce(j] [1] *u[j] (1+1]
+d(j] (1) *uj] 1-1)
te[j] [1] *ufj] 121]
-f£(j101;
anorm += fabs(resid);
ulj] [1] -= omega*resid/e[j] [1];
}
1sw=3-lsw;
3
jsw=3-jsw;
omega=(n == 1 && ipass == 1 ? 1.0/(1.0-0.5#rjactrjac)
1.0/(1.0-0.25*rjac*rjac*omega)) ;
}
if (anorm < EPS*anormf) return;
+
nrerror("MAXITS exceeded") ;

The main advantage of SOR is that it is very easy to program. Its main
disadvantage is that it is still very inefficient on large problems.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

870 Chapter 19. Partial Differential Equations

ADI (Alternating-Direction Implicit) Method

The ADI method of §19.3 for diffusion equations can be turned into a relaxation
method for elliptic equations [1-4]. In §19.3, we discussed ADI as a method for
solving the time-dependent heat-flow equation

Ou

pune (19.5.31)

By letting t — oo one also gets an iterative method for solving the elliptic equation
V-u=p (19.5.32)

In either case, the operator splitting is of the form
L=Ly+Ly (19.5.33)

where £,, represents the differencing in x and CL, that in y.
For example, in our model problem (19.0.6) with Az = Ay = A, we have

Let = Quy — Uysal — Uj—11

(19.5.34)

Lyt = Quy, — Uji4a — Uz,1-1

More complicated operators may be similarly split, but there is some art involved.
A bad choice of splitting can lead to an algorithm that fails to converge. Usually
one tries to base the splitting on the physical nature of the problem. We know for
our model problem that an initial transient diffuses away, and we set up the x and
y splitting to mimic diffusion in each dimension.

Having chosen a splitting, we difference the time-dependent equation (19.5.31)
implicitly in two half-steps:

urtt/2 _ yn Lyurth/2 4 Lyu”
At/2 “ (19.5.35)
urth — yntl/2 Lyurth/2 4 Lyurtt
At/2 1X ~P

(cf. equation 19.3.16). Here we have suppressed the spatial indices (j,/). In matrix
notation, equations (19.5.35) are

(Le + rl)-u"t!/? = (rl —Ly)-u" — A?p (19.5.36)
(Ly +rl)-u"*? = (rl —L,) -u"t!/? — Ap (19.5.37)
where
2A?
= 19.5.38
"= AE (19.538)

The matrices on the left-hand sides of equations (19.5.36) and (19.5.37) are
tridiagonal (and usually positive definite), so the equations can be solved by the

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.6 Multigrid Methods for Boundary Value Problems 871

standard tridiagonal algorithm. Given u”, one solves (19.5.36) foru”*1/2, substitutes
on the right-hand side of (19.5.37), and then solves for u"*+. The key question
is how to choose the iteration parameter r, the analog of a choice of timestep for
an initial value problem.

As usual, the goal is to minimize the spectral radius of the iteration matrix.
Although it is beyond our scope to go into details here, it turns out that, for the
optimal choice of 7, the ADI method has the same rate of convergence as SOR.
The individual iteration steps in the ADI method are much more complicated than
in SOR, so the ADI method would appear to be inferior. This is in fact true if we
choose the same parameter r for every iteration step. However, it is possible to
choose a different r for each step. If this is done optimally, then ADI is generally
more efficient than SOR. We refer you to the literature [1-4] for details.

Our reason for not fully implementing ADI here is that, in most applications,
it has been superseded by the multigrid methods described in the next section. Our
advice is to use SOR for trivial problems (e.g., 20 x 20), or for solving a larger
problem once only, where ease of programming outweighs expense of computer
time. Occasionally, the sparse matrix methods of §2.7 are useful for solving a set
of difference equations directly. For production solution of large elliptic problems,
however, multigrid is now almost always the method of choice.

CITED REFERENCES AND FURTHER READING:

Hockney, R.W., and Eastwood, J.W. 1981, Computer Simulation Using Particles (New York:
McGraw-Hill), Chapter 6.

Young, D.M. 1971, Iterative Solution of Large Linear Systems (New York: Academic Press). [1]

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New York: Springer-Verlag),
§§8.3-8.6. [2]

Varga, R.S. 1962, Matrix Iterative Analysis (Englewood Cliffs, NJ: Prentice-Hall). [3]

Spanier, J. 1967, in Mathematical Methods for Digital Computers, Volume 2 (New York: Wiley),
Chapter 11. [4]

19.6 Multigrid Methods for Boundary Value
Problems

Practical multigrid methods were first introduced in the 1970s by Brandt. These
methods can solve elliptic PDEs discretized on N grid points in O(N) operations.
The “rapid” direct elliptic solvers discussed in §19.4 solve special kinds of elliptic
equations in O(N log N) operations. The numerical coefficients in these estimates
are such that multigrid methods are comparable to the rapid methods in execution
speed. Unlike the rapid methods, however, the multigrid methods can solve general
elliptic equations with nonconstant coefficients with hardly any loss in efficiency.
Even nonlinear equations can be solved with comparable speed.

Unfortunately there is not a single multigrid algorithm that solves all elliptic
problems. Rather there is a multigrid technique that provides the framework for
solving these problems. You have to adjust the various components of the algorithm
within this framework to solve your specific problem. We can only give a brief

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

872 Chapter 19. Partial Differential Equations

introduction to the subject here. In particular, we will give two sample multigrid
routines, one linear and one nonlinear. By following these prototypes and by
perusing the references [1-4], you should be able to develop routines to solve your
own problems.

There are two related, but distinct, approaches to the use of multigrid techniques.
The first, termed “the multigrid method,” is a means for speeding up the convergence
of a traditional relaxation method, as defined by you on a grid of pre-specified
fineness. In this case, you need define your problem (e.g., evaluate its source terms)
only on this grid. Other, coarser, grids defined by the method can be viewed as
temporary computational adjuncts.

The second approach, termed (perhaps confusingly) “the full multigrid (FMG)
method,” requires you to be able to define your problem on grids of various sizes
(generally by discretizing the same underlying PDE into different-sized sets of finite-
difference equations). In this approach, the method obtains successive solutions on
finer and finer grids. You can stop the solution either at a pre-specified fineness, or
you can monitor the truncation error due to the discretization, quitting only when
it is tolerably small.

In this section we will first discuss the “multigrid method,” then use the concepts
developed to introduce the FMG method. The latter algorithm is the one that we
implement in the accompanying programs.

From One-Grid, through Two-Grid, to Multigrid

The key idea of the multigrid method can be understood by considering the
simplest case of a two-grid method. Suppose we are trying to solve the linear
elliptic problem

Lu=f (19.6.1)

where L is some linear elliptic operator and f is the source term. Discretize equation
(19.6.1) on a uniform grid with mesh size h. Write the resulting set of linear
algebraic equations as

Lnun = fr (19.6.2)

Let tp, denote some approximate solution to equation (19.6.2). We will use the
symbol w,;, to denote the exact solution to the difference equations (19.6.2). Then
the error in w,, or the correction is

Un = Un — Un (19.6.3)

The residual or defect is
dn =Lntn — fr (19.6.4)
(Beware: some authors define residual as minus the defect, and there is not universal
agreement about which of these two quantities 19.6.4 defines.) Since £;, is linear,

the error satisfies

Lrvn = —dp, (19.6.5)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 873

At this point we need to make an approximation to Lp, in order to find v;,. The
classical iteration methods, such as Jacobi or Gauss-Seidel, do this by finding, at
each stage, an approximate solution of the equation

Ln = —dy (19.6.6)

where Ln is a “simpler” operator than £;,. For example, Ly is the diagonal part of
Ly, for Jacobi iteration, or the lower triangle for Gauss-Seidel iteration. The next
approximation is generated by

Te” = Tn + Tp (19.6.7)

Now consider, as an alternative, a completely different type of approximation
for Lp, one in which we “coarsify” rather than “simplify.” That is, we form some
appropriate approximation Ly of Lp, on a coarser grid with mesh size H (we will
always take H = 2h, but other choices are possible). The residual equation (19.6.5)
is now approximated by

Lyvy = —dy (19.6.8)

Since £y has smaller dimension, this equation will be easier to solve than equation
(19.6.5). To define the defect d;; on the coarse grid, we need a restriction operator
R that restricts d;, to the coarse grid:

dy = Rd (19.6.9)

The restriction operator is also called the fine-to-coarse operator or the injection
operator. Once we have a solution 7 to equation (19.6.8), we need a prolongation
operator P that prolongates or interpolates the correction to the fine grid:

on = Pou (19.6.10)

The prolongation operator is also called the coarse-to-fine operator or the inter-
polation operator. Both R and P are chosen to be linear operators. Finally the
approximation w;, can be updated:

TRY = tin +O, (19.6.11)

One step of this coarse-grid correction scheme is thus:
Coarse-Grid Correction

Compute the defect on the fine grid from (19.6.4).
Restrict the defect by (19.6.9).

Solve (19.6.8) exactly on the coarse grid for the correction.
Interpolate the correction to the fine grid by (19.6.10).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

874 Chapter 19. Partial Differential Equations

e Compute the next approximation by (19.6.11).

Let’s contrast the advantages and disadvantages of relaxation and the coarse-grid
correction scheme. Consider the error v;, expanded into a discrete Fourier series. Call
the components in the lower half of the frequency spectrum the smooth components
and the high-frequency components the nonsmooth components. We have seen that
relaxation becomes very slowly convergent in the limit h — 0, i.e., when there are a
large number of mesh points. The reason turns out to be that the smooth components
are only slightly reduced in amplitude on each iteration. However, many relaxation
methods reduce the amplitude of the nonsmooth components by large factors on
each iteration: They are good smoothing operators.

For the two-grid iteration, on the other hand, components of the error with
wavelengths < 2H are not even representable on the coarse grid and so cannot be
reduced to zero on this grid. But it is exactly these high-frequency components that
can be reduced by relaxation on the fine grid! This leads us to combine the ideas
of relaxation and coarse-grid correction:

Two-Grid Iteration

e Pre-smoothing: Compute t, by applying v; > 0 steps of a relaxation

method to tp.

e Coarse-grid correction: As above, using tp, to give wpe".

e Post-smoothing: Compute u?°” by applying v2 > 0 steps of the relaxation

method to u7°”.

It is only a short step from the above two-grid method to a multigrid method.
Instead of solving the coarse-grid defect equation (19.6.8) exactly, we can get an
approximate solution of it by introducing an even coarser grid and using the two-grid
iteration method. If the convergence factor of the two-grid method is small enough,
we will need only a few steps of this iteration to get a good enough approximate
solution. We denote the number of such iterations by y. Obviously we can apply
this idea recursively down to some coarsest grid. There the solution is found
easily, for example by direct matrix inversion or by iterating the relaxation scheme
to convergence.

One iteration of a multigrid method, from finest grid to coarser grids and back
to finest grid again, is called a cycle. The exact structure of a cycle depends on the
value of y, the number of two-grid iterations at each intermediate stage. The case
7 = lis called a V-cycle, while y = 2 is called a W-cycle (see Figure 19.6.1). These
are the most important cases in practice.

Note that once more than two grids are involved, the pre-smoothing steps after
the first one on the finest grid need an initial approximation for the error v. This
should be taken to be zero.

Smoothing, Restriction, and Prolongation Operators

The most popular smoothing method, and the one you should try first, is
Gauss-Seidel, since it usually leads to a good convergence rate. If we order the mesh
points from 1 to NV, then the Gauss-Seidel scheme is

N
ui = Os - it i=1,...,N (19.6.12)
j=l u

Ai

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 875

© ©
2-grid [E|

3-grid
E
Q © teva
le] le) se} Ed
y=2
Figure 19.6.1. | Structure of multigrid cycles. S denotes smoothing, while E denotes exact solution

on the coarsest grid. Each descending line \ denotes restriction (R) and each ascending line / denotes
prolongation (P). The finest grid is at the top level of each diagram. For the V-cycles (7 = 1) the E
step is replaced by one 2-grid iteration each time the number of grid levels is increased by one. For the
W-cycles (y = 2), each E step gets replaced by two 2-grid iterations.

where new values of wu are used on the right-hand side as they become available. The
exact form of the Gauss-Seidel method depends on the ordering chosen for the mesh
points. For typical second-order elliptic equations like our model problem equation
(19.0.3), as differenced in equation (19.0.8), it is usually best to use red-black
ordering, making one pass through the mesh updating the “even” points (like the red
squares of a checkerboard) and another pass updating the “odd” points (the black
squares). When quantities are more strongly coupled along one dimension than
another, one should relax a whole line along that dimension simultaneously. Line
relaxation for nearest-neighbor coupling involves solving a tridiagonal system, and
so is still efficient. Relaxing odd and even lines on successive passes is called zebra
relaxation and is usually preferred over simple line relaxation.

Note that SOR should not be used as a smoothing operator. The overrelaxation
destroys the high-frequency smoothing that is so crucial for the multigrid method.

A succint notation for the prolongation and restriction operators is to give their
symbol. The symbol of P is found by considering vz; to be 1 at some mesh point
(x,y), zero elsewhere, and then asking for the values of Puy. The most popular
prolongation operator is simple bilinear interpolation. It gives nonzero values at the

9 points (x,y), (cu + h,y),-..,(@ — h,y — h), where the values are 1, Byes a

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

876 Chapter 19. Partial Differential Equations

Its symbol is therefore

(19.6.13)

BIR NIE AIR
NIE Re
AIR IE BIE

The symbol of R is defined by considering vj, to be defined everywhere on the
fine grid, and then asking what is Rv», at (x,y) as a linear combination of these
values. The simplest possible choice for R is straight injection, which means simply
filling each coarse-grid point with the value from the corresponding fine-grid point.
Its symbol is “[1].” However, difficulties can arise in practice with this choice. It
turns out that a safe choice for R is to make it the adjoint operator to P. To define the
adjoint, define the scalar product of two grid functions u, and v;, for mesh size h as

(unlondn =? Sour (a, y)un (2, y) (19.6.14)

Ty
Then the adjoint of P, denoted P', is defined by
(wi|Pl on) # = (Pun|vn)n (19.6.15)

Now take P to be bilinear interpolation, and choose u 7; = 1 at (x, y), zero elsewhere.
Set Pt = R in (19.6.15) and H = 2h. You will find that

(Rv) (x,y) fun(x,y) } bun (x th,y) 4 igvn(x th,yt+h)+-:- (19.6.16)

so that the symbol of R is

Aor
16 8 16
1 1 1
soa 8 (19.6.17)
toad
16 8 16

Note the simple rule: The symbol of R is + the transpose of the matrix defining the

symbol of P, equation (19.6.13). This rule is general whenever R = Pt and H = 2h.

The particular choice of R in (19.6.17) is called full weighting. Another popular
choice for R is half weighting, “halfway” between full weighting and straight
injection. Its symbol is

(19.6.18)

Caro
Oo] tol ol
cole

A similar notation can be used to describe the difference operator £;,. For
example, the standard differencing of the model problem, equation (19.0.6), is
represented by the five-point difference star

Lr=s}1 -4 1 (19.6.19)
0

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

19.6 Multigrid Methods for Boundary Value Problems 877

If you are confronted with a new problem and you are not sure what P and R
choices are likely to work well, here is a safe rule: Suppose m, is the order of the
interpolation P (ie., it interpolates polynomials of degree m_, — 1 exactly). Suppose
mr is the order of R, and that R is the adjoint of some P (not necessarily the P you
intend to use). Then if m is the order of the differential operator £;,, you should
satisfy the inequality m, + m, > m. For example, bilinear interpolation and its
adjoint, full weighting, for Poisson’s equation satisfy m» +m, = 4 > m= 2.

Of course the P and R operators should enforce the boundary conditions for
your problem. The easiest way to do this is to rewrite the difference equation to
have homogeneous boundary conditions by modifying the source term if necessary
(cf. §19.4). Enforcing homogeneous boundary conditions simply requires the P
operator to produce zeros at the appropriate boundary points. The corresponding
R is then found by R = Pt.

Full Multigrid Algorithm

So far we have described multigrid as an iterative scheme, where one starts
with some initial guess on the finest grid and carries out enough cycles (V-cycles,
W-cycles,...) to achieve convergence. This is the simplest way to use multigrid:
Simply apply enough cycles until some appropriate convergence criterion is met.
However, efficiency can be improved by using the Full Multigrid Algorithm (FMG),
also known as nested iteration.

Instead of starting with an arbitrary approximation on the finest grid (e.g.,
Un = 0), the first approximation is obtained by interpolating from a coarse-grid
solution:

un = Pun (19.6.20)

The coarse-grid solution itself is found by a similar FMG process from even coarser
grids. At the coarsest level, you start with the exact solution. Rather than proceed as
in Figure 19.6.1, then, FMG gets to its solution by a series of increasingly tall “N’s.”
each taller one probing a finer grid (see Figure 19.6.2).

Note that P in (19.6.20) need not be the same P used in the multigrid cycles.
It should be at least of the same order as the discretization Lp, but sometimes a
higher-order operator leads to greater efficiency.

It turns out that you usually need one or at most two multigrid cycles at each
level before proceeding down to the next finer grid. While there is theoretical
guidance on the required number of cycles (e.g., [2]), you can easily determine it
empirically. Fix the finest level and study the solution values as you increase the
number of cycles per level. The asymptotic value of the solution is the exact solution
of the difference equations. The difference between this exact solution and the
solution for a small number of cycles is the iteration error. Now fix the number of
cycles to be large, and vary the number of levels, i.e., the smallest value of h used. In
this way you can estimate the truncation error for a given h. In your final production
code, there is no point in using more cycles than you need to get the iteration error
down to the size of the truncation error.

The simple multigrid iteration (cycle) needs the right-hand side f only at the
finest level. FMG needs f at all levels. If the boundary conditions are homogeneous,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

878 Chapter 19. Partial Differential Equations

4-grid
neycle = 2

Figure 19.6.2. Structure of cycles for the full multigrid (FMG) method. This method starts on the

coarsest grid, interpolates, and then refines (by “V’s”), the solution onto grids of increasing fineness.

you can use f7; = Rf},. This prescription is not always safe for inhomogeneous
boundary conditions. In that case it is better to discretize f on each coarse grid.

Note that the FMG algorithm produces the solution on all levels. It can therefore
be combined with techniques like Richardson extrapolation.

We now give a routine mglin that implements the Full Multigrid Algorithm
for a linear equation, the model problem (19.0.6). It uses red-black Gauss-Seidel as
the smoothing operator, bilinear interpolation for P, and half-weighting for R. To
change the routine to handle another linear problem, all you need do is modify the
functions relax, resid, and slvsml appropriately. A feature of the routine is the
dynamical allocation of storage for variables defined on the various grids.

#include "nrutil.h"

#define NPRE 1 Number of relaxation sweeps before ...
#define NPOST 1 ...and after the coarse-grid correction is com-
#define NGMAX 15 puted.

void mglin(double **u, int n, int ncycle)
Full Multigrid Algorithm for solution of linear elliptic equation, here the model problem (19.0.6).
On input u[1. .n] [1..n] contains the right-hand side p, while on output it returns the solution.
The dimension n must be of the form 27 + 1 for some integer j. (j is actually the number of
grid levels used in the solution, called ng below.) ncycle is the number of V-cycles to be
used at each level.
{

void addint(double **uf, double **uc, double **res, int nf);

void copy(double **aout, double **ain, int n);

void £1110(double **u, int n);

void interp(double **uf, double **uc, int nf);

void relax(double **u, double **rhs, int n);

void resid(double **res, double **u, double **rhs, int n);

void rstrct(double **uc, double **uf, int nc);

void slvsml(double **u, double **rhs);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 879

unsigned int j,jcycle,jj,jpost,jpre,nf,ng=0,ngrid,nn;
double **ires [NGMAX+1] , **irho[NGMAX+1] ,**irhs [NGMAX+1] , **iu[NGMAX+1] ;

nn=n;

while (nn >>= 1) ngt+;

if (mn != 1+(1L << ng)) nrerror("n-1 must be a power of 2 in mglin.");

if (mg > NGMAX) nrerror("increase NGMAX in mglin.");

nn=n/2+1;

ngrid=ng-1;

irho[ngrid]=dmatrix(1,nn,1,nn); Allocate storage for r.h.s. on grid ng — 1,

rstrct(irho[ngrid] ,u,nn) ; and fill it by restricting from the fine grid.

while (nn > 3) { Similarly allocate storage and fill r.h.s. on all
nn=nn/2+1; coarse grids.

irho[--ngrid]=dmatrix(1,nn,1,nn);
rstrct (irho[ngrid] ,irho[ngrid+1] ,nn) ;
+
nn=3;
iu(1]=dmatrix(1,nn,1,nn);
irhs [1]=dmatrix(1,nn,1,nn) ;
slvsml(iu[1] ,irho[1]);
free_dmatrix(irho[1],1,nn,1,nn);
ngrid=ng;
for (j=2;j<=ngrid;jt++) { Nested iteration loop.
nn=2*nn-1;
iu(j]=dmatrix(1,nn,1,nn);
irhs[j]=dmatrix(1,nn,1,nn);
ires[j]=dmatrix(1,nn,1,nn);
interp(iu[j] ,iu[j-1] ,nn);
Interpolate from coarse grid to next finer grid.

Initial solution on coarsest grid.

copy(irhs[j],(j != ngrid ? irho[j] : u),nn); Set up r.h.s.
for (jcycle=1;jcycle<=ncycle;jcycle++) { V-cycle loop.
nf=nn;
for (jj=j3jj>=2;jj--) { Downward stoke of the V.

for (jpre=1; jpre<=NPRE; jpre++) Pre-smoothing.
relax(iu[jj],irhs[jj] ,nf);

resid(ires[jj],iuljj],irhs[jj],nf);

nf=nf/2+1;

rstrct (irhs[jj-1],ires[jj] ,nf);

Restriction of the residual is the next r.h.s.

£1110(iul[jj-1] ,nf); Zero for initial guess in next
relaxation.

slvsml(iu[1] ,irhs[1]); Bottom of V: solve on coars-

nf=3; est grid.

for (jj=2;jj<=j;jjt) { Upward stroke of V.

nf=2*nf-1;

addint (iuljjJ],iuljj-1],ires[jj] ,nf);
Use res for temporary storage inside addint.
for (jpost=1;jpost<=NPOST; jpost++) Post-smoothing.
relax(iuljj],irhs[jj] ,nf);
3
}
+
copy(u,iu[ngrid] ,n) ; Return solution in u.
for (nn=n, j=ng;j>=2;j--,nn=nn/2+1) {
free_dmatrix(ires[j],1,nn,1,nn);
free_dmatrix(irhs[j],1,nn,1,nn);
free_dmatrix(iu[j],1,nn,1,nn);
if (j != ng) free_dmatrix(irho[j],1,nn,1,nn);
}
free_dmatrix(irhs[1],1,3,1,3);
free_dmatrix(iu[1] ,1,3,1,3);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

880 Chapter 19. Partial Differential Equations

void rstrct(double **uc, double **uf, int nc)

Half-weighting restriction. nc is the coarse-grid dimension. The fine-grid solution is input in
uf [1..2*nc-1] [1..2*nc-1], the coarse-grid solution is returned in uc[1..nc] [1..nc].
{

int ic,iif,jc,jf,ncc=2*nc-1;

for (jf=3,jc=2;jc<nc;jct+,jf+=2) { Interior points.
for (iif=3,ic=2;ic<nc;ic++,iift+=2) {
uc [ic] [jc] =0.5*uf [iif] [j£]+0. 125+ (uf [iift1] [jf] +uf [iif-1] [jf]
tuf [iif] [jf+1]+uf [iif] [jf-1]);

}

}

for (jc=1,ic=1;ic<=nc;ict+,jct=2) { Boundary points.
uc [ic] (1]=uf [jc] [1];
uc [ic] [nc] =uf [jc] [nec] ;

}

for (jc=1,ic=1;ic<=nc;ict+,jct=2) {
uc [1] [ic] =uf [1] [jc];
uc [nc] [ic] =uf [ncc] [jc];

void interp(double **uf, double **uc, int nf)

Coarse-to-fine prolongation by bilinear interpolation. nf is the fine-grid dimension. The coarse-
grid solution is input as uc[1..nc] [1..nc], where nc = nf/2 +1. The fine-grid solution is
returned in uf [1..nf][1..nf].

t
int ic,iif,jc,jf,nc;
nce=nf/2+1;
for (jc=1,jf=1; jc<=nc;jct+, jf+=2) Do elements that are copies.
for (ic=1;ic<=nc;ict+) uf[2*ic-1] [jf]=uc[ic] [jc];
for (jf=1;jf<=nf ;jf+=2) Do odd-numbered columns, interpolat-
for (iif=2;iif<nf;iif+=2) ing vertically.
uf [iif] (jf£]=0.5* (uf [iif+1] (jf#]+uf [iif-1] [j£]);
for (jf=2;jf<nf ;jf+=2) Do even-numbered columns, interpolat-
for (iif=1;iif <= nf;iift++) ing horizontally.
uf [iif] [jf]=0.5* (uf [iif] [jf+1]+uf [iif] [jf-1]);
}

void addint(double **uf, double **uc, double **res, int nf)
Does coarse-to-fine interpolation and adds result to uf. nf is the fine-grid dimension. The
coarse-grid solution is input as uc[1..nc] [1..nc], where nc = nf/2+1. The fine-grid solu-
tion is returned in uf [1..nf][1..nf]. res[1..nf][1..nf] is used for temporary storage.
{

void interp(double **uf, double **uc, int nf);

int i,j;

interp(res,uc,nf) ;
for (j=1;j<=nf; j++)
for (i=1;i<=nf;i++)
uf [iJ [j] += res[i] (j];

void slvsml(double **u, double **rhs)
Solution of the model problem on the coarsest grid, where h = 3. The right-hand side is input
in rhs[1..3][1..3] and the solution is returned in u[1..3] [1..3].
{
void f1110(double **u, int n);
double h=0.5;

£1110(u,3);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 881

u(2][2] = -h*h*rhs[2] [2]/4.0;

void relax(double **u, double **rhs, int n)

Red-black Gauss-Seidel relaxation for model problem. Updates the current value of the solution
ul1..n][1..n], using the right-hand side function rhs[1..n] [1..n].

{

int i,ipass,isw,j,jsw=1;

double h,h2;
h=1.0/(n-1);
h2=h*h;
for (ipass=1;ipass<=2;ipass++,jsw=3-jsw) { Red and black sweeps.
isw=jsw;
for (j=2;j<n;j++,isw=3-isw)
for (i=iswt1;i<n;i+=2) Gauss-Seidel formula.

uli] (j]=0.25*(uli+1] (j]+uli-1] (j]+ulil [j+1]
tu[i] [j-1] -h2*rhs [i] [j]);

void resid(double **res, double **u, double **rhs, int n)
Returns minus the residual for the model problem. Input quantities are u[1..n][1..n] and
rhs[1..n][1..n], while res[1..n] [1..n] is returned.

t
int i,j;
double h,h2i;
h=1.0/(n-1);
h2i=1.0/(h*h) ;
for (j=2;j<n;j++) Interior points.
for (i=2;i<n;i++)
res(i](j] = -h2i+(ulita] (j]+u[i-1] (j]+uli] (j+1]+ulil (j-1]-
4.0*u[i] (j])+rhs [i] (31;
for (i=1;i<=n;it++) Boundary points.
res [i] [1]=res [i] [n]=res[1] [i]=res[n] [i]=0.0;
}

void copy(double **aout, double **ain, int n)
Copies ain[1..n][1..n] to aout[1..n] [1..n].

{
int i,j;
for (i=1;i<=n;it++)
for (j=1;j<=n; j++)
aout [j] ([iJ=ain[j] [i];
3

void fi110(double **u, int n)
Fills u[1..n][1..n] with zeros.

{
int i,j;
for (j=1;j<=n; j++)
for (i=1;i<=n;it++)
uli] [j]=0.0;
3

The routine mglin is written for clarity, not maximum efficiency, so that it is
easy to modify. Several simple changes will speed up the execution time:

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

882 Chapter 19. Partial Differential Equations

e The defect d;, vanishes identically at all black mesh points after a red-black
Gauss-Seidel step. Thus dy; = Rady, for half-weighting reduces to simply
copying half the defect from the fine grid to the corresponding coarse-grid
point. The calls to resid followed by rstrct in the first part of the
V-cycle can be replaced by a routine that loops only over the coarse grid,
filling it with half the defect.

e Similarly, the quantity u}°” = u;, + Puy need not be computed at red
mesh points, since they will immediately be redefined in the subsequent
Gauss-Seidel sweep. This means that addint need only loop over black
points.

e You can speed up relax in several ways. First, you can have a special
form when the initial guess is zero, and omit the routine £1110. Next,
you can store h? f), on the various grids and save a multiplication. Finally,
it is possible to save an addition in the Gauss-Seidel formula by rewriting
it with intermediate variables.

e On typical problems, mglin with ncycle = 1 will return a solution with
the iteration error bigger than the truncation error for the given size of h.
To knock the error down to the size of the truncation error, you have to
set ncycle = 2 or, more cheaply, npre = 2. A more efficient way turns
out to be to use a higher-order P in (19.6.20) than the linear interpolation
used in the V-cycle.

Implementing all the above features typically gives up to a factor of two
improvement in execution time and is certainly worthwhile in a production code.

Nonlinear Multigrid: The FAS Algorithm

Now turn to solving a nonlinear elliptic equation, which we write symbolically as
L(u) =0 (19.6.21)
Any explicit source term has been moved to the left-hand side. Suppose equation (19.6.21)
is suitably discretized:
L£i,(un) =0 (19.6.22)

We will see below that in the multigrid algorithm we will have to consider equations where a
nonzero right-hand side is generated during the course of the solution:

Lr(un) = fr (19.6.23)

One way of solving nonlinear problems with multigrid is to use Newton’s method, which
produces linear equations for the correction term at each iteration. We can then use linear
multigrid to solve these equations. A great strength of the multigrid idea, however, is that it
can be applied directly to nonlinear problems. All we need is a suitable nonlinear relaxation
method to smooth the errors, plus a procedure for approximating corrections on coarser grids.
This direct approach is Brandt’s Full Approximation Storage Algorithm (FAS). No nonlinear
equations need be solved, except perhaps on the coarsest grid.

To develop the nonlinear algorithm, suppose we have a relaxation procedure that can
smooth the residual vector as we did in the linear case. Then we can seek a smooth correction
vn to solve (19.6.23):

Lr(tin +n) = fr (19.6.24)
To find v,, note that
Lyr(tn + Un) — Ln (tin) = fr — Ln (tin)

19.6.25
a, ( )

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 883

The right-hand side is smooth after a few nonlinear relaxation sweeps. Thus we can transfer
the left-hand side to a coarse grid:

Lu(un) — Lu (Run) = —Rdp (19.6.26)
that is, we solve
Lu(un) = La(Rutn) — Rdn (19.6.27)

on the coarse grid. (This is how nonzero right-hand sides appear.) Suppose the approximate
solution is uy. Then the coarse-grid correction is

Ou = tin — Rin (196.28)
and
Tp = tin + P(t — Run) (19.6.29)

Note that PR # 1 in general, so wj,°" # Pin. This is a key point: In equation (19.6.29) the
interpolation error comes only from the correction, not from the full solution a.

Equation (19.6.27) shows that one is solving for the full approximation uz, not just the
error as in the linear algorithm. This is the origin of the name FAS.

The FAS multigrid algorithm thus looks very similar to the linear multigrid algorithm.
The only differences are that both the defect d;, and the relaxed approximation wu, have to
be restricted to the coarse grid, where now it is equation (19.6.27) that is solved by recursive
invocation of the algorithm. However, instead of implementing the algorithm this way, we
will first describe the so-called dual viewpoint, which leads to a powerful alternative way
of looking at the multigrid idea.

The dual viewpoint considers the local truncation error, defined as

tT =Li(u)— fr (19.6.30)
where w is the exact solution of the original continuum equation. If we rewrite this as
L£y(u) = fr +7 (19.6.31)

we see that 7 can be regarded as the correction to f;, so that the solution of the fine-grid
equation will be the exact solution u.

Now consider the relative truncation error 7;,, which is defined on the H-grid relative
to the h-grid:

Th = Lu(Run) — RL»(un) (19.6.32)
Since £Lpr(un) = fh, this can be rewritten as
Lu(un) = fu ttn (19.6.33)

In other words, we can think of 7, as the correction to fy that makes the solution of the
coarse-grid equation equal to the fine-grid solution. Of course we cannot compute 7, but we
do have an approximation to it from using w, in equation (19.6.32):

Th & Tr = Lu (Run) — RLa(tn) (19.6.34)
Replacing 7, by 7, in equation (19.6.33) gives
Lu(un) = Lu (Rup) — Rdn (19.6.35)

which is just the coarse-grid equation (19.6.27)!
Thus we see that there are two complementary viewpoints for the relation between
coarse and fine grids:

e Coarse grids are used to accelerate the convergence of the smooth components
of the fine-grid residuals.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

884 Chapter 19. Partial Differential Equations

e Fine grids are used to compute correction terms to the coarse-grid equations,
yielding fine-grid accuracy on the coarse grids.
One benefit of this new viewpoint is that it allows us to derive a natural stopping criterion
for a multigrid iteration. Normally the criterion would be
Ilda || S€ (19.6.36)

and the question is how to choose «. There is clearly no benefit in iterating beyond the
point when the remaining error is dominated by the local truncation error 7. The computable
quantity is 7,. What is the relation between 7 and 7;,? For the typical case of a second-order
accurate differencing scheme,

7 =Ly(u) —Ly(un) = h?72(2,y) ++ (19.6.37)

Assume the solution satisfies u, = u + h?ue(a,y) +--+. Then, assuming R is of high
enough order that we can neglect its effect, equation (19.6.32) gives

Th & Lu(ut h'uz) — Lr(u+ hur)

= Lu(u) — Lr(u) +h? [Lig (uz) — Lj, (u2)] ++ (19.6.38)
= (H? —h’)r2 + O(h*)

For the usual case of H = 2h we therefore have

TY Eth 3Th (19.6.39)
The stopping criterion is thus equation (19.6.36) with
€=all7n|l, ant (19.6.40)

We have one remaining task before implementing our nonlinear multigrid algorithm:
choosing a nonlinear relaxation scheme. Once again, your first choice should probably be
the nonlinear Gauss-Seidel scheme. If the discretized equation (19.6.23) is written with
some choice of ordering as

Li(ui,...,un) = fi, .N (19.6.41)
then the nonlinear Gauss-Seidel schemes solves
Li(ui,...,Ui-1, uy Wign,..., un) = fi (19.6.42)

for u?*”. As usual new u’s replace old w’s as soon as they have been computed. Often equation
(19.6.42) is linear in uj” , since the nonlinear terms are discretized by means of its neighbors.
If this is not the case, we replace equation (19.6.42) by one step of a Newton iteration:

new old Li (ug!) — fi

i = eS 19.6.43
tS OL, (U4) Oui ( )

For example, consider the simple nonlinear equation
Vutw=p (19.6.44)

In two-dimensional notation, we have
Luis) = (Wigan g + Uinay + Ui gta + Wag — 4uig)/h? + uz; — pig =O (19.645)

Since

= —4/h? + Qui,j (19.6.46)
the Newton Gauss-Seidel iteration is
new Luis)
j= Wy — 19.6.47
Mig wis —4/h? + Qui; ( )

Here is a routine mgfas that solves equation (19.6.44) using the Full Multigrid Algorithm
and the FAS scheme. Restriction and prolongation are done as in mglin. We have included
the convergence test based on equation (19.6.40). A successful multigrid solution of a problem
should aim to satisfy this condition with the maximum number of V-cycles, maxcyc, equal to
1 or 2. The routine mgfas uses the same functions copy, interp, and rstrct as mglin.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 885

#include "nrutil.h"
#define NPRE 1 Number of relaxation sweeps before ...
#define NPOST 1 ...and after the coarse-grid correction is computed.

#define ALPHA 0.33

Relates the estimated truncation error to the norm

#define NGMAX 15 of the residual.

void mgfas(double **u, int n, int maxcyc)

Full Multigrid Algorithm for FAS solution of nonlinear elliptic equation, here equation (19.6.44).
On input u[1. .n] [1..n] contains the right-hand side p, while on output it returns the solution.
The dimension n must be of the form 27 + 1 for some integer j. (j is actually the number of
grid levels used in the solution, called ng below.) maxcyc is the maximum number of V-cycles
to be used at each level.

{

double anorm2(double **a, int n);

void copy(double **aout, double **ain, int n);

void interp(double **uf, double **uc, int nf);

void lop(double **out, double **u, int n);

void matadd(double **a, double **b, double **c, int n);

void matsub(double **a, double **b, double **c, int n);

void relax2(double **u, double **rhs, int n);

void rstrct(double **uc, double **uf, int nc);

void slvsm2(double **u, double **rhs) ;

unsigned int j,jcycle,jj,jm1,jpost,jpre,nf,ng=0,ngrid,nn;

double **irho[NGMAX+1] ,**irhs [NGMAX+1] ,**itau[NGMAX+1],
**itemp [NGMAX+1] ,**iu[NGMAX+1] ;

double res,trerr;

nn=n;

while (nn >>= 1) ngt+;

if (mn != 1+(1L << ng)) nrerror("n-1 must be a power of 2 in mgfas.");

if (mg > NGMAX) nrerror("increase NGMAX in mglin.");

nn=n/2+1;

ngrid=ng-1;

irho[ngrid]=dmatrix(1,nn,1,nn); Allocate storage for r.h.s. on grid ng — 1,

rstrct(irho[ngrid] ,u,nn) ; and fill it by restricting from the fine grid.

while (nn > 3) { Similarly allocate storage and fill r.h.s. on all
nn=nn/2+1; coarse grids.

irho[--ngrid]=dmatrix(1,nn,1,nn);
rstrct (irho[ngrid] ,irho[ngrid+1] ,nn) ;
+
nn=3;
iu(1]=dmatrix(1,nn,1,nn);
irhs [1]=dmatrix(1,nn,1,nn) ;
itau[1]=dmatrix(1,nn,1,nn) ;
itemp[1]=dmatrix(1,nn,1,nn);
slvsm2(iu[1] ,irho[1]);
free_dmatrix(irho[1],1,nn,1,nn);
ngrid=ng;
for (j=2;j<=ngrid;jt++) { Nested iteration loop.
nn=2*nn-1;
iu(j]=dmatrix(1,nn,1,nn);
irhs[j]=dmatrix(1,nn,1,nn);
itau[j]=dmatrix(1,nn,1,nn);
itemp[j]=dmatrix(1,nn,1,nn);
interp(iu[j] ,iu[j-1] ,nn);
Interpolate from coarse grid to next finer grid.

Initial solution on coarsest grid.

copy(irhs[j],(j != ngrid ? irho[j] : u),nn); Set up r.h.s.
for (jcycle=1;jcycle<=maxcyc;jcycle++) { V-cycle loop.
nf=nn;
for (jj=jsjj>=2;jj--) { Downward stoke of the V.
for (jpre=1;jpre<=NPRE; jprett+) Pre-smoothing.
relax2(iu[jj],irhs[jj] ,nf);
lop(itemp[jj],iuljj],nf); Lp (un).

nf=nf/2+1;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

886 Chapter 19.

Partial Differential Equations

jmi=jj-1;

rstrct(itemp[jm1] ,itemp[jj] ,nf);
rstret(iu[jm1] ,iuljj] ,nf);
lop(itau[jm1] ,iuljm1] ,nf);

Ly (Rup) stored temporarily in 7).

matsub(itau[jm1] ,itemp[jm1] ,itau[jm1] ,nf) ;

if (jj == i)
trerr=ALPHA*anorm2(itau[jm1] ,nf) ;
rstrct(irhs[jm1] ,irhs[jj] ,nf);

matadd(irhs[jm1] ,itau[jm1] ,irhs[jm1] ,nf);

3

slvsm2(iu[1],irhs[1]);

nf=3;

for (jj=2;jj<=jiji+4) ¢

jmi=jj-1;

rstrct (itemp[jm1] ,iuljj] ,nf);
matsub(iu[jm1] ,itemp[jm1] ,itemp[jm1] ,nf);
nf=2*nf-1;

interp(itau[jj] ,itemp[jm1] ,nf);

matadd(iu[jj] ,itaul[jj],iuljj] nf);

for (jpost=1;jpost<=NPOST; jpost++)
relax2(iul[jj],irhs[jj],nf);

}

lop(itemp[j] ,iulj] ,nf);

matsub(itemp[j] ,irhs[j] ,itemp[j] ,nf);

res=anorm2(itemp[j] ,nf);

if (res < trerr) break;

+

+

copy(u, iu[ngrid] ,n) ;

for (nn=n, j=ng;j>=1;j--,nn=nn/2+1) {
free_dmatrix(itemp[j],1,nn,1,nn);
free_dmatrix(itau[j],1,nn,1,nn);
free_dmatrix(irhs[j],1,nn,1,nn);
free_dmatrix(iu[j],1,nn,1,nn);

RL} (un).

Rup.

Form 7),-

Estimate truncation error T.

H- _

futtn-

Bottom of V: Solve on coars-
est grid.

Upward stroke of V.

Riin.
iy — Rip.

P(t —Riu,, ) stored in 7),.
qpew.

Form uj
Post-smoothing.

Form residual ||d),||-
No more V-cycles needed if

residual small enough.

Return solution in u.

if (j != ng && j != 1) free_dmatrix(irho[j],1,nn,1,nn);

void relax2(double **u, double **rhs, int n)

Red-black Gauss-Seidel relaxation for equation (19.6.44). The current value of the solution
ul[1..n][1..n] is updated, using the right-hand side function rhs[1..n] [1..n] .

{
int i,ipass,isw,j,jsw=1;
double foh2,h,h2i,res;

h=1.0/(n-1);
h2i=1.0/(h*h) ;
foh2 = -4.0*h2i;
for (ipass=1;ipass<=2;ipass++,jsw=3-jsw) {
isw=jsw;
for (j=2;j<n;j++,isw=3-isw) {
for (i=iswti;i<n;it=2) {

Red and black sweeps.

res=h2i*(u[it1] (j]+uli-1] (j]+ufi] (j+1]+u la] (j-1]-
4.0*u [i] (j])+uli] [5] *u li] (j]-rhs [i] (51;

uli] [j] -= res/(foh2+2.0*u[i] [j]);

Newton Gauss-Seidel formula.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

19.6 Multigrid Methods for Boundary Value Problems 887

#include <math.h>

void slvsm2(double **u, double **rhs)
Solution of equation (19.6.44) on the coarsest grid, where h = 3. The right-hand side is input
in rhs[1..3][1..3] and the solution is returned in u[1..3] [1..3].

{
void fill0(double **u, int n);
double disc,fact ,h=0.5; =
Fike)
acs
£1110(u,3); &
fact=2.0/(h*h) ; e
disc=sqrt (fact*fact+rhs [2] [2]) ; 2
u(2] [2] = -rhs[2] [2]/(fact+disc) ; =
t a
€
a
void lop(double **out, double **u, int n) a

Given u[1..n] [1..n], returns £),(%;,) for equation (19.6.44) in out [1..n] [1..n].
{

int i,j;

double h,h2i;

h=1.0/(n-1);
h2i=1.0/(h*h) ;
for (j=2;j<n; j++) Interior points.

for (i=2;i<n;it+)

out [i] (j] =h2ix(ulit1] (j]+uli-1] (j]+u la) [j+1] +u [a] (j-1]-
4.0*u[i] [j])+ulil Cj] *ulil [5];

for (i=1;i<=n;it++) Boundary points.

out [i] [1] =out [i] [n]=out [1] [i] =out [n] [i] =0.0;

}

void matadd(double **a, double **b, double **c, int n)
Adds a[1..n][1..n] to b[1..n][1..n] and returns result in c({1..n] [1..n].

{
int i,j;
for (j=1;j<=n; j++)
for (i=1;i<=n;i++)
efi] (j]=alil (j]+b Ci] (31;
3

void matsub(double **a, double **b, double **c, int n)
Subtracts b[1..n][1..n] from a[1..n][1..n] and returns result in c[1..n] [1..n].
{

int i,j;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

for (j=1;j<=n; j++)
for (i=1;i<=n;it++)

¢ [i] (j]=alil (j]-b [i] (51;

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

#include <math.h>

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

double anorm2(double **a, int n)
Returns the Euclidean norm of the matrix a[1..n] [1..n].
{

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

int i,j;
double sum=0.0;

for (j=1;j<=n; j++)
for (i=1;i<=n;it++)
sum += a[i] (j] *alil [jl];
return sqrt (sum) /n;


888 Chapter 19. Partial Differential Equations

CITED REFERENCES AND FURTHER READING:

Brandt, A. 1977, Mathematics of Computation, vol. 31, pp. 333-390. [1]

Hackbusch, W. 1985, Multi-Grid Methods and Applications (New York: Springer-Verlag). [2]

Stuben, K., and Trottenberg, U. 1982, in Multigrid Methods, W. Hackbusch and U. Trottenberg,
eds. (Springer Lecture Notes in Mathematics No. 960) (New York: Springer-Verlag), pp. 1—
176. [3]

Brandt, A. 1982, in Multigrid Methods, W. Hackbusch and U. Trottenberg, eds. (Springer Lecture
Notes in Mathematics No. 960) (New York: Springer-Verlag). [4]

Baker, L. 1991, More C Tools for Scientists and Engineers (New York: McGraw-Hill).

Briggs, W.L. 1987, A Multigrid Tutorial (Philadelphia: S.1.A.M.).

Jespersen, D. 1984, Multigrid Methods for Partial Differential Equations (Washington: Mathe-
matical Association of America).

McCormick, S.F. (ed.) 1988, Multigrid Methods: Theory, Applications, and Supercomputing (New
York: Marcel Dekker).

Hackbusch, W., and Trottenberg, U. (eds.) 1991, Multigrid Methods III (Boston: Birkhauser).
Wesseling, P. 1992, An Introduction to Multigrid Methods (New York: Wiley).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Chapter 20. Less-Numerical
Algorithms

20.0 Introduction

You can stop reading now. You are done with Numerical Recipes, as such. This
final chapter is an idiosyncratic collection of “less-numerical recipes” which, for one
reason or another, we have decided to include between the covers of an otherwise
more-numerically oriented book. Authors of computer science texts, we’ve noticed,
like to throw in a token numerical subject (usually quite a dull one — quadrature, for
example). We find that we are not free of the reverse tendency.

Our selection of material is not completely arbitrary. One topic, Gray codes, was
already used in the construction of quasi-random sequences (§7.7), and here needs
only some additional explication. Two other topics, on diagnosing a computer’s
floating-point parameters, and on arbitrary precision arithmetic, give additional
insight into the machinery behind the casual assumption that computers are useful
for doing things with numbers (as opposed to bits or characters). The latter of these
topics also shows a very different use for Chapter 12’s fast Fourier transform.

The three other topics (checksums, Huffman and arithmetic coding) involve
different aspects of data coding, compression, and validation. If you handle a large
amount of data — numerical data, even — then a passing familiarity with these
subjects might at some point come in handy. In §13.6, for example, we already
encountered a good use for Huffman coding.

But again, you don’t have to read this chapter. (And you should learn about
quadrature from Chapters 4 and 16, not from a computer science text!)

20.1 Diagnosing Machine Parameters

A convenient fiction is that a computer’s floating-point arithmetic is “accurate
enough.” If you believe this fiction, then numerical analysis becomes a very clean
subject. Roundoff error disappears from view; many finite algorithms become
“exact”; only docile truncation error (§1.3) stands between you and a perfect
calculation. Sounds rather naive, doesn’t it?

Yes, it is naive. Notwithstanding, it is a fiction necessarily adopted throughout
most of this book. To do a good job of answering the question of how roundoff error

889

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

890 Chapter 20. _Less-Numerical Algorithms

propagates, or can be bounded, for every algorithm that we have discussed would be
impractical. In fact, it would not be possible: Rigorous analysis of many practical
algorithms has never been made, by us or anyone.
Proper numerical analysts cringe when they hear a user say, “I was getting
roundoff errors with single precision, so I switched to double.” The actual meaning
is, “for this particular algorithm, and my particular data, double precision seemed
able to restore my erroneous belief in the ‘convenient fiction’.” We admit that most
of the mentions of precision or roundoff in Numerical Recipes are only slightly more
quantitative in character. That comes along with our trying to be “practical.”
It is important to know what the limitations of your machine’s floating-point
arithmetic actually are — the more so when your treatment of floating-point roundoff
error is going to be intuitive, experimental, or casual. Methods for determining
useful floating-point parameters experimentally have been developed by Cody [1],
Malcolm [2], and others, and are embodied in the routine machar, below, which
follows Cody’s implementation.
All of machar’s arguments are returned values. Here is what they mean:
e ibeta (called B in §1.3) is the radix in which numbers are represented,
almost always 2, but occasionally 16, or even 10.

e it is the number of base-ibeta digits in the floating-point mantissa MZ
(see Figure 1.3.1).

e machep is the exponent of the smallest (most negative) power of ibeta
that, added to 1.0, gives something different from 1.0.

e eps is the floating-point number ibeta™@Ch€P , Joosely referred to as the
“floating-point precision.”

e negep is the exponent of the smallest power of ibeta that, subtracted
from 1.0, gives something different from 1.0.

e epsneg is ibeta®°S°P, another way of defining floating-point precision.
Not infrequently epsneg is 0.5 times eps; occasionally eps and epsneg
are equal.

e iexp is the number of bits in the exponent (including its sign or bias).

e minexp is the smallest (most negative) power of ibeta consistent with

there being no leading zeros in the mantissa.

e xmin is the floating-point number ibeta™!2©*P,, generally the smallest

(in magnitude) useable floating value.

e maxexp is the smallest (positive) power of ibeta that causes overflow.

e xmax is (l1—epsneg) x ibeta™4*XP | generally the largest (in magnitude)

useable floating value.

e irnd returns a code in the range 0. . .5, giving information on what kind of

rounding is done in addition, and on how underflow is handled. See below.

e ngrd is the number of “guard digits” used when truncating the product of

two mantissas to fit the representation.

There is a lot of subtlety in a program like machar, whose purpose is to ferret
out machine properties that are supposed to be transparent to the user. Further, it must
do so avoiding error conditions, like overflow and underflow, that might interrupt
its execution. In some cases the program is able to do this only by recognizing
certain characteristics of “standard” representations. For example, it recognizes
the IEEE standard representation [3] by its rounding behavior, and assumes certain
features of its exponent representation as a consequence. We refer you to [1] and

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.1 Diagnosing Machine Parameters 891

Sample Results Returned by machar

typical IEEE-compliant machine | DEC VAX

precision single double single
ibeta 2 2 2
it 24 53 24
machep —23 —52 —24

eps |1.19x 1077] 2.22x 10-16 | 5.96 x 10-8

negep —24 —53 —24

epsneg | 5.96 x 10-8} 1.11 x 107-!® | 5.96 x 1078

iexp 8 11 8

minexp —126 —1022 —128

xmin |1.18 x 10-38) 2.23 x 107908 |2.94 x 107-39

maxexp 128 1024 127

xmax | 3.40x 1035] 1.79 x 109° | 1.70 x 10°8

irnd 5 5 1

ngrd 0 0 0

references therein for details. Be aware that machar can give incorrect results on
some nonstandard machines.

The parameter irnd needs some additional explanation. In the IEEE standard,
bit patterns correspond to exact, “representable” numbers. The specified method
for rounding an addition is to add two representable numbers “exactly,” and then
round the sum to the closest representable number. If the sum is precisely halfway
between two representable numbers, it should be rounded to the even one (low-order
bit zero). The same behavior should hold for all the other arithmetic operations,
that is, they should be done in a manner equivalent to infinite precision, and then
rounded to the closest representable number.

If irnd returns 2 or 5, then your computer is compliant with this standard. If it
returns | or 4, then it is doing some kind of rounding, but not the IEEE standard. If
irnd returns 0 or 3, then it is truncating the result, not rounding it — not desirable.

The other issue addressed by irnd concerns underflow. If a floating value is
less than xmin, many computers underflow its value to zero. Values irnd = 0,1,
or 2 indicate this behavior. The IEEE standard specifies a more graceful kind of
underflow: As a value becomes smaller than xmin, its exponent is frozen at the
smallest allowed value, while its mantissa is decreased, acquiring leading zeros and
“gracefully” losing precision. This is indicated by irnd = 3,4, or 5.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

892 Chapter 20. Less-Numerical Algorithms

#include <math.h>
#define CONV(i) ((float) (i))
Change float to double here and in declarations below to find double precision parameters.

void machar(int *ibeta, int *it, int *irnd, int *ngrd, int *machep, int *negep,

int *iexp, int *minexp, int *maxexp, float *eps, float *epsneg,

float *xmin, float *xmax)
Determines and returns machine-specific parameters affecting floating-point arithmetic. Re-
turned values include ibeta, the floating-point radix; it, the number of base-ibeta digits in
the floating-point mantissa; eps, the smallest positive number that, added to 1.0, is not equal
to 1.0; epsneg, the smallest positive number that, subtracted from 1.0, is not equal to 1.0;
xmin, the smallest representable positive number; and xmax, the largest representable positive
number. See text for description of other returned parameters.
t

int i,itemp,iz,j,k,mx,nxres;

float a,b,beta,betah,betain,one,t,temp,temp1,tempa,two,y,z,zero;

one=CONV(1) ;
two=onetone;
zero=one-one;
a=one; Determine ibeta and beta by the method of M.
do { Malcolm.
a += a;
temp=atone;
tempi=temp-a;

} while (tempi-one == zero);
b=one;
do {

b += b;

temp=a+tb;

itemp=(int) (temp-a) ;
} while (itemp == 0);
*ibeta=itemp;
beta=CONV(*ibeta) ;
Determine it and irnd.

b *= beta;
temp=b+one;
temp1=temp-b;
} while (tempi-one == zero);
*irnd=0;
betah=beta/two;
temp=atbetah;
if (temp-a != zero) *irnd=1;
tempa=atbeta;
temp=tempatbetah;
if (*irnd == 0 && temp-tempa != zero) *irnd=2;
anegep=(*it)+3; Determine negep and epsneg.
betain=one/beta;
a=one;
for (i=1;i<=(*negep);i++) a *= betain;
b=a;
for (;;) {
temp=one-a;
if (temp-one != zero) break;
a *= beta;
--(#negep) ;

+

anegep = —(*negep) ;

*epsneg=a;

xmachep = -(*it)-3; Determine machep and eps.
a=b;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

20.1 Diagnosing Machine Parameters 893

for G3) f
temp=oneta;
if (temp-one != zero) break;
a *= beta;
++(#machep) ;
+
xeps=a;
‘ngrd=0; Determine ngrd.
temp=one+(*eps) ;
if (*irnd == 0 && temp*one-one != zero) *ngrd=1;
i=0; Determine iexp.
k=1;
z=betain;
t=onet(*eps) ;
nxres=0;
for (;;) { Loop until an underflow occurs, then exit.
yazi
Z=y*y;
a=z*one; Check here for the underflow.
temp=z*t ;
if (ata == zero || fabs(z) >= y) break;
temp1=temp*betain;
if (tempi*beta == z) break;

t+ti;
k += k;
+
if (*ibeta
*iexp=it+1;
mx=k+k;
} else { For decimal machines only.
*iexp=2;
iz=(*ibeta) ;
while (k >= iz) {
iz *= *ibeta;
++ (#iexp) ;
}
mx=iztiz-1;
}
for (;;) { To determine minexp and xmin, loop until an
axmin=y ; underflow occurs, then exit.
y *= betain;
a=y*one; Check here for the underflow.
temp=y*t;
if (ata != zero && fabs(y) < *xmin) {
++k;
temp1=temp*betain;
if (tempixbeta == y && temp != y) {
nxres=3;
*xmi 3
break;
3
}
else break;
+
‘minexp = -k; Determine maxexp, xmax.
if (mx k+k-3 && *ibeta != 10) {
mx += mx;
++(#iexp) ;
+
4maxexp=mx+ (*minexp) ;
*irnd += nxres; Adjust irnd to reflect partial underflow.
if (*irnd >= 2) *maxexp -= 2; Adjust for IEEE-style machines.

i=(*maxexp)+(*minexp) ;
Adjust for machines with implicit leading bit in binary mantissa, and machines with radix

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

894 Chapter 20. _Less-Numerical Algorithms

point at extreme right of mantissa.
if (*ibeta == 2 && !i) --(*maxexp);
if (i > 20) --(*maxexp) ;
if (a != y) *maxexp -= 2;
*xmax=one- (*epsneg) ;
if ((+xmax)*one != *xmax) *xmax=one-beta*(*epsneg) ;
*xmax /= (*xmin*beta*beta*beta) ;
i=(+*maxexp)+(*minexp)+3;
for (j=1;j<=i;j++) {
if (*ibeta == 2) *xmax += *xmax;
else *xmax *= beta;

Some typical values returned by machar are given in the table, above. IEEE-
compliant machines referred to in the table include most UNIX workstations (SUN,
DEC, MIPS), and Apple Macintosh IIs. IBM PCs with floating co-processors
are generally IEEE-compliant, except that some compilers underflow intermediate
results ungracefully, yielding irnd = 2 rather than 5. Notice, as in the case of a VAX
(fourth column), that representations with a “phantom” leading | bit in the mantissa
achieve a smaller eps for the same wordlength, but cannot underflow gracefully.

CITED REFERENCES AND FURTHER READING:

Goldberg, D. 1991, ACM Computing Surveys, vol. 23, pp. 5—48.

Cody, W.J. 1988, ACM Transactions on Mathematical Software, vol. 14, pp. 303-311. [1]
Malcolm, M.A. 1972, Communications of the ACM, vol. 15, pp. 949-951. [2]

IEEE Standard for Binary Floating-Point Numbers, ANSI/IEEE Std 754-1985 (New York: IEEE,
1985). [3]

20.2 Gray Codes

A Gray code is a function G(i) of the integers i, that for each integer N > 0
is one-to-one for 0 < i < 2% — 1, and that has the following remarkable property:
The binary representation of G(i) and G(i + 1) differ in exactly one bit. An example
of a Gray code (in fact, the most commonly used one) is the sequence 0000, 0001,
0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001,
and 1000, for i = 0,...,15. The algorithm for generating this code is simply to
form the bitwise exclusive-or (XOR) of i with 1/2 (integer part). Think about how
the carries work when you add one to a number in binary, and you will be able to see
why this works. You will also see that G(i) and G(i + 1) differ in the bit position of
the rightmost zero bit of 7 (prefixing a leading zero if necessary).

The spelling is “Gray,” not “gray”: The codes are named after one Frank Gray,
who first patented the idea for use in shaft encoders. A shaft encoder is a wheel with
concentric coded stripes each of which is “read” by a fixed conducting brush. The
idea is to generate a binary code describing the angle of the wheel. The obvious,
but wrong, way to build a shaft encoder is to have one stripe (the innermost, say)
conducting on half the wheel, but insulating on the other half; the next stripe is
conducting in quadrants | and 3; the next stripe is conducting in octants 1, 3, 5,

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.2 Gray Codes 895

MSB
4 4
3 3

2 Gi)

VVVV

0 0
LSB
(a)
MSB
4
3
Gti) 2 i
1
0
LSB
(b)
Figure 20.2.1. Single-bit operations for calculating the Gray code G(i) from i (a), or the inverse (b).

LSB and MSB indicate the least and most significant bits, respectively. XOR denotes exclusive-or.

and 7; and so on. The brushes together then read a direct binary code for the
position of the wheel.

The reason this method is bad, is that there is no way to guarantee that all the
brushes will make or break contact exactly simultaneously as the wheel turns. Going
from position 7 (0111) to 8 (1000), one might pass spuriously and transiently through
6 (0110), 14 (1110), and 10 (1010), as the different brushes make or break contact.
Use of a Gray code on the encoding stripes guarantees that there is no transient state
between 7 (0100 in the sequence above) and 8 (1100).

Of course we then need circuitry, or algorithmics, to translate from G(i) to i.
Figure 20.2.1 (b) shows how this is done by a cascade of XOR gates. The idea is
that each output bit should be the XOR of all more significant input bits. To do
N bits of Gray code inversion requires NV — 1 steps (or gate delays) in the circuit.
(Nevertheless, this is typically very fast in circuitry.) In a register with word-wide
binary operations, we don’t have to do N consecutive operations, but only Ing N.
The trick is to use the associativity of XOR and group the operations hierarchically.
This involves sequential right-shifts by 1,2,4,8,... bits until the wordlength is

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

896 Chapter 20. _Less-Numerical Algorithms

exhausted. Here is a piece of code for doing both G‘(i) and its inverse.

unsigned long igray(unsigned long n, int is)
For zero or positive values of is, return the Gray code of n; if is is negative, return the inverse
Gray code of n.

t
int ish;
unsigned long ans, idiv;
if (is >= 0) This is the easy direction!
return n ~ (n >> 1);
ish=1; This is the more complicated direction: In hierarchical
ans=n; stages, starting with a one-bit right shift, cause each
for (;;) { bit to be XORed with all more significant bits.
ans “= (idiv=ans >> ish);
if (idiv <= 1 || ish == 16) return ans;
ish <<= 1; Double the amount of shift on the next cycle.
+
}

In numerical work, Gray codes can be useful when you need to do some task
that depends intimately on the bits of 7, looping over many values of 7. Then, if there
are economies in repeating the task for values differing by only one bit, it makes
sense to do things in Gray code order rather than consecutive order. We saw an
example of this in $7.7, for the generation of quasi-random sequences.

CITED REFERENCES AND FURTHER READING:

Horowitz, P., and Hill, W. 1989, The Art of Electronics, 2nd ed. (New York: Cambridge University
Press), §8.02.

Knuth, D.E. Combinatorial Algorithms, vol. 4 of The Art of Computer Programming (Reading,
MA: Addison-Wesley), §7.2.1. [Unpublished. Will it be always so?]

20.3 Cyclic Redundancy and Other Checksums

When you send a sequence of bits from point A to point B, you want to know
that it will arrive without error. A common form of insurance is the “parity bit,”
attached to 7-bit ASCII characters to put them into 8-bit format. The parity bit is
chosen so as to make the total number of one-bits (versus zero-bits) either always
even (“even parity”) or always odd (“odd parity”). Any single bit error in a character
will thereby be detected. When errors are sufficiently rare, and do not occur closely
bunched in time, use of parity provides sufficient error detection.

Unfortunately, in real situations, a single noise “event” is likely to disrupt more
than one bit. Since the parity bit has two possible values (0 and 1), it gives, on
average, only a 50% chance of detecting an erroneous character with more than one
wrong bit. That probability, 50%, is not nearly good enough for most applications.
Most communications protocols [1] use a multibit generalization of the parity bit
called a “cyclic redundancy check” or CRC. In typical applications the CRC is 16
bits long (two bytes or two characters), so that the chance of a random error going
undetected is 1 in 2'© = 65536. Moreover, M-bit CRCs have the mathematical

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.3 Cyclic Redundancy and Other Checksums 897

property of detecting all errors that occur in M or fewer consecutive bits, for any
length of message. (We prove this below.) Since noise in communication channels
tends to be “bursty,” with short sequences of adjacent bits getting corrupted, this
consecutive-bit property is highly desirable.

Normally CRCs lie in the province of communications software experts and
chip-level hardware designers — people with bits under their fingernails. However,
there are at least two kinds of situations where some understanding of CRCs can be
useful to the rest of us. First, we sometimes need to be able to communicate with
a lower-level piece of hardware or software that expects a valid CRC as part of its
input. For example, it can be convenient to have a program generate XMODEM
or Kermit [2] packets directly into the communications line rather than having to
store the data in a local file.

Second, in the manipulation of large quantities of (e.g., experimental) data, it
is useful to be able to tag aggregates of data (whether numbers, records, lines, or
whole files) with a statistically unique “key,” its CRC. Aggregates of any size can
then be compared for identity by comparing only their short CRC keys. Differing
keys imply nonidentical records. Identical keys imply, to high statistical certainty,
identical records. If you can’t tolerate the very small probability of being wrong, you
can do a full comparison of the records when the keys are identical. When there is a
possibility of files or data records being inadvertently or irresponsibly modified (for
example, by a computer virus), it is useful to have their prior CRCs stored externally
on a physically secure medium, like a floppy disk.

Sometimes CRCs can be used to compress data as it is recorded. If identical data
records occur frequently, one can keep sorted in memory the CRCs of previously
encountered records. A new record is archived in full if its CRC is different,
otherwise only a pointer to a previous record need be archived. In this application
one might desire a 4- or 8-byte CRC, to make the odds of mistakenly discarding
a different data record be tolerably small; or, if previous records can be randomly
accessed, a full comparison can be made to decide whether records with identical
CRCs are in fact identical.

Now let us briefly discuss the theory of CRCs. After that, we will give
implementations of various (related) CRCs that are used by the official or de facto
standard protocols [1-3] listed in the accompanying table.

The mathematics underlying CRCs is “polynomials over the integers modulo
2.” Any binary message can be thought of as a polynomial with coefficients 0 and 1.
For example, the message “1100001101” is the polynomial 2° + 2° + 2° + 2? +1.
Since 0 and | are the only integers modulo 2, a power of x in the polynomial is
either present (1) or absent (0). A polynomial over the integers modulo 2 may be
irreducible, meaning that it can’t be factored. A subset of the irreducible polynomials
are the “primitive” polynomials. These generate maximum length sequences when
used in shift registers, as described in §7.4. The polynomial a? + 1 is not irreducible:
x? +1 = (x+1)(x+1), so itis also not primitive. The polynomial x4 +23 +a?+2+1
is irreducible, but it turns out not to be primitive. The polynomial x4 + x + 1 is
both irreducible and primitive.

An M-bit long CRC is based on a primitive polynomial of degree M, called
the generator polynomial. Alternatively, the generator is chosen to be a primitive
polynomial times (1 + 2) (this finds all parity errors). For 16-bit CRC’s, the CCITT
(Comité Consultatif International Télégraphique et Téléphonique) has anointed the

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

898 Chapter 20. _Less-Numerical Algorithms

Conventions and Test Values for Various CRC Protocols

icre args | Test Values (C2C{ in hex) Packet
Protocol |jinit|jrev| T  |CatMouse987654321 Format CRC
XMODEM 0 1 |1A71 E556 S1S2...SnNC2C1 0
X.25 255 | —1 |1B26 FS6E S192...SnC1C2 FOB8
(noname) | 255 | —1 |1B26 F56E S1S2...SnNCiC2 0
SDLC (IBM) same as X.25
HDLC (ISO) same as X.25
CRC-CCITT| 0 —1 |14Al C28D S1S2...SnNC1C2 0
(no name) 0 —1 |14Al C28D $1S2...SnC1C2 FOB8
Kermit same as CRC-CCITT see Notes
Notes: Overbar denotes bit complement. S,...Sy are character data. Cj is CRC’s least

significant 8 bits, C2 is its most significant 8 bits, so CRC = 256 C2 + C1 (shown
in hex). Kermit (block check level 3) sends the CRC as 3 printable ASCII characters|
(sends value +32). These contain, respectively, 4 most significant bits, 6 middle bits
6 least significant bits.

“CCITT polynomial,” which is x1° + x!? + 2° + 1. This polynomial is used by all of
the protocols listed in the table. Another common choice is the “CRC-16” polynomial
x'6 4 15 + 2? + 1, which is used for EBCDIC messages in IBM’s BISYNCH [1].
A common 12-bit choice, “CRC-12,” is 2!2 + a!! + 2° + «+1. A common 32-bit
choice, “AUTODIN-II.” is 73? + 12° + 173 + 29? +a! 4a pal 4 910 4 a8 4
xv +a°+4+a4+2?+2+1. Fora table of some other primitive polynomials, see §7.4.

Given the generator polynomial G' of degree M (which can be written either in
polynomial form or as a bit-string, e.g., 10001000000100001 for CCITT), here is
how you compute the CRC for a sequence of bits S: First, multiply S$ by «™, that is,
append M zero bits to it. Second divide — by long division — G into Sx™. Keep
in mind that the subtractions in the long division are done modulo 2, so that there
are never any “borrows”: Modulo 2 subtraction is the same as logical exclusive-or
(XOR). Third, ignore the quotient you get. Fourth, when you eventually get to a
remainder, it is the CRC, call it C. C will be a polynomial of degree M — 1 or less,
otherwise you would not have finished the long division. Therefore, in bit string
form, it has M bits, which may include leading zeros. (C' might even be all zeros,
see below.) See [3] for a worked example.

If you work through the above steps in an example, you will see that most of
what you write down in the long-division tableau is superfluous. You are actually just
left-shifting sequential bits of S', from the right, into an M/-bit register. Every time a 1
bit gets shifted off the left end of this register, you zap the register by an XOR with the
M low order bits of G (that is, all the bits of G except its leading 1). When a 0 bit is
shifted off the left end you don’t zap the register. When the last bit that was originally
part of S gets shifted off the left end of the register, what remains is the CRC.

You can immediately recognize how efficiently this procedure can be imple-
mented in hardware. It requires only a shift register with a few hard-wired XOR
taps into it. That is how CRCs are computed in communications devices, by a single

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.3 Cyclic Redundancy and Other Checksums 899

chip (or small part of one). In software, the implementation is not so elegant, since
bit-shifting is not generally very efficient. One therefore typically finds (as in our
implementation below) table-driven routines that pre-calculate the result of a bunch
of shifts and XORs, say for each of 256 possible 8-bit inputs [4].

We can now see how the CRC gets its ability to detect all errors in MW consecutive
bits. Suppose two messages, S and T, differ only within a frame of M bits. Then
their CRCs differ by an amount that is the remainder when G' is divided into
(S —T)a™ = D. Now D has the form of leading zeros (which can be ignored),
followed by some 1’s in an /-bit frame, followed by trailing zeros (which are just
multiplicative factors of x): D = x" F where F is a polynomial of degree at most
M —1andn > 0. Since G is always primitive or primitive times (1 + 2), it is not
divisible by x. So G cannot divide D. Therefore S' and T must have different CRCs.

In most protocols, a transmitted block of data consists of some N data bits,
directly followed by the M bits of their CRC (or the CRC XORed with a constant,
see below). There are two equivalent ways of validating a block at the receiving end.
Most obviously, the receiver can compute the CRC of the data bits, and compare it to
the transmitted CRC bits. Less obviously, but more elegantly, the receiver can simply
compute the CRC of the total block, with N + M bits, and verify that a result of zero
is obtained. Proof: The total block is the polynomial Sa ™/ + C (data left-shifted to
make room for the CRC bits). The definition of C is that Sx™ = QG + C, where
Q is the discarded quotient. But then Sa” + C = QG + C +C = QG (remember
modulo 2), which is a perfect multiple of G. It remains a multiple of G when it gets
multiplied by an additional a on the receiving end, so it has a zero CRC, q.e.d.

A couple of small variations on the basic procedure need to be mentioned [1,3]:
First, when the CRC is computed, the /-bit register need not be initialized to zero.
Initializing it to some other M-bit value (e.g., all 1’s) in effect prefaces all blocks by
a phantom message that would have given the initialization value as its remainder.
It is advantageous to do this, since the CRC described thus far otherwise cannot
detect the addition or removal of any number of initial zero bits. (Loss of an initial
bit, or insertion of zero bits, are common “clocking errors.”) Second, one can add
(XOR) any /-bit constant K to the CRC before it is transmitted. This constant
can either be XORed away at the receiving end, or else it just changes the expected
CRC of the whole block by a known amount, namely the remainder of dividing G
into Kx™. The constant K is frequently “all bits?’ changing the CRC into its ones
complement. This has the advantage of detecting another kind of error that the CRC
would otherwise not find: deletion of an initial 1 bit in the message with spurious
insertion of a 1 bit at the end of the block.

The accompanying function icrc implements the above CRC calculation,
including the possibility of the mentioned variations. Input to the function is a
pointer to an array of characters, and the length of that array. icrc has two “switch”
arguments that specify variations in the CRC calculation. A zero or positive value
of jinit causes the 16-bit register to have each byte initialized with the value
jinit. A negative value of jrev causes each input character to be interpreted as
its bit-reverse image, and a similar bit reversal to be done on the output CRC. You
do not have to understand this; just use the values of jinit and jrev specified in
the table. (If you insist on knowing, the explanation is that serial data ports send
characters least-significant bit first (!), and many protocols shift bits into the CRC
register in exactly the order received.) The table shows how to construct a block

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

900 Chapter 20. Less-Numerical Algorithms

of characters from the input array and output CRC of icrc. You should not need
to do any additional bit-reversal outside of icrc.

The switch jinit has one additional use: When negative it causes the input
value of the array crc to be used as initialization of the register. If you set crc to the
result of the last call to icrc, this in effect appends the current input array to that of
the previous call or calls. Use this feature, for example, to build up the CRC of a
whole file a line at a time, without keeping the whole file in memory.

The routine icrc is loosely based on the function in [4]. Here is how to
understand its operation: First look at the function icrc1. This incorporates one
input character into a 16-bit CRC register. The only trick used is that character bits
are XORed into the most significant bits, eight at a time, instead of being fed into
the least significant bit, one bit at a time, at the time of the register shift. This works
because XOR is associative and commutative — we can feed in character bits any
time before they will determine whether to zap with the generator polynomial. (The
decimal constant 4129 has the generator’s bits in it.)

unsigned short icrci(unsigned short crc, unsigned char onech)

Given a remainder up to now, return the new CRC after one character is added. This routine
is functionally equivalent to icrc(,,1,-1,1), but slower. It is used by icrc to initialize its
table.

{
int i;
unsigned short ans=(crc ~ onech << 8);
for (i=0;i<8;it++) { Here is where 8 one-bit shifts, and some XORs with the
if (ans & 0x8000) generator polynomial, are done.
ans = (ans <<= 1) * 4129;
else
ans <<= 1;
+
return ans;
3

Now look at icrc. There are two parts to understand, how it builds a table
when it initializes, and how it uses that table later on. Go back to thinking about a
character’s bits being shifted into the CRC register from the least significant end. The
key observation is that while 8 bits are being shifted into the register’s low end, all
the generator zapping is being determined by the bits already in the high end. Since
XOR is commutative and associative, all we need is a table of the result of all this
zapping, for each of 256 possible high-bit configurations. Then we can play catch-up
and XOR an input character into the result of a lookup into this table. The only other
content to icrc is the construction at initialization time of an 8-bit bit-reverse table
from the 4-bit table stored in it, and the logic associated with doing the bit reversals.
References [4-6] give further details on table-driven CRC computations.

typedef unsigned char uchar;
#define LOBYTE(x) ((uchar)((x) & OxFF))
#define HIBYTE(x) ((uchar)((x) >> 8))

unsigned short icrc(unsigned short crc, unsigned char *bufptr,

unsigned long len, short jinit, int jrev)
Computes a 16-bit Cyclic Redundancy Check for a byte array bufptr[1..len], using any
of several conventions as determined by the settings of jinit and jrev (see accompanying

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.3 Cyclic Redundancy and Other Checksums 901

table). If jimit is negative, then crc is used on input to initialize the remainder register, in
effect (for crc set to the last returned value) concatenating bufptr to the previous call.
{

unsigned short icrci(unsigned short crc, unsigned char onech) ;

static unsigned short icrctb[256] ,init=0;

static uchar rchr[256];

unsigned short j,cword=crc;

static uchar it[16]={0,8,4,12,2,10,6,14,1,9,5,13,3,11,7,15};

Table of 4-bit bit-reverses.

if (linit) { Do we need to initialize tables?
init=1;
for (j=0;j<=255; j++) {
The two tables are: CRCs of all characters, and bit-reverses of all characters.
icretb[j]=icrc1(j << 8, (uchar)0);
rcehr[j]=(uchar) (it[j & OxF] << 4 | it[j >> 4]);
}
+
if (jinit >= 0) cword=((uchar) jinit) | (((uchar) jinit) << 8);
Initialize the remainder register.
else if (jrev < 0) cword=rchr[HIBYTE(cword)] | rchr[LOBYTE(cword)] << 8;
If not initializing, do we reverse the register?
for (j=1;j<=len; j++) Main loop over the characters in the array.
cword=icrctb[(jrev < 0 ? rchr[bufptr[j]]
bufptr[j]) ~ HIBYTE(cword)] ~ LOBYTE(cword) << 8;
return (jrev >= 0 ? cword : rchr[HIBYTE(cword)] | rchr[LOBYTE(cword)] << 8);
Do we need to reverse the output?

What if you need a 32-bit checksum? For a true 32-bit CRC, you will need
to rewrite the routines given to work with a longer generating polynomial. For
example, 2°? +27 +2°+23+2?+2 +1 is primitive modulo 2, and has nonleading,
nonzero bits only in its least significant byte (which makes for some simplification).
The idea of table lookup on only the most significant byte of the CRC register goes
through unchanged.

If you do not care about the /-consecutive bit property of the checksum, but
rather only need a statistically random 32 bits, then you can use icrc as given
here: Call it once with jrev = | to get 16 bits, and again with jrev = —1 to get
another 16 bits. The internal bit reversals make these two 16-bit CRCs in effect
totally independent of each other.

Other Kinds of Checksums

Quite different from CRCs are the various techniques used to append a decimal
“check digit” to numbers that are handled by human beings (e.g., typed into a
computer). Check digits need to be proof against the kinds of highly structured
errors that humans tend to make, such as transposing consecutive digits. Wagner and
Putter [7] give an interesting introduction to this subject,including specific algorithms.

Checksums now in widespread use vary from fair to poor. The 10-digit ISBN
(International Standard Book Number) that you find on most books, including this
one, uses the check equation

10d; + 9d2 + 8d3 +--+ 2d9+dio =0 (mod 11) (20.3.1)

where do is the right-hand check digit. The character “X” is used to represent a
check digit value of 10. Another popular scheme is the so-called “IBM check,” often

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

902 Chapter 20. Less-Numerical Algorithms

used for account numbers (including, e.g., MasterCard). Here, the check equation is

2d + dy + 2#d3+da+---=0 (mod 10) (20.3.2)

where 2#d means, “multiply d by two and add the resulting decimal digits.” United
States banks code checks with a 9-digit processing number whose check equation is

3a, + Taz + a3 + 3a4 + Ta5 + a6 + 3a7 + 7ag+a9=0 (mod 10) (20.3.3)

The bar code put on many envelopes by the U.S. Postal Service is decoded by
removing the single tall marker bars at each end, and breaking the remaining bars
into 6 or 10 groups of five. In each group the five bars signify (from left to right)
the values 7,4,2,1,0. Exactly two of them will be tall. Their sum is the represented
digit, except that zero is represented as 7 + 4. The S- or 9-digit Zip Code is followed
by a check digit, with the check equation

»

None of these schemes is close to optimal. An elegant scheme due to Verhoeff
is described in [7]. The underlying idea is to use the ten-element dihedral group D5,
which corresponds to the symmetries of a pentagon, instead of the cyclic group of
the integers modulo 10. The check equation is

d;=0 (mod 10) (20.3.4)

a1*f(az)*f?(as)*---*f""(an) = 0 (20.3.5)

where * is (noncommutative) multiplication in Ds, and f' denotes the ith iteration
of a certain fixed permutation. Verhoeff’s method finds all single errors in a string,
and all adjacent transpositions. It also finds about 95% of twin errors (aa — bb),
jump transpositions (acb — bca), and jump twin errors (aca — bcb). Here is an
implementation:

int decchk(char string[], int n, char *ch)
Decimal check digit computation or verification. Returns as ch a check digit for appending
to string[1..n], that is, for storing into string[nt1]. In this mode, ignore the returned
boolean (integer) value. If string[1..n] already ends with a check digit (string[n]), re-
turns the function value true (1) if the check digit is valid, otherwise false (0). In this mode,
ignore the returned value of ch. Note that string and ch contain ASCII characters corre-
sponding to the digits 0-9, not byte values in that range. Other ASCII characters are allowed in
string, and are ignored in calculating the check digit.
{

char c;

int j,k=0,m=0;

6,3,6,3,6,3,7,0,1,5,
static int ij[10][10]={o,
»4,0,1,7,8,9,5,6,
»8,7,6,0,4,3,2,1,
7,6,5,9,3,2,1,0,4,
Group multiplication and
for (j=0;j<n;jt++) { Look at successive characters.
c=string[j];

if (c >= 48 && c <= 57) Ignore everything except digits.

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.4 Huffman Coding and Compression of Data 903

k=ij(k] [ip[(c+2) % 10] [7 & m++]];

}

for (j=0;j<=9; j++) Find which appended digit will check properly.
if (!ij(k] [ip(j][m & 7]]) break;

*ch=j+48 Convert to ASCII.

return k:

CITED REFERENCES AND FURTHER READING:

McNamara, J.E. 1982, Technical Aspects of Data Communication, 2nd ed. (Bedford, MA: Digital
Press). [1]

da Cruz, F. 1987, Kermit, A File Transfer Protocol (Bedford, MA: Digital Press). [2]

Morse, G. 1986, Byte, vol. 11, pp. 115-124 (September). [3]

LeVan, J. 1987, Byte, vol. 12, pp. 339-341 (November). [4]

Sarwate, D.V. 1988, Communications of the ACM, vol. 31, pp. 1008-1013. [5]

Griffiths, G., and Stones, G.C. 1987, Communications of the ACM, vol. 30, pp. 617-620. [6]

Wagner, N.R., and Putter, P'S. 1989, Communications of the ACM, vol. 32, pp. 106-110. [7]

20.4 Huffman Coding and Compression of Data

A lossless data compression algorithm takes a string of symbols (typically
ASCII characters or bytes) and translates it reversibly into another string, one that is
on the average of shorter length. The words “on the average” are crucial; it is obvious
that no reversible algorithm can make all strings shorter — there just aren’t enough
short strings to be in one-to-one correspondence with longer strings. Compression
algorithms are possible only when, on the input side, some strings, or some input
symbols, are more common than others. These can then be encoded in fewer bits
than rarer input strings or symbols, giving a net average gain.

There exist many, quite different, compression techniques, corresponding to
different ways of detecting and using departures from equiprobability in input strings.
In this section and the next we shall consider only variable length codes with defined
word inputs. In these, the input is sliced into fixed units, for example ASCII
characters, while the corresponding output comes in chunks of variable size. The
simplest such method is Huffman coding [1], discussed in this section. Another
example, arithmetic compression, is discussed in §20.5.

At the opposite extreme from defined-word, variable length codes are schemes
that divide up the input into units of variable length (words or phrases of English text,
for example) and then transmit these, often with a fixed-length output code. The most
widely used code of this type is the Ziv-Lempel code [2]. References [3-6] give the
flavor of some other compression techniques, with references to the large literature.

The idea behind Huffman coding is simply to use shorter bit patterns for more
common characters. We can make this idea quantitative by considering the concept
of entropy. Suppose the input alphabet has ',;, characters, and that these occur in
the input string with respective probabilities p;, i = 1,...,.Nen, so that )> p; = 1.
Then the fundamental theorem of information theory says that strings consisting of

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

904 Chapter 20. _Less-Numerical Algorithms

independently random sequences of these characters (a conservative, but not always
realistic assumption) require, on the average, at least

H =—)J> pi logs pi (20.4.1)

bits per character. Here H is the entropy of the probability distribution. Moreover,
coding schemes exist which approach the bound arbitrarily closely. For the case of
equiprobable characters, with all p; = 1/Ncn, one easily sees that H = logy Nen,
which is the case of no compression at all. Any other set of p;’s gives a smaller
entropy, allowing some useful compression.

Notice that the bound of (20.4.1) would be achieved if we could encode character
i with a code of length L; = — logy p; bits: Equation (20.4.1) would then be the
average )* p;L;. The trouble with such a scheme is that — log, p; is not generally
an integer. How can we encode the letter “Q” in 5.32 bits? Huffman coding makes
a stab at this by, in effect, approximating all the probabilities p; by integer powers
of 1/2, so that all the D,’s are integral. If all the p;’s are in fact of this form, then
a Huffman code does achieve the entropy bound H.

The construction of a Huffman code is best illustrated by example. Imagine
a language, Vowellish, with the N., = 5 character alphabet A, E, I, O, and U,
occurring with the respective probabilities 0.12, 0.42, 0.09, 0.30, and 0.07. Then the
construction of a Huffman code for Vowellish is accomplished in the following table:

Node | Stage: 1 2 3 4 5
1 A: 0.12 0.128
2 E: 0.42 0.42 0.42 0425
3 I 0.098
4 0.30 0.30 0.30 8
5 0.078
6 UL 0.168
7 AUL 0.28 8
8 AUIO: 0.58 8
9 EAUIO: 1.00

Here is how it works, proceeding in sequence through N.;, stages, represented
by the columns of the table. The first stage starts with N.», nodes, one for each
letter of the alphabet, containing their respective relative frequencies. At each stage,
the two smallest probabilities are found, summed to make a new node, and then
dropped from the list of active nodes. (A “block” denotes the stage where a node is
dropped.) All active nodes (including the new composite) are then carried over to
the next stage (column). In the table, the names assigned to new nodes (e.g., AUI)
are inconsequential. In the example shown, it happens that (after stage 1) the two

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

20.4 Huffman Coding and Compression of Data 905

9 | EAUIO | 1.00

2 0.42 8| AUIO |0.58

7| AU! {0.28 4(Q)0.30

1 0.12 6 Ul 0.16

5 0.07 3 0.09

Figure 20.4.1. | Huffman code for the fictitious language Vowellish, in tree form. A letter (A, E, I,
O, or U) is encoded or decoded by traversing the tree from the top down; the code is the sequence of
0’s and 1’s on the branches. The value to the right of each node is its probability; to the left, its node
number in the accompanying table.

smallest nodes are always an original node and a composite one; this need not be
true in general: The two smallest probabilities might be both original nodes, or both
composites, or one of each. At the last stage, all nodes will have been collected into
one grand composite of total probability 1.

Now, to see the code, you redraw the data in the above table as a tree (Figure
20.4.1). As shown, each node of the tree corresponds to a node (row) in the table,
indicated by the integer to its left and probability value to its right. Terminal nodes,
so called, are shown as circles; these are single alphabetic characters. The branches
of the tree are labeled 0 and 1. The code for a character is the sequence of zeros and
ones that lead to it, from the top down. For example, E is simply 0, while U is 1010.

Any string of zeros and ones can now be decoded into an alphabetic sequence.
Consider, for example, the string 1011111010. Starting at the top of the tree we
descend through 1011 to I, the first character. Since we have reached a terminal
node, we reset to the top of the tree, next descending through 11 to O. Finally 1010
gives U. The string thus decodes to IOU.

These ideas are embodied in the following routines. Input to the first routine
hufmak is an integer vector of the frequency of occurrence of the nchin = N <p,
alphabetic characters, i.e., a set of integers proportional to the p;’s. hufmak, along
with hufapp, which it calls, performs the construction of the above table, and also the
tree of Figure 20.4.1. The routine utilizes a heap structure (see §8.3) for efficiency;
for a detailed description, see Sedgewick [7].

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

906 Chapter 20. Less-Numerical Algorithms

#include "nrutil.h"

typedef struct {
unsigned long *icod,*ncod,*left,*right ,nch,nodemax;
} huffcode;

void hufmak(unsigned long nfreq[], unsigned long nchin, unsigned long *ilong,
unsigned long *nlong, huffcode *hcode)
Given the frequency of occurrence table nfreq[1..nchin] of nchin characters, construct the
Huffman code in the structure hcode. Returned values ilong and nlong are the character
number that produced the longest code symbol, and the length of that symbol. You should
check that nlong is not larger than your machine's word length.
{
void hufapp(unsigned long index[], unsigned long nprob[], unsigned long n,
unsigned long i);
int ibit;
long node, *up;
unsigned long j,k,*index,n,nused,*nprob;
static unsigned long setbit[32]={0x1L,0x2L,0x4L,0x8L,0x10L,0x20L,
0x40L, 0x80L,0x100L ,0x200L,0x400L ,0x800L,0x1000L,0x2000L,
0x4000L , 0x8000L ,0x10000L ,0x20000L , 0x40000L , 0x80000L ,0x100000L,
0x200000L , 0x400000L , 0x800000L , 0x1000000L,,0x2000000L ,,0x4000000L,
0x8000000L , 0x10000000L , 0x20000000L , 0x40000000L , 0x80000000L} ;

hcode->nch=nchin; Initialization.

index=lvector (1, (long) (2*hcode->nch-1)) ;

up=(long *)lvector(1, (long) (2*hcode->nch-1)) ; Vector that will keep track of
nprob=lvector (1, (long) (2*hcode->nch-1)) ; heap.

for (nused=0,j=1;j<=hcode->nch; j++) {
nprob[j]=nfreq[j];
hcode->icod[j]=hcode->ncod[j]
if (nfreq[j]) index[++nused]=j;

}

for (j=nused;j>=1;j--) hufapp(index,nprob,nused, j) ;

Sort nprob into a heap structure in index.

k=hcode->nch;

while (nused > 1) { Combine heap nodes, remaking
node=index [1]; the heap at each stage.
index [1] =index [nused--] ;
hufapp (index ,nprob,nused, 1);
nprob[++k]=nprob[index[1]]+nprob [node] ;

hcode->left [k]=node; Store left and right children of a
hcode->right [k] =index[1] ; node.

up[index[1]] = -(long)k; Indicate whether a node is a left
up [node] =index [1] =k; or right child of its parent.

hufapp (index ,nprob,nused, 1);
+
up [hcode->nodemax=k]=0;
for (j=1;j<=hcode->nch; j++) { Make the Huffman code from
if (mprob[j]) { the tree.
for (n=0,ibit=0,node=up[j] ;node;node=up [node] ,ibit++) {
if (node < 0) {
n |= setbit[ibit];
node = -node;
}
}
hcode->icod[jJ=n;
hcode->ncod[j]=ibit;

3j<=hcode->nch; j++) {
if (hcode->ncod[j] > *nlong) {
*nlong=hcode->ncod [j] ;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.4 Huffman Coding and Compression of Data 907

*ilong=j-1;

+
+
free_lvector(nprob,1, (long) (2*hcode->nch-1)) ;
free_lvector((unsigned long *)up,1, (long) (2*hcode->nch-1)) ;
free_lvector(index,1, (long) (2*hcode->nch-1)) ;

void hufapp(unsigned long index[], unsigned long nprob[], unsigned long n,
unsigned long i)

Used by hufmak to maintain a heap structure in the array index[1..1].

{
unsigned long j,k;

k=index [i] ;
while (i <= (n>>1)) {
if ((j =i << 1) <n && nprob[index[j]] > nprob[index[j+1]]) j++;
if (nprob[k] <= nprob[index[j]]) break;
index [i] =index[j] ;
irji
+

index [i] =k;

Note that the structure hcode must be defined and allocated in your main
program with statements like this:

#include "nrutil.h"
#define MC 512 Maximum anticipated value of nchin in hufmak.
#define MQ (2*MC-1)
typedef struct {
unsigned long *icod,*ncod,*left,*right ,nch,nodemax;
} huffcode;

huffcode hcode;

hcode.icod=(unsigned long *)lvector(1,MQ) ; Allocate space within hcode.
hcode.ncod=(unsigned long *)lvector(1,MQ) ;

hcode.left=(unsigned long *)lvector(1,MQ) ;

hcode. =(unsigned long *)lvector(1,MQ);

3j++) hcode.icod[j]=hcode.ncod[j]=0;

Once the code is constructed, one encodes a string of characters by repeated calls
to hufenc, which simply does a table lookup of the code and appends it to the
output message.

#include <stdio.h>
#include <stdlib.h>

typedef struct {
unsigned long *icod,*ncod,*left,*right ,nch,nodemax;
} huffcode;

void hufenc(unsigned long ich, unsigned char **codep, unsigned long *lcode,
unsigned long *nb, huffcode *hcode)

Huffman encode the single character ich (in the range 0..nch-1) using the code in the

structure hcode, write the result to the character array *codep[1..1code] starting at bit

nb (whose smallest valid value is zero), and increment nb appropriately. This routine is called

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

908 Chapter 20. Less-Numerical Algorithms

repeatedly to encode consecutive characters in a message, but must be preceded by a single
initializing call to hufmak, which constructs hcode.

{
void nrerror(char error_text[]);
int 1,n;
unsigned long k,nc;
static unsigned long setbit[32]={0x1L,0x2L,0x4L,0x8L,0x10L,0x20L,
0x40L, 0x80L,0x100L ,0x200L,0x400L ,0x800L,0x1000L,0x2000L,
0x4000L , 0x8000L ,0x10000L ,0x20000L , 0x40000L , 0x80000L ,0x100000L,
0x200000L , 0x400000L , 0x800000L , 0x1000000L,,0x2000000L ,,0x4000000L,
0x8000000L , 0x10000000L , 0x20000000L , 0x40000000L , 0x80000000L} ;
k=ich+1;
Convert character range 0..nch-1 to array index range 1..nch.
if (k > hcode->nch || k < 1) nrerror("ich out of range in hufenc.");
for (n=hcode->ncod[k]-1;n>=0;n--,++(*nb)) { Loop over the bits in the stored
nc=(*nb >> 3); Huffman code for ich.
if (++nc >= *lcode) {
fprintf(stderr,"Reached the end of the ’code’ array.\n");
fprintf(stderr,"Attempting to expand its size.\n");
*lcode *= 1.5;
if ((*codep=(unsigned char *)realloc(*codep,
(unsigned) (*lcode*sizeof (unsigned char)))) == NULL) {
nrerror("Size expansion failed.");
3
}
1=(*nb) & 7;
if (!1) (*codep) [nc]=0; Set appropriate bits in code.
if (hcode->icod[k] & setbit[n]) (*codep) [nc] |= setbit[1];
+
}

Decoding a Huffman-encoded message is slightly more complicated. The
coding tree must be traversed from the top down, using up a variable number of bits:

typedef struct {
unsigned long *icod,*ncod,*left,*right ,nch,nodemax;
} huffcode;

void hufdec(unsigned long *ich, unsigned char *code, unsigned long lcode,
unsigned long *nb, huffcode *hcode)
Starting at bit number nb in the character array code[1..1code], use the Huffman code stored
in the structure hcode to decode a single character (returned as ich in the range 0. .nch-1)
and increment nb appropriately. Repeated calls, starting with nb = 0 will return successive
characters in a compressed message. The returned value ich=nch indicates end-of-message.
The structure hcode must already have been defined and allocated in your main program, and
also filled by a call to hufmak.
{
long nc,node;
static unsigned char setbit[8]={0x1,0x2,0x4,0x8,0x10,0x20,0x40,0x80};

node=hcode->nodemax ;

for (;;) { Set node to the top of the decoding tree, and loop
nc=(*nb >> 3); until a valid character is obtained.
if (++nc > Icode) { Ran out of input; with ich=nch indicating end of
*ich=hcode->nch; message.
return;
}

node=(code[nc] & setbit[7 & (*nb)++] 7
hcode->right [node] : hcode->left [node]) ;
Branch left or right in tree, depending on its value.
if (node <= hcode->nch) {If we reach a terminal node, we have a complete
character and can return.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.4 Huffman Coding and Compression of Data 909

*ich=node-1;
return;

For simplicity, hufdec quits when it runs out of code bytes; if your coded
message is not an integral number of bytes, and if NV.;, is less than 256, hufdec can
return a spurious final character or two, decoded from the spurious trailing bits in
your last code byte. If you have independent knowledge of the number of characters
sent, you can readily discard these. Otherwise, you can fix this behavior by providing
a bit, not byte, count, and modifying the routine accordingly. (When N <p, is 256 or
larger, hufdec will normally run out of code in the middle of a spurious character,
and it will be discarded.)

Run-Length Encoding

For the compression of highly correlated bit-streams (for example the black or
white values along a facsimile scan line), Huffman compression is often combined
with run-length encoding: Instead of sending each bit, the input stream is converted to
a series of integers indicating how many consecutive bits have the same value. These
integers are then Huffman-compressed. The Group 3 CCITT facsimile standard
functions in this manner, with a fixed, immutable, Huffman code, optimized for a
set of eight standard documents [8,9].

CITED REFERENCES AND FURTHER READING:

Gallager, R.G. 1968, Information Theory and Reliable Communication (New York: Wiley).

Hamming, R.W. 1980, Coding and Information Theory (Englewood Cliffs, NJ: Prentice-Hall).

Storer, J.A. 1988, Data Compression: Methods and Theory (Rockville, MD: Computer Science
Press).

Nelson, M. 1991, The Data Compression Book (Redwood City, CA: M&T Books).

Huffman, D.A. 1952, Proceedings of the Institute of Radio Engineers, vol. 40, pp. 1098-1101. [1]

Ziv, J., and Lempel, A. 1978, IEEE Transactions on Information Theory, vol. IT-24, pp. 530-536.
[2]

Cleary, J.G., and Witten, I.H. 1984, IEEE Transactions on Communications, vol. COM-32,
pp. 396-402. [3]

Welch, T.A. 1984, Computer, vol. 17, no. 6, pp. 8-19. [4]

Bentley, J.L., Sleator, D.D., Tarjan, R.E., and Wei, V.K. 1986, Communications of the ACM,
vol. 29, pp. 320-330. [5]

Jones, D.W. 1988, Communications of the ACM, vol. 31, pp. 996-1007. [6]

Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley), Chapter 22. [7]
Hunter, R., and Robinson, A.H. 1980, Proceedings of the IEEE, vol. 68, pp. 854-867. [8]
Marking, M.P. 1990, The C Users’ Journal, vol. 8, no. 6, pp. 45-54. [9]

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

910 Chapter 20. _Less-Numerical Algorithms

20.5 Arithmetic Coding

We saw in the previous section that a perfect (entropy-bounded) coding scheme
would use L; = — logy p; bits to encode character i (in the range 1 < i < Nn),
if p; is its probability of occurrence. Huffman coding gives a way of rounding the
L,’s to close integer values and constructing a code with those lengths. Arithmetic
coding [1], which we now discuss, actually does manage to encode characters using
noninteger numbers of bits! It also provides a convenient way to output the result
not as a stream of bits, but as a stream of symbols in any desired radix. This latter
property is particularly useful if you want, e.g., to convert data from bytes (radix
256) to printable ASCII characters (radix 94), or to case-independent alphanumeric
sequences containing only A-Z and 0-9 (radix 36).

In arithmetic coding, an input message of any length is represented as a real
number R in the range 0 < R < 1. The longer the message, the more precision
required of R. This is best illustrated by an example, so let us return to the fictitious
language, Vowellish, of the previous section. Recall that Vowellish has a 5 character
alphabet (A, E, I, O, U), with occurrence probabilities 0.12, 0.42, 0.09, 0.30, and
0.07, respectively. Figure 20.5.1 shows how a message beginning “JOU” is encoded:
The interval [0,1) is divided into segments corresponding to the 5 alphabetical
characters; the length of a segment is the corresponding p;. We see that the first
message character, “I’’, narrows the range of R to 0.37 < R < 0.46. This interval is
now subdivided into five subintervals, again with lengths proportional to the p ;’s. The
second message character, “O”, narrows the range of R to 0.3763 < R < 0.4033.
The “U” character further narrows the range to 0.37630 < R < 0.37819. Any value
of R in this range can be sent as encoding “IOU”. In particular, the binary fraction
-011000001 is in this range, so “IOU” can be sent in 9 bits. (Huffman coding took
10 bits for this example, see $20.4.)

Of course there is the problem of knowing when to stop decoding. The fraction
-011000001 represents not simply “IOU,” but “IOU. .. ”” where the ellipses represent
an infinite string of successor characters. To resolve this ambiguity, arithmetic
coding generally assumes the existence of a special N.,, + 1th character, EOM
(end of message), which occurs only once at the end of the input. Since EOM
has a low probability of occurrence, it gets allocated only a very tiny piece o:
the number line.

In the above example, we gave F as a binary fraction. We could just as wel
have output it in any other radix, e.g., base 94 or base 36, whatever is convenient
for the anticipated storage or communication channel.

You might wonder how one deals with the seemingly incredible precision
required of R for a long message. The answer is that R is never actually represented
all at once. At any give stage we have upper and lower bounds for R represente:
as a finite number of digits in the output radix. As digits of the upper and lower
bounds become identical, we can left-shift them away and bring in new digits at the
low-significance end. The routines below have a parameter NWK for the number of
working digits to keep around. This must be large enough to make the chance o:
an accidental degeneracy vanishingly small. (The routines signal if a degeneracy
ever occurs.) Since the process of discarding old digits and bringing in new ones is
performed identically on encoding and decoding, everything stays synchronized.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.5 Arithmetic Coding 911

10 5 0.46 - 0.4033 -— 0.37819
4A 4A 4A A
09 4 J 0.3780
q q 0.400 4
08 4 4 7 ; =
: 044 4 J 0.3778
ote Ie le 0.3776 4 E
4 043 4 0.395 4
06 4 4 4 03774 4
a 042 4 7
05 4 J 0.390 4 03772
J 041 4 7
o4 4! 4! J! 03770 4
J 040 J
034 q 0.385 >| 0.3768 +
jo jo jo [e)
024 0.39 ; 7 0.3766 4
| q 0.380 4
014 0.38 5 7 0.3764 4
ju JU jdu U
0.0 0.37 0.3763 0.37630
Figure 20.5.1. Arithmetic coding of the message “IOU...” in the fictitious language Vowellish.

Successive characters give successively finer subdivisions of the initial interval between 0 and 1. The final
value can be output as the digits of a fraction in any desired radix. Note how the subinterval allocated
to a character is proportional to its probability of occurrence.

The routine arcmak constructs the cumulative frequency distribution table used
to partition the interval at each stage. In the principal routine arcode, when an
interval of size jdif is to be partitioned in the proportions of some n to some ntot,
say, then we must compute (n*jdif) /ntot. With integer arithmetic, the numerator
is likely to overflow; and, unfortunately, an expression like jdif/(ntot/n) is not
equivalent. In the implementation below, we resort to double precision floating
arithmetic for this calculation. Not only is this inefficient, but different roundoff
errors can (albeit very rarely) make different machines encode differently, though any
one type of machine will decode exactly what it encoded, since identical roundoff
errors occur in the two processes. For serious use, one needs to replace this floating
calculation with an integer computation in a double register (not available to the
C programmer).

The internally set variable minint, which is the minimum allowed number
of discrete steps between the upper and lower bounds, determines when new low-
significance digits are added. minint must be large enough to provide resolution of
all the input characters. That is, we must have p; X minint > 1 for alli. A value
of 100N-p, or 1.1/ min p;, whichever is larger, is generally adequate. However, for
safety, the routine below takes minint to be as large as possible, with the product
minint*nradd just smaller than overflow. This results in some time inefficiency,
and in a few unnecessary characters being output at the end of a message. You can

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

912 Chapter 20. _Less-Numerical Algorithms

decrease minint if you want to live closer to the edge.

A final safety feature in arcmak is its refusal to believe zero values in the table
nfreq; a 0 is treated as if it were a 1. If this were not done, the occurrence in a
message of a single character whose nfreq entry is zero would result in scrambling
the entire rest of the message. If you want to live dangerously, with a very slightly
more efficient coding, you can delete the IMAX( ,1) operation.

#include "nrutil.h"

#include <limits.h> ANSI header file containing integer ranges.

#define MC 512

#ifdef ULONG_MAX Maximum value of unsigned long.

#tdefine MAXINT (ULONG_MAX >> 1)

#else

#define MAXINT 2147483647

#endif

Here MC is the largest anticipated value of nchh; MAXINT is a large positive integer that does
not overflow.

typedef struct {
unsigned long *ilob,*iupb, *ncumfq, jdif,nc,minint ,nch,ncum,nrad;
} arithcode;

void arcmak(unsigned long nfreq[], unsigned long nchh, unsigned long nradd,
arithcode *acode)

Given a table nfreq[1..nchh] of the frequency of occurrence of nchh symbols, and given

a desired output radix nradd, initialize the cumulative frequency table and other variables for

arithmetic compression in the structure acode.

{

unsigned long j;

if (mchh > MC) nrerror("input radix may not exceed MC in arcmak.");
if (mradd > 256) nrerror("output radix may not exceed 256 in arcmak.");

acode->minint=MAXINT/nradd;
acode->nch=nchh;
acode->nrad=nradd
acode->ncumfq[1]=0;
for (j=2;j<acode->nch+1 ; j++)

acode—>ncumfq[j]=acode->ncumfq[j-1] +IMAX(nfreq[j-1],1);
acode->ncum=acode->ncumf q [acode->nch+2]=acode->ncumf q[acode->nch+1]+1;

The structure acode must be defined and allocated in your main program with
statements like this:

#include "nrutil.h"
#define MC 512 Maximum anticipated value of nchh in arcmak.
#define NWK 20 Keep this value the same as in arcode, below.
typedef struct {

unsigned long *ilob,*iupb, *ncumfq, jdif,nc,minint ,nch,ncum,nrad;
} arithcode;

arithcode acode;
acode.ilob=(unsigned long *)lvector(1,NWK) ; Allocate space within acode.

acode.iupb=(unsigned long *)lvector(1,NWK) ;
acode.ncumfq=(unsigned long *)lvector(1,MC+2) ;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

20.5 Arithmetic Coding 913

Individual characters in a message are coded or decoded by the routine arcode,
which in turn uses the utility arcsum.

#include <stdio.h>

#include <stdlib.h>

#define NWK 20

#define JTRY(j,k,m) ((long) ((( (double) (k))*( (double) (j)))/((double) (m))))

This macro is used to calculate (k*j) /m without overflow. Program efficiency can be improved
by substituting an assembly language routine that does integer multiply to a double register.

typedef struct {
unsigned long *ilob,*iupb, *ncumfq, jdif,nc,minint ,nch,ncum,nrad;
} arithcode;

void arcode(unsigned long *ich, unsigned char **codep, unsigned long *lcode,

unsigned long *lcd, int isign, arithcode *acode)
Compress (isign = 1) or decompress (isign = —1) the single character ich into or out of
the character array *codep[1..1lcode], starting with byte *codep[1cd] and (if necessary)
incrementing lcd so that, on return, lcd points to the first unused byte in *codep. Note
that the structure acode contains both information on the code, and also state information on
the particular output being written into the array *codep. An initializing call with isign=0
is required before beginning any *codep array, whether for encoding or decoding. This is in
addition to the initializing call to arcmak that is required to initialize the code itself. A call
with ich=nch (as set in arcmak) has the reserved meaning “end of message.”
{

void arcsum(unsigned long iin[], unsigned long iout[], unsigned long ja,

int nwk, unsigned long nrad, unsigned long nc);

void nrerror(char error_text[]);

int j,k;

unsigned long ihi,ja,jh,jl,m;

if (!isign) { Initialize enough digits of the upper and lower bounds.
acode->jdif=acode->nrad-1;
for (j=NWK;j>=1;j--) {

acode->iupb[j]=acode->nrad-1;

acode->ilob[j]=0;

acode->nc=j;

if (acode->jdif > acode->minint) return; Initialization complete.
acode->jdif=(acode->jdif+1) *acode->nrad-1;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

}
nrerror("NWK too small in arcode.");
} else {
if (isign > 0) { If encoding, check for valid input character.
if (*ich > acode->nch) nrerror("bad ich in arcode.");
}
else { If decoding, locate the character ich by bisection.
ja=(*codep) [*1cd]-acode->ilob[acode->nc] ;
for (j=acode->nct1; j<=NWK; j++) {
ja *= acode->nrad;
ja += ((*codep) [*lcd+j-acode->nc] -acode->ilob[j]);
}
ihi=acode->nch+1;
*ich=0;
while (ihi-(*ich) > 1) {
m=(*ich+ihi)>>1;
if (ja >= JTRY(acode->jdif ,acode->ncumfq[mt+1] ,acode->ncum) )
*ic) 3
else ih ;
}
if (*ich == acode->nch) return; Detected end of message.
}

Following code is common for encoding and decoding. Convert character ich to a new
subrange [ilob,iupb).

914 Chapter 20. _Less-Numerical Algorithms

jh=JTRY (acode->jdif ,acode->ncumfq[*ich+2] ,acode->ncum) ;
j1=JTRY (acode->jdif ,acode->ncumfq[*ich+1] , acode->ncum) ;
acode->jdif=jh-jl;
arcsum(acode->ilob,acode->iupb, jh, NWK,acode->nrad,acode->nc) ;
arcsum(acode->ilob,acode->ilob, j1,NWK,acode->nrad,acode->nc) ;
How many leading digits to output (if encoding) or skip over?
for (j=acode->nc; j<=NWK; j++) {
if (*ich != acode->nch && acode->iupb[j] != acode->ilob[j]) break;
if (*lcd > *lcode) {
fprintf(stderr,"Reached the end of the ’code’ array.\n");
fprintf(stderr,"Attempting to expand its size.\n");
*lcode += *lcode/2;
if ((*codep=(unsigned char *)realloc(*codep,
(unsigned) (*lcode*sizeof (unsigned char)))) == NULL) {
nrerror("Size expansion failed");

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

}
3
if (isign > 0) (*codep) [*1cd]=(unsigned char)acode->ilob[j];
++(#1cd) ;
}
if (j > NWK) return; Ran out of message. Did someone forget to encode a
acode->nc=j; terminating ncd?
for (j=0;acode->jdif<acode->minint ; j++) How many digits to shift?

acode->jdif *= acode->nrad;
if (acode->nc-j < 1) nrerror("NWK too small in arcode.");
if (j) { Shift them.
for (k=acode->nc;k<=NWK;k++) {
acode->iupb[k-j]=acode->iupb[k] ;
acode->ilob[k-j]=acode->ilob[k] ;

3
}
acode->ne -= j;
for (k=NWK-j+1;k<=NWK;k++) acode->iupb[k] =acode->ilob[k]=0;
+
return; Normal return.

void arcsum(unsigned long iin[], unsigned long iout[], unsigned long ja,

int nwk, unsigned long nrad, unsigned long nc)
Used by arcode. Add the integer ja to the radix nrad multiple-precision integer iin[nc. .nwk].
Return the result in iout (nc. .nwk] .

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

t
int j,karry=0;
unsigned long jtmp;
for (j=nwk;j>nc;j--) {
jtm a;
ja /= nrad;
iout [j]=iin[j]+(jtmp-ja*nrad)+karry;
if Ciout[j] >= nrad) {
iout[j] -= nrad;
karry=1;
} else karry=0;
+
iout [nc] =iin[nc]+jatkarry;
}

If radix-changing, rather than compression, is your primary aim (for example
to convert an arbitrary file into printable characters) then you are of course free to
set all the components of nfreq equal, say, to 1.

20.6 Arithmetic at Arbitrary Precision 915

CITED REFERENCES AND FURTHER READING:

Bell, T.C., Cleary, J.G., and Witten, I.H. 1990, Text Compression (Englewood Cliffs, NJ: Prentice-
Hall).

Nelson, M. 1991, The Data Compression Book (Redwood City, CA: M&T Books).

Witten, |.H., Neal, R.M., and Cleary, J.G. 1987, Communications of the ACM, vol. 30, pp. 520—
540. [1]

20.6 Arithmetic at Arbitrary Precision

Let’s compute the number 7 to a couple of thousand decimal places. In doing
so, we’ll learn some things about multiple precision arithmetic on computers and
meet quite an unusual application of the fast Fourier transform (FFT). We’ll also
develop a set of routines that you can use for other calculations at any desired level
of arithmetic precision.

To start with, we need an analytic algorithm for 7. Useful algorithms
are quadratically convergent, i.e., they double the number of significant digits at
each iteration. Quadratically convergent algorithms for 7 are based on the AGM
(arithmetic geometric mean) method, which also finds application to the calculation
of elliptic integrals (cf. §6.11) and in advanced implementations of the ADI method
for elliptic partial differential equations (§19.5). Borwein and Borwein [1] treat this
subject, which is beyond our scope here. One of their algorithms for 7 starts with
the initializations

Xo = v2
Tm =2+ V2 (20.6.1)
Yo = V2
and then, for i = 0,1,..., repeats the iteration
1 => 1
Xigi = 3 (ve + =)
. _n Xigitl
ann Os Aral (20.6.2)

1
Yi Xin +
* VXi41

Yi+1

Yin =

The value 7 emerges as the limit 7.

Now, to the question of how to do arithmetic to arbitrary precision: In a
high-level language like C, a natural choice is to work in radix (base) 256, so that
character arrays can be directly interpreted as strings of digits. At the very end of
our calculation, we will want to convert our answer to radix 10, but that is essentially
a frill for the benefit of human ears, accustomed to the familiar chant, “three point

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
So
33
gg
ae
=o
32.
8a
3a
aa
so
es
az
oe
$3
22
osc
OG
as
26
<*
oe
Ae
gx
38
oo
35
23
eB
go
a0
a
28
23
a2
<3
Ue
oz
$3
Pe
fa
as
3
ae
28
s
SB
25
co
§5
an
ga
ok
g8F
3
a5
23
3g
3s
oe
gs
o
a2
gg
2
ss
2
Do
Oo
ae
2s
<a
Bo
=3
3B
$8
25
eo
°3

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

916 Chapter 20. Less-Numerical Algorithms

one four one five nine. .. 2’ For any less frivolous calculation, we would likely never
leave base 256 (or the thence trivially reachable hexadecimal, octal, or binary bases).

We will adopt the convention of storing digit strings in the “human” ordering,
that is, with the first stored digit in an array being most significant, the last stored digit
being least significant. The opposite convention would, of course, also be possible.
“Carries,” where we need to partition a number larger than 255 into a low-order
byte and a high-order carry, present a minor programming annoyance, solved, in the
routines below, by the use of the macros LOBYTE and HIBYTE.

It is easy at this point, following Knuth [2], to write a routine for the “fast”
arithmetic operations: short addition (adding a single byte to a string), addition,
subtraction, short multiplication (multiplying a string by a single byte), short
division, ones-complement negation; and a couple of utility operations, copying
and left-shifting strings. (On the diskette, these functions are all in the single
file mpops.c.)

#define LOBYTE(x) ((unsigned char) ((x) & Oxff))
#define HIBYTE(x) ((unsigned char) ((x) >> 8 & Oxff))

Multiple precision arithmetic operations done on character strings, interpreted as radix 256
numbers. This set of routines collects the simpler operations.

void mpadd(unsigned char w[], unsigned char u[], unsigned char v[], int n)
Adds the unsigned radix 256 integers u[1..n] and v[1..n] yielding the unsigned integer
w(t. .nt1].

t
int j;
unsigned short ireg=0;
for (j= =1;j--) ¢
ireg=u[j]+v[j]+HIBYTE(ireg) ;
w[j+1] =LOBYTE(ireg) ;
+
w(1]=HIBYTE(ireg) ;
}

void mpsub(int *is, unsigned char w[], unsigned char u[], unsigned char v[],
int n)

Subtracts the unsigned radix 256 integer v[1..n] from u[1..n] yielding the unsigned integer

w[1..n]. If the result is negative (wraps around), is is returned as —1; otherwise it is returned

as 0.

{
int j;
unsigned short ireg=256;
3j>=1;j--) {
55+u[j]-v(j]+HIBYTE(ireg) ;
w[j]=LOBYTE(ireg) ;
+
*is=HIBYTE(ireg)-1;
3

void mpsad(unsigned char w[], unsigned char u[], int n, int iv)
Short addition: the integer iv (in the range 0 < iv < 255) is added to the unsigned radix 256
integer u[1..n], yielding w[1..n+1].
{
int j;
unsigned short ireg;

ireg=256*iv;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.6 Arithmetic at Arbitrary Precision 917

3j>=1;j--) f
(j] +HIBYTE(ireg) ;
w(j+1] =LOBYTE(ireg) ;

+
w(1]=HIBYTE(ireg) ;

}

void mpsmu(unsigned char w[], unsigned char u[], int n, int iv)
Short multiplication: the unsigned radix 256 integer u[1..n] is multiplied by the integer iv
(in the range 0 < iv < 255), yielding w[1..n+1].

{
int j;
unsigned short ireg=0;
for (j=n;j>=1;j--) {
ireg=u[j] *iv+HIBYTE(ireg) ;
w[j+1] =LOBYTE(ireg) ;
+
w(1]=HIBYTE(ireg) ;
3

void mpsdv(unsigned char w[], unsigned char u[], int n, int iv, int *ir)
Short division: the unsigned radix 256 integer u[1..n] is divided by the integer iv (in the
range 0 < iv < 255), yielding a quotient w[1..n] and a remainder ir (with 0 < ir < 255).

for (j=1;j<=n;j++) {
i=256*(*ir)+ulj];
(unsigned char) (i/iv);

void mpneg(unsigned char u[], int n)
Ones-complement negate the unsigned radix 256 integer u[1..n].

{
int j;
unsigned short ireg=256;
for (j=n;j>=1;j--) {
ireg=255-u[j]+HIBYTE(ireg) ;
u[j]=LOBYTE(ireg) ;
+
3

void mpmov(unsigned char u[], unsigned char v[], int n)
Move v[1..n] onto u[1..n].
{

int j;

for (j=1;j<=n;j++) uljl=v[jl;
}

void mplsh(unsigned char u[], int n)
Left shift u(2..n+1) onto u[1..n].
t

int j;

for (j=1;j<=n;j++) ulj]=uljt+i];

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

918 Chapter 20. Less-Numerical Algorithms

Full multiplication of two digit strings, if done by the traditional hand method,
is not a fast operation: In multiplying two strings of length N, the multiplicand
would be short-multiplied in turn by each byte of the multiplier, requiring O(N ?)
operations in all. We will see, however, that all the arithmetic operations on numbers
of length N can in fact be done in O(N x log N x log log N) operations.

The trick is to recognize that multiplication is essentially a convolution (§13.1)
of the digits of the multiplicand and multiplier, followed by some kind of carry
operation. Consider, for example, two ways of writing the calculation 456 x 789:

456

45 6
x _189 x7 8 9
sain. 36 45 54
; 32 40 48
3192 28 35 42
359784

28 67 118 93 54
3.5 9 7 8 4

The tableau on the left shows the conventional method of multiplication, in which
three separate short multiplications of the full multiplicand (by 9, 8, and 7) are
added to obtain the final result. The tableau on the right shows a different method
(sometimes taught for mental arithmetic), where the single-digit cross products are
all computed (e.g. 8 x 6 = 48), then added in columns to obtain an incompletely
carried result (here, the list 28, 67, 118, 93, 54). The final step is a single pass from
right to left, recording the single least-significant digit and carrying the higher digit
or digits into the total to the left (e.g. 93 + 5 = 98, record the 8, carry 9).

You can see immediately that the column sums in the right-hand method are
components of the convolution of the digit strings, for example 118 = 4x 9+5 x
846 x 7. In §13.1 we learned how to compute the convolution of two vectors by
the fast Fourier transform (FFT): Each vector is FFT’d, the two complex transforms
are multiplied, and the result is inverse-FFT’d. Since the transforms are done with
floating arithmetic, we need sufficient precision so that the exact integer value of
each component of the result is discernible in the presence of roundoff error. We
should therefore allow a (conservative) few times log,(log, N) bits for roundoff
in the FFT. A number of length N bytes in radix 256 can generate convolution
components as large as the order of (256)?.N, thus requiring 16 + log, N bits of
precision for exact storage. If it is the number of bits in the floating mantissa
(cf. §20.1), we obtain the condition

16 + log, N + few x log, logy N < it (20.6.3)

We see that single precision, say with it = 24, is inadequate for any interesting
value of N, while double precision, say with it = 53, allows N to be greater
than 10°, corresponding to some millions of decimal digits. The following routine
therefore presumes double precision versions of realft ($12.3) and four1 (§12.2),
here called drealft and dfour1. (These routines are included on the Numerical
Recipes diskettes.)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

20.6 Arithmetic at Arbitrary Precision 919

#include "nrutil.h"
#define RX 256.0

void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,

int m)
Uses Fast Fourier Transform to multiply the unsigned radix 256 integers u[1..n] and v[1..m],
yielding a product w[1..n+m].

void drealft(double data[], unsigned long n, int isign); double version of realft.
int j,mn,nn=1;
double cy,t,*a,*b;

mn=IMAX(m,n) ;
while (nn < mn) nn <<= 1; Find the smallest usable power of two for the trans-
nn <<= 1; form.

a=dvector(1,nn);

b=dvector(1,nn) ;

for (j=1;j<=n; j++) Move U to a double precision floating array.
j]=(double)ulj] ;

+1; j<=nn; j++) alj]=0.0;

3 j<=m; j++) Move V to a double precision floating array.
(double) v[j];
for (j=m+1;j<=nn;j++) b[j]=0.0;
drealft(a,nn,1); Perform the convolution: First, the two Fourier trans-
drealft(b,nn,1); forms.
b[1] *= ali]; Then multiply the complex results (real and imagi-
b[2] *= a[2]; nary parts).
for (j =nn;j+=2) {

blj]=(t=b(j])*alj]-blj+1] *alj+1] ;
b[j+1] =t*alj+1]+b[j+1] *alj];

3
drealft(b,nn,-1); Then do the inverse Fourier transform.
Make a final pass to do all the carries.
sjr=1ij--) ft
t=b[j]/(nn>>1)+cy+0.5; The 0.5 allows for roundoff error.
cy=(unsigned long) (t/RX);
bl j]=t-cy*RX;
+
if (cy >= RX) nrerror("cannot happen in fftmul");
wli]=(unsigned char) cy; Copy answer to output.
for (j= =n+m; j++)

wljl]=(unsigned char) b[j-1];
free_dvector(b,1,nn);
free_dvector(a,1,nn);

With multiplication thus a “fast” operation, division is best performed by
multiplying the dividend by the reciprocal of the divisor. The reciprocal of a value
V is calculated by iteration of Newton’s rule,

Ui =Ui(2-VU;) (20.6.4)

which results in the quadratic convergence of U., to 1/V, as you can easily
prove. (Many supercomputers and RISC machines actually use this iteration to
perform divisions.) We can now see where the operations count N log N log log NV,
mentioned above, originates: N log JN is in the Fourier transform, with the iteration
to converge Newton’s rule giving an additional factor of log log NV.

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

920 Chapter 20. Less-Numerical Algorithms

#include "nrutil.h"
#define MF 4
#define BI (1.0/256)

void mpinv(unsigned char u[], unsigned char v[], int n, int m)

Character string v[1..m] is interpreted as a radix 256 number with the radix point after
(nonzero) v[1]; u[1..n] is set to the most significant digits of its reciprocal, with the radix
point after u[1].

t
void mpmov(unsigned char u[], unsigned char v[], int n);
void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,
int m);
void mpneg(unsigned char u[], int n);
unsigned char *rr,*s;
int i,j,maxmn,mm;
float fu,fv;
maxmn=IMAX (n,m) ;
rr=cvector(1,1+(maxmn<<1));
s=cvector(1,maxmn) ;
mm=IMIN (MF ,m) ;
fv=(float) v[mm] ; Use ordinary floating arithmetic to get an initial ap-
for (j=mm-1;j>=1;j--) { proximation.
fv *= BI;
fv += vj];
}
fu=1.0/fv;
for (j=1;j<=n;j++) {
i=(int) fu;
ulj]=(unsigned char) i;
fu=256.0*(fu-i) ;
}
for (;;) { Iterate Newton's rule to convergence.
mpmul (rr,u,v,n,m) ; Construct 2— UV in S.
mpmov(s,&rr [1] ,n);
mpneg(s ,n) ;
s[1] -= 254; Multiply SU into U.
mpmul(rr,s,u,n,n) ;
mpmov(u,&rr [1] ,n) ;
for (j=2;j<n; j++) If fractional part of S is not zero, it has not converged
if (s[j]) break; to 1.
if (j==n) {
free_cvector(s,1,maxmn) ;
free_cvector(rr,1,1+(maxmn<<1)) ;
return;
}
+
}

Division now follows as a simple corollary, with only the necessity of calculating
the reciprocal to sufficient accuracy to get an exact quotient and remainder.

#include "nrutil.h"
#define MACC 6

void mpdiv(unsigned char q[], unsigned char r[], unsigned char u[],
unsigned char v[], int n, int m)
Divides unsigned radix 256 integers u[1..n] by v[1..m] (with m < n required), yielding a
quotient q[1..n-m+1] and a remainder r[1..m].
{
void mpinv(unsigned char u[], unsigned char v[], int n, int m);
void mpmov(unsigned char u[], unsigned char v[], int n);

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.6 Arithmetic at Arbitrary Precision 921

void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,
int m);

void mpsad(unsigned char w[], unsigned char u[], int n, int iv);

void mpsub(int *is, unsigned char w[], unsigned char u[], unsigned char v[],
int n);

int is;

unsigned char *rr,*s;

rr=cvector(1, (n+MACC)<<1) ;

s=cvector (1, (n+MACC)<<1) ;

mpinv(s,v,n+MACC,m) ; Set S = 1/V.
mpmul(rr,s,u,n+MACC,n) ; Set Q = SU.
mpsad(s,rr,n+MACC-1,1);

mpmov(q,&s [2] ,n-m+1) ;

mpmul (rr,q,v,n-m+1,m) ; Multiply and subtract to get the remainder.
mpsub(&is ,&rr(1] ,u,&rr[1] ,n);

if (is) nrerror("MACC too small in mpdiv");
mpmov(r,&rr[n-m+1] ,m) ;

free_cvector(s,1, (n+MACC)<<1) ;
free_cvector(rr,1, (n+MACC)<<1);

Square roots are calculated by a Newton’s rule much like division. If
Lai 2
Vina = guil3 —VU;) (20.6.5)

then U, converges quadratically to 1//V. A final multiplication by V gives VV.

#include <math.h>
#include "nrutil.h"
#define MF 3
#define BI (1.0/256)

void mpsqrt (unsigned char w[], unsigned char u[], unsigned char v[], int n,

int m)
Character string v[1..m] is interpreted as a radix 256 number with the radix point after v[1];
w([1..n] is set to its square root (radix point after w[1]), and u[1. .n] is set to the reciprocal
thereof (radix point before u[1]). w and u need not be distinct, in which case they are set
to the square root.
{

void mplsh(unsigned char u[], int n);

void mpmov(unsigned char u[], unsigned char v[], int n);

void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,

int m);

void mpneg(unsigned char u[], int n);

void mpsdv(unsigned char w[], unsigned char u[], int n, int iv, int *ir);

int i,ir,j,mm;

float fu,fv;

unsigned char *r,*s;

r=cvector(1,n<<1);
s=cvector(1,n<<1);
mm=IMIN(m,MF) ;

fv=(float) v[mm] ; Use ordinary floating arithmetic to get an initial ap-
for (j=mm-1;j>=1;j--) { proximation.
fv BI;
fv += vj];
+
fu=1.0/sqrt (fv);
for (j=1;j<=n;j++) {

i=(int) fu;
uljl]=(unsigned char) i;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

922 Chapter 20. Less-Numerical Algorithms

fu=256.0*(fu-i) ;

for (;;) { Iterate Newton's rule to convergence.
mpmul(r,u,u,n,n); Construct S = (3 — VU?)/2.
mplsh(r,n) ;
mpmul(s,r,v,n,IMIN(m,n));
mplsh(s,n) ;
mpneg(s,n) ;
s[1] -= 253;
mpsdv(s,s,n,2,&ir) ;
for (j=2;j<n;j++) { If fractional part of S is not zero, it has not converged
if (s[j]) { to 1.

mpmul(r,s,u,n,n); Replace U by SU.

mpmov(u,&r[1] ,n) ;

break;

}

}
if (j<n) continue;
mpmul(r,u,v,n,IMIN(m,n)); Get square root from reciprocal and return.
mpmov(w,&r[1] ,n);
free_cvector(s,1,n<<1);
free_cvector(r,1,n<<1);
return;

We already mentioned that radix conversion to decimal is a merely cosmetic
operation that should normally be omitted. The simplest way to convert a fraction to
decimal is to multiply it repeatedly by 10, picking off (and subtracting) the resulting
integer part. This, has an operations count of O(N”), however, since each liberated
decimal digit takes an O(N) operation. It is possible to do the radix conversion as
a fast operation by a “divide and conquer” strategy, in which the fraction is (fast)
multiplied by a large power of 10, enough to move about half the desired digits
to the left of the radix point. The integer and fractional pieces are now processed
independently, each further subdivided. If our goal were a few billion digits of 7,
instead of a few thousand, we would need to implement this scheme. For present
purposes, the following lazy routine is adequate:

#define IAZ 48

void mp2dfr(unsigned char a[], unsigned char s[], int n, int *m)
Converts a radix 256 fraction a[1..n] (radix point before a[1]) to a decimal fraction rep-
resented as an ascii string s[1..m], where m is a returned value. The input array a[1. .n]
is destroyed. NOTE: For simplicity, this routine implements a slow («x N?) algorithm. Fast
(x Nin N), more complicated, radix conversion algorithms do exist.
{

void mplsh(unsigned char u[], int n);

void mpsmu(unsigned char w[], unsigned char u[], int n, int iv);

int j;

wm=(int) (2.408*n) ;

for (j=1;j<=(+m); j++) {
mpsmu(a,a,n,10);
s(j]=al1]+1IAaz;
mplsh(a,n) ;

Finally, then, we arrive at a routine implementing equations (20.6.1) and (20.6.2):

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.6 Arithmetic at Arbitrary Precision 923

3.141592653589793238462643383279502884 197 169399375 1058209749445923078 164062
862089986280348253421 17067982 1480865 1328230664709384460955058223 17253594081
2848 1117450284 102701938521 10555964462294895493038 19644288 10975665933446 1284
75648233786783 16527 1201909 1456485669234603486 104543266482 13393607260249 1412
737245870066063 1558817488 152092096282925409 17 1536436789259036001 13305305488
2046652138414695 19415 1160943305727036575959 19530921861 173819326117931051185
4807446237996274956735 188575272489 1227938 18301 1949 1298336733624406566430860
21394946395224737 1907021798609437027705392 17176293 1767523846748 184676694051
32000568 127 14526356082778577 1342757789609 173637 17872 146844090 1224953430 1465
4958537 10507922796892589235420 199561 1212902 1960864034418 1598 1362977477 13099
6051870721 134999999837297804995 10597317328 160963 185950244594553469083026425
2230825334468503526193 1188171010003 1378387528865875 3320838 1420617 177669 1473
0359825349042875546873 | 15956286388235378759375 195778 185778053217 12268066130
0192787661 11959092 16420198938095257201065485863278865936 15338 18279682303019
520353018529689957736225994 1389 124972 1775283479 1315 15574857242454 1506959508
295331 1686172785588907509838 17546374649393 1925506040092770 1671 1390098488240
12858361603563707660 1047 1018 1942955596 198946767837449448255379774726847 1040
4753464620804668425906949 12933 13677028989 152104752 162056966024058038 1501935
112533824300355876402474964732639 14 19927260426992279678235478 16360093417216
412199245863 150302861829745557067498385054945885869269956909272 107975093029
55321165344987202755960236480665499 | 1988 18347977535663698074265425278625518
1841757467289097777279380008 16470600 16145249 1921732172147723501414419735685
48161361157352552133475741849468438523323907394 1 433345477624 1 68625 189835694.
855620992 19222 18427255025425688767 179049460 165346680498862723279 | 7860857843
8382796797668 1454 100953883786360950680064225 125205 1 17392984896084 1284886269
4560424 19652850222 10661 186306744278622039 194945047 1237 1378696095636437 19172
874677646575739624 1 38908658326459958 1339047802759009946576407895 | 2694683983
525957098258226205224894077267 1 947826848260 1476990902640 I 363944374553050682
0349625245 174939965 143 1429809 19065925093722 16964615 157098583874 105978859597
72975498930 161753928468 1 38268683868942774 15599 185592524595395943 10499725246
80845987273644695848653836736222626099 12460805 124388439045 1244 1365497627807
97715691435997700 1296 1 608944 1694868555848406353422072225828488648 1584560285

Figure 20.6.1. The first 2398 decimal digits of 7, computed by the routines in this section.

#include <stdio.h>
#include "nrutil.h"
#define IAOFF 48

void mppi(int n)
Demonstrate multiple precision routines by calculating and printing the first n bytes of 7.

{

void mp2dfr(unsigned char a[], unsigned char s[], int n, int *m);

void mpadd(unsigned char w[], unsigned char u[], unsigned char v[], int n);

void mpinv(unsigned char u[], unsigned char v[], int n, int m);
void mplsh(unsigned char u[], int n);
void mpmov(unsigned char u[], unsigned char v[], int n);

void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,

int m);
void mpsdv(unsigned char w[], unsigned char u[], int n, int iv, int *ir);

void mpsqrt(unsigned char w[], unsigned char u[], unsigned char v[], int n,

int m);
int ir,j,m;
unsigned char mm,*x,*y,¥*sx,*sxi,*t,*s,*pi;

x=cvector(1,n+1);

y=cvector(1,n<<1);

sx=cvector(1,n);

sxi=cvector(1,n);

t=cvector(1,n<<1);

s=cvector(1,3*n) ;

pi=cvector(1,n+1);

t[1]=2; Set T = 2.
for (j=2;j<=n;j++) t[j]=0;

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

924 Chapter 20. _Less-Numerical Algorithms

mpsqrt (x,x,t,n,n); Set Xp = V2.
mpadd(pi,t,x,n); Set m) = 24+ V2.
mplsh(pi,n) ;

mpsqrt (sx,sxi,x,n,n); Set Yo = 21/4.
mpmov(y,sx,n) ;
for G3) f
mpadd(x,sx,sxi,n); Set Xj41. = (x}? + x) /?)/2.

mpsdv(x,&x([1] ,n,2,&ir) ;
mpsqrt (sx,sxi,x,n,n);
mpmul(t,y,sx,n,n);
mpadd(&t [1] ,&t[1] ,sxi,n);
x[1]++; Increment X;41 and Y; by 1.
yli]++;
mpinv(s,y,n,n) ;
mpmul(y,&t[2],s,n,n);
mplsh(y,n) ;
mpmul(t,x,s,n,n); Form temporary T = (Xj41 + 1)/(Y¥i +1).
mm=t [2]-1; If T = 1 then we have converged.
for (j=3;j<=n;jt+) {

if (t[j] != mm) break;

1/2

Form the temporary T = Y¥iX; {4 + x7 ih?

itl

Set Yig1 = T/(Y; +1).

}

m=t (n+1]-mm;

if (j<-nllm>illm<-if
mpmul(s,pi,&t [1] ,n,n); Set 41 = Tr.
mpmov(pi,&s[1] ,n);
continue;

}

printf ("pi=\n") ;

s[1]=pi[1]+IAOFF;

>

mp2dfr(&pi[1] ,&s[2] ,n-1,&m) ;

Convert to decimal for printing. NOTE: The conversion routine, for this demonstration
only, is a slow (x N?) algorithm. Fast («x N InN), more complicated, radix conversion
algorithms do exist.

s [m+3]=0;

printf(" %64s\n",us[1]);

free_cvector(pi,1,n+1);

free_cvector(s,1,3*n) ;

free_cvector(t,1,n<<1);

free_cvector(sxi,1,n);

free_cvector(sx,1,n);

free_cvector(y,1,n<<1) ;

free_cvector(x,1,n+1);

return;

Figure 20.6.1 gives the result, computed with n = 1000. As an exercise, you
might enjoy checking the first hundred digits of the figure against the first 12 terms
of Ramanujan’s celebrated identity [3]

(20.6.6)

1 V8 SG (4n)! (1103 + 26390n)
mz 9801 » (n! 396”)4

using the above routines. You might also use the routines to verify that the

number 2°!2 + 1 is not a prime, but has factors 2,424,833 and

7 A55 602,825 647 884,208 337,395 736,200,454 ,918,783,366,342,657 (which are

in fact prime; the remaining prime factor being about 7.416 x 10°) [4].

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn euossed uMO 4194} 10} Adoo seded auo ayeW! 0} suasn JeUsE}UI 10} payue6 si

“aremyog sedivay /eouawinn Aq 2661-8861 (9) 1UBUIAdOD suIesBolg ‘ssalg AyIsIanlu eBPUqUIeD Kq Z661-BB6L (9) 1YBUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

20.6 Arithmetic at Arbitrary Precision 925

CITED REFERENCES AND FURTHER READING:

Borwein, J.M., and Borwein, P.B. 1987, Pi and the AGM: A Study in Analytic Number Theory and
Computational Complexity (New York: Wiley). [1]

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of Computer Programming
(Reading, MA: Addison-Wesley), $4.3. [2]

Ramanujan, S. 1927, Collected Papers of Srinivasa Ramanujan, G.H. Hardy, P.V. Seshu Aiyar,
and B.M. Wilson, eds. (Cambridge, U.K.: Cambridge University Press), pp. 23-39. [3]

Kolata, G. 1990, June 20, The New York Times. [4]
Kronsj6, L. 1987, Algorithms: Their Complexity and Efficiency, 2nd ed. (New York: Wiley).

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

ov
$e
33
85
og
23
=.
Ba
3a
23
ca
3
ge
es
az
38
33
23
osc
COG
2
Za
<@
a6
A
ae
a8
86
35
33
a3
ss
ano)
a
28
a3
a2
<¢
iad
oz
$2
#2
3
as
3
ae
a
as
SB
<
Par
8
gs
an
ag
ok
BS
3
Pa
23
sg
3
oe
gs
rj
aS
gg
>
ss
2
Do
Oo
as
eS
<a
Bo
23
a3
8B
2s
eo
°8

le)
o
3
ss
é

“alemyos Sedioay JeouewWNN Aq 2661-886 (O) 1UBUAdOD suresBOlg “ssald AysienluN eGpuquied Aq Z661-8861 (O

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

References

The references collected here are those of general usefulness, usually cited in
more than one section of this book. More specialized sources, usually cited in a
single section, are not repeated here.

We first list a small number of books that form the nucleus of a recommended
personal reference collection on numerical methods, numerical analysis, and closely
related subjects. These are the books that we like to have within easy reach.

Abramowitz, M., and Stegun, I.A. 1964, Handbook of Mathematical Func-
tions, Applied Mathematics Series, Volume 55 (Washington: National
Bureau of Standards; reprinted 1968 by Dover Publications, New
York)

Acton, F.S. 1970, Numerical Methods That Work; 1990, corrected edition
(Washington: Mathematical Association of America)

Ames, W.F. 1977, Numerical Methods for Partial Differential Equations,
2nd ed. (New York: Academic Press)

Bratley, P., Fox, B.L., and Schrage, E.L. 1983, A Guide to Simulation (New
York: Springer-Verlag)

Dahlquist, G., and Bjorck, A. 1974, Numerical Methods (Englewood
Cliffs, NJ: Prentice-Hall)

Delves, L.M., and Mohamed, J.L. 1985, Computational Methods for Inte-
gral Equations (Cambridge, U.K.: Cambridge University Press)
Dennis, J.E., and Schnabel, R.B. 1983, Numerical Methods for Uncon-
strained Optimization and Nonlinear Equations (Englewood Cliffs,

NJ: Prentice-Hall)

Gill, P.E., Murray, W., and Wright, M.H. 1991, Numerical Linear Algebra
and Optimization, vol. 1 (Redwood City, CA: Addison-Wesley)
Golub, G.H., and Van Loan, C.F. 1989, Matrix Computations, 2nd ed.

(Baltimore: Johns Hopkins University Press)

Oppenheim, A.V., and Schafer, R.W. 1989, Discrete-Time Signal Process-

ing (Englewood Cliffs, NJ: Prentice-Hall)

Ralston, A., and Rabinowitz, P. 1978, AFirst Course in Numerical Analysis,

2nd ed. (New York: McGraw-Hill)

Sedgewick, R. 1988, Algorithms, 2nd ed. (Reading, MA: Addison-Wesley)

Stoer, J., and Bulirsch, R. 1980, Introduction to Numerical Analysis (New

York: Springer-Verlag)

Wilkinson, J.H., and Reinsch, C. 1971, Linear Algebra, vol. Il of Handbook
for Automatic Computation (New York: Springer-Verlag)

926

vU
g
3

S
S
a
sy
om
oO
2
a
=
2
€
2
S

a
°
S
2
ios
2
E}
<
a
oO
3
g
Q
is}
3
3
=
g
G

=
4
acl
=
=
=
3
Q
is}
3
i=}
°
2
&
f=
2
e
g
n
z
io
@
Z
é
a
s
>
3
iy
=
iy
°
2
=
i=}
a
oO
3S
a
©
3
2
3S
2
o
Q
fs}
i
Q
a
g
2
®
9
9
3
ica
fom
a
©
ic}
a
ry
=
g
[oa
oO
Z
g
s
>
3
g
=
2

BJISGOM SIA ‘SWIOHGO 10 Syoog sedioay jeOUOWINN JapPso O| “payiqiyoud Aj 1
-auiyoeul jo BulAdoo Aue Jo ‘uolonpoidas Jayyin4 ‘asn jeuosiad UMO sau} 410} Adoo saded auo ayew 0} Siasn JaUEU! 10} payueB SI

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUOWNN Aq Z661-8B6l (D) IYBUAdOD swesBOlY “‘ssalg AysIaAIUN EBpLqueD Aq 2661-8861 (D) jyBUAdOD

References 927

We next list the larger collection of books, which, in our view, should be included
in any serious research library on computing, numerical methods, or analysis.

Bevington, P.R. 1969, Data Reduction and Error Analysis for the Physical Sciences
(New York: McGraw-Hill)

Bloomfield, P. 1976, Fourier Analysis of Time Series — An Introduction (New York:
Wiley)

Bowers, R.L., and Wilson, J.R. 1991, Numerical Modeling in Applied Physics and
Astrophysics (Boston: Jones & Bartlett)

Brent, R.P. 1973, Algorithms for Minimization without Derivatives (Englewood
Cliffs, NJ: Prentice-Hall)

Brigham, E.O. 1974, The Fast Fourier Transform (Englewood Cliffs, NJ: Prentice-
Hall)

Brownlee, K.A. 1965, Statistical Theory and Methodology, 2nd ed. (New York:
Wiley)

Bunch, J.R., and Rose, D.J. (eds.) 1976, Sparse Matrix Computations (New York:
Academic Press)

Canuto, C., Hussaini, M.Y., Quarteroni, A., and Zang, T.A. 1988, Spectral Methods
in Fluid Dynamics (New York: Springer-Verlag)

Carnahan, B., Luther, H.A., and Wilkes, J.O. 1969, Applied Numerical Methods
(New York: Wiley)

Champeney, D.C. 1973, Fourier Transforms and Their Physical Applications (New
York: Academic Press)

Childers, D.G. (ed.) 1978, Modern Spectrum Analysis (New York: IEEE Press)

Cooper, L., and Steinberg, D. 1970, Introduction to Methods of Optimization
(Philadelphia: Saunders)

Dantzig, G.B. 1963, Linear Programming and Extensions (Princeton, NJ: Princeton
University Press)

Devroye, L. 1986, Non-Uniform Random Variate Generation (New York: Springer-
Verlag)

Dongarra, J.J., et al. 1979, LINPACK User’s Guide (Philadelphia: S.I.A.M.)

Downie, N.M., and Heath, R.W. 1965, Basic Statistical Methods, 2nd ed. (New
York: Harper & Row)

Duff, 1.S., and Stewart, G.W. (eds.) 1979, Sparse Matrix Proceedings 1978
(Philadelphia: S.I.A.M.)

Elliott, D.F., and Rao, K.R. 1982, Fast Transforms: Algorithms, Analyses, Appli-
cations (New York: Academic Press)

Fike, C.T. 1968, Computer Evaluation of Mathematical Functions (Englewood
Cliffs, NJ: Prentice-Hall)

Forsythe, G.E., Malcolm, M.A., and Moler, C.B. 1977, Computer Methods for
Mathematical Computations (Englewood Cliffs, NJ: Prentice-Hall)

Forsythe, G.E., and Moler, C.B. 1967, Computer Solution of Linear Algebraic
Systems (Englewood Cliffs, NJ: Prentice-Hall)

Gass, S.T. 1969, Linear Programming, 3rd ed. (New York: McGraw-Hill)

Gear, C.W. 1971, Numerical Initial Value Problems in Ordinary Differential Equa-
tions (Englewood Cliffs, NJ: Prentice-Hall)

Goodwin, E.T. (ed.) 1961, Modern Computing Methods, 2nd ed. (New York:
Philosophical Library)

Gottlieb, D. and Orszag, S.A. 1977, Numerical Analysis of Spectral Methods:
Theory and Applications (Philadelphia: S.1-A.M.)

Hackbusch, W. 1985, Multi-Grid Methods and Applications (New York: Springer-
Verlag)

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD
(S-801€b-1ZS-0 NES!) DNILAdNOO OISILNSIOS 4O LHW SHL !O NI SAdIO4Y TWOIMSWNN Woy aBed ejdwes

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

928

References

Hamming, R.W. 1962, Numerical Methods for Engineers and Scientists; reprinted
1986 (New York: Dover)

Hart, J.F., et al. 1968, Computer Approximations (New York: Wiley)

Hastings, C. 1955, Approximations for Digital Computers (Princeton: Princeton
University Press)

Hildebrand, F.B. 1974, Introduction to Numerical Analysis, 2nd ed.; reprinted 1987
(New York: Dover)

Hoel, P.G. 1971, Introduction to Mathematical Statistics, 4th ed. (New York: Wiley)

Horn, R.A., and Johnson, C.R. 1985, Matrix Analysis (Cambridge: Cambridge
University Press)

Householder, A.S. 1970, The Numerical Treatment of a Single Nonlinear Equation
(New York: McGraw-Hill)

Huber, P.J. 1981, Robust Statistics (New York: Wiley)

Isaacson, E., and Keller, H.B. 1966, Analysis of Numerical Methods (New York:
Wiley)

Jacobs, D.A.H. (ed.) 1977, The State of the Art in Numerical Analysis (London:
Academic Press)

Johnson, L.W., and Riess, R.D. 1982, Numerical Analysis, 2nd ed. (Reading, MA:
Addison-Wesley)

Kahaner, D., Moler, C., and Nash, S. 1989, Numerical Methods and Software
(Englewood Cliffs, NJ: Prentice Hall)

Keller, H.B. 1968, Numerical Methods for Two-Point Boundary-Value Problems
(Waltham, MA: Blaisdell)

Knuth, D.E. 1968, Fundamental Algorithms, vol. 1 of The Art of Computer
Programming (Reading, MA: Addison-Wesley)

Knuth, D.E. 1981, Seminumerical Algorithms, 2nd ed., vol. 2 of The Art of
Computer Programming (Reading, MA: Addison-Wesley)

Knuth, D.E. 1973, Sorting and Searching, vol. 3 of The Art of Computer Program-
ming (Reading, MA: Addison-Wesley)

Koonin, S.E., and Meredith, D.C. 1990, Computational Physics, Fortran Version
(Redwood City, CA: Addison-Wesley)

Kuenzi, H.P., Tzschach, H.G., and Zehnder, C.A. 1971, Numerical Methods of
Mathematical Optimization (New York: Academic Press)

Lanczos, C. 1956, Applied Analysis; reprinted 1988 (New York: Dover)

Land, A.H., and Powell, S. 1973, Fortran Codes for Mathematical Programming
(London: Wiley-Interscience)

Lawson, C.L., and Hanson, R. 1974, Solving Least Squares Problems (Englewood
Cliffs, NJ: Prentice-Hall)

Lehmann, E.L. 1975, Nonparametrics: Statistical Methods Based on Ranks (San
Francisco: Holden-Day)

Luke, Y.L. 1975, Mathematical Functions and Their Approximations (New York:
Academic Press)

Magnus, W., and Oberhettinger, F. 1949, Formulas and Theorems for the Functions
of Mathematical Physics (New York: Chelsea)

Martin, B.R. 1971, Statistics for Physicists (New York: Academic Press)

Mathews, J., and Walker, R.L. 1970, Mathematical Methods of Physics, 2nd ed.
(Reading, MA: W.A. Benjamin/Addison-Wesley)

von Mises, R. 1964, Mathematical Theory of Probability and Statistics (New York:
Academic Press)

Murty, K.G. 1976, Linear and Combinatorial Programming (New York: Wiley)

Norusis, M.J. 1982, SPSS Introductory Guide: Basic Statistics and Operations; and
1985, SPSS-X Advanced Statistics Guide (New York: McGraw-Hill)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

References

929

Nussbaumer, H.J. 1982, Fast Fourier Transform and Convolution Algorithms (New
York: Springer-Verlag)

Ortega, J., and Rheinboldt, W. 1970, Iterative Solution of Nonlinear Equations in
Several Variables (New York: Academic Press)

Ostrowski, A.M. 1966, Solutions of Equations and Systems of Equations, 2nd ed.
(New York: Academic Press)

Polak, E. 1971, Computational Methods in Optimization (New York: Academic
Press)

Rice, J.R. 1983, Numerical Methods, Software, and Analysis (New York: McGraw-
Hill)

Richtmyer, R.D., and Morton, K.W. 1967, Difference Methods for Initial Value
Problems, 2nd ed. (New York: Wiley-Interscience)

Roache, P.J. 1976, Computational Fluid Dynamics (Albuquerque: Hermosa)

Robinson, E.A., and Treitel, S. 1980, Geophysical Signal Analysis (Englewood
Cliffs, NJ: Prentice-Hall)

Smith, B.T., et al. 1976, Matrix Eigensystem Routines — EISPACK Guide, 2nd ed.,
vol. 6 of Lecture Notes in Computer Science (New York: Springer-Verlag)

Stuart, A., and Ord, J.K. 1987, Kendall’s Advanced Theory of Statistics, Sth ed.
(London: Griffin and Co.) [previous eds. published as Kendall, M., and Stuart,
A., The Advanced Theory of Statistics]

Tewarson, R.P. 1973, Sparse Matrices (New York: Academic Press)

Westlake, J.R. 1968, A Handbook of Numerical Matrix Inversion and Solution of
Linear Equations (New York: Wiley)

Wilkinson, J.H. 1965, The Algebraic Eigenvalue Problem (New York: Oxford
University Press)

Young, D.M., and Gregory, R.T. 1973, ASurvey of Numerical Mathematics, 2 vols.;
reprinted 1988 (New York: Dover)

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SIWOHGD JO Sy00q sedioey jedaWINN Japso OF “payiqiyoud Ajyouys SI ‘“aNdwios JaAsas Aue 0} (aUO Siu} Bulpnjoul) sajy ajqepea.
-eulyoew jo BulAdoo Aue Jo ‘uojonpoides sayyin4 “asn seuosied UMO 4194} 10} Adoo seded auo eyeW 0} suas JeUJE}U! 10} PayUeJGH SI UOISSILAg

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Appendix A: Table of Prototype
Declarations

We here list ANSI prototype declarations for all the routines in Numerical
Recipes in C. If, as is preferred, you are using a compiler that implements the ANSI
standard, then you should #include this listing (or the relevant lines thereof) in
each separately compiled source file that contains or references any routine from this
book. That will alert your compiler to the fact that our routines do not generally
expect argument conversions, and also allow your compiler to point out possible
errors in your invocation of our routines.

On the diskette, this Appendix is in the file nr.h. An important point about
the file on the diskette is that it contains both ANSI C and “traditional” or “K&R”
style declarations. The ANSI forms are invoked if any of the following macros are
defined: _.STDC__, ANSI, or NRANSI. (The purpose of the last one is to give you
an invocation that does not conflict with other possible uses of the first two names.)
If you do have an ANSI compiler it is essential that you invoke it with one or more
of these macros defined. The typical means for doing so is to include a switch like
“-DANSI” on the compiler command line.

If you have a “traditional” or “K&R” C compiler, then the above discussion
does not apply to you: you do not need the ANSI header file listed here. If you
have the diskette nr.h file, you will probably find it helpful to #include nr.h
anyway, without setting any of the macros __STDC__, ANSI, or NRANSI. That will
at least alert your compiler to the returned value types of our routines. You should
of course be sure to use the K&R versions of our programs, as included (along with
the primary ANSI versions) on the Numerical Recipes C Diskette. Your compiler
will do the “usual argument conversions” whether you like it or not, but it will also
undo them upon entering routines whose arguments have been declared differently
from the usual conversions.

Here is a listing of the file nr.h:

#ifndef _NR_H_
#define _NR_H_

#ifndef _FCOMPLEX_DECLARE_T_

typedef struct FCOMPLEX {float r,i;} fcomplex;
#define _FCOMPLEX_DECLARE_T_

#endif /* _FCOMPLEX_DECLARE_T_ */

#ifndef _ARITHCODE_DECLARE_T_
typedef struct {

930

SILO

*(eoewy YON epis}no) B10'eBpuquies @AJes}sno}O~1IP 0} [ea PUSS JO ‘(AJUO BOVBWIY YON) EZPZ-ZZ8-008- | [129 10 WOoU'MMMy/:dAY,
O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

2
@
3
2
3
a
s
=
8
3
2
©
&
8
3
3
3
a
ES
3
°
s
3
3
3
3
g
3
fe}
3
2
s
Fa
2
°
=
5
3
8
Fa
°
5
2
©
&
®
n
s
Ea
2
3
3
°
Qa
2
S
&
P
fa
2
e}
2
8
fo}
3
s.
5
a
g
3
3
8
2
5
?

uAdog

n
Ey
3
3
g
3
Ey
a
3
fei
3
Zz
i
=
m
2
fe)
>
=
D
m
Q
vu
m
n
2
2
4
x
m
>
D
4
fe)
9
n
Q
m
Zz
a
al
ie)
Q
fe}
=
vu
Cc
a
2
fo)
D
jes)
Zz
2
a
X
h
g
3
ha
Ro)

2
o
2
e
o
S
8

z
1?)
2
3
ic
z

a
o
Cc
Es
Fe
g
gZ

=
3
3
S
9
2
3

Q
Ba
3
a
9
S

3

Ss

2
E

2
o
2
2
o
3S
nn
z

<
Zz
é
3
g
3
Ea
DD
2
&

3
3
3
n
ot
=
FA
a
8

Table of Prototype Declarations (nr.h) 931

unsigned long *ilob,*iupb, *ncumfq, jdif,nc,minint ,nch,ncum,nrad;
} arithcode;
#define _ARITHCODE_DECLARE_T_
#endif /* _ARITHCODE_DECLARE_T_ */

#ifndef _HUFFCODE_DECLARE_T_
typedef struct {
unsigned long *icod,*ncod,*left,*right ,nch,nodemax;
} huffcode;
#define _HUFFCODE_DECLARE_T_
#endif /* _HUFFCODE_DECLARE_T_ */

#include <stdio.h>
#if defined(__STDC__) || defined(ANSI) || defined(NRANSI) /* ANSI */

void addint(double **uf, double **uc, double **res, int nf);

void airy(float x, float *ai, float *bi, float *aip, float *bip);

void amebsa(float **p, float y[], int ndim, float pb[], float *yb,
float ftol, float (*funk)(float []), int *iter, float temptr);

void amoeba(float **p, float y[], int ndim, float ftol,
float (*funk) (float []), int *iter);

float amotry(float **p, float y[], float psum[], int ndim,
float (*funk) (float []), int ihi, float fac);

float amotsa(float **p, float y[], float psum[], int ndim, float pb[],
float *yb, float (*funk)(float []), int ihi, float *yhi, float fac);

void anneal(float x[], float y[], int iorder[], int ncity);

double anorm2(double **a, int n);

void arcmak(unsigned long nfreq[], unsigned long nchh, unsigned long nradd,
arithcode *acode);

void arcode(unsigned long *ich, unsigned char **codep, unsigned long *lcode,
unsigned long *lcd, int isign, arithcode *acode);

void arcsum(unsigned long iin[], unsigned long iout[], unsigned long ja,
int nwk, unsigned long nrad, unsigned long nc);

void asolve(unsigned long n, double b[], double x[], int itrnsp);

void atimes(unsigned long n, double x[], double r[], int itrnsp);

void avevar(float data[], unsigned long n, float *ave, float *var);

void balanc(float **a, int n);

void banbks(float **a, unsigned long n, int m1, int m2, float **al,
unsigned long indx[], float b[]);

void bandec(float **a, unsigned long n, int m1, int m2, float **al,
unsigned long indx[], float *d);

void banmul(float **a, unsigned long n, int m1, int m2, float x[], float b[]);

void beucof(float y[], float yi[], float y2[], float y12[], float di,
float d2, float **c);

void bcuint(float y[], float yi[], float y2[], float y1i2[],
float x11, float x1lu, float x21, float x2u, float x1,
float x2, float *ansy, float *ansy1, float *ansy2);

void beschb(double x, double *gam1, double *gam2, double *gampl,
double *gammi) ;

float bessi(int n, float x);

float bessi0(float x);

float bessi1(float x);

void bessik(float x, float xnu, float *ri, float *rk, float *rip,
float *rkp);

float bessj(int n, float x);

float bessjO(float x);

float bessji(float x);

void bessjy(float x, float xnu, float *rj, float *ry, float *rjp,
float *ryp);

float bessk(int n, float x);

float besskO(float x);

float besski(float x);

float bessy(int n, float x);

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

932 Appendix A

float bessy0(float x);
float bessy1(float x);
float beta(float z, float w);
float betacf(float a, float b, float x);
float betai(float a, float b, float x);
float bico(int n, int k);
void bksub(int ne, int nb, int jf, int k1, int k2, float ***c);
float bnldev(float pp, int n, long *idum);
float brent(float ax, float bx, float cx,
float (*f) (float), float tol, float *xmin);
void broydn(float x[], int n, int *check,
void (*vecfunc) (int, float [], float []));
void bsstep(float y[], float dydx[], int nv, float *xx, float htry,
float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []));
void caldat(long julian, int *mm, int *id, int *iyyy);
void chder(float a, float b, float c[], float cder[], int n);
float chebev(float a, float b, float c[], int m, float x);
void chebft(float a, float b, float c[], int n, float (*func)(float));
void chebpc(float c[], float d[], int n);
void chint(float a, float b, float c{], float cint[], int n);
float chixy(float bang) ;
void choldc(float **a, int n, float p[]);
void cholsl(float **a, int n, float p[], float b[], float x[]);
void chsone(float bins[], float ebins[], int nbins, int knstrn,
float *df, float *chsq, float *prob);
void chstwo(float binsi[], float bins2[], int nbins, int knstrn,
float *df, float *chsq, float *prob);
void cisi(float x, float *ci, float *si);
void cntabi(int **nn, int ni, int nj, float *chisq,
float *df, float *prob, float *cramrv, float *ccc);
void cntab2(int **nn, int ni, int nj, float *h, float *hx, float *hy,
float *hygx, float *hxgy, float *uygx, float *uxgy, float *uxy);
void convlv(float data[], unsigned long n, float respns[], unsigned long m,
int isign, float ans[]);
void copy(double **aout, double **ain, int n);
void correl(float datai[], float data2[], unsigned long n, float ans[]);
void cosft(float y[], int n, int isign);
void cosft1(float y[], int n);
void cosft2(float y[], int n, int isign);
void covsrt(float **covar, int ma, int ia[], int mfit);
void crank(unsigned long n, float w[], float *s);
void cyclic(float a[], float b[], float c[], float alpha, float beta,
float r[{], float x[], unsigned long n);
void daub4(float al], unsigned long n, int isign);
float dawson(float x);
float dbrent(float ax, float bx, float cx,
float (*f) (float), float (*df)(float), float tol, float *xmin);
void ddpoly(float c[], int nc, float x, float pd[], int nd);
int decchk(char string[], int n, char *ch);
void derivs(float x, float y[], float dydx[]);
float dfidim(float x);
void dfourl(double data[], unsigned long nn, int isign);
void dfpmin(float p[], int n, float gtol, int *iter, float *fret,
float (*func) (float []), void (*dfunc) (float [], float []));
float dfridr(float (*func) (float), float x, float h, float *err);
void dftcor(float w, float delta, float a, float b, float endpts[],
float *corre, float *corim, float *corfac);
void dftint(float (*func) (float), float a, float b, float w,
float *cosint, float *sinint);
void difeq(int k, int ki, int k2, int jsf, int isi, int isf,
int indexv[], int ne, float **s, float *+y);
void dlinmin(float p[], float xi[], int n, float *fret,
float (*func) (float []), void (*dfunc) (float [], float[]));

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Table of Prototype Declarations (nr.h) 933

double dpythag(double a, double b);

void drealft(double data[], unsigned long n, int isign);

void dsprsax(double sa[], unsigned long ija[], double x[], double b[],
unsigned long n);

void dsprstx(double sa[], unsigned long ija[], double x[], double b[],
unsigned long n);

void dsvbksb(double **u, double w[], double **v, int m, int n, double bf],
double x[]);

void dsvdemp(double **a, int m, int n, double w[], double **v);

void eclass(int nf[], int n, int lista[], int listb[], int m);

void eclazz(int nf[], int n, int (*equiv) (int, int));

float ei(float x);

void eigsrt(float d[], float **v, int n);

float elle(float phi, float ak);

float ellf(float phi, float ak);

float ellpi(float phi, float en, float ak);

void elmhes(float **a, int n);

float erfcc(float x);

float erff(float x);

float erffc(float x);

void eulsum(float *sum, float term, int jterm, float wksp[]);

float evlmem(float fdt, float d[], int m, float xms);

float expdev(long *idum) ;

float expint(int n, float x);

float f1(float x);

float fidim(float x);

float £2(float y);

float £3(float z);

float factln(int n);

float factrl(int n);

void fasper(float x[], float y[], unsigned long n, float ofac, float hifac,
float wki[], float wk2[], unsigned long nwk, unsigned long *nout,
unsigned long *jmax, float *prob);

void fdjac(int n, float x[], float fvec[], float **df,
void (*vecfunc) (int, float [], float []));

void fgauss(float x, float a[], float *y, float dyda[], int na);

void fi110(double **u, int n);

void fit(float x[], float y[], int ndata, float sig[], int mwt,
float *a, float *b, float *siga, float *sigb, float *chi2, float ¥*q);

void fitexy(float x[], float y[], int ndat, float sigx[], float sigy[],
float *a, float *b, float *siga, float *sigb, float *chi2, float ¥*q);

void fixrts(float d[], int m);

void fleg(float x, float pl[], int nl);

void flmoon(int n, int nph, long *jd, float *frac);

float fmin(float x[]);

void four1(float data[], unsigned long nn, int isign);

void fourew(FILE *file[5], int *na, int *nb, int *nc, int *nd);

void fourfs(FILE *file[5], unsigned long nn[], int ndim, int isign);

void fourn(float data[], unsigned long nn[], int ndim, int isign);

void fpoly(float x, float p[], int np);

void fred2(int n, float a, float b, float t{], float f[], float wf,
float (*g)(float), float (*ak) (float, float));

float fredin(float x, int n, float a, float b, float t[], float f[], float wO,
float (*g)(float), float (*ak) (float, float));

void frenel(float x, float *s, float ¥*c);

void frprmn(float p[], int n, float ftol, int *iter, float *fret,
float (*func) (float []), void (*dfunc) (float [], float []));

void ftest(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *f, float *prob);

float gamdev(int ia, long *idum);

float gammln(float xx);

float gammp(float a, float x);

float gammq(float a, float x);

float gasdev(long *idum) ;

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


934 Appendix A

void gaucof(int n, float a[], float b[], float amu0, float x[], float wf]);

void gauher(float x[], float w[], int n);

void gaujac(float x[], float w[], int n, float alf, float bet);

void gaulag(float x[], float w[], int n, float alf);

void gauleg(float x1, float x2, float x[], float w[], int n);

void gaussj(float **a, int n, float **b, int m);

void gcf(float *gammcf, float a, float x, float *gln);

float golden(float ax, float bx, float cx, float (*f)(float), float tol,
float *xmin);

void gser(float *gamser, float a, float x, float *gln);

void hpsel(unsigned long m, unsigned long n, float arr[], float heap[]);

void hpsort (unsigned long n, float ra[]);

void hqr(float **a, int n, float wr[], float wil]);

void hufapp(unsigned long index[], unsigned long nprob[], unsigned long n,
unsigned long i);

void hufdec(unsigned long *ich, unsigned char *code, unsigned long lcode,
unsigned long *nb, huffcode *hcode) ;

void hufenc(unsigned long ich, unsigned char **codep, unsigned long *lcode,
unsigned long *nb, huffcode *hcode) ;

void hufmak(unsigned long nfreq[], unsigned long nchin, unsigned long *ilong,
unsigned long *nlong, huffcode *hcode) ;

void hunt(float xx[], unsigned long n, float x, unsigned long *jlo);

void hypdrv(float s, float yy[], float dyyds[]);

fcomplex hypgeo(fcomplex a, fcomplex b, fcomplex c, fcomplex z);

void hypser(fcomplex a, fcomplex b, fcomplex c, fcomplex z,
fcomplex *series, fcomplex *deriv);

unsigned short icrc(unsigned short crc, unsigned char *bufptr,
unsigned long len, short jinit, int jrev);

unsigned short icrci(unsigned short crc, unsigned char onech) ;

unsigned long igray(unsigned long n, int is);

void iindexx(unsigned long n, long arr[], unsigned long indx[]);

void indexx(unsigned long n, float arr[], unsigned long indx[]);

void interp(double **uf, double **uc, int nf);

int irbit1(umsigned long *iseed) ;

int irbit2(umsigned long *iseed) ;

void jacobi(float **a, int n, float da[], float **v, int *nrot);

void jacobn(float x, float y[], float dfdx[], float **dfdy, int n);

long julday(int mm, int id, int iyyy);

void kendli(float datai[], float data2[], unsigned long n, float *tau, float *z,
float *prob) ;

void kend12(float **tab, int i, int j, float *tau, float *z, float *prob);

void kermom(double w[], double y, int m);

void ks2dis(float x1[], float yi[], unsigned long ni,
void (*quadvl) (float, float, float *, float +, float *, float *),
float *d1, float *prob);

void ks2d2s(float x1[], float yi[], unsigned long ni, float x2[], float y2[],
unsigned long n2, float *d, float *prob);

void ksone(float data[], unsigned long n, float (*func) (float), float *d,
float *prob) ;

void kstwo(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *d, float *prob);

void laguer(fcomplex a[], int m, fcomplex *x, int *its);

void lfit(float x[], float y[], float sig[], int ndat, float al], int ia[],
int ma, float **covar, float *chisq, void (*funcs) (float, float [], int));

void linbcg(unsigned long n, double b[], double x[], int itol, double tol,
int itmax, int *iter, double *err);

void linmin(float p[], float xi[], int n, float *fret,
float (*func) (float []));

void Insrch(int n, float xold[], float fold, float g[], float p[], float x[],
float *f, float stpmax, int *check, float (*func) (float []));

void load(float x1, float v[], float y[]);

void loadi(float x1, float vif], float y[]);

void load2(float x2, float v2[], float y[]);

void locate(float xx[], unsigned long n, float x, unsigned long *j);

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Table of Prototype Declarations (nr.h) 935

void lop(double **out, double **u, int n);

void lubksb(float **a, int n, int *indx, float b[]);

void ludemp(float **a, int n, int *indx, float ¥*d);

void machar(int *ibeta, int *it, int *irnd, int *ngrd,
int *machep, int *negep, int *iexp, int *minexp, int *maxexp,
float *eps, float *epsneg, float *xmin, float *xmax);

void matadd(double **a, double **b, double **c, int n);

void matsub(double **a, double **b, double **c, int n);

void medfit(float x[], float y[], int ndata, float *a, float *b, float *abdev);

void memcof(float data[], int n, int m, float *xms, float d[]);

int metrop(float de, float t);

void mgfas(double **u, int n, int maxcyc);

void mglin(double **u, int n, int ncycle);

float midexp(float (*funk) (float), float aa, float bb, int n);

float midinf(float (*funk) (float), float aa, float bb, int n);

float midpnt(float (*func) (float), float a, float b, int n);

float midsql(float (*funk) (float), float aa, float bb, int n);

float midsqu(float (*funk) (float), float aa, float bb, int n);

void miser(float (*func)(float []), float regn[], int ndim, unsigned long npts,
float dith, float *ave, float *var);

void mmid(float y[], float dydx[], int nvar, float xs, float htot,
int nstep, float yout[], void (*derivs)(float, float[], float[]));

void mnbrak(float *ax, float *bx, float *cx, float *fa, float *fb,
float *fc, float (*func) (float));

void mnewt(int ntrial, float x[], int n, float tolx, float tolf);

void moment(float data[], int n, float *ave, float *adev, float *sdev,
float *var, float *skew, float *curt);

void mp2dfr(unsigned char a[], unsigned char s[], int n, int *m);

void mpadd(unsigned char w[], unsigned char u[], unsigned char v[], int n);

void mpdiv(unsigned char q[], unsigned char r[], unsigned char u[],
unsigned char v[], int n, int m);

void mpinv(unsigned char u[], unsigned char v[], int n, int m);

void mplsh(unsigned char u[], int n);

void mpmov(unsigned char u[], unsigned char v[], int n);

void mpmul(unsigned char w[], unsigned char u[], unsigned char v[], int n,
int m);

void mpneg(unsigned char u[], int n);

void mppi(int n);
void mprove(float **a, float **alud, int n, int indx[], float b[],

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

float x{]);

void mpsad(unsigned char w[], unsigned char u[], int n, int iv);

void mpsdv(unsigned char w[], unsigned char u[], int n, int iv, int *ir);

void mpsmu(unsigned char w[], unsigned char u[], int n, int iv);

void mpsqrt(unsigned char w[], unsigned char u[], unsigned char v[], int n,
int m);

void mpsub(int *is, unsigned char w[], unsigned char u[], unsigned char v[],
int n);

void mrqcof (float x[], float y[], float sig[], int ndata, float al],
int ia[], int ma, float **alpha, float beta[], float *chisq,
void (*funcs) (float, float [], float *, float [], int));
void mrqmin(float x[], float y[], float sig[], int ndata, float al],
int ia{], int ma, float **covar, float **alpha, float *chisq,
void (*funcs) (float, float [], float *, float [], int), float *alamda);
void newt(float x[], int n, int *check,
void (*vecfunc) (int, float [], float []));
void odeint(float ystart[], int nvar, float x1, float x2,
float eps, float hi, float hmin, int *nok, int *nbad,
void (*derivs) (float, float [], float []),
void (*rkqs) (float [], float [], int, float *, float, float,
float [], float *, float *, void (*)(float, float [], float [])));
void orthog(int n, float anu[], float alpha[], float beta[], float al],
float b[]);
void pade(double cof[], int n, float *resid);
void pccheb(float d[], float c[], int n);

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

936 Appendix A

void peshft(float a, float b, float d[], int n);

void pearsn(float x[], float y[], unsigned long n, float *r, float *prob,
float *z);

void period(float x[], float y[], int n, float ofac, float hifac,
float px[], float py[], int np, int *nout, int *jmax, float *prob);

void piksr2(int n, float arr[{], float brr[]);

void piksrt(int n, float arr[]);

void pinvs(int ie1, int ie2, int jel, int jsf, int jcl, int k,
float ***c, float **s);

float plgndr(int 1, int m, float x);

float poidev(float xm, long *idum);

void polcoe(float x[], float y[], int n, float cof[]);

void polcof(float xa[], float ya[], int n, float cof[]);

void poldiv(float u[], int n, float v[], int nv, float q[], float r[]);

void polin2(float xia[], float x2a[], float **ya, int m, int n,
float x1, float x2, float *y, float *dy);

void polint(float xa[], float ya[], int n, float x, float *y, float *dy);

void powell(float p[], float **xi, int n, float ftol, int *iter, float *fret,
float (*func) (float []));

void predic(float data[], int ndata, float d[], int m, float future[], int nfut);

float probks(float alam);

void psdes(unsigned long *lword, unsigned long *irword) ;

void pwt(float a[], unsigned long n, int isign);

void pwtset(int n);

float pythag(float a, float b);

void pzextr(int iest, float xest, float yest[], float yz[], float dy[],
int nv);

float qgaus(float (*func) (float), float a, float b);

void qrdcmp(float **a, int n, float *c, float *d, int *sing);

float qromb(float (*func) (float), float a, float b);

float gromo(float (*func) (float), float a, float b,
float (*choose) (float (*)(float), float, float, int));

void qgroot(float p[], int n, float *b, float *c, float eps);

void qrsolv(float **a, int n, float c[], float d[], float b[]);

void qrupdt(float **r, float **qt, int n, float ul], float v[]);

float qsimp(float (*func) (float), float a, float b);

float qtrap(float (*func) (float), float a, float b);

float quad3d(float (*func) (float, float, float), float x1, float x2);

void quadct(float x, float y, float xx[], float yy[], unsigned long nn,
float *fa, float *fb, float *fc, float *fd);

void quadmx(float **a, int n);

void quadvl(float x, float y, float *fa, float *fb, float *fc, float *fd);

float ranO(long *idum) ;

float rani(long *idum) ;

float ran2(long *idum) ;

float ran3(long *idum) ;

float ran4(long *idum) ;

void rank(unsigned long n, unsigned long indx[], unsigned long irank[]);

void ranpt(float pt[], float regn[], int n);

void ratint(float xa[], float ya[], int n, float x, float *y, float *dy);

void ratlsq(double (*fn) (double), double a, double b, int mm, int kk,
double cof[], double *dev);

double ratval(double x, double cof[], int mm, int kk);

float rc(float x, float y);

float rd(float x, float y, float z);

void realft(float data[], unsigned long n, int isign);

void rebin(float rc, int nd, float r[{], float xin[], float xif[]);

void red(int izi, int iz2, int jz1, int jz2, int jm1, int jm2, int jmf,
int ici, int jc1, int jcf, int kc, float ***c, float **s);

void relax(double **u, double **rhs, int n);

void relax2(double **u, double **rhs, int n);

void resid(double **res, double **u, double **rhs, int n);

float revcst(float x[], float y[], int iorder[], int ncity, int n[]);

void reverse(int iorder[], int ncity, int n[]);

doa,

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII


Table of Prototype Declarations (nr.h) 937

float rf(float x, float y, float z);

float rj(float x, float y, float z, float p);

void rk4(float y[], float dydx[], int n, float x, float h, float yout[],
void (*derivs)(float, float [], float []));

void rkck(float y[], float dydx[], int n, float x, float h,
float yout[], float yerr[], void (*derivs) (float, float [], float []));

void rkdumb(float vstart[], int nvar, float x1, float x2, int nstep,
void (*derivs)(float, float [], float []));

void rkqs(float y[], float dydx[], int n, float *x,
float htry, float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float [], float []));

void rlft3(float ***data, float **speq, unsigned long nn1,
unsigned long nn2, unsigned long nn3, int isign);

float rofunc(float b);

void rotate(float **r, float **qt, int n, int i, float a, float b);

void rsolv(float **a, int n, float d[], float b[]);

void rstrct(double **uc, double **uf, int nc);

float rtbis(float (*func) (float), float x1, float x2, float xacc);

float rtflsp(float (*func) (float), float x1, float x2, float xacc);

float rtnewt(void (*funcd) (float, float *, float *), float x1, float x2,
float xacc);

float rtsafe(void (*funcd) (float, float *, float *), float x1, float x2,
float xacc);

float rtsec(float (*func) (float), float x1, float x2, float xacc);

void rzextr(int iest, float xest, float yest[], float yz[], float dy[], int nv);

void savgol(float c[], int np, int nl, int nmr, int 1d, int m);

void score(float xf, float y[], float f[]);

void scrsho(float (*fx)(float));

float select(unsigned long k, unsigned long n, float arr[]);

float selip(unsigned long k, unsigned long n, float arr[]);

void shell(unsigned long n, float al[]);

void shoot(int n, float v[], float f[]);

void shootf(int n, float v[], float f[]);

void simp1(float **a, int mm, int 11[], int nll, int iabf, int *kp,
float *bmax);

void simp2(float **a, int m, int n, int *ip, int kp);

void simp3(float **a, int il, int ki, int ip, int kp);

void simplx(float **a, int m, int n, int m1, int m2, int m3, int *icase,
int izrov[], int iposv[]);

void simpr(float y[], float dydx[], float dfdx[], float *+#dfdy,
int n, float xs, float htot, int nstep, float yout[],
void (*derivs)(float, float [], float []));

void sinft(float y[], int n);

void slvsm2(double **u, double **rhs) ;

void slvsml(double **u, double **rhs) ;

void sncndn(float uu, float emmc, float *sn, float *cn, float *dn);

double snrm(unsigned long n, double sx[], int itol);

void sobseq(int *n, float x[]);

void solvde(int itmax, float conv, float slowc, float scalv[],
int indexv[], int ne, int nb, int m, float **y, float **#c, float *#*s);

void sor(double **a, double **b, double **c, double **d, double **e,
double **f, double **u, int jmax, double rjac);

void sort(unsigned long n, float arr[]);

void sort2(unsigned long n, float arr{], float brr[]);

void sort3(unsigned long n, float ra[], float rb[], float rc[]);

void spctrm(FILE *fp, float p[], int m, int k, int ovrlap);

void spear(float datai[], float data2[], unsigned long n, float *d, float *zd,
float *probd, float *rs, float *probrs) ;

void sphbes(int n, float x, float *sj, float *sy, float *sjp, float *syp);

void splie2(float x1a[], float x2a[], float *+*ya, int m, int n, float **y2a);

void splin2(float x1a[], float x2a[], float **ya, float **y2a, int m, int n,
float x1, float x2, float *y);

void spline(float x[], float y[], int n, float yp1, float ypn, float y2[]);

void splint(float xa[], float ya[], float y2a[], int n, float x, float *y);

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

938 Appendix A

void spread(float y, float yy[], unsigned long n, float x, int m);
void sprsax(float sa[], unsigned long ijal], float x[], float bf],
unsigned long n);
void sprsin(float **a, int n, float thresh, unsigned long nmax, float sa[],
unsigned long ija[]);
void sprspm(float sa[], unsigned long ija[], float sb[], unsigned long ijb[],
float sc[], unsigned long ijc[]);
void sprstm(float sa[], unsigned long ija[], float sb[], unsigned long ijb[],
float thresh, unsigned long nmax, float sc[], unsigned long ijc[]);
void sprstp(float sa[], unsigned long ija[], float sb[], unsigned long ijb[]);
void sprstx(float sa[], unsigned long ija[], float x[], float bf],
unsigned long n);
void stifbs(float y[], float dydx[], int nv, float *xx,
float htry, float eps, float yscal[], float *hdid, float *hnext,
void (*derivs) (float, float » float []));
void stiff(float y[], float dydx[], int n, float *x,
float htry, float eps, float yscal[], float *hdid, float *hnext,

void (*derivs) (float, float » float []));
void stoerm(float y[], float d2y[], int nv, float xs,
float htot, int nstep, float yout[],
void (*derivs) (float, float [], float []));
void svbksb(float **u, float wf], float **v, int m, int n, float b[],
float x{]);

void svdcemp(float **a, int m, int n, float w[], float **v);
void svdfit(float x[], float y[], float sig[], int ndata, float al],
int ma, float **u, float **v, float w[], float *chisq,

void (*funcs) (float, float [], int));
void svdvar(float **v, int ma, float wl], float **cvm);
void toeplz(float r[], float x[], float y[], int n);

void tptest(float datai[], float data2[], unsigned long n, float *t, float *prob);

void tqli(float d[], float e[], int n, float **z);

float trapzd(float (*func) (float), float a, float b, int n);

void tred2(float **a, int n, float d[], float e[]);

void tridag(float a[], float b[], float c{], float r[], float uf],
unsigned long n);

float trncst(float x[], float y[], int iorder[], int ncity, int n[]);

void trnspt(int iorder[], int ncity, int n[]);

void ttest(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *t, float *prob);

void tutest(float datai[], unsigned long ni, float data2[], unsigned long n2,
float *t, float *prob);

void twofft(float datai{], float data2[], float fftif], float fft2[],
unsigned long n);

void vander(double x[], double w[], double q[], int n);

void vegas(float regn[], int ndim, float (*fxn)(float [], float), int init,
unsigned long ncall, int itmx, int nprn, float *tgral, float *sd,
float *chi2a);

void voltra(int n, int m, float tO, float h, float *t, float **f,
float (*g)(int, float), float (*ak)(int, int, float, float));

void wt1(float af], unsigned long n, int isign,
void (*wtstep) (float [], unsigned long, int));

void wtn(float a[], unsigned long nn[], int ndim, int isign,
void (*wtstep) (float [], unsigned long, int));

void wwghts(float wghts[], int n, float h,
void (*kermom) (double [], double ,int));

int zbrac(float (*func) (float), float *x1, float *x2);

void zbrak(float (*fx)(float), float x1, float x2, int n, float xbi[],
float xb2[], int *nb);

float zbrent(float (*func) (float), float x1, float x2, float tol);

void zrhqr(float a[], int m, float rtr[], float rti[]);

float zriddr(float (*func) (float), float x1, float x2, float xacc);

void zroots(fcomplex a[], int m, fcomplex roots[], int polish);

#else /* ANSI */

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

939

Table of Prototype Declarations (nr.h)

/* traditional - K&R */

void addint();
void airy();

Rest of traditional declarations are here on the diskette.

#endif /* ANSI */

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge Univer: Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, is strictly prohibited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

#endif /* _NR_H_ */

Appendix B: — Utility Routines

Non-Copyright Notice: This Appendix and its utility routines are herewith
placed into the public domain. Anyone may copy them freely for any purpose.
We of course accept no liability whatsoever for any such use.

The routines listed below are used by many of the Recipes in this book. The first
routine, nrerror, is invoked to terminate program execution — with an appropriate
message — when a fatal error is encountered. The other routines are used to allocate
and deallocate memory for vectors and matrices, as explained in detail in §1.2. All
memory allocation and deallocation in Numerical Recipes in C is done using these
routines as intermediaries. Therefore, if you want to allocate memory in some
different way, you need change only these routines, not the programs themselves.

On the diskette, these routines are in the file nrutil.c. Also listed here is
the header file nrutil.h. Besides the declarations for the routines in nrutil.h, it
contains several macros that are used throughout the book. The ideas behind some
of these macros were described in §1.2.

Memory Allocation: Advanced Topics

Two issues regarding vector and matrix memory allocation are worth discussing here:
first, a “back-door” compatibility between our matrices and those used by other C software;
second, the question of whether our pointer arithmetic for unit-offset vectors and matrices
violates the ANSI C standard.

1. Back-door compatibility. As explained in §1.2, we always allocate matrices via the
scheme “pointer to an array of pointers to rows” (shown in Figure 1.2.1). With this scheme,
once storage for a matrix is allocated, the top level pointer (the “name” of the matrix) can
be used to store a matrix of any size whose dimensions are less than or equal to the size of
the dimensions allocated. The only time that the allocated size ever again becomes relevant
is when the matrix storage is finally deallocated.

In this scheme, the rows of a matrix need not be stored in physically contiguous storage;
each row has its own pointer addressing it (see Figure 1.2.1). However, the allocation routines
printed below in fact (and by design) allocate an entire matrix as one contiguous block, via
a single call to malloc(). This “back-door” fact allows you to use matrices created with
our routines directly with other software: The address of the first element in a matrix **a
(usually &a[1] [1] if a has been allocated with a statement like a=matrix(1,m,1,n); but
possibly &a[0] [0] if a was created with zero-offsets) is guaranteed to point to the beginning
of a contiguous block containing the full physical matrix, stored by rows. This address can be
used as an argument to other software. (Note that this other software will generally also need
to know the physical size of the matrix, or at least the number of columns.)

2. Unit-offset vectors. In §1.2, we described how a unit-offset vector bb [1. .4] could be
obtained from a zero-offset array b[0. .3] by the pointer arithmetic operation bb=b-1. Since
bb points to one location before b, bb[1] addresses the same element as b[0], and so on.

940

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'SGOM IISIA ‘SWWOHGD 40 Sy00q sedivay jEOUAWINN Japs0 OF “payiqiyoud Ajolys si ‘wayndwios Ja/as Aue 0} (auUO sy} Buipnjoul) s:
-aulyoeul jo BulAdoo Aue Jo ‘uolonpoidas sayjin4 ‘“asn jeuosiad UMO JIAaU} 10} Adoo Jaded auo ayew O} Suasn JeUJE\U! 10} payuRJB S|

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

“AIEMYOS Sadioay ;eOUVOWNN Aq Z661-8B6L (D) IUBUAdOD sweIBolY “ssalg AYSIBAIUN eBplqueD Aq Z661-8861 (D) }

Utility Routines (nrutil.c) 941

Strictly speaking, this scheme is not blessed by the ANSI C standard. The problem
is not the fact that b-1 points to unallocated storage: location b-1 will never be referenced
without an increment back into the allocated region. Rather, the problem is that it might
happen in rare cases (and probably only on a segmented machine) that the expression b-1
has no representation at all. If this occurs, then there is no guarantee that the relation
b=(b-1)+1 is satisfied.

In practice, this is much less of a problem than one might think. We are not aware of
any compiler or machine on which b=(b-n) +n fails to be true for small integer n. Even on a
segmented machine, what typically happens is that the compiler stores some (perhaps illegal)
representation of b-n, and that b is recovered when n is added back to this representation. The
memory allocation routines in the First Edition of Numerical Recipes in C, in wide use since
1988, all have this “problem”, and we have had not even a single report of their failure in this
respect (notwithstanding the many readers who have told us that theoretically it could fail).
We have also communicated to standards bodies the desirability of blessing “b=(b-n) +n” (at
least for some range of n, say n representable as type short) in a future standard, since there
would seem to be no conflict with existing compilers in doing so.

Despite the absence of any experimental (as opposed to theoretical) problem, we have
taken some steps in this edition to make our vector and matrix allocation routines more ANSI
compliant. In the listing that follows, the parameter NR_END is used as a number of extra storage
locations allocated at the beginning of every vector or matrix block, simply for the purpose
of making offset pointer references guaranteed-representable. We set NR_END to a default
value of 1. This has the effect of making all unit-offset allocations (e.g., b=vector (1,7) ;
or a=matrix(1,128,1,128) ;) be strictly ANSI compliant. With NR_LEND = 1, the number
of storage locations wasted is fairly negligible. Allocations with offsets other than 1 (e.g.,
b=vector(2,10)) are still theoretically non-compliant, but are virtually unknown in our
routines. If you need to make such allocations, you may wish to consider increasing the value
of NR_END (noting that larger values increase the amount of wasted storage).

Here is a listing of nrutil.h:

#ifndef _NR_UTILS_H_
#define _NR_UTILS_H_

static float sqrarg;
#define SQR(a) ((sqrarg=(a)) == 0.0 ? 0.0 : sqrarg*sqrarg)

static double dsgrarg;
#define DSQR(a) ((dsqrarg=(a)) == 0.0 ? 0.0 : dsqrarg*dsqrarg)

static double dmaxarg1 ,dmaxarg2;
#define DMAX(a,b) (dmaxargi=(a) ,dmaxarg2=(b),(dmaxarg1) > (dmaxarg2) ?\
(dmaxargi) : (dmaxarg2))

static double dminarg1,dminarg2;
#define DMIN(a,b) (dminargi=(a) ,dminarg2=(b),(dminarg1) < (dminarg2) ?\
(dminargi) : (dminarg2))

static float maxarg1,maxarg2;
#define FMAX(a,b) (maxargi=(a) ,maxarg2=(b),(maxarg1) > (maxarg2) ?\
(maxargi) : (maxarg2))

static float minarg1,minarg2;
#define FMIN(a,b) (minargi=(a) ,minarg2=(b),(minarg1) < (minarg2) ?\
(minargi) : (minarg2))

static long lmaxarg1,1lmaxarg2;
#define LMAX(a,b) (lmaxargi=(a) ,lmaxarg2=(b) ,(1maxarg1) > (lmaxarg2) ?\
(imaxargi1) : (Imaxarg2))

static long lminarg1,lminarg2;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

doa,

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

942 Appendix B

#define LMIN(a,b) (lminargi=(a) ,lminarg2=(b),(lminarg1) < (lminarg2) ?\
(Iminargi) : (lminarg2))

static int imaxarg1,imaxarg2;
#define IMAX(a,b) (imaxargi=(a) ,imaxarg2=(b),(imaxarg1) > (imaxarg2) ?\
(imaxarg1) : (imaxarg2))

static int iminarg1,iminarg2;
#define IMIN(a,b) (iminargi=(a) ,iminarg2=(b),(iminarg1) < (iminarg2) ?\
(iminarg1) : (iminarg2))

doa,

#define SIGN(a,b) ((b) >= 0.0 ? fabs(a) : -fabs(a))

#if defined(__STDC__) || defined(ANSI) || defined(NRANSI) /* ANSI */

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

void nrerror(char error_text[]);

float *vector(long nl, long nh);

int *ivector(long nl, long nh);

unsigned char *cvector(long nl, long nh);

unsigned long *lvector(long nl, long nh);

double *dvector(long nl, long nh);

float **matrix(long nrl, long nrh, long ncl, long nch);

double **dmatrix(long nrl, long nrh, long ncl, long nch);

int **imatrix(long nrl, long nrh, long ncl, long nch);

float **submatrix(float **a, long oldrl, long oldrh, long oldcl, long oldch,
long newrl, long newcl);

float **convert_matrix(float *a, long nrl, long nrh, long ncl, long nch);

float ***f3tensor(long nrl, long nrh, long ncl, long nch, long ndl, long ndh);

void free_vector(float *v, long nl, long nh);

void free_ivector(int *v, long nl, long nh);

void free_cvector(unsigned char *v, long nl, long nh);

void free_lvector(unsigned long *v, long nl, long nh);

void free_dvector(double *v, long nl, long nh);

void free_matrix(float **m, long nrl, long nrh, long ncl, long nch);

void free_dmatrix(double **m, long nrl, long nrh, long ncl, long nch);

void free_imatrix(int **m, long nrl, long nrh, long ncl, long nch);

void free_submatrix(float **b, long nrl, long nrh, long ncl, long nch);

void free_convert_matrix(float **b, long nrl, long nrh, long ncl, long nch);

void free_f3tensor(float ***t, long nrl, long nrh, long ncl, long nch,
long ndl, long ndh);

#else /* ANSI */
/* traditional - K&R */

void nrerror();
float *vector();
Rest of traditional declarations are here on the diskette.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

#endif /* ANSI */

#endif /* _NR_UTILS_H_ */

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

And here is nrutil.c:

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

#include <stdio.h>
#include <stddef.h>
#include <stdlib.h>
#tdefine NR_END 1
#tdefine FREE_ARG char*

void nrerror(char error_text[])
/* Numerical Recipes standard error handler */

{

Utility Routines (nrutil.c) 943

fprintf(stderr,"Numerical Recipes run-time error...\n");
fprintf(stderr,"%s\n",error_text) ;
fprintf(stderr,"...now exiting to system...\n");
exit(1);

}

float *vector(long nl, long nh)
/* allocate a float vector with subscript range v[nl..nh] */

t
float *v;
v=(float *)malloc((size_t) ((nh-nl+1+NR_END)*sizeof (float) ));
if (!v) nrerror("allocation failure in vector()");
return v-nl+NR_END;
}

int *ivector(long nl, long nh)
/* allocate an int vector with subscript range v[nl..nh] */

t
int *v;
v=(int *)malloc((size_t) ((nh-nl+1+NR_END)*sizeof(int)));
if (!v) nrerror("allocation failure in ivector()");
return v-nl+NR_END;

}

unsigned char *cvector(long nl, long nh)
/* allocate an unsigned char vector with subscript range v[nl..nh] */

t
unsigned char *v;
v=(unsigned char *)malloc((size_t) ((nh-nl+1+NR_END)*sizeof (unsigned char)));
if (!v) nrerror("allocation failure in cvector()");
return v-nl+NR_END;
}

unsigned long *lvector(long nl, long nh)
/* allocate an unsigned long vector with subscript range v[nl..nh] */

t
unsigned long *v;
v=(unsigned long *)malloc((size_t) ((nh-n1+1+NR_END)*sizeof (long))) ;
if (!v) nrerror("allocation failure in lvector()");
return v-nl+NR_END;
}

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

double *dvector(long nl, long nh)
/* allocate a double vector with subscript range v[nl..nh] */

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

t
double *v;
v=(double *)malloc((size_t) ((nh-n1+1+NR_END) *sizeof (double) )) ;
if (!v) nrerror("allocation failure in dvector()");
return v-nl+NR_END;
}

float **matrix(long nrl, long nrh, long ncl, long nch)
/* allocate a float matrix with subscript range m[nrl..nrh][ncl..nch] */
t

long i, nrow=nrh-nrl+1,ncol=nch-ncl+1;

float **m;

/* allocate pointers to rows */
m=(float **) malloc((size_t) ((nrow+NR_END) *sizeof (float*)));

944 Appendix B

if (!m) nrerror("allocation failure 1 in matrix()");
m += NR_END;
m -= nrl;

/* allocate rows and set pointers to them */

m(nrl]=(float *) malloc((size_t) ((nrow*ncol+NR_END) *sizeof (float) ));
if (!m[nrl]) nrerror("allocation failure 2 in matrix()");

m{nrl] += NR_END;

m{nrl] -= ncl;

for (i=nrl+1;i<=nrh;it++) m[i]=m[i-1]+ncol;

/* return pointer to array of pointers to rows */
return m;

}

double **dmatrix(long nrl, long mrh, long ncl, long nch)
/* allocate a double matrix with subscript range m[nrl..nrh][ncl..nch] */
t

long i, nrow=nrh-nrl+1,ncol=nch-ncl+1;

double **m;

/* allocate pointers to rows */

m=(double **) malloc((size_t) ((nrow+NR_END)*sizeof (double*) )) ;
if (!m) nrerror("allocation failure 1 in matrix()");

m += NR_END;

m -= nrl;

/* allocate rows and set pointers to them */

m{(nrl]=(double *) malloc((size_t) ((nrow*ncol+NR_END) *sizeof (double) )) ;
if (!m[nrl]) nrerror("allocation failure 2 in matrix()");

m{nrl] += NR_END;

m{nrl] -= ncl;

for (i=nrl+1;i<=nrh;it++) m[i]=m[i-1]+ncol;

/* return pointer to array of pointers to rows */
return m;

}

int **imatrix(long nrl, long nrh, long ncl, long nch)
/* allocate a int matrix with subscript range m[mrl..nrh][ncl..nch] */
t

long i, nrow=nrh-nrl+1,ncol=nch-ncl+1;

int **m;

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

/* allocate pointers to rows */

m=(int **) malloc((size_t) ((nrow+NR_END) *sizeof(int*)));
if (!m) nrerror("allocation failure 1 in matrix()");

m += NR_END;

m -= nrl;

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

/* allocate rows and set pointers to them */

m(nrl]=(int *) malloc((size_t) ((nrow*ncol+NR_END) *sizeof (int)));
if (!m[nrl]) nrerror("allocation failure 2 in matrix()");
m{nr1] NR_END ;

m{nrl] -= ncl;

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

for (i=nrl+1;i<=nrh;it++) m[i]=m[i-1]+ncol;

/* return pointer to array of pointers to rows */
return m;

Utility Routines (nrutil.c) 945

float **submatrix(float **a, long oldrl, long oldrh, long oldcl, long oldch,
long newrl, long newcl)
/* point a submatrix [newrl..][newcl..] to afoldrl..oldrh] [oldcl..oldch] */
{
long i,j,nrow=oldrh-oldr1+1,ncol=oldcl-newcl;
float **m;

/* allocate array of pointers to rows */

m=(float **) malloc((size_t) ((nrow+NR_END)*sizeof(float*)));
if (!m) nrerror("allocation failure in submatrix()");

m += NR_END;

m newrl;

/* set pointers to rows */
for (i=oldrl, j=newrl;i<=oldrh;i++,j++) m{j]=ali]+ncol;

/* return pointer to array of pointers to rows */
return m;

}

float **convert_matrix(float *a, long nrl, long nrh, long ncl, long nch)
/* allocate a float matrix m[{nrl..nrh][ncl..nch] that points to the matrix
declared in the standard C manner as a[nrow][ncol], where nrow=nrh-nrl+1
and ncol=nch-ncl+1. The routine should be called with the address
&a[0][0] as the first argument. */
{

long i,j,nrow=nrh-nrl+1,ncol=nch-ncl+1;

float **m;

/* allocate pointers to rows */

m=(float **) malloc((size_t) ((nrow+NR_END) *sizeof(float*)));
if (!m) nrerror("allocation failure in convert_matrix()");
NR_END;

= nrl;

m

/* set pointers to rows */

m{nrl1]=a-ncl;

for (i=1, j=nrl+1;i<nrow;it++,j++) m{j]=m[j-1]+ncol;
/* return pointer to array of pointers to rows */
return m;

}

float ***f3tensor(long nrl, long nrh, long ncl, long nch, long ndl, long ndh)
/* allocate a float 3tensor with range t[nrl..nrh] [ncl..nch] [ndl..ndh] */
{

long i,j,nrow=nrh-nrl+1,ncol=nch-ncl+1,ndep=ndh-nd1+1;

float ***t;

/* allocate pointers to pointers to rows */

t=(float ***) malloc((size_t) ((nrow+NR_END)*sizeof (float**))) ;
if (!t) nrerror("allocation failure 1 in f3tensor()");

t NR_END;

t nrl;

/* allocate pointers to rows and set pointers to them */
t[nrl]=(float **) malloc((size_t) ((nrow*ncol+NR_END) *sizeof (float*))) ;
if (!t[mrl]) nrerror("allocation failure 2 in f3tensor()");

t{nrl] += NR_END;

t{nrl] -= ncl;

/* allocate rows and set pointers to them */

t{nrl] [ncl]=(float *) malloc((size_t) ((nrow*ncol*ndep+NR_END)*sizeof(float)));

if (!t{mrl][mcl]) nrerror("allocation failure 3 in f3tensor()");

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

946 Appendix B

t{nrl] [ncl] += NR_END;
t{nrl] [ncl] -= ndl;

for(j=nclt+1 nch;j++) t{nrl] [j]=t[nrl] [j-1]+ndep;
for(i=nrl+1; nrh;i++) {

t(i]=t[i-1]+ncol;

+ [i] Incl] =t [i-1] [ncl]+ncol*ndep;

for (j=ncl+1;j<=nch; j++) tli] [j]=tli] [j-1]+ndep;

+

/* return pointer to array of pointers to rows */
return t;

}

void free_vector(float *v, long nl, long nh)
/* free a float vector allocated with vector() */
t
free((FREE_ARG) (v+nl-NR_END));
}

void free_ivector(int *v, long nl, long nh)
/* free an int vector allocated with ivector() */
t
free((FREE_ARG) (v+nl-NR_END));
}

void free_cvector(unsigned char *v, long nl, long nh)
/* free an unsigned char vector allocated with cvector() */
t
free((FREE_ARG) (v+nl-NR_END));
}

void free_lvector(unsigned long *v, long nl, long nh)
/* free an unsigned long vector allocated with lvector() */
t
free((FREE_ARG) (v+nl-NR_END));
}

void free_dvector(double *v, long nl, long nh)
/* free a double vector allocated with dvector() */
t
free((FREE_ARG) (v+nl-NR_END));
}

void free_matrix(float **m, long nrl, long nrh, long ncl, long nch)
/* free a float matrix allocated by matrix() */
t
free((FREE_ARG) (m[nrl]+ncl-NR_END)) ;
free((FREE_ARG) (m+nr1-NR_END)) ;
}

void free_dmatrix(double **m, long nrl, long nrh, long ncl, long nch)
/* free a double matrix allocated by dmatrix() */
t
free((FREE_ARG) (m[nrl]+ncl-NR_END)) ;
free((FREE_ARG) (m+nr1-NR_END)) ;
}

void free_imatrix(int **m, long nrl, long nrh, long ncl, long nch)
/* free an int matrix allocated by imatrix() */
t

free((FREE_ARG) (m[nrl]+ncl-NR_END)) ;

free((FREE_ARG) (m+nr1-NR_END)) ;

Buipnjoul) say ajqepeos

yew Jo BulAdoo Aue Jo ‘uononpoidas sayyin4 “asn yeuossed UMO 4194} 10} Adoo seded auo eyeW 0} SuasN yeUsE}U! 10} PayUeJG SI UOISSIWAg

*(eouaWy YLON epis}no) Bio’ aBpuquieo® les}sno}0~a11p 0} |fewWa PUBS JO ‘(KjUO BOUV@WIY YUON) EZPZ-ZZ8-008- | [29 10 WOO™U'MMy/: cy
JOM USIA ‘SAIOHGD 40 Sy00g sadioay jeOOWNN Japs OJ “payiqiyoud Ajjoujs Ss! ‘’a}ndwod JaAsas Aue 0} (BU SII

“aremyog sedivay jeouawinn Aq 2661-8861 (9) 1UBUIAdoD suIesBolg ‘ssalq Ayan eBpUquIeD kq Z661-86L (9) 1U

dog

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

Utility Routines (nrutil.c) 947

void free_submatrix(float **b, long nrl, long nrh, long ncl, long nch)
/* free a submatrix allocated by submatrix() */
{
free((FREE_ARG) (b+nr1-NR_END)) ;
3

void free_convert_matrix(float **b, long nrl, long nrh, long ncl, long nch)
/* free a matrix allocated by convert_matrix() */
{
free((FREE_ARG) (b+nr1-NR_END)) ;
3

void free_f3tensor(float ***t, long nrl, long nrh, long ncl, long nch,
long ndl, long ndh)
/* free a float f3tensor allocated by f3tensor() */

{
free((FREE_ARG) (t[nrl] [ncl]+nd1-NR_END)) ;
free((FREE_ARG) (t[nrl]+ncl-NR_END)) ;
free((FREE_ARG) (t+nr1-NR_END)) ;

3

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“(eoueWy YON apis}no) Bioabpuquieo@ MasjsnojoauIp 0} |PewWe PUas JO ‘(AJUO BOLOWY YON) EZPZ-ZZ8-008- I [129 40 Woo" JU-MMy/:d)Y
“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

OUSGOM IISIA ‘SWWOHGD JO Sy00q sedioay jeoaWINN Japso OY “payiqiyoud Ajyouys si ‘“a;Nndwios JaAsas Aue 0} (aUO siy} Bulpnjoul) s
-eulyoew jo BulAdoo Aue Jo ‘uononpoides seyin4 “asn yeuossed UMO 4194} 10} Adoo Jaded auo eyeW 0} suasn jeUsa}U! 40} payUeJB Ss}

Appendix C: Complex Arithmetic

The functions listed below are used by the Recipes cisi, frenel, hypdrv,
hypgeo, hypser, laguer, zroots, and fixrts for complex arithmetic. A complex
number is defined to be a structure containing two float values, the real (.r) and
imaginary (.i) parts. Complex arguments are passed and returned by value. See
additional discussion in §1.2.

On the diskette, this Appendix is in the file complex.c.

#include <math.h>
typedef struct FCOMPLEX {float r,i;} fcomplex;

fcomplex Cadd(fcomplex a, fcomplex b)

{
fcomplex c;
c.r=a.rtb.r;
c. .itb.i;
return c;
3
fcomplex Csub(fcomplex a, fcomplex b)
{
fcomplex c;
.r-b.r;
.i-b.i;
return c;
3

fcomplex Cmul(fcomplex a, fcomplex b)

{
fcomplex c;
c.r=a.r*b.r-a.i*b.i;
c.isa.i*b.rta.r*b.i;
return c;
3
fcomplex Complex(float re, float im)
{
fcomplex c;
c.r=re;
c.i=im;
return c;
3

fcomplex Conjg(fcomplex z)
{

fcomplex c;

C.r=Z.r;

948

*(eoLaWy YON episyno) Bio‘eBpuquies@AJesysno}Oa1Ip 0} [EWE PUas JO ‘(AJUO BOVEWY YVON) E%PZ-ZZ8-008-| [fe 40 WOO UMMM): daY

O'USGOM IISIA ‘SWWOHGD 40 Sy00g sedisay jeOUEWINN Japso OF “payiqiyoid Ajolys si ‘ayndwios Ja/as Aue 0} (UO Siu} Bulpnjoul) sally ajqepees

-auiyoeul jo BulAdoo Aue Jo ‘uolonpoidas Jayyin4 ‘asn jeuosiad UMO sau} 410} Adoo saded auo ayew 0} Siasn JaUEU! 10} payueB SI

“aremyog sedioay /eouewnn Aq 2661-886} (9) 1UBUAdOD swWesBoIg ‘ssalq AysianlU eBpUIquIeD Aq 2661-8861 (9) 1YBUAdOD

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

Complex Arithmetic (complex.c) 949

fcomplex Csqrt(fcomplex z)
t
fcomplex c;
float x,y,w,r;
if ((z.r == 0.0) && (z.i == 0.0)) {
c.r=0.0;
c.i=0.0;
return c;
} else {
x=fabs(z.r);
y=fabs(z.i);
if (x >= y) {
r=y/x;
wesqrt (x) *sqrt (0.5*(1.0+sqrt (1.0+r*r))) ;
} else {
r=x/y;
we=sqrt (y) *sqrt (0.5*(rtsqrt (1.0+r#r)));

c.i = -z.i;
return c;
}
fcomplex Cdiv(fcomplex a, fcomplex b)
t
fcomplex c;
float r,den; = on
if (fabs(b.r) >= fabs(b.i)) { = gs
r=b.i/b.r; SBRES8
den=b.r+r*b.i; = $s
c.r=(a.r+r*a.i)/den; = 38
=(a.i- : 28 aL
c.i=(a.i-r*a.r)/den; @aeaus
} else { g a3 8$
r=b.r/b.i; gash2
den=b.it+r*b.r; $2 ses
c.r=(a.r*rta.i)/den; asig0
i=(a.i : poarZ
c.i=(a.i*r-a.r)/den; SoE55
} ezsn2
el azgr
return c; See oD
3 NOESm
+ RES Tae)
S< 400
Gegcom
float Cabs(fcomplex z) 335 22
{ gaRez
float x,y,ans,temp; 38 ean
x=fabs(z.r); 32 30T
y=fabs(z.i); 353 8 z
if (x == 0.0) pean) “3
w
ans°yi 228350
<=28
else if (y == 0.0) S339 a
ans=x; Se2e3 is}
else if (x >y) f{ 83508
temp=y/x; e283 =
ans=x*sqrt (1.0+temp*temp) ; 25265
} else { Ee 2 20
temp=x/y; ge ENCES)
ans=y*sqrt (1.0+temp*temp) ; aes ze
+ 2QRe5
return ans; G25 36
} BR3° O-
SamBe
<acog
@gsSz
98325
32256
z8Ser
2093
as 88S
egars
aoos
SsGrdo
2008
&
aosg
eos?
528 D
SaS°
b<G
22
re
3.
£28
ee

}
if (z.r >= 0.0) {
c.r=w;
c.i=z.i/(2.0*w) ;
} else {
c.i=(z.i >= 0) ? wi: -w;

Appendix C

950

c.r=z.i/(2.0*c.i);

+

return c;

fcomplex RCmul(float x, fcomplex a)

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

Copyright (C) 1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

Permission is granted for internet users to make one paper copy for their own personal use. Further reproduction, or any copying of machine-
readable files (including this one) to any server computer, ited. To order Numerical Recipes books or CDROMs, visit website
http:/Avww.nr.com or call 1-800-872-7423 (North America only), or send email to directcustserv @cambridge.org (outside North America).

X#*a.T;
x*a.i;

fcomplex c;
c.r

ci

return c;

Index of Programs and Dependencies

The following table lists, in alphabetical order, all the routines in Numerical
Recipes. When a routine requires subsidiary routines, either from this book or else
user-supplied, the full dependency tree is shown: A routine calls directly all routines
to which it is connected by a solid line in the column immediately to its right; it
calls indirectly the connected routines in all columns to its right. Typographical

conventions: Routine:

s from this book are in typewriter font (e.g., eulsum, gammln).

The smaller, slanted font is used for the second and subsequent occurences of a routine
in a single dependency tree. (When you are getting routines from the Numerical

Recipes diskettes, or

their archive files, you need only specify names in the larger,

upright font.) User-supplied routines are indicated by the use of text font and square

brackets, e.g., [funcv

. Consult the text for individual specifications of these routines.

The right-hand side of the table lists section and page numbers for each program.

addint —— interp §19.6 (p. 880)
airy bessik Boe ee 86.7 (p. 250)
bessjy beschb chebev
amebsa —— rani . oe §10.9 (p. 452)
amotsa [funk]
— ranl
'— [funk]
amoeba—— amotry—— [funk] . . . . . . .) .) . 8104 (p.411)
'— [funk]
amotry —— [funk] §10.4 (p. 412)
amotsa —— [funk] §10.9 (p. 454)
— rani
anneal —— ran3 §10.9 (p. 448)
-irbitt
[- trncst
;—- metrop ran3
[- trnspt
[- revcst
'— reverse
anorm2 §19.6 (p. 887)
arcmak §20.5 (p. 912)
arcode —— arcsum §20.5 (p. 913)
arcsum §20.5 (p. 914)
avevar §14.2 (p.617)
badluk —— julday 81.1 (p. 13)
"— flmoon
balanc §11.5 (p. 483)

951

OMA YUON) €ZpZ-228-008

}ayNdwod Ja/uas Aue Oo}
lo swei6olg ‘ssalg Aysuaniuy eBpuquueg Aq 2661-8861 (D) 1yBUAdOD

($-80L€b-12S-0 NS!) ONILNdWOOD OISILNAIOS 4O LYV SHL 'O NI SAdIOSY TWOINSWNN Woy e6ed ajdwes

MOWNN Japso OL
ayn “asn jeuosiad

“aremyos sedicay jeouewnn Aq 2661-8861 (O)

‘SIWOHGD 40 syoog sedio:

~aulyoeW Jo julAdoo Aue Jo ‘uononpoid:

WY YON apisyno) B10’ e6puquies@AJesysno\oauIp O}

aqISqom 3!

952 Index of Programs and Dependencies

banbks Be §2.4 (p.54)
bandec Be §2.4 (p.53)
banmul Be §2.4 (p.52)
beucof Be §3.6 (p. 126)
beuint——bcucof . .... . .. . . . 33.6 (p. 127) zaPog
beschb——chebev . . . . . . . 2. 1 ww 86.7 (p. 245) Ss 23
aZss
bessi——bessiO . ...... . 2 1 86.6 (p. 239) Zo 23
bessi0 ww. 86.6 (p. 237) 28 oO
es3gos
bessil Ce ek 86.6 (Pp. 238) ar2e3
bessik —— beschb —— chebev Ce 86.7 (p. 248) 8238
bessj —— bessj0 Bo 86.5 (p. 235) a? 228
—bessji Saas Pa
' NogsaA
bessjO Ce §6.5 (p. 232) Lg 825
63
bessjt1 ow. 86.5 (p. 233) Be so8
Zsz2=
bessjy —— beschb —— chebev So ee 86.7 (p. 243) ge Ay 3 5
bessk—— besskO——bessiO . . . . . . . §6.6 (p. 239) Fe EF
_ bessk1 —bessit gabe
a8!
besskO——bessiO . . . . . . . . ew 86.6 (p. 237) 825 36
Sassen
besskl1—bessil . . . .. ...... . §6.6 (p. 238) e288
bessy——bessyi——bessji . . . ...... 86.5 (p. 234) S398
— bessyO —— bessj0O gs eosin
. aasge
bessyO——bessjO . 2. . 2... ee en 86.5 (p. 232) SIBSO
of0
bessyi——bessjt1 . . . . . . . ee ee §6.5 (p. 233) PEE ze
2oR
beta——gammin. . ..... . ee 86.1 (p. 216) 225 az
gg?
betacf www ee 86.4 (p. 227) $2 28B
betai——gammln . .... . . . ee 86.4 (p. 227) gesz5
— betacf 38328
e338
bico——factln——gammln .......... 86.1 (p. 215) geeks
esgzao
bksub 2. ee ee 817.3 (p. 770) SssRe
. Look
bnidev——rani. . 2. 2... we en §7.3 (p. 295) ae o
‘— gamm1n 92 Fa
288
brent [func] . . . 2. . . 810.2 (p. 404) 2588
broydn fmin re §9.7 (p. 390) 35S
L- fdjac — [funcv] 3s 3
2s
[- qrdcmp . gs
[- qrupdt —— rotate °
[- rsolv
'— Insrch —— fmin [funcv]
bsstep——mmid——[derivs] . . . . . . . . . 8164 (p.728)
— pzextr

caldat . 2... we BLL (pe 14)

Index of Programs and Dependencies 953

chder
chebev
chebft —— [func]
chebpc
chint
chixy
choldc
cholsl
chsone gammq gser
get
chstwo gammq gser
get
cisi
cntab1i gammq gser
get
cntab2
convlv twofft :
realft fourt
copy
correl twofft :
realft fourt
cosft1—— realft —— fourl
cosft2—— realft —— fourl
covsrt
crank
cyclic —— tridag
daub4
dawson
dbrent —— [func]
'— [dfunc]
ddpoly
decchk :
df 1dim — [dfunc]
dfourt1
dfpmin —— [func]
t— [dfunc]
'— Insrch —— [func]
dfridr — [func]
dftcor

gammln

gammln

gammln

§5.9 (p. 195)
§5.8 (p. 193)
§5.8 (p. 192)
§5.10 (p. 197)
§5.9 (p. 195)
§15.3 (p.670)
§2.9 (p.97)
§2.9 (p.97)
§143 (p.621)

§143 (p.622)

§6.9 (p.258)
§14.4 (p.631)

§14.4 (p. 635)
§13.1 (p. 543)

§19.6 (p. 881)
§13.2 (p. 546)

§12.3 (p. 518)
§12.3 (p. 520)
§15.4 (p. 675)
§14.6 (p. 642)

§2.7 (p.75)
§13.10 (p. 595)
§6.10 (p. 260)
§10.3 (p.406)

§5.3 (p. 174)

§20.3 (p. 902)

§10.6 (p. 425)

double version of four1, q.v.
§10.7 (p. 428)

§5.7 (p. 188)
§13.9 (p. 587)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

954 Index of Programs and Dependencies

dftint —— [func] Be ee 813.9 (p. 588)
| realft —— fourl
[- polint
'— dftcor
difeq : §17.4 (p.778)
dlinmin mnbrak fidim Lo. §10.6 (p. 424) z Q
dbrent dfidim [func] $ 2
F oO
dpythag . . . . . . . . . +. double version of pythag, q.v. 2 8
drealft . . ....... . double version of realft, q.v. Beane
as
dsprsax . . . . . . . . . . ~~ double version of sprsax, q.v. 3 cz 8 z
dsprstx  . 2... wee double version of sprstx, q.v. Bas 8 =
dsvbksb . . . . . . . . . «~~ double version of svbksb, q.v. geste
8338s
dsvdcemp . . . . . . double version of svdcmp, q.v. at a 3 5
npogs.
eclass . . . . . ee ee ee ee 88.6 (p. 346) x3 $29
. + e®ascm
eclazz——[equiv] . . . . . . . ee ee, §8.6 (p. 346) 23 3 22
°
eigstt 2... eee ee ee LLL (p. 468) 238 vi
. FEB
elle+-rf ..... . ee eee 86.11 (p. 268) gs 3 83
rd 22889
ellf—rf ...... ee ee 86.11 (p. 268) ‘ 28
ellpit-rf.... ee ee 86.11 (p. 269) 28894
. ®
rj re ea
—rf 2 ZO
Ss oo
elmhes . . ... . . ee ee ee 81S (p. 485) S as
erff gammp gser Ce §6.2 (p. 220) a eS
gct gammln g ge
n
erffc gammp gser Ce §6.2 (p. 220) 8 <2
ect gammln 8 5 &
s88aN
gcf gammln $ xy a
S @
erfec . 2 1. ee 86.2 (p. 221) e gs
eulsum Be §5.1 (p. 167) fa %
" o Fa
evlmem Be 813.7 (9. 575) z S
expdev——rani. . 2... we 87.2 (p. 287) p23
expint . . . . 0... 86.3 (p. 223) 3
fidim——[func] . 2. 2. 2 ww eee 810.5 (p. 419) £
factln—gammin . ...... . 86.1 (p.215)
factrl—gammin . ...... . . 86.1 (p.214)
fasper—7—avevar . . . . . . . . . 813.8 (p. 582)
[- spread
'— realft —— fourl

fdjac —— [funcv] Se §9.7 (p. 388)

Index of Programs and Dependencies

955

fgauss
£1110
fit gammq gser :
gcft gammln
fitexy —— avevar soe
}- fit gammq gser
gcf gammln
-- chixy
[- mnbrak
|~ brent
; gammq gser
get gammln
'— zbrent —— chixy
fixrts —— zroots —— laguer
fleg
flmoon
fmin — [funcv]
fourl
fourew
fourfs —— fourew
fourn
fpoly
fred2 —— gauleg
r [ak]
rls]
;— ludcmp
— lubksb
fredex quadmx wwghts kermom
|~ ludcmp
"— lubksb
fredin —— [ak]
— [gl
frenel
frprmn —— [func]
t— [dfunc]
linmin mnbrak
brent fidim [func]
ftest —— avevar :
betai gammin
— betacf
gamdev —— ran1

gammln

8
8
8

8

8
8
8
§
§
8
8
§

8
8

8

8

15.5 (p.688)
19.6 (p.881)
15.2 (p.665)

15.3 (p. 668)

13.6 (p. 569)
5.4 (p. 680)
81.0 (p.1)
§9.7 (p.388)
2.2 (p.507)
2.6 (p.535)
12.6 (p. 532)
24 (p.523)
5.4 (p. 680)
18.1 (p. 793)

18.3 (p. 802)

18.1 (p. 793)

§6.9 (p.256)
10.6 (p.423)

142 (p.619)

87.3 (p.292)
86.1 (p.214)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
8
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uonjonpoides seyyin4 “asn yeuosied uMO

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

956

Index of Programs and Dependencies

gammp gser . §6.2 (p. 218)
gcf gammin
gammq gser . §6.2 (p. 218)
gcf gammin
gasdev —— ranl §7.2 (p. 289)
gaucof —— tqli—— pythag 84.5 (p. 157)
'— eigsrt
gauher 84.5 (p. 154)
gaujac —— gammln 84.5 (p. 155)
gaulag —— gammln 84.5 (p. 152)
gauleg 84.5 (p. 152)
gaussj §2.1  (p.39)
gcf —— gammln 86.2 (p. 219)
golden —— [func] §10.1 (p. 401)
gser—— gammln . 86.2 (p. 218)
hpsel —— sort 88.5 (p. 344)
hpsort §8.3 (p. 337)
hqr §11.6 (p. 491)
hufapp §20.4 (p. 907)
hufdec §20.4 (p. 908)
hufenc §20.4 (p. 907)
hufmak —— hufapp §20.4 (p. 906)
hunt §3.4 (p. 118)
hypdrv 86.12 (p. 273)
hypgeo —— hypser . : 86.12 (p.272)
odeint bsstep mmid
— pzextr
— hypdrv
hypser 86.12 (p. 273)
icre ——icrcl §20.3 (p. 900)
icrct §20.3 (p. 900)
igray §20.2 (p. 896)
iindexx . long int version of indexx, q.v.
indexx 88.4 (p. 338)
interp §19.6 (p. 880)
irbit1 87.4 (p. 297)
irbit2 87.4 (p. 298)
jacobi §11.1 (p. 467)
jacobn §16.6 (p. 742)
julday §1.1 (p.1)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

Index of Programs and Dependencies 957

kendli—— erfcc Se ee 814.6 (p. 643)
kend12 —— erfcc Soe ee 814.6 (p. 644)
kermom Be 818.3 (p. 801)
ks2dis—7—-quadct . . . . . . 2. 1 1... 814.7 (p. 647)
f- quadvl 2 2
pearsn betai gammin $ 2
— betacf 3 3
'_ probks 2 3
8 az
ks2d2s—7—quadct . . . . . . . 1... 814.7 (p. 649) 3 2 8 3
[~ pearsn —— betai —— gammln 8 ees
_betacf sou
eezce
'— probks 83 FA g Pe
ksone—— sort . . . . . . . ee. 814.3 (p. 625) RSs ain
- [func] s< as o
— probks 3 222
kstwo——sort . . . . 2... ee ee 814.3 (p. 625) 38 ean
333 0r
— probks sua
BSBSR
aguer a 89.5 (p.373) gaz
22889
fit——[funces] . 2. 2 we ee 81S.4 (p. 674) P ae
| gaussj 8 ° i
— covsrt gesea
inbcg—7—atimes . . . . . . ee §2.7 (p. 86) SIBSO
I- snrm Fy <5
— asolve S 3
inmin mnbrak Soe ee ee 810.5 (p. 419) é g2
brent fidim [func] agacs
8 Zo
msrch——[func] . . . . . 2 2. ew ee 89.7 (p. 385) 28328
a Oe
ocate «ww ee 83.4 (p. 117) 8 ze
Op ee $19.6 (p. 887) eeege
ubksbo 5. ee 82.3 (p47) e g
Zz
udemp  . we $2.3 (p. 46) 2285
>
machar ww wee ee 820.1 (p. 892) 3
matadd | ww we 819.6 (p. 887)
matsub Be ee ee 819.6 (p. 887)
medfit —— rofunc —— select woe ee ee. 815.7 (p. 704)
memcof Be ee. 813.6 (p. 568)
metrop——ran3... . . . . . 2. . «810.9 (p. 451)


958 Index of Programs and Dependencies

mgfas —— rstrct foe
;- slvsm2 —— fi110
;- interp

p— Copy

;~ relax2

;—- lop

;- matsub

;~ anorm2

— matadd

mglin—— rstrct foe
;- slvsml —— fi110
;- interp
;_ Copy
;—- relax
;- resid
r— £i110
— addint

interp
midinf —— [func
midpnt —— [func

miser —— ranpt —— rani
— [func]

mmid —— [derivs]
mnbrak —— [func

mnewt —— [usrfun]
;— ludcmp
— lubksb

moment

mp2dfr —— mpops

dfourt

mpdiv mpinv mpmul
—— mpops

drealft

mpmul drealft

—— mpops

dfourl

mpinv mpmul drealft
—— mpops

mpmul —— drealft —— dfour1

mpops

dfourt

mppi mpsqrt mpmul
—— mpops

[— mpops

drealft

mpmul drealft

dfourl

mpinv mpmul

"— mp2dfr —— mpops

mprove —— lubksb

drealft

dfourt

dfourl

§19.6 (p. 885)

§19.6 (p.878)

84.4 (p. 144)
84.4 (p. 142)
§7.8 (p.325)

§16.3 (p. 723)
§10.1 (p.400)
§9.6 (p. 381)

§14.1 (p. 613)
§20.6 (p. 922)
§20.6 (p. 920)

§20.6 (p. 920)

§20.6 (p. 918)
§20.6 (p. 916)
§20.6 (p. 923)

§2.5  (p.56)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

Index of Programs and Dependencies

959

mpsqrt mpmul drealft dfourt
"— mpops
mrqcof —— [funcs]
mrqmin—— mrqcof —— [funcs]
| gaussj
— covsrt
newt fmin :
[- fdjac -— [funcv]
|~ ludcmp
| lubksb
'— Insrch —— fmin [funcv]
odeint —— [derivs]
rkqs [derivs]
'— rkck —— [derivs]
orthog
pade —— ludcmp
| lubksb
"— mprove lubksb
pecheb
peshft
pearsn —— betai —— gammln
— betacf
period —— avevar
piksr2
piksrt
pinvs
plgndr
poidev—— rant
— gammln
polcoe
polcof —— polint
poldiv
polin2 —— polint
polint
powell —— [func] an
linmin mnbrak
brent fidim [func]
predic
probks
psdes
pwt

pwtset

§20.6 (p.921)

§15.5 (p.687)
§15.5 (p. 685)

§9.7 (p. 386)

§16.2 (p.

§4.5 (p.
§5.12 (p.

85.11 (p.
§5.10 (p.
§14.5 (p.

§13.8 (
§8.1 (
§8.1 (

817.3 (p.

56.8 (

87.3 (p

§3.5 (

§3.5 (p.

§5.3 (

§3.6 (p.
p.

§3.1 (

p

p

p.

p.

p.

p.

721)

159)
202)

199)
198)
638)

.579)
.331)

330)
770)
254)
294)

121)
121)
175)
124)
109)

§10.5 (p.417)

§13.6 (p.
§14.3 (p.

§7.5 (p.
§13.10 (p.

570)
626)
302)
597)

§13.10 (p. 596)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

960 Index of Programs and Dependencies

pythag Be §2.6 (p.70)
pzextr Be 816.4 (p. 731)
qgaus——[func] . . 2. ee 84.5 (p. 148)
qrdcmp Be ee 82.10 (p. 99)
qromb—— trapzd——[func] . . . . . . . 84.3 (p. 140)
— polint 8 ge
qromo——midpnt—— [func] . . . . . . . 84.4 (p. 143) 28 as
— polint 5 3 358
qroot——poldiv . . ... . . ee §9.5 (p.378) 833 ss
qrsolv —— rsolv Se ee 82.10 (p. 100) gauge
qrupdt——rotate . . . .. . . . 2...) . 82.10 (p. 101) +3 soK
qsimp ——trapzd——[func] . . . . . . .. §4.2 (p. 139) 233 ge
qtrap——trapzd——[func] . . . . . . 2... §4.2 (p. 137) Regan
quad3d——qgaus—— [func] . . . . . . . . §4.6 (p. 164) s< ae A
L [z1] 33302
quadct 7 we ee 8147 (p. 648) S289
quadmx —— wwghts —— kermom foe ee eee 818.3 (p. 802) 2 ass 2
guadvl we ee 8147 (p. 648) 3 z92
vanO . 1 we ee 87.1 (p. 279) 2g2a0
rant. we ee 87 (p. 280) 53858
ran2 ww ee 87 (p. 282) abs
ran3 we 87 (p. 283) 5Z5a5
ran4—psdes . . . . . . ee ee 87.5 (p. 303) s2208
rank 2. we ee ee 88.4 (p34) Dee
ranpt——rani ....... . . eee §7.8 (p. 327) 233 aa
ratint 2 ow. ee 83.2 (D112) gaze
ratlsq—- [fn] Lo §5.13 (p. 206) Bases
|[- dsvdcmp —— dpythag Boao
L asvbksb 8923
'_ ratval g23 8
ratval 2... eee ee 85.3 (p. 176) Peed
TO. ee 86.11 (p. 267) £83
rd. ee $6.11 (p. 264) #3
realft —— four1 Se 812.3 (p. 513) ,
rebin 2... ee §7.8 (p. 323)
red ww 817.3 (PTT)
relax 2.) ww ee 819.6 (p. 881)

relax2 Se ee ee. 819.6 (p. 886)

Index of Programs and Dependencies 961

resid 2... ww ee 819.6 (p. 881)
revcst Be ee ee 810.9 (p. 449)
reverse 2. 1... ww ee ee ee 810.9 (p. 449)
re 86.11 (p. 264)
yjaore . we 86.11 (p. 265)
xf gagge
rk4 — [derivs] ee 8161 (p. 712) sages
rkck — [derivs] ee $16.2 (p.719) SF208
rkdumb —— [derivs] Coe ee 816.1 (p. 713) PETES]
__ rk4 — [derivs] gauge
rkqs —— rkck —— [derivs] woe ee ee. 816.2 (p. 719) +3 soK
rlft3——foun. ..... . . . . . 2 «$12.5 (p.528) ECE:
rofunc——select . . . . . . . . . . . .  §15,7 (p.704) Regan
rotate... ww ee ee 82.10 (p. 101) Been
rsolv 2... ee ee ee $2.10 (p. 100) 3 222
rstret wwe ee $19.6 (p. 880) 38 ge4
rtbis—[func] . . 2... ee ee 89.1 (p. 354) aes gh
rtflsp—[func] . . . . . . . 2 we. 89.2 (p. 356) ga: 23
rtnewt——[funced] . . . 2... 2. we. 89-4 (p. 365) 2aes a
rtsafe——ffuned]) . . . . . 2 2 ee es 89-4 (p. 366) g3 290
rtsec——[func] . 2. 2... we ee 89.2 (p. 357) 3 goa2
rzextr Be 816.4 (p. 731) sR 58
savgol—j—ludcmp . ...... . . . . .) §14.8 (p..652) 22825
'_ lubksb re
scrsho——[func] . 2. 1 1 ew ew ee 89.0 (p. 349) 5 oe
select 2 1. ee ee ee 88.5 (342) agacs
selip——shell . . . .. . . 2. 1... 88.5 (p.343) 22328
sfroid——plgndr .. . 817.4 (p.777) &8 aes
solvde difeq Q 8298
— pinvs zeege
-- red Zoxe
_ bksub Case
shell . 2... ee §8.1 (p. 332) g7s5
shoot——[load] . . 2. 2 ee ee ee BTA (p.758) 3 &o
odeint [derivs] 3 5 Fy
— rkqs —— rkck —— [derivs] : a3
_ [score] ,
shootf——[loadl] . . 2... 2 2. eee. 8172 (p.761)
I~ odeint —— [derivs]
— rkqs —— rkck —— [derivs]
t— [score]

— [load2]

962 Index of Programs and Dependencies

simp1
simp2
simp3
simplx—— simp1
|- simp2
—— simp3
simpr —— ludcmp
;—- lubksb
‘— [derivs]
sinft —— realft —— fourl

slvsm2 —— fi110
slvsml —— fi110

sncndn

snrm

sobseq

solvde —— difeq
}~ pinvs
t- red
'— bksub

sor

sort

sort2

sort3 —— indexx

spctrm—— fourl

spear —— sort2
p— crank
p- erfcc

betai gammin

— betacf

sphbes bessjy beschb chebev
sphfpt —— newt —— fdjac — shootf (q.v.)
;—- lnsrch
;- fmin |
;— ludcmp
— lubksb

'shootf (q.v.)

sphoot —— newt —— fdjac —— shoot (q.v.)
;— lnsrch

;- fmin |
r— ludcmp
— lubksb

‘shoot (q.V.)

splie2—— spline
splin2—— splint
'— spline

§10.8 (p. 441)
§10.8 (p. 442)
§10.8 (p. 442)
§10.8 (p.439)

§16.6 (p. 743)

§12.3 (p.517)
§19.6 (p. 887)
§19.6 (p. 880)
§6.11 (p.270)
§2.7 (p.88)
87.7 (p.312)
817.3 (p. 768)

§19.5 (p. 869)
§8.2 (p.333)
§8.2 (p. 334)
§8.4 (p. 340)

§13.4 (p.557)

§14.6 (p. 641)

§6.7 (p.251)
8174 (p.781)

§17.4 (p. 780)

§3.6 (p. 128)
§3.6 (p. 128)

Jeo JO Woo suMMy/:dy

‘(Ajuo eouewy YUON) EZrZ-228-008-

*(eoveWY YUON episyno) B10‘eBpuquiex® Mes}snojoauIp 0} |reWa pues

“sayndwiod Jansas Aue 0} (aU0 S

($-801€b-12S-0 NASI) ONILNdWOD OISILNSIOS JO LYV SHL:0 NI SAdIO3Y TWOIMSINNN Woy eBed aides

“aremyos sedisay jeouewnn Aq Z661-B861. (9) 1UBUAdOD susesBolg "ssaid AysienluN eBpuquued Kq Z661-BB6l

Index of Programs and Dependencies

963

spline
splint
spread
sprsax
sprsin
sprspm
sprstm
sprstp——iindexx .
sprstx
stifbs —— jacobn
|~ simpr —— ludcmp
;— lubksb
— [derivs]
— pzextr
stiff —— jacobn
;— ludcmp
;— lubksb
— [derivs]
stoerm —— [derivs]
svbksb
svdcmp —— pythag
svdfit —— [funcs] .
|- svdcmp —— pythag
"— svbksb
svdvar
toep1z
tptest —— avevar .
betai gammln
— betacf
tqli——pythag .
trapzd —— [func]
tred2
tridag
trncst
trnspt
ttest —— avevar .
betai gammin
— betacf
tutest —— avevar .
betai gammln
— betacf
twofft ——fourl

§3.3 (p. 115)
§3.3 (p. 116)
§13.8 (p. 583)
§2.7. (p.79)
§2.7. (p.79)
§2.7 (p.82)
§2.7 (p.82)
§2.7 (p. 80)
§2.7 (p.80)

§16.6 (p. 744)

§16.6 (p. 739)

§16.5 (p. 733)
§2.6 (p.64)
§2.6 (p.67)
§15.4 (p. 678)

§15.4 (p. 679)
§2.8 (p.95)
§14.2 (p. 618)

§11.3 (p. 480)
§4.2 (p. 137)
§11.2 (p.474)
§2.4 (p.51)
§10.9 (p.450)
§10.9 (p.450)
§14.2 (p. 616)

§14.2 (p. 617)

§12.3 (p. 511)

Buipnjoul) say ajqepeos

2
@
Fy
2
5
a
s
=
g
3
2
©
5
g
3
io
3
B
R
é
°
s
3
9
B
3
g
3
fo}
3
g
sz
Fg

*(eoveWY YON episyno) Bio‘eBpuquied® Mesisnojoauip 0} |eWe Pues JO ‘(AJUO BOLEWY YUON) EzbPZ-ZZ8-008-| [129 JO WOo'U'MMy/:cY

BJISGOM SIA ‘SWIOHGD 40 Sy00g sadioay jeoUaWINN Japso O| “payiqiyoud Ajjouys Ss! “4a}NdWOd JaAsas Aue 0} (U0 S|

-aulyoew jo BulAdoo Aue Jo ‘uojonposdes sayyin4 “asn yeuosiad uMO.

($-80L€b-L2S-0 NASI) DNILAAWOD SISILNSIOS SO LYW SHL:O NI SAdIOSY TVOINSWNN Woy ebed ejdwes

“aleMYyOs sedioay JeouewWNnN Aq 2661-8861 (O) 1UBUAdoD suresBOIg “Ssalg AysienluN eGpuquied Aq 2661-8861 (O) 1BUAdOD

Index of Programs and Dependencies

964

§2.8 (p.92)
§7.8 (p. 320)

vander

;~ ran2
— [fxn]

vegas —— rebin

Sample page from NUMERICAL RECIPES IN C: THE ART OF SCIENTIFIC COMPUTING (ISBN 0-521-43108-5)

http://www. | tr. com or call

~~ ~~
a was
a as Ss
& mS 6
S aa a
o eon
is eon
20 7 Tt
= an
om) = a &
tor to
a..0
aa 8
oO fs
a esau y 8
SSaa 22 x
LI| | «6 6
| U0 8D
3 a
4 re)
p a
q 4 a
° Pp pe
ES BB BS

§9.1 (p. 352)
§9.1 (p. 352)
§9.3 (p. 361)

zbrac —— [func]

zbrak —— [func]

zbrent —— [func]

§9.5 (p.375)

— hqr

zrhqr —— balanc

§9.2 (p. 358)
§9.5 (p.374)

zriddr —— [func]

is one) to any server computer,
-800-872-7423 (North America only),

zroots —— laguer

1988-1992 by Cambridge University Press. Programs Copyright (C) 1988-1992 by Numerical Recipes Software.

send email to directcustserv@cambridge.org (outside North America).

General Index

Accelerated convergence of series 166ff.
Accuracy 28f.
achievable in minimization 398, 404, 410
achievable in root finding 353
contrasted with fidelity 841, 849
CPU different from memory 186
vs. stability 710, 736, 839, 853
Acknowledgments — xii
Adams-Bashford-Moulton method 749
Adams’ stopping criterion 373
Adaptive integration 129, 141, 709, 714ff.,
725ff., 733f., 737, 744, 749f., 797
Adaptive Monte Carlo integration 316ff.,
319ff.
Addition, multiple precision 916
Addition theorem, elliptic integrals 262
ADI (alternating direction implicit) method
856, 870f., 915
Adjoint operator 876
Adobe Illustrator xiii, xvii
Advective equation 835
AGM (arithmetic geometric mean) 915
Airy function 210, 240, 250
routine for 250f.
Aitken’s delta squared process 166
Aitken’s interpolation algorithm 108
Algorithms, non-numerical 889ff.
Aliasing 501, 576
see also Fourier transform
All-poles model 573
see also Maximum entropy method (MEM)
All-zeros model 573
see also Periodogram
Allocation of storage 19, 21f., 940ff.
Alternating-direction implicit method (ADI)
856, 870f., 915
Alternating series 166f.
Alternative extended Simpson’s rule 134
Amoeba 410
see also Simplex, method of Nelder and
Mead

Amplification factor 837, 839, 841, 849, 854f.

Amplitude error 840
Analog-to-digital converter 821, 894
Analyticity 201
Analyze/factorize/operate package 71f., 833
Anderson-Darling statistic 626f.
Andrew’s sine 702
Annealing, method of simulated 394f., 444ff.
as ment 454f.
for continuous variables 444, 451f.

schedule 445
thermodynamic analogy 444f.
traveling salesman problem 445ff.
ANSI C standard 2f., 14, 25, 930, 941
ANSI macro 17, 930
Antonov-Saleev variant of Sobol’ sequence
310ff.
Apple xvii
Macintosh 894
Approximate inverse of matrix 57
Approximation of functions 105f.
by Chebyshev polynomials 191f., 519
Padé approximant 200ff.
by rational functions 204ff.
by wavelets 601f., 791
see also Fitting
Arguments, conversion of data types 24f., 930
Arithmetic
arbitrary precision 889, 915ff.
complex 23f., 948ff.
floating point 889
TEEE standard 285, 890f.
rounding 890
Arithmetic coding 889, 910ff.
Arithmetic-geometric mean (AGM) method
915
Array
centered subarray of 119
how to allocate 19
index range 18
one-dimensional 18
relation to C pointer 18
three-dimensional 23
two-dimensional 20f.
unit-offset 18, 940f.
variable dimension 20
zero-offset 18
Artificial viscosity 840, 846
Ascending transformation, elliptic integrals
262
ASCII character set 5, 896, 903, 910
Assembly language 278
Associated Legendre polynomials 252f., 773
recurrence relation for 253
relation to Legendre polynomials 252
Association, measures of 610, 628ff.
Asymptotic series 167
exponential integral 224
Attenuation factors 590
Autocorrelation
in linear prediction 565


966

Index

use of FFT 545
Wiener-Khinchin theorem 498, 574
AUTODIN-II polynomial 898
Autonomous differential equations 735f.
Autoregressive model (AR) see Maximum en-
tropy method (MEM)
Average deviation of distribution 611
Averaging kernel, in Backus-Gilbert method
816

Backsubstitution 42, 47, 50, 98
in band diagonal matrix 54
in Cholesky decomposition 97
complex equations 49
direct for computing A! -B 48
relaxation solution of boundary value prob-
lems 764
in singular value decomposition 64
Backtracking 427
in quasi-Newton methods 384
Backus-Gilbert method 815ff.
Backward deflation 370
Bader-Deuflhard method 737, 742f.
Bairstow’s method 371, 376f.
Balancing 483
Band diagonal matrix 50, 51ff.
backsubstitution 54
LU decomposition 53f.
multiply by vector 52f.
storage 52
Band-pass filter 558, 562
wavelets 592, 599f.
Bandwidth limited function 501
Bank accounts, checksum for 902
Bar codes, checksum for 902
Bartlett window 554
Base of representation 28, 890
BASIC, Numerical Recipes in xv, 1
Basis functions in general linear least squares
671
Bayes’ Theorem 819
Bayesian
approach to inverse problems 808, 820,
825f.
contrasted with frequentist 819
vs. historic maximum entropy method
825f.
views on straight line fitting 670
Bays’ shuffle 280
Bernoulli number 138
Bessel functions 230ff., 240ff.
asymptotic form 230, 236
complex 210
continued fraction 240f., 246f.
double precision 230
fractional order 230, 240ff.
Miller’s algorithm 181, 234
modified 236ff.
modified, fractional order 246ff.
modified, normalization formula 239, 246
modified, routines for 237ff.
normalization formula 181
recurrence relation 178, 231, 239, 241f.
reflection formulas 242

reflection formulas, modified functions
247
routines for 232ff., 243ff.
routines for modified functions 248f.
series for 166, 230
series for K, 247
series for Y, 242
spherical 240, 251
turning point 241
Wronskian 240, 246
Best-fit parameters 656, 662, 666, 703
see also Fitting
Beta function 213
incomplete see Incomplete beta function
BFGS algorithm see Broyden-Fletcher-Goldfarb-
Shanno algorithm
Bias, of exponent 28
Bias, removal in linear prediction 570
Biconjugacy 84
Biconjugate gradient method
elliptic partial differential equations 833
preconditioning 85f., 833
for sparse system 84f., 606
Bicubic interpolation 125f.
Bicubic spline 127f.
Big-endian 302
Bilinear interpolation 123f.
Binomial coefficients 213
recurrences for 215
Binomial probability function 215
cumulative 229
deviates from 290, 295f.
Binormal distribution 637, 695
Biorthogonality 84
Bisection 117, 366
compared to minimum bracketing 397f.,
399.
minimum finding with derivatives 406
root finding 350, 353f., 359ff., 397, 476
BISYNCH 898
Bit 28
reversal in fast Fourier transform (FFT)
505f., 532
Bitwise logical functions 296ff., 898f.
Block-by-block method 797
Block of statements 6
Bode’s rule 132
Boltzmann probability distribution 445
Boltzmann’s constant 445
Bootstrap method 691f.
Bordering method for Toeplitz matrix 92f.
Borwein and Borwein method for 7 915
Boundary 161f., 432f., 753
Boundary conditions
for differential equations 707f.
initial value problems 708
in multigrid method 877f.
partial differential equations 514, 828ff.,
857ff.
for spheroidal harmonics 774
two-point boundary value problems 708,
753ff.
Boundary value problems see Differential
equations; Elliptic partial differential


Index

967

equations; Two-point boundary value
problems
Box-Muller algorithm for normal deviate 289
Bracketing
of function minimum 350, 397ff., 409
of roots 348, 350ff., 360, 369, 371, 376,
397
Branch cut, for hypergeometric function 209f.
Branching 8
Break iteration 12f.
Brenner, N.M. 506, 522
Brent’s method
minimization 395f., 402ff., 666
minimization, using derivative 396, 406
root finding 348, 356, 666
Broyden-Fletcher-Goldfarb-Shanno algorithm
397, 426ff.
Broyden’s method 380, 389ff., 393
singular Jacobian 393
Bubble sort 330
Bugs
in compilers xiii
how to report iv, xviii
Bulirsch-Stoer
algorithm for rational function interpolation
111.
method (differential equations) 209, 272,
708f., 712, 722, 724ff., 733, 747
method (differential equations), stepsize
control 725, 733f.
for second order equations
Burg’s LP algorithm 568
Byte 28

733

Crt 7,24
C (programming language) 11
ANSI 2f., 14, 25, 930, 941
C++ 7,24
compilers 3
control structures 5
deficiencies 16, 24f., 26f.
external functions 25
features 15f.
function declaration 17
function definition 17
header (.h) file 17
implicit conversions 24f., 930
Kernighan and Ritchie 2, 16, 24, 930
nature of 15f.
Numerical Recipes in xv, 1
operator associativity 25f.
operator precedence 25f.
prototypes 2, 25, 930
vectors in 18
Calendar algorithms
Calibration 659
is, sorting a hand of 330
on’s elliptic integrals 261f.
Cash-Karp parameters 716f.
Cauchy probability distribution see Lorentzian
probability distribution
Cauchy problem for partial differential equa-
tions 827f.
Cayley’s representation of exp(—iHt)

If., 11 ff.

853

CCITT (Comité Consultatif International Télé-
graphique et Téléphonique) 897f., 909

CCITT polynomial 897f.

Center of mass 305ff.

Central limit theorem 658f.

Central tendency, measures of 610ff.

Change of variable
in integration 144ff., 797
in Monte Carlo integration 307f.
in probability distribution 287ff.

Characteristic polynomial
digital filter 561
eigensystems 456, 475f.
linear prediction 567
matrix with a specified 375
of recurrence relation 180

Characteristics of partial differential equations

827

Chebyshev acceleration in successive over-

relaxation (SOR) 868f.

Chebyshev approximation 91, 130, 189, 190ff.
Clenshaw-Curtis quadrature 196
Clenshaw’s recurrence formula 193
coefficients for 191
contrasted with Padé approximation 201

derivative of approximated function 189,
195

economization of series 198ff., 201

for error function 220f.

even function 194

and fast cosine transform 519

gamma functions 242

integral of approximated function 195

odd function 194
polynomial fits derived from
rational function 204ff.
Remes exchange algorithm for filter 560
Chebyshev polynomials 190ff.
continuous orthonormality
discrete orthonormality 191
explicit formulas for 190
formula for «* in terms of
Check digit 901
Checksum 889, 896
cyclic redundancy (CRC) 896ff.
Cherry, sundae without a 818
Chi-by-eye 657
Chi-square fitting see Fitting; Least squares
fitting
Chi-square probability function 216, 221,
621, 660, 806
as boundary of confidence region 693f.
related to incomplete gamma function 221
Chi-square test 620f.
for binned data 620f.
chi-by-eye 657
and confidence limit estimation 693f.
for contingency table 630ff.
degrees of freedom 621f.
for inverse problems 806
least squares fitting 659ff.
nonlinear models 681 ff.
tule of thumb 661
for straight line fitting 661ff.

197

190f.

199

968

Index

for straight line fitting, errors in both coor-
dinates 666
for two binned data sets 622
unequal size samples 623
Chip rate 300
Chirp signal 563
Cholesky decomposition 96ff., 430, 462
backsubstitution 97
operation count 97
pivoting 97

solution of normal equations 674
Circulant 592
Class, data type 7
Clenshaw-Curtis quadrature 130, 196, 518,
519
Clenshaw’s recurrence formula 181ff., 196

for Chebyshev polynomials 193
stability 181ff.

Clocking errors 899

cn function 269

to-fine operator 873

grid correction 873f.

Coding
arithmetic 9 10ff.

checksums 896
decoding a Huffman-encoded message
905
Huffman 903ff.
run-length 909
variable length code 903
Ziv-Lempel 903
see also Arithmetic coding; Huffman cod-
ing
Coefficients
binomial 215
ian quadrature 147ff.
in quadrature, nonclassical weight
function 157ff., 797
for quadrature formulas
Column degeneracy 32
Column operations on matrix 37, 40f.
Column totals 630
Combinatorial minimization see Annealing
Comité Consultatif International Télégraphique
et Téléphonique (CCITT) 897f., 909
Communication theory, use in adaptive integra-
tion 727
Communications protocol 896
Comparison function for rejection method
2908.
Complementary error function
see Error function
Complete elliptic integral see Elliptic integrals
Complex arithmetic 23f., 176ff., 948ff.
avoidance of in path integration 209
cubic equations 185
for linear equations 49f.
quadratic equations 184
Complex error function 259
Complex plane
fractal structure for Newton’s rule 367f.
path integration for function evaluation
208ff., 271
poles in 111, 166, 208f., 213, 561, 573,
724f.

131ff., 797

Complex systems of linear equations 49f.
complex.c utility functions 23f., 948ff.
Compression of data 603, 889, 903ff., 910ff.
Concordant pair for Kendall’s tau 642f.
Condition number 61, 85
Confidence level 692f., 696ff.
Confidence limits
bootstrap method 692f.
and chi-square 693f.
confidence region, confidence interval 692
on estimated model parameters 689ff.
by Monte Carlo simulation 689ff.
from singular value decomposition (SVD)
698
Confluent hypergeometric function 210, 246
Conjugate directions 414f., 421 ff.
Conjugate gradient method
biconjugate 84
compared to variable metric method 425f.
elliptic partial differential equations 833
for minimization 396f., 420ff., 812f., 824
minimum residual method 85
preconditioner 85f.
for sparse system 83ff., 606
and wavelets 606
Conservative differential equations 732f.
Constrained linear inversion method 808ff.
Constrained linear optimization see Linear pro-
gramming
Constrained optimization 394
Constraints, deterministic 813ff.
Constraints, linear 431
Contingency coefficient C 631
Contingency table 628ff., 644
sta based on chi-square 630ff.
statistics based on entropy 632ff.
continue construction 14
Continued fraction 169ff.
Bessel functions 240f.
convergence criterion 171
equivalence transformation 172
evaluation 169ff.
evaluation along with normalization condi-
tion 247
even and odd parts
even part 255, 257
exponential integral
Fresnel integral 255
incomplete beta function 227
incomplete gamma function 217
Lentz’s method 171, 219
modified Lentz’s method 171
Pincherle’s theorem 181
ratio of Bessel functions 246
rational function approximation
227
recurrence for evaluating 170f.
and recurrence relation 181
sine and cosine integrals 257
Steed’s method 170f.
tangent function 169
typography for 169
Continuous variable (statistics) 628
Control structures 6, 8ff.
bad 14

172, 217, 222

222

170, 217,

Index

969

Conventions in C programs 25ff.
Convergence

accelerated, for series

of algorithm for 7 915

criteria for 353, 398f., 410, 489f., 495,

684f., 767

eigenvalues accelerated by shifting 477f.

golden ratio 354, 406

of golden section search 398f.

of Levenberg-Marquardt method 684f.

linear 353, 400

of QL method 477f.

quadratic 57, 358, 364f., 415f., 427, 915

rate 353, 359f., 364f.

recurrence relation 181

of Ridders’ method 358

series vs. continued fraction 169f.

and spectral radius 865ff., 871
convert_matrix() utility 945
Convex sets, use in inverse problems
Convolution

denoted by asterisk 498

finite impulse response (FIR) 538

of functions 498, 509

of large data sets 543f.

for multiple precision arithmetic 918

multiplication as 918

necessity for optimal filtering 542

overlap-add method 544

overlap-save method 543f.

and polynomial interpolation 120

relation to wavelet transform 592

theorem 498, 538ff., 553

theorem, discrete 538ff.

treatment of end effects

use of FFT 530, 538ff.

wraparound problem 540
Cooley-Tukey FFT algorithm 509
Co-processor, floating point 894
Copyright rules xvi
Cornwell-Evans algorithm 825
Corporate promotion ladder 336f.
Corrected two-pass algorithm 613
Correction, in multigrid method 872
Correlation coefficient (linear) 636ff.
Correlation function 498

autocorrelation 498, 546, 565

166ff.

813f.

540

and Fourier transforms 498
theorem 498, 545
treatment of end effects 546

using FFT 545f.
Wiener-Khinchin theorem 498, 574
Correlation, statistical 609f., 628
Kendall’s tau 640, 642ff.
linear correlation coefficient 636ff., 664
linear related to least square fitting 636,
664
nonparametric or rank statistical 639ff.
among parameters in a fit 663, 673, 676
in random number generators 277
Spearman rank-order coefficient 640f.
sum squared difference of ranks 640
Cosine function, recurrence 178
Cosine integral 255, 257ff.
continued fraction 257

routine for 258f.
series 257
Cosine transform see Fast Fourier transform
(FFT); Fourier transform
Coulomb wave function 210, 240
Courant condition 838, 841, 843, 845
multidimensional 855
Courant-Friedrichs-Lewy stability criterion see
Courant condition
Covariance
a priori 705
in general linear least squares 673, 677
matrix, by Cholesky decomposition 98,
673
matrix, of errors 805, 817
matrix, is inverse of Hessian matrix 685
matrix, when it is meaningful 695ff.
in nonlinear models 685, 687
relation to chi-square 695ff.
from singular value decomposition (SVD)
698
in straight line fitting 663
CR method see Cyclic reduction (CR)
Cramer’s V_ 631
Crank-Nicolson method 848, 853, 855
CRC (cyclic redundancy check) 896ff.
CRC-12 898
CRC-16 polynomial
CRC-CCITT 898
Creativity, essay on 8
Critical (Nyquist) sampling 500, 550
Cross (denotes matrix outer product) 73
Crosstabulation analysis 629
see also Contingency table
Crout’s algorithm 44ff., 53
Cubic equations 183ff., 367
Cubic spline interpolation 113ff.
see also Spline
Cumulative binomial distribution 226, 229
Cumulative Poisson function 221
related to incomplete gamma function 221
Curvature matrix see Hessian matrix
cvector() utility 943
Cycle, in multigrid method 874
Cyclic Jacobi method 466
Cyclic reduction (CR) 857f., 861f.
Cyclic redundancy check (CRC) 896ff.
Cyclic tridiagonal systems 74f.

898

Danielson-Lanczos lemma 504f., 532
Data

assigning keys to 897

continuous vs. binned 620

entropy 632ff., 903f.

essay on 609

fitting 656ff.

fraudulent 661

glitches in 659

iid (independent and identically distributed)

691

modeling 656ff.

serial port 899

smoothing 610, 65Off.

atistical tests 6009ff.


970

Index

unevenly or irregularly sampled 576,
S581f., 654
use of CRCs in manipulating 897
windowing 553ff.
see also Statistical tests
Data compression 603, 889
arithmetic coding 910ff.
cosine transform 519
Huffman coding 903ff., 910
linear predictive coding (LPC) S7If.
lossless 903
Data Encryption Standard (DES) 300ff.
Data type 28
DAUB4 592ff., 595, 598f., 601
DAUB6 593
DAUBI2 605
DAUB20 598f.
Daubechies wavelet coefficients
598f., 601, 605

592ff., 596,

Davidon-Fletcher-Powell algorithm 397, 426f.

Dawson’s integral 259f., 606
approximation for 259
routine for 260

D.C. (direct current) 498

Debugging 7

DEC (Digital Equipment Corp.) xvii, 3, 894

Declarations 930ff.

Decomposition see Cholesky decomposition;
LU decomposition; QR decomposition;
Singular value decomposition (SVD)

Deconvolution 542, 549

see also Convolution; Fast Fourier trans-
form (FFT); Fourier transform

Defect, in multigrid method 872

Deferred approach to the limit see Richard-
son’s deferred approach to the limit

Deflation

of matrix 478
of polynomials 369ff., 377, 378

Degeneracy of linear algebraic equations 32,
61, 65, 676

Degenerate kernel 794

Degenerate minimization principle 804

Degrees of freedom 621f., 660, 695f.

Demonstration programs 3

Derivatives

computation via Chebyshev approximation
189, 195

computation via Savitzky-Golay filters
189, 651

matrix of first partial see Jacobian determi-
nant

matrix of second partial see Hessian ma-
trix

numerical computation 186ff., 386, 651,
738, 758, 779

of polynomial 173f.

use in optimization 395f., 406
DES see Data Encryption Standard
Descending transformation, elliptic integrals

262

Descent direction 383, 390, 427
Descriptive statistics 609ff.

see also Statistical tests
Design matrix 651, 671, 804, 809

Determinant 34, 49
Deviates, random see Random deviates
DFP algorithm see Davidon-Fletcher-Powell
algorithm
Diagonal dominance 51, 684, 789, 865
Difference equations, finite see Finite differ-
ence equations (FDEs)
Difference operator 167
Differential equations 707ff.
accuracy vs. stability 710, 736
Adams-Bashforth-Moulton schemes 749
adaptive stepsize control 709, 714ff., 725,
733f., 738, 744, 749f., 751
algebraically difficult sets 772
backward Euler’s method 735
Bader-Deuflhard method for stiff 737,
742f.
boundary conditions
760, 7798.
Bulirsch-Stoer method 209, 272, 708f.,
712, 722, 724ff., 747
Bulirsch-Stoer method for conservative
equations 733
comparison of methods
conservative 732f.
danger of too small stepsize 720
eigenvalue problem 756, 772ff., 779f.,
781
embedded Runge-Kutta method 715f., 738
equivalence of multistep and multivalue
methods 751
Euler’s method 708, 710, 735
forward Euler’s method 735
free boundary problem 756, 785
high-order implicit methods 737ff.
implicit differencing 735f., 749
initial value problems 708
internal boundary conditions 784ff.
internal singular points 784ff.
interpolation on right-hand sides 117
Kaps-Rentrop method for stiff 737
local extrapolation 715
modified midpoint method 722f., 726
multistep methods 747ff.
multivalue methods 747
order of method 710f., 725
path integration for function evaluation

707f., 753ff., 757,

708f., 747, 751

208ff., 271

predictor-corrector methods 708, 737,
TATE.

reduction to first-order sets 707, 753

relaxation method 754f., 762ff.

relaxation method, example of 772ff.

rh.s. independent of « 735

Rosenbrock methods for stiff 737

Runge-Kutta method 708f., 710ff., 714ff.,
738, 747

Runge-Kutta method, high-order 711

Runge-Kutta-Fehlberg method 715f.

scaling stepsize to required accuracy 716f.

second order 732f.

semi-implicit differencing 737

semi-implicit Euler method 737, 743

semi-implicit extrapolation method 737,
743

Index

971

semi-implicit midpoint rule 743
shooting method 754, 757ff.
shooting method, example 779f., 781
similarity to Volterra integral equations
794.
singular points 724f., 760, 784ff.
step doubling 715
stepsize control 709, 714ff., 724, 733f.,
738, 744, 749f., 751
stiff 709, 734ff.
stiff methods compared 747
Stoermer’s rule 732f.
see also Partial differential equations; Two-
point boundary value problems
Diffusion equation 827, 847ff., 864
Crank-Nicolson method 848, 853, 855
Forward Time Centered Space (FTCS)
847ff., 850f., 864
implicit differencing 848
multidimensional 855f.
Digamma function 222
Digital filtering see Filter
Dihedral group Ds 902
Dimensions (units) 683f.
Diminishing increment sort 331
Dirac delta function 293, 789
Direct method see Periodogram
Direct methods for linear algebraic equations
35
Direct product see Outer product of matrices
Direction of largest decrease 416f.
Direction numbers, Sobol’s sequence 311
Direction-set methods for minimization 396,
412ff.
Dirichlet boundary conditions
865, 867
Disclaimer of warranty xvi
Discordant pair for Kendall’s tau 643
Discrete convolution theorem 538ff.
Discrete Fourier transform (DFT) 500ff.
as approximate continuous transform 503
see also Fast Fourier transform (FFT)
Discrete optimization 444ff.
Discriminant 184, 464
Diskettes, how to order xvi, 996f.
Dispersion 840
DISPO see Savitzky-Golay filters
Dissipation, numerical 839
Divergent series 167
Division
complex 177
multiple precision 919f.
of polynomials 175, 369, 377
dmatrix() utility 944
dn function 269
Do-while iteration 12
Dogleg step methods 393
Domain of integration 161f.
Dominant solution of recurrence relation 179
Dot (denotes matrix multiplication) 33
Double exponential error distribution 701
Double precision
as refuge of scoundrels 890
use in iterative improvement 56
Double root 348

829, 849, 859,

Downhill simplex method see Simplex, method
of Nelder and Mead

Driver programs 3

Dual viewpoint, in multigrid method 883

Duplication theorem, elliptic integrals 262f.

dvector() utility 943

DWT (discrete wavelet transform) see Wavelet
transform

Eardley, D.M. 346
EBCDIC 898
Economization of power series
Eigensystems 456ff.
balancing matrix 483
bounds on eigenvalues 58
calculation of few eigenvectors or eigenval-
ues 461, 494
canned routines 461
characteristic polynomial 456, 475f.
completeness 457
defective 457, 482, 494
deflation 478
degenerate eigenvalues 456, 458
elimination method 460, 485
factorization method 460
fast Givens reduction 470
generalized eigenproblem 462
Givens reduction 469f.
Hermitian matrix 481f.
Hessenberg matrix 460, 477, 482ff., 494
Householder transformation 460, 469ff.,
476, 480, 481, 484f.
ill-conditioned eigenvalues
implicit shifts 478ff.
and integral equations 788ff., 794
invariance under similarity transform 459
inverse iteration 462, 476, 483, 493ff.
Jacobi transformation 460, 463ff., 469,
481, 495
left eigenvalues
list of tasks 461
multiple eigenvalues
nonlinear 462
nonsymmetric matrix 482ff.
operation count of balancing 483
operation count of Givens reduction 470
operation count of Householder reduction
474
operation count of inverse iteration 494
operation count of Jacobi method 467
operation count of QL method 477, 480
operation count of QR method for Hessen-
berg matrices 490
operation count of reduction to Hessenberg
form 485
orthogonality 457
polynomial roots and 375
QL method 476ff., 481, 494f.
QL method with implicit shifts 478ff.
QR method 60, 460, 463, 476ff.
QR method for Hessenberg matrices
real, symmetric matrix 156, 474, 794
reduction to Hessenberg form 484f.
right eigenvalues 458
shifting eigenvalues 456, 477f., 486f.

198ff., 201

483

458

495

486ff.

972

Index

special matrices 461
termination criterion 489f., 495
tridiagonal matrix 460, 475ff., 494
Eigenvalue and eigenvector, defined 456
Eigenvalue problem for differential equations
756, 772ff., 779, 781
Eigenvalues and polynomial root finding 375
EISPACK 461, 482
Electromagnetic potential 525
Elimination see Gaussian elimination
Ellipse in confidence limit estimation 693
Elliptic integrals 261 ff., 915
addition theorem 262
Carlson’s forms and algorithms 261 ff.
Cauchy principal value 263
duplication theorem 262f.
Legendre 261 ff., 267ff.
routines for 264ff.
symmetric form 261f.
Weierstrass 262

Elliptic partial differential equations 827

alternating-direction implicit method (ADI)
870f., 915

analyze/factorize/operate package 833

biconjugate gradient method 833

boundary conditions 828f.

comparison of rapid methods 863

conjugate gradient method 833

cyclic reduction 857f., 861f.

Fourier analysis and cyclic reduction (FACR)
857ff., 863

Gauss-Seidel method 864, 873, 874f., 884

incomplete Cholesky conjugate gradient
method (ICCG) 833

Jacobi’s method 864f., 873

matrix methods 833

multigrid method 833, 871 ff.

rapid (Fourier) method 833, 857ff.

relaxation method 832, 863ff.

strongly implicit procedure 833

successive over-relaxation (SOR) 866ff.,
871, 875

Emacs, GNU _ xiii

Embedded Runge-Kutta method 715f., 738

Encapsulation, in programs 6f.

Encryption 300

Entropy 903f.
of data 632ff., 820

EOM (end of message) 910

Equality constraints 431

Equations

cubic 183ff., 367

normal (fitting) 651, 672ff., 809f.

quadratic 29, 183ff.

see also Differential equations; Partial dif-
ferential equations; Root finding

Equivalence classes 345f.
Equivalence transformation 172
Error

checksums for preventing 899
clocking 899

double exponential distribution 701
local truncation 883

Lorentzian distribution 701If.

in multigrid method 872

nonnormal 659, 695, 699ff.

relative truncation 883

roundoff 185, 889f.

series, advantage of an even 138f., 723
systematic vs. statistical 659
truncation 30, 186, 406, 715, 889f.
varieties found by check digits 902
varieties of, in PDEs 840ff.

see also Roundoff error

Error function 220f., 607

approximation via sampling theorem 607f.
Chebyshev approximation 220f.

complex 259
for Fisher’s z-transformation 638
relation to Dawson’s integral 259
relation to Fresnel integrals 255
relation to incomplete gamma function
220
routine for 220f.
for significance of correlation 636
for sum squared difference of ranks 641
Error handling in programs 2, 940
Estimation of parameters see Fitting; Maxi-
mum likelihood estimate
Estimation of power spectrum 549ff., 572ff.
Euler equation (fluid flow) 840
Euler-Maclaurin summation formula 138, 142
Euler’s constant 223f., 257
Euler’s method for differential equations 708,
710, 735
Euler’s transformation 1 66ff.
generalized form 168f.
Evaluation of functions see Function
Even and odd parts, of continued fraction
172, 217, 222
Even parity 896
Exception handling in programs 2, 940
exit() function 2
Explicit differencing 836
Exponent in floating point format 28, 890
Exponential deviate 287f.
Exponential integral 222ff.

asymptotic expansion 224

continued fraction 222

recurrence relation 178

related to incomplete gamma function 222
relation to cosine integral 257

routine for En(x) 223f.

routine for Ei(a) 225

series 222

Exponential probability distribution 577
Extended midpoint rule 130f., 135, 141f.
Extended Simpson’s rule 134, 796, 799
Extended Simpson’s three-eighths rule 797
Extended trapezoidal rule 131f., 133, 136ff.,
141, 795

roundoff error 138
extern storage class 25
Extirpolation (so-called) S8If.
Extrapolation 105ff.

in Bulirsch-Stoer method 724ff., 731

differential equations 708

by linear prediction 564ff.

local 715

maximum entropy method as type of 574

Index

973

polynomial 728, 730f., 748
rational function 724ff., 731
relation to interpolation 105
for Romberg integration 140
see also Interpolation
Extremization see Minimization

F -distribution probability function 226, 229

F-test for differences of variances 617, 619

FACR see Fourier analysis and cyclic reduc-
tion (FACR)

Facsimile standard 909

Factorial
double (denoted “!!") 253
evaluation of 165
relation to gamma function 213
routine for 214f.

False position 354ff.

Family tree 345

FAS (full approximation storage algorithm)

882ff.

Fast Fourier transform (FFT) 504ff., 889
alternative algorithms S09f.
applications 537ff.
as approximation to continuous transform

503
Bartlett window 554
bit reversal 505f., 532
and Clenshaw-Curtis quadrature 196
convolution 509, 530, 538ff., 918
convolution of large data sets 543f.
Cooley-Tukey algorithm 509
correlation 545f.
cosine transform 196, 517ff., 860f.
cosine transform, second form 519, 861
Danielson-Lanczos lemma 504f., 532
da is not a power of 2 509
data smoothing 650
data windowing 553ff.
decimation-in-frequency algorithm 509
decimation-in-time algorithm 509
discrete autocorrelation 546
discrete convolution theorem 538ff.
discrete correlation theorem 545
at double frequency 582
endpoint corrections 585f.
external storage 532
figures of merit for data windows 554
filtering S58ff.
FIR filter 559f.
Fourier integra 584 ff.
Fourier integrals, infinite range 590f.
Hamming window 554
Hann window 554
history 504
TIR filter 559
image processing 812, 814
integrals using 130
inverse of cosine transform 518f.
inverse of sine transform 517
large data sets 532
leakage 551
memory-local algorithm 535f.
multidimensional 521ff.
for multiple precision arithmetic 915

for multiple precision multiplication 918
number-theoretic transforms 509f.
operation count 504
optimal (Wiener) filtering 547ff., 565f.
order of storage in 507
partial differential equations
Parzen window 554
periodicity of 503
periodogram S50ff., 574
power spectrum estimation 549ff.
for quadrature 130
of real data in 2D and 3D 525ff.
of real functions 510ff., S25ff.
related algorithms 509f.
Sande-Tukey algorithm 509
sine transform 514ff., 859
Singleton’s algorithm 532
square window 553
treatment of end effects in convolution
540
treatment of end effects in correlation 546
Tukey’s trick for frequency doubling 582
use in smoothing data 650
used for Lomb periodogram 581f.
variance of power spectrum estimate 552,
556
virtual memory machine 535f.
Welch window 554
Winograd algorithms 509
see also Discrete Fourier transform (DFT);
Fourier transform; Spectral density
Faure sequence 310
Fax (facsimile) Group 3 standard 909
fcomplex (data type) 24, 948
Feasible vector 431
FFT see Fast Fourier transform (FFT)
Field, in data record 338
Figure-of-merit function 656
Filon’s method 590
Filter 558ff.
acausal 559
bilinear transformation method 561
causal 559, 650
characteristic polynomial 561
data smoothing 650
digital 558ff.
DISPO 650
by fast Fourier transform (FFT) 530,
S58 ff.
finite impulse response (FIR) 538, 559f.
homogeneous modes of 561
infinite impulse response (IIR) 559, 573
Kalman 705
linear 559ff.
low-pass for smoothing 650
nonrecursive 559f.
optimal (Wiener) 542, 547ff., 565f., 650
quadrature mirror 592, 600
realizable 559, 561
recursive 559, 573
Remes exchange algorithm 560
Savitzky-Golay 189, 6SOff.
stability of 561
in the time domain SS8ff.
Fine-to-coarse operator 873

833, 857ff.


974

Index

Finite difference equations (FDEs) 762, 772,

783
alternating-direction implicit method (ADI)
856, 870f.
art, not science 838
Cayley’s form for unitary operator 853
Courant condition 838, 841, 845

standard (probable) errors on fitted parame-
ters 663, 667f., 673, 677, 689ff.

straight line 661ff., 673f., 703

straight line, errors in both coordinates
666ff.

see also Error; Least squares fitting; Max-
imum likelihood estimate; Robust esti-

mation
Five-point difference star 876
Fixed point format 28
Fletcher-Powell algorithm see Davidon-Fletcher-
Powell algorithm
Fletcher-Reeves algorithm 396f., 421ff.
float to double conversion 24f.
Floating point co-processor 894
Floating point format 28, 890
care in numerical derivatives 186
TEEE 285, 890f.
Flux-conservative initial value problems
FMG (full multigrid method) 872, 877f.
for iteration 8, 11
Formats of numbers 28, 890
FORTRAN 16, 20
Numerical Recipes in xv, |
Forward deflation 370
Forward difference operator 167
Forward Euler differencing 835f.
Forward Time Centered Space see FTCS
Fourier analysis and cyclic reduction (FACR)
858, 863
Fourier and spectral applications
Fourier integrals

Courant condition (multidimensional) 855
Crank-Nicolson method 848, 853, 855
eigenmodes of 836f.
explicit vs. implicit schemes 836
forward Euler 835f.
Forward Time Centered Space (FTCS)
836ff., 847ff., 852, 864
implicit scheme 848
Lax method 837ff., 845
Lax method (multidimensional) 854f.
mesh drifting instability 843f.
numerical derivatives 186
partial differential equations
in relaxation methods 762ff.
staggered leapfrog method 842f.
two-step Lax-Wendroff method 844ff.
upwind differencing 841f., 846
see also Partial differential equations
Finite element methods, partial differential
equations 833f.
Finite impulse response (FIR) 538
Finkelstein, S. xii
FIR (finite impulse response) filter 559f.
Fisher’s z-transformation 637f.
Fitting 656ff.

834ff.
830ff.

537ff.

basis functions 671

by Chebyshev approximation

chi-square 659ff.

confidence levels related to chi-square val-
ues 696ff.

confidence levels from singular value de-
composition (SVD) 698

confidence limits on fitted parameters 689ff.

covariance matrix not always meaningful
657, 695

degeneracy of parameters 679

an exponential 679

freezing parameters in 674, 705

Gaussians, a sum of 687f.

general linear least squares 67 1ff.

Kalman filter 705

K-S test, caution regarding 627

least squares 657ff.

Legendre polynomials 680

Levenberg-Marquardt method 683ff., 825

linear regression 661 ff.

maximum likelihood estimation 658,
699ff.

Monte Carlo simulation 627, 660, 689ff.

multidimensional 680

nonlinear models 681 ff.

nonlinear models, advanced methods 688

nonlinear problems that are linear 679

nonnormal errors 662, 695, 699ff.

polynomial 90, 120, 197, 6SOf., 671, 679f.

by rational Chebyshev approximation 204ff.

robust methods 699ff.

of sharp spectral features 573

191f.

attenuation factors 590
endpoint corrections 585f.
tail integration by parts 591
use of fast Fourier transform (FFT) 584ff.
Fourier transform 105, 496ff.
aliasing 501,576
approximation of Dawson’s integral 259
autocorrelation 498
basis functions compared 514f.
contrasted with wavelet transform 591f.,
601
convolution 498, 509, 538ff., 918
correlation 498, 545f.
cosine transform 196, 517ff., 860f.
cosine transform, second form 519, 861
critical sampling 500, 550, 552
definition 496
discrete Fourier transform (DFT) 190,
S00ff.
Gaussian function 607
image processing 812, 814
infinite range 590f.
inverse of discrete Fourier transform 503
method for partial differential equations
857ff.
issing data 576
mi g data, fast algorithm 581f.
Nyquist frequency SO0ff., 526, 550, 552,
576, 579
optimal (Wiener) filtering 547ff., 565f.
Parseval’s theorem 498, 504, 551
power spectral density (PSD) 498f.
power spectrum estimation by FFT 549ff.


Index

975

power spectrum estimation by maximum
entropy method 572ff.
properties of 497f.
sampling theorem 501, 550, 552, 606f.
scalings of 497
significance of a peak in 577f.
sine transform 514ff., 859
symmetries of 497
uneven sampling, fast algorithm 581f.
unevenly sampled data 575ff., 581f.
and wavelets 599f.
Wiener-Khinchin theorem 498, 566, 574
see also Fast Fourier transform (FFT);
Spectral density
Fractal region 367f.
Fractional step methods 856f.
Fredholm alternative 789
Fredholm equations 788f.
eigenvalue problems 789, 794
error estimate in solution 793
first kind 788
Fredholm alternative 789
homogeneous, second kind 793f.
homogeneous vs. inhomogeneous 789
ill-conditioned 789
infinite range 797f.
inverse problems 789, 804ff.
kernel 788f.
nonlinear 790
Nystrom method 791ff., 797f.
product Nystrom method 797
second kind 789, 791f.
with singularities 797
with singularities, worked example 801
subtraction of singularity 798
symmetric kernel 794
see also Inverse problems
Freeing of storage 19, 21f., 940ff.
free_matrix() utility 946
free_vector() utility 946
Frequency domain 496
Frequency spectrum see Fast Fourier transform
(FFT)
Frequentist, contrasted with Bayesian 819
Fresnel integrals 255ff.
asymptotic form 255
continued fraction 255
routine for 256f.
series 255
Friday the Thirteenth 13f.
FTCS (forward time centered space) 836ff.,
847ff., 852
stability of 836ff., 847ff., 864
Full approximation storage (FAS) algorithm
882ff.
Full moon = 13f.
Full multigrid method (FMG) 872, 877f.
Full Newton methods, nonlinear least squares
688
Full pivoting 38
Full weighting 876
Function
Airy 210, 240, 250
approximation 105f., 190ff.

associated Legendre polynomial 252f.,
773

autocorrelation of 498

bandwidth limited 501

Bessel 178, 210, 230ff., 240ff.

beta 215f.

branch cuts of 209f.

chi-square probability 221, 806

complex 208

confluent hypergeometric 210, 246

convolution of 498

correlation of 498

Coulomb wave 210, 240

cumulative binomial probability 226, 229

cumulative Poisson 216

Dawson’s integral 259f., 606
declaration 17

definition 17

digamma 222

elliptic integrals 261ff., 915

error 220f., 255, 259, 607, 636, 641
evaluation 165ff.
evaluation by path integration 208ff., 271
exponential integral 178, 222ff., 257
external 25
F-distribution probability 226, 229
Fresnel integral 255ff.
gamma 213f.
hypergeometric 208ff., 271 ff.
incomplete beta 226ff., 616
incomplete gamma 216ff., 621, 660, 663f.
inverse hyperbolic 184, 262
inverse trigonometric 262
Jacobian elliptic 261, 269f.
Kolmogorov-Smirnov probability 624f.,
646f.

Legendre polynomial
logarithm 262
modified Bessel 236ff.
modified Bessel, fractional order 246ff.
path integration to evaluate 208ff.
pathological 105f., 350f.
Poisson cumulant 221
prototypes 16f., 25, 930
representations of 496
routine for plotting a 349f.
sine and cosine integrals 255, 257ff.
sn,dn,cn 269
spherical Bessel 240
spherical harmonics 252f.
spheroidal harmonic 772ff., 779f., 781
Student’s probability 226, 228
Weber 210

Functional iteration, for implicit equations

748
FWHM (full width at half maximum) 555

178, 252f., 680

G amma deviate 290ff.
Gamma function 213ff.
incomplete see Incomplete gamma func-
tion
s-Chebyshev integration 147, 151, 518f.
lermite integration 151, 798
and weights 153
normalization 153


976

Index

Gauss-Jacobi integration 151
abscissas and weights 154
Gauss-Jordan elimination 36ff., 41, 71
operation count 42, 48
solution of normal equations 673
storage requirements 38f.
Kronrod quadrature 160
Laguerre integration 151, 798
Legendre integration 151
e also Gai in integration
s-Lobatto quadrature 160, 196, 518
Radau quadrature 160
s-Seidel method (relaxation) 864, 866,
873, 874f.
nonlinear 884
ransformation 262
in (normal) distribution 275, 658, 807
central limit theorem 658f.
deviates from 288f., 578
kurtosis of 612
multivariate 695
semi-invariants of 614
tails compared to Poisson 659
two-dimensional (binormal) 637
variance of skewness of 612
Gaussian elimination 41f., 59, 63
fill-in 53,71
integral equations 795
operation count 42
in reduction to Hessenberg form 485
relaxation solution of boundary value prob-
lems 762ff., 785
sian function
Hardy’s theorem on Fourier transforms
607
see also Gaussian (normal) distribution
n integration 133, 147ff., 798
calculation of absc
error estimate in solution 793
extensions of 160
Golub-Welsch algorithm for weights and
abs 156f.
for integral equations 790, 792
from known recurrence relation 156f.
nonclassical weight function 157ff., 797
and orthogonal polynomials 148
preassigned nodes 160
weight function log 159
weight functions 147ff., 797
Gear’s method (stiff ODEs) 737
Geiger counter 274
Generalized eigenvalue problems 462
Generalized minimum residual method (GM-

RES) 85
Geophysics, use of Backus-Gilbert method
818

Gerchberg-Saxton algorithm 814f.
Gilbert and Sullivan 720
Givens reduction 469f., 480
fast 470
operation count 470
Glassman, A.J. 185
Global optimization 394f., 444ff., 656
continuous variables 451f.

s and weights 1 SOff.

Globally convergent
minimization 425ff.
root finding 380, 383ff., 390, 757f., 761
GMRES (generalized minimum residual method)
85
GNU Emacs _ xiii
Godunov’s method 846
Golden mean (golden ratio) 30, 354, 399, 406
Golden section search 348, 396, 397ff., 403
Golub-Welsch algorithm, for Gaussian quadra-
ture 156f.
Goodness-of-fit 656, 660, 663f., 668, 695
goto statements, danger of 8
Gram-Schmidt
biorthogonalization 421f.
orthogonalization 100, 457, 458
SVD as alternative to 66
Graphics, function plotting 349f.
Gravitational potential 525
Gray code 311, 889, 894ff.
Greenbaum, A. 86
Gregorian calendar 12, 15
Grid square 123f.
Group, dihedral 902
Guard digits 890

Halt weighting 876
Halton’s quasi-random sequence 309f.
Hamming window 554
Hamming’s motto 348
Hann window 554
Harmonic analy:
Hashing 303
HDLC checksum 898
Header (.h) files 16f.
Heap (data structure) 336f., 344, 905
Heapsort 329, 336f., 344
Helmholtz equation 861
Hermite polynomials 151, 153
Hermitian matrix 457ff., 481f.
Hertz (unit of frequency) 496
Hessenberg matrix 100, 460, 477, 482, 494
e also Matrix
Hessian matrix 389, 414, 422, 427, 681ff.,
812, 824
is inverse of covariance matrix 673, 685
second derivatives in 683
Hexadecimal constants 285, 303
Hierarchically band diagonal matrix 606
Hierarchy of program structure 5ff.
High-order not same as high-accuracy 106f.,
130, 396, 406, 711, 715, 748f.
High-pass filter 558
Hilbert matrix 90
Historic maximum entropy method 825f.
Homogeneous linear equations 61
Hook step methods 393
Hotelling’s method for matrix inverse 57, 606
Householder transformation 60, 460, 469ff.,
476, 480, 481, 484f., 488ff.
operation count 474
in QR decomposition 99
Huffman coding 571, 889, 903ff., 910
Hyperbolic functions, explicit formulas for
inverse 184

ee Fourier transform


Index

977

Hyperbolic partial differential equations 827
advective equation 835
flux-conservative initial value problems
834ff.
Hypergeometric function 208ff., 271 ff.
routine for 272f.
Hypothesis, null 609

IBM xvii
bad random number generator 277
PC 3, 285, 303, 894
radix base for floating point arithmetic
483
IBM checksum 901f.
ICCG (incomplete Cholesky conjugate gradient
method) 833
ICF (intrinsic correlation function) model 826
Identity (unit) matrix 34
IEEE floating point format 285, 890f.
if structure 11
warning about nesting 11
IIR (infinite impulse response) filter 559, 573
Ill-conditioned integral equations 789
Image processing 525, 812
cosine transform 519
fast Fourier transform (FFT) 525, 530,
812
as an inverse problem 812
maximum entropy method (MEM) 818ff.
from modulus of Fourier transform 814
wavelet transform 603
imatrix() utility 944
Implicit
conversion of data types 24f., 930
function theorem 347
pivoting 38
shifts in QL method 478ff.
Implicit differencing 836
for diffusion equation 848
for stiff equations 735f., 749
Importance sampling, in Monte Carlo 316f.
Improper integrals 141 ff.
Impulse response function 538, 549, 559
IMSL xvii, 35, 72, 212, 371, 376, 461
In-place selection 342
Include files 17, 930
Incomplete beta function 226ff.
for F-test 619
routine for 227f.
for Student’s t 616, 618
Incomplete Cholesky conjugate gradient method
(ICCG) 833
Incomplete gamma function 216
for chi-square 621, 660, 663f.
deviates from 290ff.
in mode estimation 616
routine for 218f.
Increment of linear congruential generator
276
Indentation of blocks 11
Index 965ff.
this entry 977
Index table 329, 338
Inequality constraints 431

Inheritance 7

Initial value problems 708, 827f.
see also Differential equations;
Partial differential equations

Injection operator 873

Instability see Stability

Integer programming 443

Integral equations 788ff.
adaptive stepsize control 797
block-by-block method 797

correspondence with linear algebraic equa-

tions 788ff.
degenerate kernel 794
eigenvalue problems 789, 794
error estimate in solution 793
Fredholm = 788f., 791f.
Fredholm alternative 789

homogeneous, second kind 793f.

ill-conditioned 789

infinite range 797f.

inverse problems 789, 804ff.
kernel 788f.

nonlinear 790, 796

Nystrom method 791f., 797
product Nystrom method 797
with singularities 797ff.

with singularities, worked example 801

subtraction of singularity 798
symmetric kernel 794
unstable quadrature 796
Volterra 789f., 794f.
wavelets 791

see also Inverse problems

Integral operator, wavelet approximation of

603f., 791
Integration of functions 129ff.
cosine integrals 257
Fourier integrals 584ff.

Fourier integrals, infinite range 590f.

Fresnel integrals 255
Gauss-Hermite 153
Gauss-Jacobi 154
Gauss-Laguerre 152
Gauss-Legendre 151

integrals that are elliptic integrals 261

path integration 208ff.

sine integrals 257

see also Quadrature
Integro-differential equations 791
Interface, in programs 7
Intermediate value theorem 350
Internet xvii
Interpolation 105ff.

Aitken’s algorithm 108

avoid 2-stage method 106

avoid in Fourier analysis 576

bicubic 125f.

bilinear 123f.

caution on high-order 106f.

coefficients of polynomial 106, 120ff.,

197, 582
for computing Fourier integrals
error estimates for 106
of functions with poles 11 1ff.
inverse quadratic 360, 402ff.

586

978 Index

multidimensional 107f., 123ff.
in multigrid method 876
Neville’s algorithm 108f., 188
Nystrom 792
offset arrays 110, 119
operation count for 106
operator 873
order of 106
and ordinary differential equations 107
oscillations of polynomial 106, 120, 396, Jacobi matrix, for Gaussian quadrature 156
406 Jacobi transformation (or rotation) 100, 460,
parabolic, for minimum finding 402 463ff., 469, 481, 495
polynomial 105, 108ff., 188 Jacobian determinant 288f., 783
rational Chebyshev approximation 204ff. Jacobian elliptic functions 261, 269f.
rational function 105, 111ff., 200ff., 231f., Jacobian matrix 381, 383, 386, 389, 738
724ff.. 731 singular in Newton’s rule 393
reverse (extirpolation) 5slf. Jacobi’s method (relaxation) 864f., 866, 873
spline 106, 113ff., 1278. Jenkins-Traub method 376
trigonometric 105 Julian Day 1, 12, 14
see also Fitting Jump transposition errors 902
Interval variable (statistics) 628
Intrinsic correlation function (ICF) model 826
Inverse hyperbolic function 184, 262
Inverse iteration see Eigensystems
Inverse problems 789, 804ff.
Backus-Gilbert method 815ff.
Bayesian approach 808, 820, 825f.
central idea 808
constrained linear inversion method 808ff.
data inversion 816
deterministic constraints 813ff.
in geophysics 818
Gerchberg-Saxton algorithm 814f.
incomplete Fourier coefficients 822 symmetric 793f.
and integral equations 789 Kernighan & Ritchie C (K&R C) 2, 16, 24,
linear regularization 808ff. 930
maximum entropy method (MEM) 818ff., Keys used in sorting 338, 897
MEM demystiied 823 Kolmogorov-Smirnov test 620, 623ff., 699
Phillips-Twomey method 808ff. two-dimensiontl eat
principal solution 806 Kuiper's tic 627
regularization 805ff. Kurtosis 612, 614
regularizing operator 807
stabilizing functional 807
Tikhonov-Miller regularization 808ff.
trade-off curve 804
trade-off curve, Backus-Gilbert method

for linear algebraic equations 35
required for two-point boundary value
problems 753
in root finding 347f.
Iteration matrix 865
ITPACK 78
ivector() utility 943

K -s test see Kolmogorov-Smirnov test
Kalman filter 705
Kaps-Rentrop method 737
Kendall’s tau 640, 642ff.
Kermit checksum 897
Kernel 788f.
averaging, in Backus-Gilbert method 816f.
degenerate 794
finite rank 794
inverse response 816f.
separable 794
singular 797f.

L-estimate 699
Labels, statement 8
Lag 498, 545f., 560
818 Lagrange multiplier 804
two-dimensional regularization 812 Lagrange’s formula for polynomial interpola-
use of conjugate gradient minimization tion 91, 108, 582, 585
812f., 824 Laguerre’s method 348, 371ff.
use of convex sets 813f. Lanczos lemma _ 504f.
use of Fourier transform 812, 814 Lanczos method for gamma function 213
Van Cittert’s method 813 Landen transformation 262
Inverse quadratic interpolation 360, 402ff. LAPACK 35

Inverse response kernel, in Backus-Gilbert
method 816f.
Inverse trigonometric function 262
ISBN (International Standard Book Number)
checksum 901
Iterated integrals 161
Iteration 8
functional 748
to improve solution of linear algebraic
equations 55ff., 201

Laplace’s equation 252, 827
see also Poisson equation

Las Vegas 631

Latin square or hypercube 315

Laurent series 573

Lax method 837ff., 845, 854f.
multidimensional 854f.

Lax-Wendroff method 844ff.

Leakage in power spectrum estimation 551,

554f.

Index

979

Leakage width 554f.
Leapfrog method 842f.
Least squares filters see Savitzky-Golay filters
Least squares fitting 650f., 657ff., 661ff.,
666ff., 671 ff.
contrasted to general minimization prob-
lems 689
degeneracies in 677, 679
Fourier components 577
freezing parameters in 674, 705
general linear case 671ff.
Levenberg-Marquardt method 683ff., 825
Lomb periodogram 577
as M-estimate for normal errors 701
maximum likelihood estimator 658
as method for smoothing data 650f.
multidimensional 680
nonlinear 393, 681ff., 825
nonlinear, advanced methods 688
normal equations 651, 672ff., 809f.
normal equations often singular 676, 679
optimal (Wiener) filtering 547
QR method in 100, 674
for rational Chebyshev approximation 205
relation to linear correlation 636, 664
Savitzky-Golay filter as 650f.
singular value decomposition (SVD) 34f.,
59ff., 205, 676ff.
skewed by outliers 659
for spectral analysis 577
standard (probable) errors on fitted parame-
ters 673,677
weighted 658
see also Fitting
L’Ecuyer’s long period random generator 280ff.
Left eigenvalues or eigenvectors 458
Legal matters xvi
Legendre elliptic integral see Elliptic integrals
Legendre polynomials 252f.
fitting data to 680
recurrence relation 178
shifted monic 159
see also Associated Legendre polynomials;
Spherical harmonics
Lehmer-Schur algorithm 376
Lemarie’s wavelet 600
Lentz’s method for continued fraction 171,
219
Lepage, P. 319
Leptokurtic distribution 612
Levenberg-Marquardt algorithm 393, 683ff.,
825
advanced implementation 688
Levinson’s method 92f.
Lewis, H.W. 284
License information xvi
Limbo 362
Limit cycle, in Laguerre’s method 372
Line minimization see Minimization, along a
ray
Line search see Minimization, along a ray
Linear algebraic equations 32ff.
band diagonal 51 ff.
biconjugate gradient method 84f.

Cholesky decomposition 96ff., 430, 462,
674
complex 49f.
computing A~!-B 48
conjugate gradient method 83ff., 606
cyclic tridiagonal 74f.
direct methods 35,71
Gauss-Jordan elimination 36ff.
Gaussian elimination 41f.
Hilbert matrix 90
Hotelling’s method 57, 606
and integral equations 788ff., 792
iterative improvement SSff., 201
iterative methods 35, 83ff.
large sets of 33
least squares solution 62, 65f., 205, 676
LU decomposition 43ff., 201, 393, 739,
792, 795, 810
nonsingular 33
overdetermined 34f., 205, 676, 806
partitioned 77f.
QR decomposition 98f., 389, 393, 674
row vs. column elimination 40f.
Schultz’s method 57, 606
Sherman-Morrison formula 73ff., 90
singular 32, 61, 66, 205, 676
singular value decomposition (SVD) 59ff.,
205, 676ff., 806
sparse 33, SIff., 71ff., 739, 813
summary of tasks 34
Toeplitz 90, 92ff., 201
Vandermonde 90ff., 120
wavelet solution 603ff., 791
Woodbury formula 7S5ff., 90
see also Eigensystems
Linear congruential random number generator
276f.
choice of constants for 284f.
Linear constraints 431
Linear convergence 353, 400
Linear correlation (statistics) 636ff.
Linear dependency
constructing orthonormal basis 66, 100
of directions in N-dimensional space 415
in linear algebraic equations 32f.
Linear equations see Differential equations; In-
tegral equations; Linear algebraic equa-
tions
Linear inversion method, constrained 808ff.
Linear prediction 564ff.
characteristic polynomial 567
coefficients 564ff.
compared with regularization 810
contrasted to polynomial extrapolation
567
related to optimal filtering 565f.
removal of bias in 570
stability 567
Linear predictive coding (LPC) 571f.
Linear programming 394, 430ff.
artificial variables 437
auxiliary objective function 437
basic variables 434
composite simplex algorithm 443
constraints 431


980

Index

convergence criteria 439
degenerate feasible vector 436
dual problem 443

equality constraints 431
feasible basis vector 433f.
feasible vector 431
fundamental theorem 432f.
inequality constraints 431
left-hand variables 434
nonbasic variables 434
normal form 433

objective function 431
optimal feasible vector 431
pivot element 435f.
primal-dual algorithm 443
primal problem 443
reduction to normal form 436ff.
restricted normal form 433ff.
revised simplex method 443
right-hand variables 434

simplex method 408f., 430, 433ff., 439ff.

slack variables 436
tableau 434
vertex of simplex 433
Linear regression 661ff., 666ff.
see also Fitting
Linear regularization 808ff.
LINPACK 35
Little-endian 302
Local extrapolation 715
Local extremum 394, 445
Localization of roots see Bracketing
Logarithmic function 262

Lomb periodogram method of spectral analysis

576f.
fast algorithm S8If.
Loops 8

Lorentzian probability distribution 292, 701f.

Low-pass filter 558, 650
LP coefficients see Linear prediction
LPC (linear predictive coding) S7If.
LU decomposition 43ff., 56f., 59, 63, 71,
104, 381, 673, 739
for A~!.B 48
band diagonal matrix 51ff., 53f.
complex equations 49f.
Crout’s algorithm 44ff., 53
for integral equations 792, 795
for inverse iteration of eigenvectors 494
for inverse problems 810
for matrix determinant 49
for matrix inverse 48
for nonlinear sets of equations 381, 393
operation count 44, 48
for Padé approximant 201
pivoting 45f.
repeated backsubstitution 48, 54
solution of linear algebraic equations 48
solution of normal equations 673
for Toeplitz matrix 94
Lucifer (encryption algorithm) 300
lvector() utility 943

M.estimates 699ff.

how to compute 702f.

local 700ff.

see also Maximum likelihood estimate
Machine accuracy 28f., 890
Macintosh, see Apple Macintosh
Maehly’s procedure 370, 378
Magic

in MEM image restoration 823

in Padé approximation 201
Mantissa in floating point format 28, 890,

918

Marginals 630

Marquardt method (least squares fitting) 683ff.,

825

M center of 305ff.
MasterCard checksum 901f.
Mathematical Center (Amsterdam) 360
Matrix 33ff.

allocating and freeing 21f., 940ff.

approximation of 66f., 605f.

band diagonal 50, 51 ff., 71

band triangular 71

banded 35, 461

bidiagonal 60

block diagonal 71, 762

block triangular 71

block tridiagonal 71

bordered 71

characteristic polynomial 456, 475f.

Cholesky decomposition 96ff., 430, 462,

674

column augmented 37

compatibility 940

complex 49f.

condition number 61, 85

curvature 682

cyclic banded 71

cyclic tridiagonal 74f.

defective 457, 482, 494

of derivatives see Hessian matrix; Jacobian

determinant
design (fitting) 651, 671, 809
determinant of 34, 49
diagonalization 459ff.
elementary row and column operations
finite differencing of partial differential
equations 830ff.
freeing a submatrix 23
Hermitian 457, 461, 481f.
Hermitian conjugate 457

Hessenberg 100, 460, 477, 482, 484f., 494

He in see Hessian matrix
hierarchically band diagonal 606
Hilbert 90

identity 34

ill-conditioned 61, 63, 120
indexed storage of 78f.

and integral equations 788, 792

inverse 34, 36, 42, 48f., 73ff., 77f., 1O2ff.

inverse, approximate 57

inverse by Hotelling’s method 57, 606
inverse by Schultz’s method 57, 606
inverse multiplied by a matrix 49
iteration for inverse 57, 606

Jacobi transformation 460, 463ff., 469

Index

981

Jacobian 738

lower triangular 43f., 96, 790

multiplication denoted by dot 33

norm 58

normal 457, 458

nullity 61

nullspace 34, 61, 63, 456, 804

orthogonal 98, 457, 470, 594

orthogonal transformation 459, 470ff., 477

orthonormal basis 66, 100

outer product denoted by ® 73, 427

partitioning for determinant 78

partitioning for inverse 77f.

pattern multiply of sparse 81f.

positive definite 35, 96, 674

QR decomposition 98f., 389, 393, 674

range 61

rank 61

residual 57

row and column indices 33

row vs. column operations 40f.

self-adjoint 457

similarity transform 459ff., 463, 483, 485,
488

singular 61, 63, 66, 456

singular value decomposition 34f., 59ff.,
806

sparse 33, 71ff., 78, 606, 739, 762, 813

special forms 35

splitting in relaxation method 865f.

spread 817

square root of 430, 462

storage schemes in C 20f., 33f., 940ff.

submatrix of 22, 945

symmetric 35, 96, 457, 461, 469ff., 674,
793f.

threshold multiply of sparse 81ff.

Toeplitz 90, 92ff., 201

transpose of sparse 80f.

triangular 460

tridiagonal 35, SOf., 71, 115, 156, 460,
461, 469ff., 475ff., 494, 848f., 862,
870f.

tridiagonal with fringes 831

unitary 457

updating 100, 389f.

upper triangular 43f., 98

Vandermonde 90ff., 120

see also Eigensystems
Matrix equations see Linear algebraic equa-
tions
matrix() utility 943f.
Matterhorn 612
Maximization see Minimization
Maximum entropy method (MEM) _572ff.
algorithms for image restoration 824f.
Bayesian 825f.
Cornwell-Evans algorithm 825
demystified 823
historic vs. Bayesian 825f.
image restoration 818ff.
intrinsic correlation function (ICF) model
826
for inverse problems 81 8ff.
operation count 574

see also Linear prediction
Maximum likelihood estimate (M-estimates)
695, 699ff.
and Bayes’ Theorem 820
chi-square test 695
defined 658
how to compute 702f.
mean absolute deviation 701, 703
relation to least squares 658
Maxwell’s equations 835
Mean(s)
of distribution 610f., 614
statistical differences between two 615ff.
Mean absolute deviation of distribution 611,
701
related to median 703
Measurement errors 656
Median 329
calculating 341
of distribution 611, 614f.
as L-estimate 699
role in robust straight line fitting 703
by selection 703
Median-of-three, in Quicksort 333
MEM see Maximum entropy method (MEM)
Memory, allocating and freeing 19, 21f.,
940ff.
Merit function 656
in general linear least squares 671
for inverse problems 806
nonlinear models 681

for straight line fitting 662, 703
for straight line fitting, errors in both coor-
dinates 666
Mesh-drift instability 843f.

Mesokurtic distribution 612
Method of regularization 808ff.
Metropolis algorithm 445f.
Microsoft xvii
Midpoint method see Modified midpoint method;
Semi-implicit midpoint rule
Mikado, or Town of Titipu 720
Miller’s algorithm 181, 234
Minimal solution of recurrence relation 179
Minimax polynomial 192, 204
Minimax rational function 204
Minimization 394ff.
along aray 84, 384f., 396, 412f., 418f.,
424, 425
annealing, method of simulated 394f.,
444ff.
bracketing of minimum 397ff., 409
Brent’s method 396, 402ff., 406, 666
Broyden-Fletcher-Goldfarb-Shanno algo-
rithm 397, 426ff.
chi-square 659ff., 681ff.
choice of methods 395ff.
combinatorial 444
conjugate gradient method 396f., 420ff.,
812f., 824
convergence rate 400, 415f.
Davidon-Fletcher-Powell algorithm 397,
426f.
degenerate 804

direction-set methods 396, 412ff.

982

Index

downhill simplex method 396, 408ff.,
451f., 702f.

finding best-fit parameters 656
Fletcher-Reeves algorithm 396f., 421ff.
functional 804
global 394f., 451f., 656
globally convergent multidimensional 425ff.
golden section search 397ff., 403
multidimensional 395f., 408ff.
in nonlinear model fitting 681f.
Polak-Ribiere algorithm 396f., 422f.
Powell’s method 396, 408, 412ff.
quasi-Newton methods 383, 397, 425ff.
and root finding 382
scaling of variables 428
by searching smaller subspaces 824
steepest descent method 421, 813
termination criterion 398f., 410
use in finding double roots 348
use for sparse linear systems 84ff.
using derivatives 396f., 40Sff.
variable metric methods 397, 425ff.
see also Linear programming

Minimum residual method, for sparse system

85
MINPACK 688
MIPS 894
Missing data problem 576
Mississippi River 446, 455

Mode of distribution 611, 615

Modeling of data see Fitting

Model-trust region 393, 688

Modes, homogeneous, of recursive filters 561
Modified Bessel functions see Bessel func-

tions
Modified Lentz’s method, for continued frac-
tions 171

Modified midpoint method 722f., 726
Modified moments 158
Modula-2 7
Modular arithmetic, without overflow 278,
281, 284
Modularization, in programs  6f.
Modulus of linear congruential generator 276
Moments
of distribution 610ff.
filter that preserves 650
modified problem of 158
problem of 90f.
and quadrature formulas 799
semi-invariants 614
Monic polynomial 149
Monotonicity constraint, in upwind differenc-
ing 846
Monte Carlo 162, 275
adaptive 316ff., 319ff.
bootstrap method 691f.
comparison of sampling methods 318f.
exploration of binary tree 300
importance sampling 316f.
integration 130, 162, 304ff., 316ff.
integration, recursive 323ff.
integration, using Sobol’ sequence 313ff.
integration, VEGAS algorithm 319ff.

and Kolmogorov-Smirnoy statistic 627,
646f.
partial differential equations 833
quasi-random sequences in 309ff.
quick and dirty 691f.
recursive 316ff., 323ff.
significance of Lomb periodogram 578
simulation of data 660, 689ff., 695
stratified sampling 317f., 323
Moon, calculate phases of If., 13f.
Mother functions 591
Mother Nature 689, 691
Moving average (MA) model 573
Moving window averaging 650
Mozart 8
MS. xvii
MS-DOS xii, 3
Muller’s method 371, 379
Multidimensional
confidence levels of fitting 694
data, use of binning 629
Fourier transform 521 ff.
Fourier transform, real data 525ff.
initial value problems 853ff.
integrals 130, 161ff., 304ff., 316ff.
interpolation 123ff.
Kolmogorov-Smirnov test 645ff.
least squares fitting 680
minimization 408ff., 412ff., 420ff.
Monte Carlo integration 304ff., 3 16ff.
normal (Gaussian) distribution 695
optimization 395f.
partial differential equations 853ff.
root finding 347ff., 365, 377, 379ff., 382,
754, 757f., 761, 762
search using quasi-random sequence 309
secant method 380, 389ff.
wavelet transform 602
Multigrid method 833, 871 ff.
avoid SOR 875
boundary conditions 877f.
choice of operators 877
coarse-to-fine operator 873
coarse-grid correction 873f.
cycle 874
dual viewpoint 883
fine-to-coarse operator 873
full approximation storage (FAS) algorithm
882ff.
full multigrid method (FMG) 872, 877f.
full weighting 876
Gauss-Seidel relaxation 874f.
half weighting 876
importance of adjoint operator 876
injection operator 873
interpolation operator 873
line relaxation 875
local truncation error 883
Newton’s rule 882, 884
nonlinear equations 882ff.
nonlinear Gauss-Seidel relaxation 884
odd-even ordering 875, 878
operation count 871
prolongation operator 873
recursive nature 874


Index

983

relative truncation error 883
relaxation as smoothing operator
restriction operator 873
speeding up FMG algorithm 881f.
stopping criterion 884
straight injection 876
symbol of operator 875f.
use of Richardson extrapolation 878
V-cycle 874
W-cycle 874
zebra relaxation 875
Multiple precision arithmetic 915ff.
Multiple roots 348, 369
Multiplication, complex 177
Multiplication, multiple precision 916, 918
Multiplier of linear congruential generator
276
Multistep and multivalue methods (ODEs)
TATE.
see also Differential Equations; Predictor-
corrector methods
Multivariate normal distribution 695
Murphy’s Law 413
Musical scores 5

874

Naa wii, 35, 72, 212, 461
National Science Foundation (U.S.) xiii, xv
Natural cubic spline 115
Navier-Stokes equation 839, 840
Needle, eye of (minimization) 410
Negation, multiple precision 916
Negentropy 820, 904
Nelder-Mead minimization method 396, 408ff.
Nested iteration 877
Neumann boundary conditions
867
Neutrino 645
Neville’s algorithm 108f., 111, 140, 188
Newton-Cotes formulas 131ff., 147
open 132
Newton-Raphson method see Newton’s rule
Newton’s rule 149f., 185, 348, 362ff., 369,
371, 476
with backtracking 384f.
caution on use of numerical derivatives
365
fractal domain of convergence 367f.
globally convergent multidimensional
383ff., 389, 757f., 761
for matrix inverse 57, 606
in multidimensions 377, 379ff., 757f.,
761, 762
in nonlinear multigrid 882, 884
nonlinear Volterra equations 796
for reciprocal of number 919
safe 366
scaling of variables 389
singular Jacobian 393
solving stiff ODEs 748
for square root of number 921
Niederreiter sequence 310

829, 849, 860,

380,

NL2SOL 688
Noise
bursty 897

effect on maximum entropy method 574

equivalent bandwidth 554
fitting data which contains 653, 656
model, for optimal filtering 548
Nominal variable (statistics) 628
Nonexpansive projection operator 814
Non-interfering directions see Conjugate direc-
tions
Nonlinear eigenvalue problems 462
Nonlinear equations
finding roots of 347ff.
integral equations 790, 796
in MEM inverse problems 822f.
multigrid method for elliptic PDEs
Nonlinear instability 840
Nonlinear programming 443
Nonnegativity constraints 430f.
Nonparametric statistics 639ff.
Nonpolynomial complete (NP-complete) 445
Norm, of matrix 58
Normal (Gaussian) distribution 275, 658,
687f., 807
central limit theorem 658f.
deviates from 288f., 578
kurtosis of 612
multivariate 695
semi-invariants of 614
tails compared to Poisson 659
two-dimensional (binormal) 637
variance of skewness of 612
Normal equations (fitting) 34f., 651, 672ff.,
804, 809f.
often are singular 676
Normalization
of Bessel functions 181
of floating-point representation 28, 890
of functions 149, 774
of modified Bessel functions
Notch filter 558, 562f.
NP-complete problem 445
nr.h prototypes for Numerical Recipes 17,
930
NRANSI macro

882ff.

239

17, 930

NR_END macro, for offset arrays 941

nrerror() utility 2, 942f.

nrutil.c utility functions 2, 19, 2If., 940,
942 ff.

nrutil.h prototypes for utilities 17,27,
940ff.

Null hypothesis 609

Nullity 61

Nullspace 34, 61, 63, 456, 804

Number-theoretic transforms 509f.

Numerical derivatives 186ff., 651

Numerical integration see Quadrature
Numerical Recipes
compatibility with First Edition 3f.
compilers tested 3
Example Book 3
how to get diskettes xvi, 996f.
how to report bugs iv
license information xvi
list of all 951ff.
machines tested 3
OEM information xvii
no warranty on xvi

984

Index

programming conventions 25ff.
programs by chapter and section xix
prototypes (nr.h) 17, 930
table of dependencies 951 ff.
table of prototypes 930
as trademark xvii
utility functions 2, 940ff.
utility prototypes (nrutil.h)
940ff.
Numerical Recipes Software xi, xvii
address and fax number xvii
Nyquist frequency 500ff., 526, 550, 552, 576,
S78f.
Nystrom method 791f., 797f.
product version 797

17, 27,

Object extensibility 7
Objective function 431
Object-oriented programming 7
Oblateness parameter 773
Odd parity 896
Odd-even ordering
in Gauss-Seidel relaxation 875, 878
in successive over-relaxation (SOR) 868
OEM information xvii
One-sided power spectral density 498
Operation count
balancing 483
Bessel function evaluation 234f.
bisection method 353
Cholesky decomposition 97
coefficients of interpolating polynomial
120f.
complex multiplication 104
cubic spline interpolation 115
evaluating polynomial 174f.
fast Fourier transform (FFT) 504
ss-Jordan elimination 42, 48
n elimination 42
Givens reduction 470
Householder reduction 474
interpolation 106
inverse iteration 494
iterative improvement 56
Jacobi transformation 467
Kendall’s tau 643f.
linear congruential generator 277
LU decomposition 44, 48
matrix inversion 104
matrix multiplication 103
maximum entropy method 574
multidimensional minimization 420
multigrid method 871
multiplication 918
polynomial evaluation
QL method 477, 480
QR decomposition 98
QR method for Hessenberg matrices 490
reduction to Hessenberg form 485
selection by partitioning 341
sorting 329ff.
Toeplitz matrix 90
Vandermonde matrix 90

104, 174f.

Operator
associativity, in C 25f.
overloading 7
precedence, inC 25f.
splitting 832, 856f., 870
Optimal feasible vector 431
Optimal (Wiener) filtering 542, 547ff., 565f.,
650
compared with regularization 810
Optimization see Minimization
Ordinal variable (statistics) 628
Ordinary differential equations see Differential
equations
Orthogonal see Orthonormal functions; Or-
thonormal polynomials
Orthogonal transformation 459, 470ff., 477,
591
Orthonormal basis, constructing 66, 100
Orthonormal functions 149, 252
Orthonormal polynomials
Chebyshev 151, 190ff.
construct for arbitrary weight 157ff.
in Gauss-Hermite integration 153
and Gaussian quadrature 149
Gaussian weights from recurrence 156
Hermite 151
Jacobi 151
Laguerre 151
Legendre 151
weight function loga 159
Orthonormality 59f., 149, 470
Outer product of matrices (denoted by ®) 73,
427
Outgoing wave boundary conditions 829
Outlier 611, 659, 662, 699, 702
see also Robust estimation
Overcorrection 866
Overflow 890
how to avoid in modulo multiplication
278
in complex arithmetic 177
Overlap-add and overlap-save methods
Overrelaxation parameter 866
choice of 866f.

543f.

Paaé approximant 111, 200ff.
Parabolic interpolation 403
Parabolic partial differential equations 827,
847ff.
Parallel axis theorem 318
Parameters in fitting function 657f., 689ff.
Parity bit 896
Park and Miller minimal standard random gen-
erator 278f.
Parseval’s Theorem 498, 551
discrete form 504
Partial differential equations
advective equation 835
alternating-direction implicit method (ADI)
856, 870f.
amplification factor 837, 843
analyze/factorize/operate package 833
artificial viscosity 840, 846
biconjugate gr:
boundary conditions

827ff.


Index

985

boundary value problems 828ff., 857f.

Cauchy problem 827f.

caution on high-order methods

Cayley’s form 853

characteristics 827

Chebyshev acceleration 868f.

classification of 827ff.

comparison of rapid methods 863

conjugate gradient method 833

Courant condition 838, 841, 843, 845

Courant condition (multidimensional) 855

Crank-Nicolson method 848, 851, 853,
855

cyclic reduction (CR) method 857f., 861f.

diffusion equation 827, 847ff., 855, 864

Dirichlet boundary conditions 829, 848,
859, 865, 867

elliptic, defined 827

error, varieties of 840ff.

explicit vs. implicit differencing 836

FACR method 863

finite difference method 830ff.

finite element methods 833f.

flux-conservative initial value problems
834ff.

forward Euler differencing 835f.

Forward Time Centered Space (FTCS)
836ff., 847ff., 852, 864

Fourier analysis and cyclic reduction (FACR)
857ff., 863

Gauss-Seidel method (relaxation) 864,
873ff., 884

Godunov’s method 846

Helmholtz equation 861

hyperbolic 827, 834f.

implicit differencing 848

incomplete Cholesky conjugate gradient
method (ICCG) 833

inhomogeneous boundary conditions

initial value problems 827f.

initial value problems, recommendations on
847ff.

Jacobi’s method (relaxation) 864f., 873

Laplace’s equation 827

Lax method 837ff., 845, 854f.

Lax method (multidimensional) 854f.

matrix methods 833

mesh-drift instability 843f.

Monte Carlo methods 833

multidimensional initial value problems
853ff.

multigrid method 833, 871 ff.

Neumann boundary conditions
860, 867

nonlinear diffusion equation 851

nonlinear instability 840

numerical dissipation or viscosity 839

operator splitting 832, 856f., 870

853f.

859f.

829, 849,

outgoing wave boundary conditions 829
parabolic 827, 847ff.
periodic boundary conditions 859, 867

piecewise parabolic method (PPM) 846
Poisson equation 827, 861

rapid (Fourier) methods 514ff., 833, 857ff.
relaxation methods 832, 863ff.

Schrodinger equation 851 ff.
second-order accuracy 842ff., 848f.
shock 840, 846
sparse matrices from 71
spectral methods 833f.
spectral radius 865ff., 871
stability vs. accuracy 839
stability vs. efficiency 830
staggered grids 519, 861
staggered leapfrog method 842f.
strongly implicit procedure 833
successive over-relaxation (SOR) 866ff.,
871, 875
time splitting 856f., 870
two-step Lax-Wendroff method 844ff.
upwind differencing 841f., 846
variational methods 833
varieties of error 840ff.
von Neumann stability analysis
839, 842, 849
wave equation 827, 834f.
see also Elliptic partial differential equa-
tions; Finite difference equations (FDEs)
Partial pivoting 38
Partition-exchange 332, 341
Partitioned matrix, inverse of 77f.
Party tricks 102ff., 174f.
Parzen window 554
Pascal 16, 18, 20
Pascal, Numerical Recipes in xv, 1
Path integration, for function evaluation 208ff.,
271
Pattern multiply of sparse matrices 81f.
PBCG (preconditioned biconjugate gradient
method) 85f., 833
PC methods see Predictor-corrector methods
PCGPACK 78
PDEs see Partial differential equations
Pearson’s r 636ff.
PECE method 749
Pentagon, symmetries of 902
Percentile 329
Period of linear congruential generator 276
Periodic boundary conditions 859, 867
Periodogram 550ff., 574
Lomb’s normalized 576f., 581f.
variance of 552
Perl (programming language) xiii
Perron’s theorems, for convergence of recur-
rence relations 180f.
Perturbation methods for matrix inversion
‘73tf.
Peter Principle 337
Phase error 840
-locked loop 705
istic 631
Phillips-Twomey method 808ff.
Pi, computation of 915ff.
Piecewise parabolic method (PPM) 846
Pincherle’s theorem 181
Pivot element 38, 41, 764
in linear programming 435f.
Pivoting 36, 38ff., 54, 73, 97
full 38
implicit

8306f.,

38, 46

986

Index

in LU decomposition 45f.
partial 38, 41, 46
and QR decomposition 99
in reduction to Hessenberg form 485
in relaxation method 764
for tridiagonal systems 51
Pixel 525, 603, 812, 820
Planck’s constant 851
Plane rotation see Givens reduction; Jacobi
transformation (or rotation)
Platykurtic distribution 612
Plotting of functions 349f.

POCS (method of projection onto convex sets)

814
Poetry 5
Pointer
to array 18
use for matrices 20, 33f., 940ff.
Poisson equation 525, 827, 861
Poisson probability function
cumulative 221
deviates from 290, 293ff., 579
semi-invariants of 614
tails compared to Ga an 659
Poisson process 287, 291, 293
Polak-Ribiere algorithm 396f., 422f.
Poles see Complex plane, poles in
Polishing of roots 365, 370f., 376f.
Polymorphism 7
Polynomial interpolation 105, 108ff.
Aitken’s algorithm 108
in Bulirsch-Stoer method 728, 730f.
coefficients for 120ff.
Lagrange’s formula 91, 108f.
multidimensional 123ff.
Neville’s algorithm 108f., 111, 140, 188
pathology in determining coefficients for
120
in predictor-corrector method 748
smoothing filters 650f.
see also Interpolation
Polynomials 173ff.
algebraic manipulations 175
approximating modified Bessel functions
236

approximation from Chebyshev coefficients

197

AUTODIN-II 898

CCITT 897f.

characteristic 375

characteristic, for digital filters 561,567

characteristic, for eigenvalues of matrix
456, 475f.

Chebyshev 190ff.

CRC-16 898

deflation 369ff., 377

derivatives of 173f.

division 91, 175, 369, 377

evaluation of 173

evaluation of derivatives 173f.

extrapolation in Bulirsch-Stoer method
728, 730f.

extrapolation in Romberg integration 140

fitting 90, 120, 197, 6SOf., 671, 679F.
generator for CRC 897f.

ill-conditioned 369
matrix method for roots 375
minimax 192, 204
monic 149
multiplication 175
operation count for 174f.
orthonormal 149, 190f.
primitive modulo 2 296ff., 311f., 897
roots of 183ff., 369ff., 375
shifting of 198f.
stopping criterion in root finding 373
Port, serial data 899
Portability 2f., 16
Portable random number generator see Ran-
dom number generator
Positive definite matrix, testing for 97
Positivity constraints 431
Postal Service (U.S.), barcode 902
PostScript xiii, xvii
Powell’s method 396, 408, 412ff.
Power (in a signal) 498f.
Power series 165ff., 173f., 201
economization of 198ff.
Padé approximant of 200ff.
Power spectral density see Fourier transform;
Spectral density
Power spectrum estimation see Fourier trans-
form; Spectral density
PPM (piecewise parabolic method) 846
Precedence of operators, in C 25f.
Precision, floating point 890
Precision, multiple 915ff.
Preconditioned biconjugate gradient method
(PBCG) 85f.
Preconditioning, in conjugate gradient methods
833
Predictor-corrector methods 708, 737, 747ff.
Adams-Bashforth-Moulton schemes 749
adaptive order methods 751
compared to other methods 747f.
fallacy of multiple correction 748f.
with fixed number of iterations 749
functional iteration vs. Newton’s rule 749
multivalue compared with multistep 749f.
starting and stopping 750, 751
stepsize control 749f.
Prime numbers 924f.
Primitive polynomials modulo 2 296ff., 311f.,
897
Principal directions 414f.
Principal solution, of inverse problem 806
Prize, $1000 offered 281
Probability see Random number generator;
Statistical tests
Probability density, change of variables in
287ff.
Process loss 554
Product Nystrom method 797
Program(s)
as black boxes xiv, 5, 35, 60, 212, 348,
413
dependencies 951ff.
encapsulation 6f.
interfaces 7
modularization 6f.


Index

987

organization 5ff.
recipes by chapter and section xix
typography of 11
validation 2f.
Projection onto convex sets (POCS) 814
Projection operator, nonexpansive 814
Prolongation operator 873
Protocol, for communications
Prototypes in C  16f., 25, 930
PSD (power spectral density) see Fourier
transform; Spectral density
Pseudo-random numbers 274ff.
Puns, particularly bad 173, 752, 755
Pyramidal algorithm 594
Pythagoreans 399

896

Qu see Eigensystems

and le:
operation count 98
pivoting 99

updating 100, 389
use for orthonormal basis 66, 100
Quadratic
convergence 57, 262, 358, 364f., 415f.,
427, 915
equations 29, 183ff., 398, 464

interpolation 360, 371
programming 443
Quadrature 129ff.
adaptive 129, 196, 797
alternative extended Simpson’s rule 134

arbitrary weight function 157ff., 797
automatic 160

Bode’s rule 132

change of variable in 144 ff., 797

by Chebyshev fitting 130, 195
classical formulas for 130ff.
Clenshaw-Curtis 130, 196, 518f.
closed formulas 131, 133f.

and computer science 889

by cubic splines 130

error estimate in solution 793

extended midpoint rule 135, 141f.
extended rules 133ff., 140, 795, 797, 799
extended Simpson’s rule 134

Fourier integra 584 ff.

Fourier integrals, infinite range 590f.

Gau: hebyshev 151, 518f.

Gau: lermite 151, 798

Gauss-Jacobi 151

Gauss-Kronrod 160

Gau: 151, 798

Gau: egendre 151, 792, 797

Gau: a 160, 196, 518

Gau: 160

Gaussian integration 133, 147ff., 790,
792, 797

Gaussian integration, nonclassical weight
function 157ff., 797

for improper integrals 141ff., 797f.

for integral equations 790f., 795

Monte Carlo 130, 162, 304ff., 316ff.

multidimensional 130, 161 ff.

Newton-Cotes formulas 131 ff., 147

Newton-Cotes open formulas 132

open formulas 131, 132f., 135f., 141

related to differential equations 129

related to predictor-corrector methods 747f.

Romberg integration 130, 140f., 143, 188,
723,797

semi-open formulas 135f.

Simpson’s rule 132, 139, 143, 590, 791f.,

797, 799

Simpson’s three-eighths rule 132, 797,
799

singularity removal 144ff., 797f.

singularity removal, worked example 801
trapezoidal rule 131, 133, 136ff., 140,
586, 590, 791f., 795
using FFTs 130
weight function loga 159
see also Integration of functions
Quadrature mirror filter 592, 600
Quantum mechanics, Uncertainty Principle
607
Quartile value 329
Quasi-Newton methods for minimization 397,
425ff.
Quasi-random sequence 309ff., 327, 889, 896
Halton’s 309f.
for Monte Carlo integration 313ff., 319,
327
Sobol’s 311
see also Random number generator
Quicksort 329, 332ff., 338, 341
Quotient-difference algorithm 170

R-estimates 699f.
Radioactive decay 287
Radix base for floating point arithmetic 483,
890, 916, 922
Radix conversion 910, 914, 922
Ramanujan’s identity for 7 924
RAND_MAX macro 275f., 277
Random bits, generation of 296ff.
Random deviates 274ff.
binomial 295f.
exponential 287f.
gamma distribution 290ff.
Gaussian 275, 288f., 578, 807
normal 275, 288f., 578
Poisson 293ff., 579
quasi-random sequences
uniform 275ff.
uniform integer 280, 283ff.
Random number generator 274ff.
bitwise operations 296ff.
Box-Muller algorithm 289
Data Encryption Standard 300ff.
good choices for modulus, multiplier and
increment 284f.
for integer-valued probability distribution
293
integer vs. real implementation 283
L’Ecuyer’s long period 280f.

309ff., 889, 896

988

Index

linear congruential generator 276f.
machine language 278

Minimal Standard, Park and Miller’s 278f.

nonrandomness of low-order bits 277
perfect 281
planes, numbers lie on 277
portable 278ff.
primitive polynomials modulo 2 296ff.
pseudo-DES 300
quasi-random sequences 309ff., 889, 896
quick and dirty 283ff.
quicker and dirtier 284f.
in Quicksort 333
random access to nth number 303
random bits 296ff.
recommendations 285f.
rejection method 290ff.
shuffling procedure 280, 281
in simulated annealing method 445
spectral test 284
subtractive method 282
system-supplied 275ff.
timings 285f.
transformation method 287ff.
trick for trigonometric functions 289
Random numbers see Monte Carlo; Random
deviates
Random walk 29
RANDU, infamous routine 277
Range 61, 63
Rank (matrix) 61
kernel of finite 794
Rank (sorting) 329, 340f.
Rank (statistics) 639ff., 699f.
Kendall’s tau 642ff.
Spearman correlation coefficient 640f.
sum squared differences of 640
Ratio variable (statistics) 628
Rational Chebyshev approximation 204ff.
Rational function 105, 173ff., 200ff., 204ff.
approximation for Bessel functions 231f.
approximation for continued fraction 170,
217, 227
Chebyshev approximation 204ff.
evaluation of 176
extrapolation in Bulirsch-Stoer method
724ff., 731
interpolation and extrapolation using 105,
111 ff., 200ff., 204ff., 724ff., 731
minimax 204
as power spectrum estimate 573
Realizable (causal) 559, 561
Rearranging see Sorting
Reciprocal, multiple precision 919
Record, in data file 338
Recurrence relation 178ff.
associated Legendre polynomials 253
Bessel function 178, 231, 241f.
binomial coefficients 215
Bulirsch-Stoer 111 f.
characteristic polynomial of tridiagonal
matrix 475
Clenshaw’s recurrence formula 181 ff.
and continued fraction 181
continued fraction evaluation 170f.

convergence 181
cosine function 178, 506
dominant solution 179
exponential integrals 178
gamma function 213
generation of random bits 297f.
Golden Mean 30
Legendre polynomials 178
minimal vs. dominant solution 179
modified Bessel function 239
Neville’s 109, 188
orthonormal polynomials 149
Perron’s theorems 1 80f.
Pincherle’s theorem 181
polynomial interpolation 109, 189
primitive polynomials modulo 2 297f.
random number generator 276
rational function interpolation 11 1f.
sequence of trig functions 178f.
sine function 178, 506
spherical harmonics 253
stability of 30f., 179ff., 182f., 231, 239,
253
trig functions 579
weight of Gaussian quadrature 150f.
Recursion, in multigrid method 874
Recursive Monte Carlo integration 316ff.
Recursive stratified sampling 323ff.
Red-black see Odd-even ordering
Reduction of variance in Monte Carlo integra-
tion 308, 316ff.
References (explanation) 4
References (general bibliography) 926ff.
Reflection formula for gamma function 213
register storage cl 25
Regula falsi (false position) 354ff.
Regularity condition 784
Regularization
compared with optimal filtering 810
constrained linear inversion method 808ff.
of inverse problems 805ff.
linear 808ff.
nonlinear 822f.
objective criterion 811
Phillips-Twomey method 808ff.
Tikhonov-Miller 808ff.
trade-off curve 808
two-dimensional 812
zeroth order 805ff.
see also Inverse problems
Regularizing operator 807
Rejection method for random number genera-
tor 290ff.
Relaxation method
for algebraically difficult sets 772
automated allocation of mesh points 783f.,
786
computation of spheroidal harmonics 772ff.
for differential equations 754f., 762ff.
elliptic partial differential equations 832f.,
863ff.
example 772ff.
Gauss-Seidel method 864, 873ff., 884
internal boundary conditions 784ff.
internal singular points 784ff.


Index

989

Jacobi’s method 864f., 873
successive over-relaxation (SOR) 866ff.,
871, 875
see also Multigrid method
Remes algorithms
exchange algorithm 560
for minimax rational function 205
Residual 57, 62, 85
in multigrid method 872
Resolution function, in Backus-Gilbert method
816
Response function 538
Restriction operator 873
Reward, $1000 offered 281
Richardson’s deferred approach to the limit
140, 143, 188, 708, 724ff., 733f., 796,
878
see also Bulirsch-Stoer method
Richtmyer artificial viscosity 846
Ridders’ method, for numerical derivatives
188
Ridders’ method, root finding 348, 356, 358
Riemann shock problem 846
Right eigenvalues and eigenvectors
Rise/fall time 554f.
Robust estimation 659, 699ff., 705
Andrew’s sine 702
average deviation 611
double exponential errors 701
Kalman filtering 705
Lorentzian errors 70If.
mean absolute deviation 611
nonparametric correlation 639ff.
Tukey’s biweight 702
use of a priori covariances
see also Statistical tests
Romberg integration 130, 140f., 143, 188,
723, 797
Root finding 149f., 347ff.
advanced implementations of Newton’s rule
393
Bairstow’s method 371, 377
bisection 350, 353, 359ff., 366, 397, 476,
703
bracketing of roots
371, 376
Brent’s method 348, 356, 666
Broyden’s method 380, 389ff., 393
compared with multidimensional minimiza-
tion 382
complex analytic functions
in complex plane 210
convergence criteria 353, 381
deflation of polynomials 369ff., 377
without derivatives 361
double root 348
eigenvalue methods
false position 354ff.
Jenkins-Traub method 376
Laguerre’s method 348, 371 ff.
Lehmer-Schur algorithm 376
Maehly’s procedure 370, 378
matrix method 375
Muller’s method 371, 379
multiple roots 348

458

705

348, 350ff., 360, 369,

371

375

Newton’s rule 149f., 185, 348, 362ff.,
369, 371, 377, 379ff., 383f., 476, 749,
757£., 762, 796, 882, 884, 919, 921
pathological cases 350f., 362ff., 369, 380
polynomials 348, 369ff., 456
in relaxation method 762
Ridders’ method 348, 356, 358
root-polishing 365, 370f., 376ff., 378
safe Newton’s rule 366
secant method 354ff., 365, 371, 406
in shooting method 754, 757f.
singular Jacobian in Newton’s rule 393
stopping criterion for polynomials 373
use of minimum finding 348
using derivatives 362ff.
zero suppression 379
see also Roots
Root polishing 365, 370, 376ff.
Roots
Chebyshev polynomials
cubic equations 184f.
multiple 348, 371 ff.
nonlinear equations 347ff.
polynomials 348, 369ff., 456
quadratic equations 183f.
reflection in unit circle 567
square, multiple precision 921
see also Root finding
Rosenbrock method 737ff.
compared with semi-implicit extrapolation
TAT
stepsize control 738
Roundoff error 29, 889f.
bracketing a minimum 406
conjugate gradient method 833
eigensystems 465, 474, 476, 478, 483,
485, 489
extended trapezoidal rule 138
general linear least squares 674, 677
graceful 891
hardware aspects 890
Householder reduction 472
JEEE standard 891
interpolation 107
least squares fitting 664, 674
Levenberg-Marquardt method 685
linear algebraic equations 32f., 36, 38, 55,
64,91
linear predictive coding (LPC) 571
magnification of 29,55
maximum entropy method (MEM) 574
measuring 890
multidimensional minimization 426, 430
multiple roots 369f.
numerical derivatives 186
recurrence relations 179
reduction to Hessenberg form 485
series 170f.
straight line fitting 664
variance 613
Row degeneracy 32
Row-indexed sparse storage 78f.
transpose 80f.
Row operations on matrix 37, 40
Row totals 630

190


990

Index

RSS algorithm 323ff.
RST properties (reflexive, symmetric, transi-
tive) 345
Runge-Kutta method 708f., 710ff., 738, 747
Cash-Karp parameters 716f.
embedded 715f., 738
high-order 711
quality control 728
stepsize control 714ff.
Run-length encoding 909
Rybicki, G.B. 91f., 120, 151, 259, 528, 581,
606

Sampling
importance 316f.

Latin square or hypercube 315
recursive stratified 323ff.
stratified 317f.

uneven or irregular 576, 654

Sampling theorem 501, 550
for numerical approximation 606ff.

Sande-Tukey FFT algorithm 509

Savitzky-Golay filters
for data smoothing 650ff.
for numerical derivatives 189, 651

Scallop loss 554

Schrage’s algorithm 278

Schrodinger equation 851 ff.

Schultz’s method for matrix inverse 57, 606

SDLC checksum 898

Searching
with correlated values 117f.
an ordered table 117f.
selection 341 ff.

Secant method 348, 354ff., 365, 371, 406
Broyden’s method 389ff.
multidimensional (Broyden’s) 380, 389ff.

Second Euler-Maclaurin summation formula

142

Second order differential equations 732f.

Seed of random number generator 275

Selection 329, 341ff.
find m largest elements 344
heap algorithm 344
for median 703
operation count 341f.
by partition-exchange 341
without rearrangement 342
timings 344
use to find median 614f.

Semi-implicit Euler method 737, 743

Semi-implicit extrapolation method 737, 743
compared with Rosenbrock method 747
stepsize control 744

Semi-implicit midpoint rule 743

Semi-invariants of a distribution 614

Sentinel, in Quicksort 333, 341

Separable kernel 794

Separation of variables 252

Serial data port 899

Series 165ff.
accelerating convergence of 166ff.
alternating 166f.
asymptotic 167
Bessel function K, 247

Bessel function ¥, 242
Bessel functions 166, 230
cosine integral 257
divergent 167
economization 198ff., 201
Euler’s transformation 166ff.
exponential integral 222, 224
Fresnel integral 255
hypergeometric 208, 271
incomplete beta function 227
incomplete gamma function 217
Laurent 573
relation to continued fractions 169f.
roundoff error in 170f.
sine and cosine integrals 257
sine function 166
Taylor 362, 414, 708, 715, 763, 767
transformation of 166ff.
van Wijngaarden’s algorithm 167
Shaft encoder 894f.
Shakespeare 8
Shampine’s Rosenbrock parameters 738
Shell algorithm (Shell’s sort) 330ff.
Sherman-Morrison formula 73ff., 90, 389
Shifting of eigenvalues 456, 477f., 486f.
Shock wave 840, 846
Shooting method
computation of spheroidal harmonics 781
for differential equations 754, 757ff.,
779f., 781
for difficult cases 760
example 779f., 781
interior fitting point 760
Shuffling to improve random number generator
280f.
Sidelobe fall-off 554
Sidelobe level 554
Signal, bandwidth limited 501
Significance (numerical) 28
Significance (statistical) 615f.
one- vs. two-sided 638
peak in Lomb periodogram 577
of 2-d K-S test 646f.
two-tailed 619
Similarity transform 459ff., 463, 483, 485,
488
Simplex
defined 408f.
method in linear programming 396, 408f.,
430, 433ff., 439ff.
method of Nelder and Mead 396, 408ff.,
451f., 702f.
use in simulated annealing 451f.
Simpson’s rule 130ff., 134, 139, 143, 590,
791f., 796, 797
Simpson’s three-eighths rule 132, 797, 799
Simulated annealing see Annealing, method of
simulated
Simulation see Monte Carlo
Sine function
evaluated from tan(@/2) 179
recurrence 178
series 166
Sine integral 255, 257ff.
continued fraction 257

Index

991

series 257
see also Cosine integral
Sine transform see Fast Fourier transform
(FFT); Fourier transform
Singleton’s algorithm for FFT 532
Singular value decomposition (SVD) 33, 34f.,
Soff.
approximation of matrices 66f.
backsubstitution 64
and bases for nullspace and range 61
confidence levels from 698
covariance matrix 698
fewer equations than unknowns 65
for inverse problems 806
and least squares 62, 65f., 205, 674, 676ff.
in minimization 416
more equations than unknowns 65f.
and rational Chebyshev approximation
205
of square matrix 61ff.

use for ill-conditioned matrices 63f., 66,
456
use for orthonormal basis 66, 100
Singularities

of hypergeometric function 209, 271
in integral equations 797ff.
in integral equations, worked example 801
in integrands 141 ff., 797f.
removal in numerical integration
797E.
Singularity, subtraction of the 798
SIPSOL 833
Skewness of distribution 612, 614
Smoothing, importance in multigrid method
874
Smoothing of data
in integral equations
sn function 269
Snyder, N.L. xii
Sobol’s quasi-random sequence 311
Sonata 8
Sonnet 8
Sorting 329ff.
bubble sort cautioned against
compared to selection 341
covariance matrix 675, 687
eigenvectors 468f.
Heapsort 329, 336f., 344
index table 329, 338
operation count 329ff.
Quicksort 329, 332ff., 338, 341
rank table 329, 340f.
ranking 338
Shell’s method 330ff.
straight insertion 330f., 468
SPARC or SPARCstation xvii, 3
Sparse linear equations 33, 71ff., 739
band diagonal 51 ff.
biconjugate gradient method 84f., 606
indexed storage 78f.
in inverse problems 813
minimum residual method 85
named patterns 71, 831
partial differential equations

144ff.,

120, 650ff.
790

330

83 1ff.

relaxation method for boundary value prob-
lems 762
row-indexed storage 78f.
wavelet transform 591, 606
see also Matrix
Spearman rank-order coefficient
Special functions see Function
Spectral analysis see Fourier transform; Peri-
odogram
Spectral density 548
and data windowing 553ff.
figures of merit for data windows 554f.
normalization conventions 550
one-sided PSD 498
periodogram S50ff., 574
power spectral density (PSD) 498f.
power spectral density per unit time 499
power spectrum estimation by FFT 549ff.
power spectrum estimation by MEM 572ff.
two-sided PSD 499
variance reduction in spectral estimation
552
Spectral lines, how to smooth 650
Spectral methods for partial differential equa-
tions 833f.
Spectral radius 865ff., 871
Spectral test for random number generator
284
Spectrum see Fourier transform
Spherical Bessel functions 240
routine for 251
Spherical harmonics 252f.
orthogonality 252
routine for 254
stable recurrence for
table of 253
see also Associated Legendre polynomials
Spheroidal harmonics 772ff., 779f., 781
boundary conditions 774
normalization 774
routine for 777ff., 780f., 781f.
Spline 106
cubic 113ff.
gives tridiagonal system 115
natural 115
operation count 115
two-dimensional (bicubic)
Spread matrix 817
Spread spectrum 300
Square root, complex 177f.
Square root, multiple precision 921
Square window 553
Squaring, macro in C 27
Stability 30f.
of Clenshaw’s recurrence 182f.
Courant condition 838, 841ff., 845, 855
diffusion equation 849
of Gauss-Jordan elimination 36, 38
of implicit differencing 735f., 849
mesh-drift in PDEs 843f.
nonlinear 840, 846
partial differential equations
of polynomial deflation 370
in quadrature solution of Volterra equation
796

640f., 699f.

253

127f.

829, 836f.

992

Index

of recurrence relations 179ff., 182f., 231,

239, 253
and stiff differential equations 735f.
von Neumann analysis for PDEs 836f.,
839, 842, 849
see also Accuracy
Stabilized Kolmogorov-Smirnov test 626f.
Stabilizing functional 807
Staggered leapfrog method 842f.
Standard deviation
of a distribution 611
of Fisher’s z 637
of linear correlation coefficient 636
of sum squared difference of ranks 641
Standard (probable) errors 616, 662, 667,
673, 677, 689
Statement labels 8
Statistical error 659
Statistical tests 609ff.
Anderson-Darling 626f.
average deviation 611
bootstrap method 691f.
chi-square 620f., 630ff.
contingency coefficient C 631
contingency tables 628ff., 644
correlation 609f.
Cramer’s V 631
difference of distributions 620ff.
difference of means 615ff.
difference of variances 617, 619
entropy measures of association 632ff.
F-test 617, 619
Fisher’s z-transformation 637f.
general paradigm 609
Kendall’s tau 640, 642ff.
Kolmogorov-Smimov 620, 623ff., 645ff.,
699
Kuiper’s statistic 627
kurtosis 612, 614
L-estimates 699
linear correlation coefficient 636ff.
M-estimates 699ff.
mean 609ff., 614, 615ff.
measures of association 610, 628ff.
measures of central tendency 610ff.
median 611, 699
mode 611
moments 610ff., 614
nonparametric correlation 639ff.
Pearson’s r 636ff.
for periodic signal 577f.
phi statistic 631
R-estimates 699f.
rank correlation 639ff.
robust 611, 640, 699ff.
semi-invariants 614
for shift vs. for spread 626f.
significance 615f.
significance, one- vs. two-sided 619, 638
skewness 612, 614
Spearman rank-order coefficient 640f.,
699.
standard deviation 611
strength vs. significance 615, 628
Student’s t 616, 637

Student’s t, for correlation 637
Student’s t, paired samples 618
Student’s t, Spearman rank-order coefficient
640
Student’s t, unequal variances 617
sum squared difference of ranks 640f.
Tukey’s trimean 699
two-dimensional 645ff.
variance 609ff., 613, 618
Wilcoxon 699
see also Error; Robust estimation
_-STDC__ macro 17, 930
Steak, without sizzle 818
Steed’s method
Bessel functions 240ff., 246
continued fractions 170f.
Steepest descent method 421
in inverse problems 813
Step
doubling 136, 715
tripling 143
Stieltjes, procedure of 157
Stiff equations 709, 734ff.
Kaps-Rentrop method 737
methods compared 747
predictor-corrector method 737
rh.s. independent of « 736
Rosenbrock method 737ff.
scaling of variables 737
semi-implicit extrapolation method 737
semi-implicit midpoint rule 743
Stiff functions 106, 406
Stirling’s approximation 213, 821
Stoermer’s rule 732f.
Stopping criterion, in multigrid method 884
Stopping criterion, in polynomial root finding
373
Storage
band diagonal matrix 52
scheme for matrix in C 20f., 33f., 940f.
sparse matrices 78f.
Straight injection 876
Straight insertion 330f., 468
Straight line fitting 661ff., 673f.
errors in both coordinates 666ff.
robust estimation 703
Strassen’s fast matrix algorithms 102ff.
Stratified sampling, Monte Carlo 317f., 323
Strongly implicit procedure (SIPSOL) 833
Structured programming  Sff.
Student’s probability distribution 226, 228
Student’s t-test
for correlation 637
for difference of ranks 641
for difference of means 616
for difference of means (paired samples)
618
for difference of means (unequal variances)
617
Spearman rank-order coefficient 640
Sturmian sequence 475f.
submatrix() utility 945
Submatrix
caution on freeing 23
of existing matrix 22, 945

Index

993

Sub-random sequences see Quasi-random se-
quence
Subtraction, multiple precision 916
Subtractive method for random number genera-
tor 282
Successive over-relaxation (SOR) 866ff., 871
bad in multigrid method 875
Chebyshev acceleration 868f.
choice of overrelaxation parameter
Sum squared difference of ranks 640
Sums see Series
Sun xvii, 894
SPARCstation xvii, 3
Supernova 1987A 645
SVD see Singular value decomposition (SVD)
switch structure 14
Symbol, of operator 875f.
Synthetic division 91, 174, 369, 377
Systematic errors 659

866f.

T ableau (interpolation) 109, 189
Tangent function, continued fraction 169
Taylor series 186, 362, 414, 708, 715, 750,
763, 767
Test programs 3
TeX xiii
Thermodynamics, analogy for simulated an-
nealing 444f.
Threshold multiply of sparse matrices
Tides 568
Tikhonov-Miller regularization 808ff.
Time domain 496
Time splitting 856f., 870
Toeplitz matrix 90, 92ff., 201
LU decomposition 94
new, fast algorithms 95f.
nonsymmetric 93ff.
Tongue twisters 341
Torus 305ff., 313ff.
Trade-off curve 804, 818
Trademarks xvii
Transformation
Gauss 262
Landen 262
method for random number generator 287ff.
Transforms, number theoretic 509f.
Transport error 840
Transpose of sparse matrix 80f.
Trapezoidal rule 131, 133, 136ff., 140, 586,
590, 791f., 795
Traveling salesman problem 445ff.
Tridiagonal matrix 5O0f., 66, 156, 460f., 494
in alternating-direction implicit method

Si ff.

(ADI) 870f.
from cubic spline 115
cyclic 74f.
in cyclic reduction 862
eigenvalues 475ff.

with fringes 831

from operator splitting 870f.

reduction of symmetric matrix to 469ff.,
476

see also Matrix

Trigonometric
functions, linear sequences 178f.
functions, recurrence relation 178,579

functions, tan(@/2) as minimal 179
interpolation 105
solution of cubic equation 184f.
Truncation error 30, 406, 715, 889f.
in multigrid method 883
in numerical derivatives 186
Tukey’s biweight 702
Tukey’s trimean 699
Turbo Pascal (Borland) 7
Twin errors 902
Two-dimensional see Multidimensional
Two-dimensional K-S test 645ff.
Two-pass algorithm for variance 613

Two-point boundary value problems 708,
753ff.
automated allocation of mesh points 783f.,
786
boundary conditions 753ff., 757, 760,
T19f.

difficult cases 760
eigenvalue problem for differential equa-
tions 756, 772ff., 779, 781
free boundary problem 756, 785
grid (mesh) points 754f., 762, 783f., 786
internal boundary conditions 784ff.
internal singular points 784ff.
linear requires no iteration 759
multiple shooting 762
problems reducible to standard form 756
regularity condition 784
relaxation method 754f., 762ff.
relaxation method, example of 772ff.
shooting to a fitting point 760ff.
shooting method 754, 757ff., 779f., 781
shooting method, example of 779f., 781
singular endpoints 760, 773, 780
see also Elliptic partial differential equa-
tions
Two-sided exponential error distribution 701
Two-sided power spectral density 499
Two-step Lax-Wendroff method 844ff.

Ucrrix xvii, 3
Uncertainty coefficient 634
Uncertainty principle 607
Underflow, in IEEE arithmetic 891
Underrelaxation 866
Uniform deviates see Random deviates, uni-
form
Unit-offset array 18, 940f.
Unitary (function) 852f.
Unitary (matrix) see Matrix
UNIX. xii, xvii, 3, 16, 285, 303, 894
Upper Hessenberg matrix see Hessenberg ma-
trix
Upwind differencing 841f., 846
US. Postal Service barcode 902
Utility functions
complex.c 23f., 948ff.
mrutil.c 2, 19, 21f., 940, 942ff.

Vv -cycle 874

994

Index

Validation of Numerical Recipes procedures
2F.
Valley, long or narrow 410, 413, 416
Van Cittert’s method 813
Van Wijngaarden-Dekker-Brent method see
Brent’s method
Vandermonde matrix 90ff., 120
Variable length code 903
Variable metric method 397, 425ff.
compared to conjugate gradient method
425f.
Variable step-size integration 129, 141, 709,
713, 725ff., 733f., 738, 744, 749f.
Variance(s)
of distribution 609ff., 614, 617, 619
pooled 616

reduction of (in Monte Carlo) 308, 316ff.

statistical differences between two 615
two-pass algorithm for computing 613
see also Covariance
Variational methods, partial differential equa-
tions 833
VAX xvii, 285, 303
Vector see Array
Vectors, representation inC 18
vector() utility 943
VEGAS algorithm for Monte Carlo 319ff.
Verhoeff’s algorithm for checksums 902
Viéte’s formulas for cubic roots 184f.
Virus, computer 897
Viscosity
artificial 840, 846
numerical 839, 840, 846
VMS. xvii
void (parameter type list) 17
Volterra equations 789f.
adaptive stepsize control 797
analogy with ODEs 794f.
block-by-block method 797
first kind 790, 795
nonlinear 790, 796
second kind 790, 794f.
unstable quadrature 796
von Neumann-Richtmyer artificial viscosity
846

von Neumann stability analysis for PDEs 836f.,

839, 842, 849
Vowellish (coding example) 904f., 910

W cycle 874

Warranty, disclaimer of xvi

Wave equation 252, 827, 834f.

Wavelet transform 591 ff.
appearance of wavelets 598f.
approximation condition of order p 592f.
coefficient values 594, 596
contrasted with Fourier transform 591f.,

601

Daubechies wavelet filter coefficients 592ff.,

596, 598, 601, 605
detail information 593
discrete wavelet transform (DWT) 594f.
DWT (discrete wavelet transform) 594f.
eliminating wrap-around 594f.

fast solution of linear equations 603ff.
filters S99f.
and Fourier domain 599f.
image processing 603
for integral equations 791
inverse 594
Lemarie’s wavelet 600
of linear operator 603ff.
mother-function coefficient 594
mother functions 591
multidimensional 602
nonsmoothness of wavelets 598f.
pyramidal algorithm 594
quadrature mirror filter 592
smooth information 593
truncation 601f.
wavelet filter coefficient 592, 594
wavelets 591, 598ff.
Wavelets see Wavelet transform
Weber function 210
Weighted Kolmogorov-Smirnov test 626f.
Weighted least-squares fitting see Least squares
fitting
Weighting, full vs. half in multigrid 876
Weights for Gaussian quadrature 147ff., 797
nonclassical weight function 157ff., 797
Welch window 554
while iteration 12
Wiener filtering 542, 547ff., 565f., 650
compared to regularization 810
Wiener-Khinchin theorem 498, 566, 574
Wilcoxon test 699
Window function
Bartlett 554
flat-topped 555
Hamming 554
Hann 554
Parzen 554
square 553
Welch 554
Winograd Fourier transform algorithms 509
Woodbury formula 7S5ff., 90
Wordlength 28
Wraparound
order for storing spectrum 507
problem in convolution 540
Wronskian, of Bessel functions 240, 246

X 25 protocol 898
XMODEM checksum 897
X-ray diffraction pattern, processing of 814

Y ale Sparse Matrix Package 72, 78

Z -transform 561, 567, 572
Z-transformation, Fisher’s 637f.
Zealots 823

Zebra relaxation 875

Zero contours 379f.

Zero-offset array 18
Zeroth-order regularization 805ff.
Zip code, barcode for 902
Ziv-Lempel compression 903
